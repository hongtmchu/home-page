<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Andrew Heiss">
<meta name="description" content="For mathematical and philosophical reasons, propensity scores and inverse probability weights don’t work in Bayesian inference. But never fear! There’s still a way to do it!">
<title>How to use Bayesian propensity scores and inverse probability weights | Andrew Heiss – Andrew Heiss</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<link href="../../../../..//files/favicon-512.png" rel="icon" type="image/png">
<script src="../../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-17c909d5fd25ae21b861c0c7a49b2096.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap-5cd0811f461c8271ae6bab49df9501be.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script src="../../../../../site_libs/quarto-contrib/iconify-2.0.0/iconify-icon.min.js"></script><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-527449-5', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script><style>

      .quarto-title-block .quarto-title-banner {
        background: #170C3A;
      }
</style>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<meta property="og:title" content="How to use Bayesian propensity scores and inverse probability weights | Andrew Heiss">
<meta property="og:description" content="For mathematical and philosophical reasons, propensity scores and inverse probability weights don’t work in Bayesian inference. But never fear! There’s still a way to do it!">
<meta property="og:image" content="https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/index_files/figure-html/plot-iptw-pseudo-populations-1.png">
<meta property="og:site_name" content="Andrew Heiss">
<meta property="og:locale" content="en_US">
<meta property="og:image:height" content="1067">
<meta property="og:image:width" content="1728">
<meta name="twitter:title" content="How to use Bayesian propensity scores and inverse probability weights | Andrew Heiss">
<meta name="twitter:description" content="For mathematical and philosophical reasons, propensity scores and inverse probability weights don’t work in Bayesian inference. But never fear! There’s still a way to do it!">
<meta name="twitter:image" content="https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/index_files/figure-html/plot-iptw-pseudo-populations-1.png">
<meta name="twitter:creator" content="@andrewheiss">
<meta name="twitter:site" content="@andrewheiss">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image-height" content="1067">
<meta name="twitter:image-width" content="1728">
</head>
<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Andrew Heiss</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
<li class="nav-item">
    <a class="nav-link" href="../../../../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../cv/index.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../research/index.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../teaching/index.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../talks/index.html"> 
<span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../now/index.html"> 
<span class="menu-text">Now</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../uses/index.html"> 
<span class="menu-text">Uses</span></a>
  </li>  
</ul>
<ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item">
    <a class="nav-link" href="../../../../../atom.xml"> 
<span class="menu-text"><iconify-icon inline="" icon="bi:rss" style="font-size: 1.1em;" aria-label="Icon rss from bi Iconify.design set." title="RSS"></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="mailto:aheiss@gsu.edu"> 
<span class="menu-text"><iconify-icon inline="" icon="bi:envelope" style="font-size: 1.1em;" aria-label="Icon envelope from bi Iconify.design set." title="E-mail"></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://bsky.app/profile/andrew.heiss.phd" rel="me"> 
<span class="menu-text"><iconify-icon inline="" icon="fa6-brands:bluesky" style="font-size: 1.1em;" aria-label="Icon bluesky from fa6-brands Iconify.design set." title="Bluesky"></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://fediscience.org/users/andrew/" rel="me"> 
<span class="menu-text"><iconify-icon inline="" icon="bi:mastodon" style="font-size: 1.1em;" aria-label="Icon mastodon from bi Iconify.design set." title="Mastodon"></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/andrewheiss" rel="me"> 
<span class="menu-text"><iconify-icon inline="" icon="bi:github" style="font-size: 1.1em;" aria-label="Icon github from bi Iconify.design set." title="GitHub"></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.youtube.com/andrewheiss" rel="me"> 
<span class="menu-text"><iconify-icon inline="" icon="bi:youtube" style="font-size: 1.1em;" aria-label="Icon youtube from bi Iconify.design set." title="YouTube"></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.heissatopia.com/"> 
<span class="menu-text"><iconify-icon inline="" icon="fa6-brands:blogger" style="font-size: 1.15em;" aria-label="Icon blogger from fa6-brands Iconify.design set." title="Blogger"></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.linkedin.com/in/andrewheiss" rel="me"> 
<span class="menu-text"><iconify-icon inline="" icon="bi:linkedin" style="font-size: 1.1em;" aria-label="Icon linkedin from bi Iconify.design set." title="LinkedIn"></iconify-icon></span></a>
  </li>  
</ul>
</div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default blog-post page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">How to use Bayesian propensity scores and inverse probability weights</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          For mathematical and philosophical reasons, propensity scores and inverse probability weights don’t work in Bayesian inference. But never fear! There’s still a way to do it!
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">r</div>
                <div class="quarto-category">tidyverse</div>
                <div class="quarto-category">regression</div>
                <div class="quarto-category">statistics</div>
                <div class="quarto-category">data visualization</div>
                <div class="quarto-category">causal inference</div>
                <div class="quarto-category">do calculus</div>
                <div class="quarto-category">DAGs</div>
                <div class="quarto-category">bayes</div>
                <div class="quarto-category">brms</div>
                <div class="quarto-category">stan</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://www.andrewheiss.com/">Andrew Heiss</a> <a href="https://orcid.org/0000-0002-3948-3914" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Saturday, December 18, 2021</p>
      </div>
    </div>
    
      
      <div>
      <div class="quarto-title-meta-heading">Doi</div>
      <div class="quarto-title-meta-contents">
        <p class="doi">
          <a href="https://doi.org/10.59350/nrwsd-3jz20">10.59350/nrwsd-3jz20</a>
        </p>
      </div>
    </div>
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Contents</h2>
   
  <ul>
<li><a href="#who-this-post-is-for" id="toc-who-this-post-is-for" class="nav-link active" data-scroll-target="#who-this-post-is-for">Who this post is for</a></li>
  <li>
<a href="#general-approach-to-inverse-probability-weighting" id="toc-general-approach-to-inverse-probability-weighting" class="nav-link" data-scroll-target="#general-approach-to-inverse-probability-weighting">General approach to inverse probability weighting</a>
  <ul class="collapse">
<li><a href="#basic-frequentist-example" id="toc-basic-frequentist-example" class="nav-link" data-scroll-target="#basic-frequentist-example">Basic frequentist example</a></li>
  <li><a href="#pseudo-populations" id="toc-pseudo-populations" class="nav-link" data-scroll-target="#pseudo-populations">Pseudo-populations</a></li>
  </ul>
</li>
  <li>
<a href="#bayesian-inverse-probability-weighting" id="toc-bayesian-inverse-probability-weighting" class="nav-link" data-scroll-target="#bayesian-inverse-probability-weighting">Bayesian inverse probability weighting</a>
  <ul class="collapse">
<li><a href="#why-even-do-this-bayesianly" id="toc-why-even-do-this-bayesianly" class="nav-link" data-scroll-target="#why-even-do-this-bayesianly">Why even do this Bayesianly?</a></li>
  <li><a href="#fundamental-problem-with-bayesian-propensity-scores" id="toc-fundamental-problem-with-bayesian-propensity-scores" class="nav-link" data-scroll-target="#fundamental-problem-with-bayesian-propensity-scores">Fundamental problem with Bayesian propensity scores</a></li>
  <li><a href="#a-legal-way-to-use-weights-bayesianly" id="toc-a-legal-way-to-use-weights-bayesianly" class="nav-link" data-scroll-target="#a-legal-way-to-use-weights-bayesianly">A legal way to use weights Bayesianly</a></li>
  </ul>
</li>
  <li><a href="#things-i-dont-know-yet-but-want-to-know" id="toc-things-i-dont-know-yet-but-want-to-know" class="nav-link" data-scroll-target="#things-i-dont-know-yet-but-want-to-know">Things I don’t know yet but want to know!</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></nav>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><p>This post combines two of my long-standing interests: causal inference and Bayesian statistics. I’ve been teaching <a href="https://evalf21.classes.andrewheiss.com/">a course on program evaluation and causal inference</a> for a couple years now and it has become one of my favorite classes ever. It has reshaped how I do my research, and I’ve been trying to carefully incorporate causal approaches in my different project—as evidenced by an ever-growing series of blog posts here about different issues I run into and figure out (like <a href="https://www.andrewheiss.com/blog/2020/12/01/ipw-binary-continuous/">this</a> and <a href="https://www.andrewheiss.com/blog/2020/12/03/ipw-tscs-msm/">this</a> and <a href="https://www.andrewheiss.com/blog/2021/01/15/msm-gee-multilevel/">this</a> and <a href="https://www.andrewheiss.com/blog/2021/09/07/do-calculus-backdoors/">this</a>.)</p>
<p>Additionally, ever since stumbling on <a href="https://thinkinator.com/2016/01/12/r-users-will-now-inevitably-become-bayesians/">this blog post</a> as a PhD student back in 2016 following the invention of <strong>rstanarm</strong> and <strong>brms</strong>, both of which make it easy to use Stan with R, I’ve been as Bayesian as possible in all my research. I find Bayesian approaches to inference <em>way</em> more intuitive than frequentist null hypothesis significance testing. My work with Bayesian approaches has also led to a bunch of blog posts here (like <a href="https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/">this</a> and <a href="https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/">this</a> and <a href="https://www.andrewheiss.com/blog/2021/12/01/multilevel-models-panel-data-guide/">this</a>).</p>
<p>However, the combination of these two interests is a little fraught. In one of my projects, I’m using marginal structural models and inverse probability weights to account for confounding and make causal claims. I also want to do this Bayesianly so that I can work with posterior distributions and use Bayesian inference rather than null hypotheses and significance. But using inverse probability weights <em>and</em> Bayesian methods simultaneously seems to be impossible! <span class="citation" data-cites="RobinsHernanWasserman:2015">Robins, Hernán, and Wasserman (<a href="#ref-RobinsHernanWasserman:2015" role="doc-biblioref">2015</a>)</span> even have an article where they explicitly say that “Bayesian inference must ignore the propensity score,” effectively making it impossible to use things like inverse probability weights Bayesianly. Oh no!</p>
<p>I recently came across a new article that gives me hope though! There’s a group of epidemiologists and biostatisticians who have been working on finding ways to use a Bayesian approach to propensity scores and weights (<span class="citation" data-cites="SaarelaStephensMoodie:2015">Saarela et al. (<a href="#ref-SaarelaStephensMoodie:2015" role="doc-biblioref">2015</a>)</span>; <span class="citation" data-cites="Zigler:2016">Zigler (<a href="#ref-Zigler:2016" role="doc-biblioref">2016</a>)</span>; <span class="citation" data-cites="LiaoZigler:2020">Liao and Zigler (<a href="#ref-LiaoZigler:2020" role="doc-biblioref">2020</a>)</span>, among others), and <span class="citation" data-cites="LiaoZigler:2020">Liao and Zigler (<a href="#ref-LiaoZigler:2020" role="doc-biblioref">2020</a>)</span> provide a useful (and understandable!) approach for doing it. This post is my attempt at translating Liao and Zigler’s paper from conceptual math and <a href="https://github.com/shirleyxliao/Uncertainty-in-the-Design-Stage-of-Two-Stage-Bayesian-Propensity-Score-Analysis"><strong>MCMCPack</strong>-based R code</a> into <strong>tidyverse</strong>, <strong>brms</strong>, and Stan-based code. Here we go!</p>
<section id="who-this-post-is-for" class="level2"><h2 class="anchored" data-anchor-id="who-this-post-is-for">Who this post is for</h2>
<p>Here’s what I assume you know:</p>
<ul>
<li>You’re familiar with <a href="https://www.r-project.org/">R</a> and the <a href="https://www.tidyverse.org/">tidyverse</a> (particularly <a href="https://dplyr.tidyverse.org/">dplyr</a> and <a href="https://ggplot2.tidyverse.org/">ggplot2</a>).</li>
<li>You’re familiar with <a href="https://paul-buerkner.github.io/brms/">brms</a> for running Bayesian regression models. See <a href="https://paul-buerkner.github.io/brms/articles/index.html">the vignettes here</a>, examples like <a href="https://www.rensvandeschoot.com/tutorials/brms-started/">this</a>, or <a href="https://evalf21.classes.andrewheiss.com/resource/bayes/#resources">resources like these</a> for an introduction.</li>
<li>You know a little about DAGs and causal model-based approaches to causal inference, and you’ve heard about statistical adjustment to isolate causal effects (i.e.&nbsp;“closing backdoors in a DAG”)</li>
</ul></section><section id="general-approach-to-inverse-probability-weighting" class="level2"><h2 class="anchored" data-anchor-id="general-approach-to-inverse-probability-weighting">General approach to inverse probability weighting</h2>
<p>I won’t go into the details of how inverse probability weighting works here. For more details, check out <a href="https://evalf21.classes.andrewheiss.com/example/matching-ipw/">this fully worked out example</a> or <a href="https://evalf21.classes.andrewheiss.com/files/10-causal-inference.pdf">this chapter</a>, which has references to lots of other more detailed resources. Instead, I’ll provide a super short abbreviated overview of how inverse probability weights are used for causal inference and why and how we can use them.</p>
<p>When trying to make causal inferences with observational data, there is inevitably confounding—people self-select into (or out of) treatment conditions because of a host of external factors. We can adjust for these confounding factors with lots of different methods. Quasi-experimental approaches like difference-in-differences, regression discontinuity, and instrumental variables let us use specific (and often weird) situations to adjust for confounding and make comparable treatment and control groups. Alternatively, we can use model-based inference using DAGs and <em>do</em>-calculus, identifying which variables open up backdoor pathways between treatment and outcome, and statistically adjusting for those variables to isolate the treatment → outcome pathway.</p>
<p>One way to adjust for confounders is to use inverse probability weighting. In short, here’s how it works for a binary (0/1) treatment:</p>
<ol type="1">
<li><p>Create a model that predicts treatment (often called a <em>treatment model</em> or <em>design stage</em>). Use confounders (identified with a DAG) as the covariates. Use whatever modeling approach you want here—logistic regression, random forests, fancy machine learning things, etc.</p></li>
<li><p>Use the results of the treatment model to calculate propensity scores.</p></li>
<li>
<p>Convert those propensity scores into <em>inverse probability of treatment weights</em> (IPTW) using this formula:</p>
<p><span class="math display">\[
\frac{\text{Treatment}}{\text{Propensity}} + \frac{1 - \text{Treatment}}{1 - \text{Propensity}}
\]</span></p>
</li>
<li><p>Create a model that estimates the effect of treatment on outcome, weighted by the IPTWs (often called an <em>outcome model</em> or <em>analysis stage</em>). The coefficient for the treatment variable is the average treatment effect (ATE).</p></li>
</ol>
<section id="basic-frequentist-example" class="level3"><h3 class="anchored" data-anchor-id="basic-frequentist-example">Basic frequentist example</h3>
<p>Throughout this post, we’ll use some simulated data for a fake hypothetical social program that distributes mosquito nets in order to reduce malaria risk. I created this data for my course on <a href="https://evalf21.classes.andrewheiss.com/">program evaluation and causal inference</a> and use it for <a href="https://evalf21.classes.andrewheiss.com/example/matching-ipw/">teaching adjustment with inverse probability weighting</a>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/paul-buerkner/brms">brms</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://broom.tidymodels.org/">broom</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bbolker/broom.mixed">broom.mixed</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/r-causal/ggdag">ggdag</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">MetBrewer</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">3273</span><span class="op">)</span>  <span class="co"># From random.org</span></span>
<span></span>
<span><span class="co"># Use the delightful Isfahan1 palette from the MetBrewer package</span></span>
<span><span class="va">isfahan</span> <span class="op">&lt;-</span> <span class="fu">MetBrewer</span><span class="fu">::</span><span class="fu">met.brewer</span><span class="op">(</span><span class="st">"Isfahan1"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Custom ggplot theme to make pretty plots</span></span>
<span><span class="co"># Get Archivo Narrow at https://fonts.google.com/specimen/Archivo+Narrow</span></span>
<span><span class="va">theme_nice</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu">theme_minimal</span><span class="op">(</span>base_family <span class="op">=</span> <span class="st">"Archivo Narrow"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">theme</span><span class="op">(</span>panel.grid.minor <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span>,</span>
<span>          plot.background <span class="op">=</span> <span class="fu">element_rect</span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"white"</span>, color <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span>,</span>
<span>          plot.title <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>face <span class="op">=</span> <span class="st">"bold"</span><span class="op">)</span>,</span>
<span>          axis.title <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>face <span class="op">=</span> <span class="st">"bold"</span><span class="op">)</span>,</span>
<span>          strip.text <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>face <span class="op">=</span> <span class="st">"bold"</span>, size <span class="op">=</span> <span class="fu">rel</span><span class="op">(</span><span class="fl">0.8</span><span class="op">)</span>, hjust <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>,</span>
<span>          strip.background <span class="op">=</span> <span class="fu">element_rect</span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"grey80"</span>, color <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span>,</span>
<span>          legend.title <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>face <span class="op">=</span> <span class="st">"bold"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Use this theme on all plots</span></span>
<span><span class="fu">theme_set</span><span class="op">(</span></span>
<span>  <span class="fu">theme_nice</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Make all labels use Archivo by default</span></span>
<span><span class="fu">update_geom_defaults</span><span class="op">(</span><span class="st">"label"</span>, </span>
<span>                     <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>family <span class="op">=</span> <span class="st">"Archivo Narrow"</span>,</span>
<span>                          fontface <span class="op">=</span> <span class="st">"bold"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">nets</span> <span class="op">&lt;-</span> <span class="fu">read_csv</span><span class="op">(</span><span class="st">"https://evalf21.classes.andrewheiss.com/data/mosquito_nets.csv"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I normally use a more complicated DAG with other nodes that don’t need to be adjusted for, but for the sake of simplicity here, this DAG only includes the confounders. The relationship between net usage (measured as a 0/1 binary variable where 1 = person used a net) and malaria risk (measured on a scale of 0-100, with higher values representing higher risk) is confounded by monthly income (in USD), health (measured on a scale of 0-100, with higher values representing better health), and nighttime temperatures at the person’s home (measured in Celsius).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">mosquito_dag</span> <span class="op">&lt;-</span> <span class="fu">dagify</span><span class="op">(</span></span>
<span>  <span class="va">malaria_risk</span> <span class="op">~</span> <span class="va">net</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">health</span> <span class="op">+</span> <span class="va">temperature</span>,</span>
<span>  <span class="va">net</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">health</span> <span class="op">+</span> <span class="va">temperature</span>,</span>
<span>  <span class="va">health</span> <span class="op">~</span> <span class="va">income</span>,</span>
<span>  exposure <span class="op">=</span> <span class="st">"net"</span>,</span>
<span>  outcome <span class="op">=</span> <span class="st">"malaria_risk"</span>,</span>
<span>  coords <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>malaria_risk <span class="op">=</span> <span class="fl">7</span>, net <span class="op">=</span> <span class="fl">3</span>, income <span class="op">=</span> <span class="fl">4</span>, health <span class="op">=</span> <span class="fl">5</span>, temperature <span class="op">=</span> <span class="fl">6</span><span class="op">)</span>,</span>
<span>                y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>malaria_risk <span class="op">=</span> <span class="fl">2</span>, net <span class="op">=</span> <span class="fl">2</span>, income <span class="op">=</span> <span class="fl">3</span>, health <span class="op">=</span> <span class="fl">1</span>, temperature <span class="op">=</span> <span class="fl">3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>malaria_risk <span class="op">=</span> <span class="st">"Risk of malaria"</span>, net <span class="op">=</span> <span class="st">"Mosquito net"</span>, income <span class="op">=</span> <span class="st">"Income"</span>,</span>
<span>             health <span class="op">=</span> <span class="st">"Health"</span>, temperature <span class="op">=</span> <span class="st">"Nighttime temperatures"</span>,</span>
<span>             resistance <span class="op">=</span> <span class="st">"Insecticide resistance"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Turn DAG into a tidy data frame for plotting</span></span>
<span><span class="va">mosquito_dag_tidy</span> <span class="op">&lt;-</span> <span class="va">mosquito_dag</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">tidy_dagitty</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">node_status</span><span class="op">(</span><span class="op">)</span>   <span class="co"># Add column for exposure/outcome/latent</span></span>
<span></span>
<span><span class="va">status_colors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>exposure <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, outcome <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">7</span><span class="op">]</span>, latent <span class="op">=</span> <span class="st">"grey50"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fancier graph</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">mosquito_dag_tidy</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, xend <span class="op">=</span> <span class="va">xend</span>, yend <span class="op">=</span> <span class="va">yend</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_dag_edges</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_dag_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>color <span class="op">=</span> <span class="va">status</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_label</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>label <span class="op">=</span> <span class="va">label</span>, fill <span class="op">=</span> <span class="va">status</span><span class="op">)</span>,</span>
<span>             color <span class="op">=</span> <span class="st">"white"</span>, fontface <span class="op">=</span> <span class="st">"bold"</span>, nudge_y <span class="op">=</span> <span class="op">-</span><span class="fl">0.3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_color_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="va">status_colors</span>, na.value <span class="op">=</span> <span class="st">"grey20"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_fill_manual</span><span class="op">(</span>values <span class="op">=</span> <span class="va">status_colors</span>, na.value <span class="op">=</span> <span class="st">"grey20"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">guides</span><span class="op">(</span>color <span class="op">=</span> <span class="st">"none"</span>, fill <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme_dag</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="index_files/figure-html/basic-dag-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
</div>
<p>Following the logic of <em>do</em>-calculus, we need to adjust for three of these nodes in order to isolate the pathway between net usage and malaria risk:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.dagitty.net">dagitty</a></span><span class="op">)</span></span>
<span><span class="fu">adjustmentSets</span><span class="op">(</span><span class="va">mosquito_dag</span><span class="op">)</span></span>
<span><span class="co">## { health, income, temperature }</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can adjust for these variables using inverse probability weighting. We’ll first make a treatment model (or “design stage” in the world of biostats) that uses these confounders to predict net use, then we’ll create propensity scores and inverse probability treatment weights, and then we’ll use those weights in an outcome model (or “analysis stage” in the world of biostats) to calculate the average treatment effect (ATE) of net usage.</p>
<p>I built in a 10 point decrease in malaria risk due to nets (hooray for fake data!), so let’s see if we can recover that treatment effect:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Step 1: Create model that predicts treatment status using confounders</span></span>
<span><span class="va">model_treatment_freq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">net</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">temperature</span> <span class="op">+</span> <span class="va">health</span>,</span>
<span>                            data <span class="op">=</span> <span class="va">nets</span>,</span>
<span>                            family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span>link <span class="op">=</span> <span class="st">"logit"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 2: Use the treatment model to calculate propensity scores, and</span></span>
<span><span class="co"># Step 3: Use the propensity scores to calculate inverse probability of treatment weights</span></span>
<span><span class="va">nets_with_weights</span> <span class="op">&lt;-</span> <span class="fu">augment</span><span class="op">(</span><span class="va">model_treatment_freq</span>, <span class="va">nets</span>,</span>
<span>                             type.predict <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">rename</span><span class="op">(</span>propensity <span class="op">=</span> <span class="va">.fitted</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>iptw <span class="op">=</span> <span class="op">(</span><span class="va">net_num</span> <span class="op">/</span> <span class="va">propensity</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">net_num</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">propensity</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 4: Use the IPTWs in a model that estimates the effect of treatment on outcome</span></span>
<span><span class="va">model_outcome_freq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">malaria_risk</span> <span class="op">~</span> <span class="va">net</span>,</span>
<span>                         data <span class="op">=</span> <span class="va">nets_with_weights</span>,</span>
<span>                         weights <span class="op">=</span> <span class="va">iptw</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Coefficient for `net` should be -10ish</span></span>
<span><span class="fu">tidy</span><span class="op">(</span><span class="va">model_outcome_freq</span><span class="op">)</span></span>
<span><span class="co">## # A tibble: 2 × 5</span></span>
<span><span class="co">##   term        estimate std.error statistic  p.value</span></span>
<span><span class="co">##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;</span></span>
<span><span class="co">## 1 (Intercept)     39.7     0.468      84.7 0       </span></span>
<span><span class="co">## 2 netTRUE        -10.1     0.658     -15.4 3.21e-50</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It worked! After going through this two-step process of (1) creating propensity scores and weights, and (2) using those weights to estimate the actual effect, we successfully closed the backdoor pathways that confounded the relationship between net use and malaria risk, ending up with an unbiased ATE. Neato.</p>
</section><section id="pseudo-populations" class="level3"><h3 class="anchored" data-anchor-id="pseudo-populations">Pseudo-populations</h3>
<p>Before looking at how to do this analysis Bayesianly, it’s helpful to understand what these weights are actually doing behind the scenes. The point of these IPTWs is to create pseudo-populations of treated and untreated observations that are comparable across all the different levels of confounders. They’re essentially a way to let us fake treatment and control groups so that we can interpret the results of outcome models causally.</p>
<p>Visualizing the propensity scores for treated and untreated people can help show what’s going on. Here are the distributions of propensity scores for these two groups: the treated group is in the top half in brown; the untreated group is in the bottom half in turquoise. (Thanks to <a href="https://livefreeordichotomize.com/2019/01/17/understanding-propensity-score-weighting/">Lucy D’Agostino McGowan for this really neat way of looking at weight distributions</a>!)</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">nets_with_weights</span>, <span class="va">net_num</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">50</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">propensity</span><span class="op">)</span>, </span>
<span>                 fill <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">nets_with_weights</span>, <span class="va">net_num</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">50</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">propensity</span>, y <span class="op">=</span> <span class="op">-</span><span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 fill <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_hline</span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">annotate</span><span class="op">(</span>geom <span class="op">=</span> <span class="st">"label"</span>, x <span class="op">=</span> <span class="fl">0.1</span>, y <span class="op">=</span> <span class="fl">20</span>, label <span class="op">=</span> <span class="st">"Treated"</span>, </span>
<span>           fill <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, color <span class="op">=</span> <span class="st">"white"</span>, hjust <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">annotate</span><span class="op">(</span>geom <span class="op">=</span> <span class="st">"label"</span>, x <span class="op">=</span> <span class="fl">0.1</span>, y <span class="op">=</span> <span class="op">-</span><span class="fl">20</span>, label <span class="op">=</span> <span class="st">"Untreated"</span>, </span>
<span>           fill <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span>, color <span class="op">=</span> <span class="st">"white"</span>, hjust <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_y_continuous</span><span class="op">(</span>label <span class="op">=</span> <span class="va">abs</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.8</span><span class="op">)</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">80</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Propensity"</span>, y <span class="op">=</span> <span class="st">"Count"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="index_files/figure-html/plot-propensity-hist-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
</div>
<p>We can learn a few different things from this plot. Fewer people received the treatment than didn’t—there are more people in the untreated part of the graph. We can confirm this really quick:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">nets</span> <span class="op">%&gt;%</span> <span class="fu">count</span><span class="op">(</span><span class="va">net</span><span class="op">)</span></span>
<span><span class="co">## # A tibble: 2 × 2</span></span>
<span><span class="co">##   net       n</span></span>
<span><span class="co">##   &lt;lgl&gt; &lt;int&gt;</span></span>
<span><span class="co">## 1 FALSE  1071</span></span>
<span><span class="co">## 2 TRUE    681</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Yep. There are ≈400 more net-non-users than net-users.</p>
<p>We can also see that those who did not receive treatment tend to have a lower probability of doing so—the bulk of the untreated distribution is clustered in the low end of propensity scores. This makes sense! If people have a low chance of using a mosquito net, there should be fewer people ultimately using a bed net.</p>
<p>But these two groups—treated and untreated—aren’t exactly comparable at this point. There are confounding factors that make people who didn’t use nets less likely to use them. To make causal inferences about the effect of nets, we’d need to look at a “treatment” and a “control” group with similar characteristics and with similar probabilities of using nets.</p>
<p>To get around this, we can create two pseudo-populations of treated and untreated people. We can give less statistical importance (or weight) to those who had a low probability of being treated and who subsequently weren’t treated (since there are a ton of those people) and more statistical weight to those who had a high probability of being treated but weren’t. Similarly, we can give more weight to treated people who had a low probability of being treated (that’s surprising!) and less weight to treated people who had a high probability of being treated (that’s not surprising!). If we scale each person by their weight, given the confounders of income, temperature, and health, we can create comparable treated and untreated populations:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">nets_with_weights</span>, <span class="va">net_num</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">50</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">propensity</span>, weight <span class="op">=</span> <span class="va">iptw</span><span class="op">)</span>, </span>
<span>                 fill <span class="op">=</span> <span class="fu">colorspace</span><span class="fu">::</span><span class="fu">lighten</span><span class="op">(</span><span class="va">isfahan</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">0.35</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">nets_with_weights</span>, <span class="va">net_num</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">50</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">propensity</span>, weight <span class="op">=</span> <span class="va">iptw</span>, y <span class="op">=</span> <span class="op">-</span><span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 fill <span class="op">=</span> <span class="fu">colorspace</span><span class="fu">::</span><span class="fu">lighten</span><span class="op">(</span><span class="va">isfahan</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span>, <span class="fl">0.35</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">nets_with_weights</span>, <span class="va">net_num</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">50</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">propensity</span><span class="op">)</span>, </span>
<span>                 fill <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">nets_with_weights</span>, <span class="va">net_num</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">50</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">propensity</span>, y <span class="op">=</span> <span class="op">-</span><span class="fu">after_stat</span><span class="op">(</span><span class="va">count</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 fill <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">annotate</span><span class="op">(</span>geom <span class="op">=</span> <span class="st">"label"</span>, x <span class="op">=</span> <span class="fl">0.8</span>, y <span class="op">=</span> <span class="fl">70</span>, label <span class="op">=</span> <span class="st">"Treated (actual)"</span>, </span>
<span>           fill <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, color <span class="op">=</span> <span class="st">"white"</span>, hjust <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">annotate</span><span class="op">(</span>geom <span class="op">=</span> <span class="st">"label"</span>, x <span class="op">=</span> <span class="fl">0.8</span>, y <span class="op">=</span> <span class="fl">90</span>, label <span class="op">=</span> <span class="st">"Treated (IPTW pseudo-population)"</span>, </span>
<span>           fill <span class="op">=</span> <span class="fu">colorspace</span><span class="fu">::</span><span class="fu">lighten</span><span class="op">(</span><span class="va">isfahan</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">0.35</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"white"</span>, hjust <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">annotate</span><span class="op">(</span>geom <span class="op">=</span> <span class="st">"label"</span>, x <span class="op">=</span> <span class="fl">0.8</span>, y <span class="op">=</span> <span class="op">-</span><span class="fl">60</span>, label <span class="op">=</span> <span class="st">"Untreated (actual)"</span>, </span>
<span>           fill <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span>, color <span class="op">=</span> <span class="st">"white"</span>, hjust <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">annotate</span><span class="op">(</span>geom <span class="op">=</span> <span class="st">"label"</span>, x <span class="op">=</span> <span class="fl">0.8</span>, y <span class="op">=</span> <span class="op">-</span><span class="fl">80</span>, label <span class="op">=</span> <span class="st">"Untreated (IPTW pseudo-population)"</span>, </span>
<span>           fill <span class="op">=</span> <span class="fu">colorspace</span><span class="fu">::</span><span class="fu">lighten</span><span class="op">(</span><span class="va">isfahan</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span>, <span class="fl">0.35</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"white"</span>, hjust <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_hline</span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fl">0</span>, color <span class="op">=</span> <span class="st">"white"</span>, linewidth <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_y_continuous</span><span class="op">(</span>label <span class="op">=</span> <span class="va">abs</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.8</span><span class="op">)</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">80</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Propensity"</span>, y <span class="op">=</span> <span class="st">"Count"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="index_files/figure-html/plot-iptw-pseudo-populations-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
</div>
<p>This plot shows the original distributions for treated and untreated propensities in darker colors and overlays the weighted/adjusted propensities in lighter colors. These pseudo-populations mirror each other pretty well now! These two rescaled and reweighted groups are now much more equally sized and comparable: the low-propensity net users have a much higher weight, while high-propensity non-net users also get more weight.</p>
</section></section><section id="bayesian-inverse-probability-weighting" class="level2"><h2 class="anchored" data-anchor-id="bayesian-inverse-probability-weighting">Bayesian inverse probability weighting</h2>
<section id="why-even-do-this-bayesianly" class="level3"><h3 class="anchored" data-anchor-id="why-even-do-this-bayesianly">Why even do this Bayesianly?</h3>
<p>We successfully found the causal effect of -10 malaria risk points using regular frequentist regression, so why am I trying to make life more complex and do this with Bayesian methods instead? Mostly because I’m not a fan of null hypothesis signficance testing (NHST), or the whole process of proposing a null hypothesis, generating a statistical test, finding a p-value, and seeing if the test/p-value/confidence interval provides enough evidence to reject the null hypothesis. With frequentist statistics we test for the probability of the data given a null hypothesis, or <span class="math inline">\(P(\text{data} \mid H_0)\)</span>, while with Bayesian statistics, we get to test for the probability of a hypothesis given the data, or <span class="math inline">\(P(H \mid \text{data})\)</span>.</p>
<p>Testing a hypothesis directly with Bayesian inference is a lot more intuitive, with Bayesian <a href="https://evalf21.classes.andrewheiss.com/resource/bayes/#bayesian-credible-intervals">credible intervals</a> and inferential approaches like measuring the probability that a parameter is greater/less than 0 (i.e.&nbsp;<a href="https://evalf21.classes.andrewheiss.com/resource/bayes/#probability-of-direction">probability of direction</a>), or measuring the proportion of a posterior that falls within a null region of practical equivalence, or <a href="https://evalf21.classes.andrewheiss.com/resource/bayes/#region-of-practical-equivalence-rope">ROPE</a>. <a href="https://evalf21.classes.andrewheiss.com/resource/bayes/#">See this page for an overview of all these methods</a> and a comparison with frequentism, and check out <a href="https://www.bayesrulesbook.com/">this amazing (and free!) textbook on Bayesianism in general</a>.</p>
<p>Plus, <a href="https://paul-buerkner.github.io/brms/">the amazing <strong>brms</strong> package</a> lets us make all sorts of powerful and complex models with <a href="https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/">multilevel nested intercepts and slopes</a> and fancy families like <a href="https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/">beta</a>, <a href="https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/">zero-inflated beta</a>, hurdled lognormal, and so on.</p>
</section><section id="fundamental-problem-with-bayesian-propensity-scores" class="level3"><h3 class="anchored" data-anchor-id="fundamental-problem-with-bayesian-propensity-scores">Fundamental problem with Bayesian propensity scores</h3>
<p>However, there are serious mathematical and philosophical issues with using propensity scores (and IPTWs) in Bayesian models. Put simply, IPTWs aren’t actually part of the model! They’re a neat way to scale and shift the population into comparable pseudo-populations, but they’re not part of the data-generating process for any element of a Bayesian model.</p>
<p>Here’s how <span class="citation" data-cites="RobinsHernanWasserman:2015">Robins, Hernán, and Wasserman (<a href="#ref-RobinsHernanWasserman:2015" role="doc-biblioref">2015</a>)</span>, <span class="citation" data-cites="Zigler:2016">Zigler (<a href="#ref-Zigler:2016" role="doc-biblioref">2016</a>)</span>, and <span class="citation" data-cites="LiaoZigler:2020">Liao and Zigler (<a href="#ref-LiaoZigler:2020" role="doc-biblioref">2020</a>)</span> explain it (but substantially simplified to the point of being a little bit wrong, but acceptably wrong). We can more formally define the average treatment effect (ATE) of mosquito nets on malaria risk using this estimand:</p>
<p><span class="math display">\[
\Delta_{\text{ATE}} = E[ \overbrace{E \left( Y_i \mid T_i = 1, X_i \right)}^{\substack{\text{Average outcome } Y \text{ when} \\ \text{treated, given confounders }X}} - \overbrace{E \left( Y_i \mid T_i = 0, X_i \right)}^{\substack{\text{Average outcome } Y \text{ when} \\ \text{not treated, given confounders }X}} ]
\]</span></p>
<p>There are three moving parts in the equation for the ATE here: the treatment effect is a function of the outcome <span class="math inline">\(Y\)</span> (malaria risk), the treatment <span class="math inline">\(T\)</span> (nets), and all other covariates <span class="math inline">\(X\)</span> (health, income, temperatures). In order to calculate this ATE, we need to create some sort of function that incorporates all three, or something like this that calculates <span class="math inline">\(\Delta\)</span> given <span class="math inline">\(T\)</span>, <span class="math inline">\(X\)</span>, and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y})
\]</span></p>
<p>This works well with Bayesian methods. We don’t know what <span class="math inline">\(\Delta\)</span> is, so we can use Bayes’ theorem to estimate it given our existing data for <span class="math inline">\(T\)</span>, <span class="math inline">\(X\)</span>, and <span class="math inline">\(Y\)</span>. To quote from <span class="citation" data-cites="LiaoZigler:2020">Liao and Zigler (<a href="#ref-LiaoZigler:2020" role="doc-biblioref">2020</a>)</span>,</p>
<blockquote class="blockquote">
<p>[T]raditional Bayeisan inference for <span class="math inline">\(\Delta\)</span> would follow from specification of a likelihood for <span class="math inline">\((\boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y})\)</span> conditional on unknown parameters, <span class="math inline">\(\theta\)</span>, a prior distribution for <span class="math inline">\(\theta\)</span>, and some function relating the data and <span class="math inline">\(\theta\)</span> to the quantity <span class="math inline">\(\Delta\)</span>.</p>
</blockquote>
<p>Note that the equation below is slightly wrong—ordinarily in Bayesian modeling we’re interested in estimating an unknown <span class="math inline">\(\theta\)</span> parameter, so if we wanted to be super official we’d need to create “some function relating the data and <span class="math inline">\(\theta\)</span> to the quantity <span class="math inline">\(\Delta\)</span>”, but for the sake of simplicity and intuition we’ll skip that part and pretend that <span class="math inline">\(\Delta\)</span> is the output of the model:</p>
<p><span class="math display">\[
\overbrace{P[\Delta \mid (\boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y})]}^{\substack{\text{Posterior estimate} \\ \text{of } \Delta \text{, given data}}} \propto \overbrace{P[(\boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y}) \mid \Delta]}^{\substack{\text{Likelihood for existing} \\ \text{data, given unknown }\Delta}} \times \overbrace{P[\Delta]}^{\substack{\text{Prior} \\ \text{for }\Delta}}
\]</span></p>
<p>But guess what’s missing entirely from this equation?! Our weights! Propensity scores and inverse probability weights have no place in this kind of Bayesian estimation. They would theoretically show up in the likelihood part of the Bayesian equation, but in practice, weights aren’t part of the data-generating process and thus aren’t actually part of the likelihood. This is the key problem with Bayesian weights: these weights <strong>are not part of the model</strong> for calculating the ATE (<span class="math inline">\(f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y})\)</span>). We can’t set a prior on the weight parameter—there isn’t even a weight parameter to work with in the likelihood. In order to be a true Bayesian, we can’t use propensity scores and weighting. This is the argument that <span class="citation" data-cites="RobinsHernanWasserman:2015">Robins, Hernán, and Wasserman (<a href="#ref-RobinsHernanWasserman:2015" role="doc-biblioref">2015</a>)</span> make: because propensity scores (and thus weights) aren’t part of the likelihood, we can’t do anything Bayesian with them.</p>
<p>And that’s disappointing because Bayesian inference is great! I find it far more intuitive (and fun) to make inferences with posterior distributions rather than work with null hypothesis significance testing.</p>
</section><section id="a-legal-way-to-use-weights-bayesianly" class="level3"><h3 class="anchored" data-anchor-id="a-legal-way-to-use-weights-bayesianly">A legal way to use weights Bayesianly</h3>
<p>Fortunately, <span class="citation" data-cites="RobinsHernanWasserman:2015">Robins, Hernán, and Wasserman (<a href="#ref-RobinsHernanWasserman:2015" role="doc-biblioref">2015</a>)</span> conclude by saying that there are possible compromises that we can use to work with weights in a Bayesian framework, and <span class="citation" data-cites="LiaoZigler:2020">Liao and Zigler (<a href="#ref-LiaoZigler:2020" role="doc-biblioref">2020</a>)</span> explore one of these compromises and propose a method of incorporating propensity scores into Bayesian estimation of causal effects. Their general approach is to think of the propensity score calculation as a new parameter <span class="math inline">\(\nu\)</span> (nu). Instead of calculating a single value of <span class="math inline">\(\nu\)</span> (i.e.&nbsp;a single set of propensity scores or weights) like we did with frequentist estimation, we incorporate a range of reasonable values of <span class="math inline">\(\nu\)</span> from the posterior distribution of the treatment/design model into the outcome model, which essentially lets us get rid of the <span class="math inline">\(\nu\)</span> term. Mathematically the approach looks like this:</p>
<p><span class="math display">\[
\overbrace{f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y})}^{\substack{\text{Estimand for} \\ \text{the ATE, without } \nu}} = \int_\nu \overbrace{f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y}, \nu)}^{\substack{\text{Outcome model} \\ \text{with } \nu}}\ \overbrace{f(\nu \mid \boldsymbol{T}, \boldsymbol{X})}^{\substack{\text{Treatment model} \\ \text{creating propensity} \\ \text{scores with } T \text{ and } X}}\ \mathrm{d} \nu
\]</span></p>
<p>The <span class="math inline">\(f(\nu \mid \boldsymbol{T}, \boldsymbol{X})\)</span> part of the equation is the treatment/design model and it doesn’t have any information about the outcome <span class="math inline">\(Y\)</span> in it. In our running example, this is <code>net ~ confounders</code>. The <span class="math inline">\(f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y}, \nu)\)</span> part of the equation is the outcome model and it incorporates <span class="math inline">\(\nu\)</span>. This <em>isn’t</em> the final estimand, which doesn’t have the propensity scores (<span class="math inline">\(\nu\)</span>) in it. We ultimately get rid of that <span class="math inline">\(\nu\)</span> term by marginalizing over the distribution for <span class="math inline">\(\Delta\)</span>.</p>
<p>To simplify this process more, here’s the basic process for doing this:</p>
<ol type="1">
<li>Use a Bayesian model to estimate the likelihood of treatment and generate propensity scores (<span class="math inline">\(\nu\)</span>). This is the treatment model (or design model) and is analogous to the logistic regression model <code>model_treatment_freq</code> that we ran earlier.</li>
<li>Generate <span class="math inline">\(K\)</span> samples of propensity scores based on the posterior distribution of propensity scores. This can be whatever number you want—often it’s the number of posterior chains from the Bayesian model (like 2,000 or however many iterations you use).</li>
<li>For each of the <span class="math inline">\(K\)</span> samples, generate inverse probability weights and run an outcome model using those weights. This essentially means that we’ll be running the outcome model <code>malaria_risk ~ net</code> a bunch of times based on different weights each time.</li>
<li>Combine the results from the outcome model to create the final <span class="math inline">\(\nu\)</span>-free ATE.</li>
</ol>
<p>This process is similar to <a href="https://www.andrewheiss.com/blog/2018/03/07/amelia-tidy-melding/">multiple imputation</a> or bootstrapping: run the same model a bunch of times on slightly different data and combine the results.</p>
<p>Here’s what this looks like in practice. First we’ll predict net usage based on the confounders of income, temperature, and health using <strong>brms</strong> (with default priors and settings):</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/html/model-treatment-bayes_80e543c5cad8e56fc28761ad20e9e0d7">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">model_treatment</span> <span class="op">&lt;-</span> <span class="fu">brm</span><span class="op">(</span></span>
<span>  <span class="fu">bf</span><span class="op">(</span><span class="va">net</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">temperature</span> <span class="op">+</span> <span class="va">health</span>,</span>
<span>     decomp <span class="op">=</span> <span class="st">"QR"</span><span class="op">)</span>,  <span class="co"># QR decomposition handles scaling and unscaling for us</span></span>
<span>  family <span class="op">=</span> <span class="fu">bernoulli</span><span class="op">(</span><span class="op">)</span>,  <span class="co"># Logistic regression</span></span>
<span>  data <span class="op">=</span> <span class="va">nets</span>,</span>
<span>  chains <span class="op">=</span> <span class="fl">4</span>, cores <span class="op">=</span> <span class="fl">4</span>, iter <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">1234</span>, backend <span class="op">=</span> <span class="st">"cmdstanr"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">## Start sampling</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll then use <code>posterior_epred()</code> to plug the original data into each of the posterior draws to calculate propensity scores for each draw. This will give us a lot of propensity scores: 2,000 probabilities for each of the 1,752 people in the data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Extract posterior predicted propensity scores</span></span>
<span><span class="va">pred_probs_chains</span> <span class="op">&lt;-</span> <span class="fu">posterior_epred</span><span class="op">(</span><span class="va">model_treatment</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">pred_probs_chains</span><span class="op">)</span></span>
<span><span class="co">## [1] 2000 1752</span></span>
<span></span>
<span><span class="co"># Rows are chains; columns are individuals</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">pred_probs_chains</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]</span></span>
<span><span class="co">## [1,] 0.382 0.383 0.157 0.282 0.342 0.436 0.470 0.406 0.399 0.378</span></span>
<span><span class="co">## [2,] 0.357 0.395 0.140 0.242 0.286 0.437 0.497 0.452 0.421 0.344</span></span>
<span><span class="co">## [3,] 0.342 0.369 0.117 0.203 0.236 0.394 0.470 0.443 0.396 0.303</span></span>
<span><span class="co">## [4,] 0.339 0.352 0.124 0.206 0.237 0.372 0.439 0.415 0.373 0.298</span></span>
<span><span class="co">## [5,] 0.338 0.359 0.120 0.210 0.246 0.388 0.454 0.420 0.382 0.306</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each column here is a person in the dataset; each row is a draw from the posterior distribution. Note how there’s a lot of uncertainty in these propensity scores—we want to incorporate this uncertainty into our outcome model somehow. But as we’ve seen, there’s no weight parameter in our model, so there’s no way to directly add this uncertainty to the outcome model. Instead, we’ll run the outcome model a bunch of times, or <span class="math inline">\(K\)</span> times. For now we’ll set <span class="math inline">\(K\)</span> to 2,000—we’ll create an outcome model for each of the posterior draws that we have.</p>
<p>Doing this will require a little bit of <strong>purrr</strong> magic. We’ll make a dataset with 2,000 (<span class="math inline">\(K\)</span>) rows and put the propensity scores for all 1,752 people into their own cell to make it easier to keep track of these scores.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Put each set of individual propensity scores into its own cell</span></span>
<span><span class="va">pred_probs_nested</span> <span class="op">&lt;-</span> <span class="va">pred_probs_chains</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="co"># Convert this matrix to a data frame</span></span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span>.name_repair <span class="op">=</span> <span class="st">"unique"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="co"># Add a column for the draw number</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>draw <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">n</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="co"># Make this long so that each draw gets its own row</span></span>
<span>  <span class="fu">pivot_longer</span><span class="op">(</span><span class="op">-</span><span class="va">draw</span>, names_to <span class="op">=</span> <span class="st">"row"</span>, values_to <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="co"># Clean up the draw number </span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>row <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fu">str_remove</span><span class="op">(</span><span class="va">row</span>, <span class="st">"..."</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="co"># Group by draw and nest all the scores in a cell</span></span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">draw</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">nest</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">pred_probs_nested</span></span>
<span><span class="co">## # A tibble: 2,000 × 2</span></span>
<span><span class="co">##     draw data                </span></span>
<span><span class="co">##    &lt;int&gt; &lt;list&gt;              </span></span>
<span><span class="co">##  1     1 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">##  2     2 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">##  3     3 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">##  4     4 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">##  5     5 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">##  6     6 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">##  7     7 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">##  8     8 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">##  9     9 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">## 10    10 &lt;tibble [1,752 × 2]&gt;</span></span>
<span><span class="co">## # ℹ 1,990 more rows</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll then run an outcome model using each of these nested propensity scores. We’ll take the scores, calculate weights, and run a basic frequentist <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> model with those weights. We’ll then use <code>tidy()</code> from <strong>broom</strong> to extract different parts of the results.</p>
<div class="cell" data-layout-align="center" data-hash="index_cache/html/run-lots-of-outcome-models_2967a0d817a1fd33cdc2310929e1b1c3">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">outcome_models</span> <span class="op">&lt;-</span> <span class="va">pred_probs_nested</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>outcome_model <span class="op">=</span> <span class="fu">map</span><span class="op">(</span><span class="va">data</span>, <span class="op">~</span><span class="op">{</span></span>
<span>    <span class="co"># Add this version of propensity scores to the original data and calculate</span></span>
<span>    <span class="co"># weights. We could also do this prior to nesting everything.</span></span>
<span>    <span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">bind_cols</span><span class="op">(</span><span class="va">nets</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>      <span class="fu">mutate</span><span class="op">(</span>iptw <span class="op">=</span> <span class="op">(</span><span class="va">net_num</span> <span class="op">/</span> <span class="va">prob</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">net_num</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">prob</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Create outcome model with this iteration of weights</span></span>
<span>    <span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">malaria_risk</span> <span class="op">~</span> <span class="va">net</span>, data <span class="op">=</span> <span class="va">df</span>, weights <span class="op">=</span> <span class="va">iptw</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="co"># Extract results</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>tidied <span class="op">=</span> <span class="fu">map</span><span class="op">(</span><span class="va">outcome_model</span>, <span class="op">~</span><span class="fu">tidy</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span>,</span>
<span>         ate <span class="op">=</span> <span class="fu">map_dbl</span><span class="op">(</span><span class="va">tidied</span>, <span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">.</span>, <span class="va">term</span> <span class="op">==</span> <span class="st">"netTRUE"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">estimate</span><span class="op">)</span><span class="op">)</span>,</span>
<span>         ate_se <span class="op">=</span> <span class="fu">map_dbl</span><span class="op">(</span><span class="va">tidied</span>, <span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">.</span>, <span class="va">term</span> <span class="op">==</span> <span class="st">"netTRUE"</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="va">std.error</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">outcome_models</span></span>
<span><span class="co">## # A tibble: 2,000 × 6</span></span>
<span><span class="co">##     draw data                 outcome_model tidied              ate ate_se</span></span>
<span><span class="co">##    &lt;int&gt; &lt;list&gt;               &lt;list&gt;        &lt;list&gt;            &lt;dbl&gt;  &lt;dbl&gt;</span></span>
<span><span class="co">##  1     1 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.71  0.662</span></span>
<span><span class="co">##  2     2 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.37  0.665</span></span>
<span><span class="co">##  3     3 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.15  0.670</span></span>
<span><span class="co">##  4     4 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.89  0.665</span></span>
<span><span class="co">##  5     5 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.40  0.669</span></span>
<span><span class="co">##  6     6 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt; -10.8   0.654</span></span>
<span><span class="co">##  7     7 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.03  0.669</span></span>
<span><span class="co">##  8     8 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -8.58  0.673</span></span>
<span><span class="co">##  9     9 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.67  0.664</span></span>
<span><span class="co">## 10    10 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt; -11.0   0.651</span></span>
<span><span class="co">## # … with 1,990 more rows</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>ate</code> column here is the coefficient for <code>net</code>, and we have 2,000 of them to work with now, each based on a different set of weights. If we average all these estimates, we can get one final estimate of the ATE that successfully incorporates the uncertainty from the treatment/design model. We can’t take the direct average of the standard errors, but we can combine them using Rubin’s rules. <span class="citation" data-cites="Rubin:1987">Rubin (<a href="#ref-Rubin:1987" role="doc-biblioref">1987</a>)</span> outlines an set of rules for combining the results from multiply imputed datasets that reflects the averages and accounts for differences in standard errors (they’re essentially a fancier, more robust way of averaging standard errors across multiple models). We didn’t use multiply imputed datasets here, but the same principle applies—we used the same model on lots of slightly different datasets.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Combined average treatment effect</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">outcome_models</span><span class="op">$</span><span class="va">ate</span><span class="op">)</span></span>
<span><span class="co">## [1] -10.1</span></span>
<span></span>
<span><span class="co"># Combined standard errors (this is wrong)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">outcome_models</span><span class="op">$</span><span class="va">ate_se</span><span class="op">)</span></span>
<span><span class="co">## [1] 0.659</span></span>
<span></span>
<span><span class="co"># Combined standard errors with Rubin's rules (this is correct)</span></span>
<span><span class="va">rubin_se</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">ates</span>, <span class="va">sigmas</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">sigmas</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">ates</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu">rubin_se</span><span class="op">(</span><span class="va">outcome_models</span><span class="op">$</span><span class="va">ate</span>, <span class="va">outcome_models</span><span class="op">$</span><span class="va">ate_se</span><span class="op">)</span></span>
<span><span class="co">## [1] 1.02</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The overall average treatment effect is still −10ish, like we found earlier, but now we have a bunch of extra uncertainty from the treatment model, which is neat. We can also visualize the distribution of the ATE, almost like a Bayesian posterior:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">ggplot</span><span class="op">(</span><span class="va">outcome_models</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">ate</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_density</span><span class="op">(</span>fill <span class="op">=</span> <span class="va">isfahan</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span>, color <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Average treatment effect of using a mosquito net"</span>, y <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="index_files/figure-html/visualize-ates-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%"></p>
</figure>
</div>
</div>
</div>
<p>We did it!</p>
</section></section><section id="things-i-dont-know-yet-but-want-to-know" class="level2"><h2 class="anchored" data-anchor-id="things-i-dont-know-yet-but-want-to-know">Things I don’t know yet but want to know!</h2>
<p>So, thanks to <span class="citation" data-cites="LiaoZigler:2020">Liao and Zigler (<a href="#ref-LiaoZigler:2020" role="doc-biblioref">2020</a>)</span>, we have a fully Bayesian treatment/design model and we can incorporate the uncertainty from that model’s propensity scores and weights into the outcome/analysis model. That’s so neat!</p>
<p>However, in their paper Liao and Zigler use a frequentist outcome model, like we just did here. While combining the ATEs from 2,000 different frequentist OLS-based models feels quasi-Bayesian, I don’t know if we can legally talk about these results Bayesianly. Can we pretend that this distribution of ATEs is similar to a posterior distribution and use Bayesian inference rather than null hypothesis significance testing, or do we still need to talk about null hypotheses? These 2,000 models are essentially a mathematical transformation of the posterior, so maybe it’s legal? But the Bayesian model is for predicting treatment, not the outcome, so maybe it’s not legal? idk. (Turns out <a href="https://twitter.com/adamjnafa/status/1472269242334687236">the answer is no</a>—there’s no uncertainty in the outcome model here, since all the uncertainty comes from the treatment model. Ah! Well. Nevertheless.)</p>
<p>We could technically run a Bayesian outcome model with <code>brm()</code>, but we’d have to run it 2,000 times—one model per set of weights—and that would take literally forever and might melt my computer. There could be a way to only run a single outcome model once and use one set of weights for each of the iterations (i.e.&nbsp;use the first column of propensity scores for the first iteration of the outcome model, the second for the second, and so on), but that goes beyond my skills with <strong>brms</strong>. (<strong>UPDATE</strong>: <a href="https://twitter.com/adamjnafa">Thanks to Jordan Nafa</a>, this is actually possible! <a href="https://www.andrewheiss.com/blog/2021/12/20/fully-bayesian-ate-iptw/">See here!</a>)</p>
</section><section id="references" class="level2">


<!-- -->


</section><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-LiaoZigler:2020" class="csl-entry" role="listitem">
Liao, Shirley X., and Corwin M. Zigler. 2020. <span>“Uncertainty in the Design Stage of Two-Stage Bayesian Propensity Score Analysis.”</span> <em>Statistics in Medicine</em> 39 (17): 2265–90. <a href="https://doi.org/10.1002/sim.8486">https://doi.org/10.1002/sim.8486</a>.
</div>
<div id="ref-RobinsHernanWasserman:2015" class="csl-entry" role="listitem">
Robins, James M., Miguel A. Hernán, and Larry Wasserman. 2015. <span>“On <span>Bayesian</span> Estimation of Marginal Structural Models.”</span> <em>Biometrics</em> 71 (2): 296–99. <a href="https://doi.org/10.1111/biom.12273">https://doi.org/10.1111/biom.12273</a>.
</div>
<div id="ref-Rubin:1987" class="csl-entry" role="listitem">
Rubin, Donald B. 1987. <em>Multiple Imputation for Nonresponse in Surveys</em>. New York: John Wiley &amp; Sons. <a href="https://doi.org/10.1002/9780470316696">https://doi.org/10.1002/9780470316696</a>.
</div>
<div id="ref-SaarelaStephensMoodie:2015" class="csl-entry" role="listitem">
Saarela, Olli, David A. Stephens, Erica E. M. Moodie, and Marina B. Klein. 2015. <span>“On Bayesian Estimation of Marginal Structural Models.”</span> <em>Biometrics</em> 71 (2): 279–88. <a href="https://doi.org/10.1111/biom.12269">https://doi.org/10.1111/biom.12269</a>.
</div>
<div id="ref-Zigler:2016" class="csl-entry" role="listitem">
Zigler, Corwin Matthew. 2016. <span>“The Central Role of <span>Bayes’</span> Theorem for Joint Estimation of Causal Effects and Propensity Scores.”</span> <em>The American Statistician</em> 70 (1): 47–54. <a href="https://doi.org/10.1080/00031305.2015.1111260">https://doi.org/10.1080/00031305.2015.1111260</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{heiss2021,
  author = {Heiss, Andrew},
  title = {How to Use {Bayesian} Propensity Scores and Inverse
    Probability Weights},
  date = {2021-12-18},
  url = {https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/},
  doi = {10.59350/nrwsd-3jz20},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-heiss2021" class="csl-entry quarto-appendix-citeas" role="listitem">
Heiss, Andrew. 2021. <span>“How to Use Bayesian Propensity Scores and
Inverse Probability Weights.”</span> December 18, 2021. <a href="https://doi.org/10.59350/nrwsd-3jz20">https://doi.org/10.59350/nrwsd-3jz20</a>.
</div></div></section></div></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.andrewheiss\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><script src="https://giscus.app/client.js" data-repo="andrewheiss/ath-quarto" data-repo-id="R_kgDOIg6EJQ" data-category="Blog comments" data-category-id="DIC_kwDOIg6EJc4CSz92" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><input type="hidden" id="giscus-base-theme" value="light"><input type="hidden" id="giscus-alt-theme" value="dark"><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb15" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "How to use Bayesian propensity scores and inverse probability weights"</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2021-12-18</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "For mathematical and philosophical reasons, propensity scores and inverse probability weights don't work in Bayesian inference. But never fear! There's still a way to do it!"</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> index_files/figure-html/plot-iptw-pseudo-populations-1.png</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - r</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - tidyverse</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - regression</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - statistics</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - data visualization</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">  - causal inference</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">  - do calculus</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">  - DAGs</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co">  - bayes</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co">  - brms</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co">  - stan</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="an">doi:</span><span class="co"> 10.59350/nrwsd-3jz20</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span><span class="co"> true</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="in">```{r setup, include=FALSE}</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="in">knitr::opts_chunk$set(fig.align = "center", fig.retina = 3,</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="in">                      fig.width = 6, fig.height = (6 * 0.618),</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="in">                      out.width = "90%", collapse = TRUE)</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a><span class="in">options(digits = 3, width = 90)</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>This post combines two of my long-standing interests: causal inference and Bayesian statistics. I've been teaching <span class="co">[</span><span class="ot">a course on program evaluation and causal inference</span><span class="co">](https://evalf21.classes.andrewheiss.com/)</span> for a couple years now and it has become one of my favorite classes ever. It has reshaped how I do my research, and I've been trying to carefully incorporate causal approaches in my different project—as evidenced by an ever-growing series of blog posts here about different issues I run into and figure out (like <span class="co">[</span><span class="ot">this</span><span class="co">](https://www.andrewheiss.com/blog/2020/12/01/ipw-binary-continuous/)</span> and <span class="co">[</span><span class="ot">this</span><span class="co">](https://www.andrewheiss.com/blog/2020/12/03/ipw-tscs-msm/)</span> and <span class="co">[</span><span class="ot">this</span><span class="co">](https://www.andrewheiss.com/blog/2021/01/15/msm-gee-multilevel/)</span> and <span class="co">[</span><span class="ot">this</span><span class="co">](https://www.andrewheiss.com/blog/2021/09/07/do-calculus-backdoors/)</span>.)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>Additionally, ever since stumbling on <span class="co">[</span><span class="ot">this blog post</span><span class="co">](https://thinkinator.com/2016/01/12/r-users-will-now-inevitably-become-bayesians/)</span> as a PhD student back in 2016 following the invention of **rstanarm** and **brms**, both of which make it easy to use Stan with R, I've been as Bayesian as possible in all my research. I find Bayesian approaches to inference *way* more intuitive than frequentist null hypothesis significance testing. My work with Bayesian approaches has also led to a bunch of blog posts here (like <span class="co">[</span><span class="ot">this</span><span class="co">](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/)</span> and <span class="co">[</span><span class="ot">this</span><span class="co">](https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/)</span> and <span class="co">[</span><span class="ot">this</span><span class="co">](https://www.andrewheiss.com/blog/2021/12/01/multilevel-models-panel-data-guide/)</span>).</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>However, the combination of these two interests is a little fraught. In one of my projects, I'm using marginal structural models and inverse probability weights to account for confounding and make causal claims. I also want to do this Bayesianly so that I can work with posterior distributions and use Bayesian inference rather than null hypotheses and significance. But using inverse probability weights *and* Bayesian methods simultaneously seems to be impossible! @RobinsHernanWasserman:2015 even have an article where they explicitly say that "Bayesian inference must ignore the propensity score," effectively making it impossible to use things like inverse probability weights Bayesianly. Oh no!</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>I recently came across a new article that gives me hope though! There's a group of epidemiologists and biostatisticians who have been working on finding ways to use a Bayesian approach to propensity scores and weights (@SaarelaStephensMoodie:2015; @Zigler:2016; @LiaoZigler:2020, among others), and @LiaoZigler:2020 provide a useful (and understandable!) approach for doing it. This post is my attempt at translating Liao and Zigler's paper from conceptual math and <span class="co">[</span><span class="ot">**MCMCPack**-based R code</span><span class="co">](https://github.com/shirleyxliao/Uncertainty-in-the-Design-Stage-of-Two-Stage-Bayesian-Propensity-Score-Analysis)</span> into **tidyverse**, **brms**, and Stan-based code. Here we go!</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Who this post is for</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>Here's what I assume you know:</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You're familiar with <span class="co">[</span><span class="ot">R</span><span class="co">](https://www.r-project.org/)</span> and the <span class="co">[</span><span class="ot">tidyverse</span><span class="co">](https://www.tidyverse.org/)</span> (particularly <span class="co">[</span><span class="ot">dplyr</span><span class="co">](https://dplyr.tidyverse.org/)</span> and <span class="co">[</span><span class="ot">ggplot2</span><span class="co">](https://ggplot2.tidyverse.org/)</span>).</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You're familiar with <span class="co">[</span><span class="ot">brms</span><span class="co">](https://paul-buerkner.github.io/brms/)</span> for running Bayesian regression models. See <span class="co">[</span><span class="ot">the vignettes here</span><span class="co">](https://paul-buerkner.github.io/brms/articles/index.html)</span>, examples like <span class="co">[</span><span class="ot">this</span><span class="co">](https://www.rensvandeschoot.com/tutorials/brms-started/)</span>, or <span class="co">[</span><span class="ot">resources like these</span><span class="co">](https://evalf21.classes.andrewheiss.com/resource/bayes/#resources)</span> for an introduction.</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You know a little about DAGs and causal model-based approaches to causal inference, and you've heard about statistical adjustment to isolate causal effects (i.e. "closing backdoors in a DAG")</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a><span class="fu">## General approach to inverse probability weighting</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>I won't go into the details of how inverse probability weighting works here. For more details, check out <span class="co">[</span><span class="ot">this fully worked out example</span><span class="co">](https://evalf21.classes.andrewheiss.com/example/matching-ipw/)</span> or <span class="co">[</span><span class="ot">this chapter</span><span class="co">](https://evalf21.classes.andrewheiss.com/files/10-causal-inference.pdf)</span>, which has references to lots of other more detailed resources. Instead, I'll provide a super short abbreviated overview of how inverse probability weights are used for causal inference and why and how we can use them.</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>When trying to make causal inferences with observational data, there is inevitably confounding—people self-select into (or out of) treatment conditions because of a host of external factors. We can adjust for these confounding factors with lots of different methods. Quasi-experimental approaches like difference-in-differences, regression discontinuity, and instrumental variables let us use specific (and often weird) situations to adjust for confounding and make comparable treatment and control groups. Alternatively, we can use model-based inference using DAGs and *do*-calculus, identifying which variables open up backdoor pathways between treatment and outcome, and statistically adjusting for those variables to isolate the treatment → outcome pathway.</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>One way to adjust for confounders is to use inverse probability weighting. In short, here's how it works for a binary (0/1) treatment:</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create a model that predicts treatment (often called a *treatment model* or *design stage*). Use confounders (identified with a DAG) as the covariates. Use whatever modeling approach you want here—logistic regression, random forests, fancy machine learning things, etc.</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Use the results of the treatment model to calculate propensity scores.</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Convert those propensity scores into *inverse probability of treatment weights* (IPTW) using this formula:</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>    \frac{\text{Treatment}}{\text{Propensity}} + \frac{1 - \text{Treatment}}{1 - \text{Propensity}}</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Create a model that estimates the effect of treatment on outcome, weighted by the IPTWs (often called an *outcome model* or *analysis stage*). The coefficient for the treatment variable is the average treatment effect (ATE).</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a><span class="fu">### Basic frequentist example</span></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>Throughout this post, we'll use some simulated data for a fake hypothetical social program that distributes mosquito nets in order to reduce malaria risk. I created this data for my course on <span class="co">[</span><span class="ot">program evaluation and causal inference</span><span class="co">](https://evalf21.classes.andrewheiss.com/)</span> and use it for <span class="co">[</span><span class="ot">teaching adjustment with inverse probability weighting</span><span class="co">](https://evalf21.classes.andrewheiss.com/example/matching-ipw/)</span>. </span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a><span class="in">```{r libraries-functions, warning=FALSE, message=FALSE}</span></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a><span class="in">library(tidyverse)</span></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a><span class="in">library(brms)</span></span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a><span class="in">library(broom)</span></span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a><span class="in">library(broom.mixed)</span></span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a><span class="in">library(ggdag)</span></span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a><span class="in">library(MetBrewer)</span></span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(3273)  # From random.org</span></span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a><span class="in"># Use the delightful Isfahan1 palette from the MetBrewer package</span></span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a><span class="in">isfahan &lt;- MetBrewer::met.brewer("Isfahan1")</span></span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a><span class="in"># Custom ggplot theme to make pretty plots</span></span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a><span class="in"># Get Archivo Narrow at https://fonts.google.com/specimen/Archivo+Narrow</span></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a><span class="in">theme_nice &lt;- function() {</span></span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_minimal(base_family = "Archivo Narrow") +</span></span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a><span class="in">    theme(panel.grid.minor = element_blank(),</span></span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a><span class="in">          plot.background = element_rect(fill = "white", color = NA),</span></span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a><span class="in">          plot.title = element_text(face = "bold"),</span></span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a><span class="in">          axis.title = element_text(face = "bold"),</span></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a><span class="in">          strip.text = element_text(face = "bold", size = rel(0.8), hjust = 0),</span></span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a><span class="in">          strip.background = element_rect(fill = "grey80", color = NA),</span></span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a><span class="in">          legend.title = element_text(face = "bold"))</span></span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a><span class="in"># Use this theme on all plots</span></span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true" tabindex="-1"></a><span class="in">theme_set(</span></span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_nice()</span></span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a><span class="in"># Make all labels use Archivo by default</span></span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true" tabindex="-1"></a><span class="in">update_geom_defaults("label", </span></span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true" tabindex="-1"></a><span class="in">                     list(family = "Archivo Narrow",</span></span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true" tabindex="-1"></a><span class="in">                          fontface = "bold"))</span></span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-108"><a href="#cb15-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-109"><a href="#cb15-109" aria-hidden="true" tabindex="-1"></a><span class="in">```{r load-data-fake, eval=FALSE}</span></span>
<span id="cb15-110"><a href="#cb15-110" aria-hidden="true" tabindex="-1"></a><span class="in">nets &lt;- read_csv("https://evalf21.classes.andrewheiss.com/data/mosquito_nets.csv")</span></span>
<span id="cb15-111"><a href="#cb15-111" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true" tabindex="-1"></a><span class="in">```{r load-data-real, include=FALSE, warning=FALSE, message=FALSE}</span></span>
<span id="cb15-114"><a href="#cb15-114" aria-hidden="true" tabindex="-1"></a><span class="in">nets &lt;- read_csv("mosquito_nets.csv")</span></span>
<span id="cb15-115"><a href="#cb15-115" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-116"><a href="#cb15-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-117"><a href="#cb15-117" aria-hidden="true" tabindex="-1"></a>I normally use a more complicated DAG with other nodes that don't need to be adjusted for, but for the sake of simplicity here, this DAG only includes the confounders. The relationship between net usage (measured as a 0/1 binary variable where 1 = person used a net) and malaria risk (measured on a scale of 0-100, with higher values representing higher risk) is confounded by monthly income (in USD), health (measured on a scale of 0-100, with higher values representing better health), and nighttime temperatures at the person's home (measured in Celsius).</span>
<span id="cb15-118"><a href="#cb15-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-119"><a href="#cb15-119" aria-hidden="true" tabindex="-1"></a><span class="in">```{r basic-dag}</span></span>
<span id="cb15-120"><a href="#cb15-120" aria-hidden="true" tabindex="-1"></a><span class="in">mosquito_dag &lt;- dagify(</span></span>
<span id="cb15-121"><a href="#cb15-121" aria-hidden="true" tabindex="-1"></a><span class="in">  malaria_risk ~ net + income + health + temperature,</span></span>
<span id="cb15-122"><a href="#cb15-122" aria-hidden="true" tabindex="-1"></a><span class="in">  net ~ income + health + temperature,</span></span>
<span id="cb15-123"><a href="#cb15-123" aria-hidden="true" tabindex="-1"></a><span class="in">  health ~ income,</span></span>
<span id="cb15-124"><a href="#cb15-124" aria-hidden="true" tabindex="-1"></a><span class="in">  exposure = "net",</span></span>
<span id="cb15-125"><a href="#cb15-125" aria-hidden="true" tabindex="-1"></a><span class="in">  outcome = "malaria_risk",</span></span>
<span id="cb15-126"><a href="#cb15-126" aria-hidden="true" tabindex="-1"></a><span class="in">  coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6),</span></span>
<span id="cb15-127"><a href="#cb15-127" aria-hidden="true" tabindex="-1"></a><span class="in">                y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3)),</span></span>
<span id="cb15-128"><a href="#cb15-128" aria-hidden="true" tabindex="-1"></a><span class="in">  labels = c(malaria_risk = "Risk of malaria", net = "Mosquito net", income = "Income",</span></span>
<span id="cb15-129"><a href="#cb15-129" aria-hidden="true" tabindex="-1"></a><span class="in">             health = "Health", temperature = "Nighttime temperatures",</span></span>
<span id="cb15-130"><a href="#cb15-130" aria-hidden="true" tabindex="-1"></a><span class="in">             resistance = "Insecticide resistance")</span></span>
<span id="cb15-131"><a href="#cb15-131" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb15-132"><a href="#cb15-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-133"><a href="#cb15-133" aria-hidden="true" tabindex="-1"></a><span class="in"># Turn DAG into a tidy data frame for plotting</span></span>
<span id="cb15-134"><a href="#cb15-134" aria-hidden="true" tabindex="-1"></a><span class="in">mosquito_dag_tidy &lt;- mosquito_dag %&gt;% </span></span>
<span id="cb15-135"><a href="#cb15-135" aria-hidden="true" tabindex="-1"></a><span class="in">  tidy_dagitty() %&gt;%</span></span>
<span id="cb15-136"><a href="#cb15-136" aria-hidden="true" tabindex="-1"></a><span class="in">  node_status()   # Add column for exposure/outcome/latent</span></span>
<span id="cb15-137"><a href="#cb15-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-138"><a href="#cb15-138" aria-hidden="true" tabindex="-1"></a><span class="in">status_colors &lt;- c(exposure = isfahan[2], outcome = isfahan[7], latent = "grey50")</span></span>
<span id="cb15-139"><a href="#cb15-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-140"><a href="#cb15-140" aria-hidden="true" tabindex="-1"></a><span class="in"># Fancier graph</span></span>
<span id="cb15-141"><a href="#cb15-141" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(mosquito_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +</span></span>
<span id="cb15-142"><a href="#cb15-142" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_dag_edges() +</span></span>
<span id="cb15-143"><a href="#cb15-143" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_dag_point(aes(color = status)) +</span></span>
<span id="cb15-144"><a href="#cb15-144" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_label(aes(label = label, fill = status),</span></span>
<span id="cb15-145"><a href="#cb15-145" aria-hidden="true" tabindex="-1"></a><span class="in">             color = "white", fontface = "bold", nudge_y = -0.3) +</span></span>
<span id="cb15-146"><a href="#cb15-146" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_color_manual(values = status_colors, na.value = "grey20") +</span></span>
<span id="cb15-147"><a href="#cb15-147" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_fill_manual(values = status_colors, na.value = "grey20") +</span></span>
<span id="cb15-148"><a href="#cb15-148" aria-hidden="true" tabindex="-1"></a><span class="in">  guides(color = "none", fill = "none") +</span></span>
<span id="cb15-149"><a href="#cb15-149" aria-hidden="true" tabindex="-1"></a><span class="in">  theme_dag()</span></span>
<span id="cb15-150"><a href="#cb15-150" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-151"><a href="#cb15-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-152"><a href="#cb15-152" aria-hidden="true" tabindex="-1"></a>Following the logic of *do*-calculus, we need to adjust for three of these nodes in order to isolate the pathway between net usage and malaria risk:</span>
<span id="cb15-153"><a href="#cb15-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-154"><a href="#cb15-154" aria-hidden="true" tabindex="-1"></a><span class="in">```{r show-adjustment-sets}</span></span>
<span id="cb15-155"><a href="#cb15-155" aria-hidden="true" tabindex="-1"></a><span class="in">library(dagitty)</span></span>
<span id="cb15-156"><a href="#cb15-156" aria-hidden="true" tabindex="-1"></a><span class="in">adjustmentSets(mosquito_dag)</span></span>
<span id="cb15-157"><a href="#cb15-157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-158"><a href="#cb15-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-159"><a href="#cb15-159" aria-hidden="true" tabindex="-1"></a>We can adjust for these variables using inverse probability weighting. We'll first make a treatment model (or "design stage" in the world of biostats) that uses these confounders to predict net use, then we'll create propensity scores and inverse probability treatment weights, and then we'll use those weights in an outcome model (or "analysis stage" in the world of biostats) to calculate the average treatment effect (ATE) of net usage. </span>
<span id="cb15-160"><a href="#cb15-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-161"><a href="#cb15-161" aria-hidden="true" tabindex="-1"></a>I built in a 10 point decrease in malaria risk due to nets (hooray for fake data!), so let's see if we can recover that treatment effect:</span>
<span id="cb15-162"><a href="#cb15-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-163"><a href="#cb15-163" aria-hidden="true" tabindex="-1"></a><span class="in">```{r freq-ipw}</span></span>
<span id="cb15-164"><a href="#cb15-164" aria-hidden="true" tabindex="-1"></a><span class="in"># Step 1: Create model that predicts treatment status using confounders</span></span>
<span id="cb15-165"><a href="#cb15-165" aria-hidden="true" tabindex="-1"></a><span class="in">model_treatment_freq &lt;- glm(net ~ income + temperature + health,</span></span>
<span id="cb15-166"><a href="#cb15-166" aria-hidden="true" tabindex="-1"></a><span class="in">                            data = nets,</span></span>
<span id="cb15-167"><a href="#cb15-167" aria-hidden="true" tabindex="-1"></a><span class="in">                            family = binomial(link = "logit"))</span></span>
<span id="cb15-168"><a href="#cb15-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-169"><a href="#cb15-169" aria-hidden="true" tabindex="-1"></a><span class="in"># Step 2: Use the treatment model to calculate propensity scores, and</span></span>
<span id="cb15-170"><a href="#cb15-170" aria-hidden="true" tabindex="-1"></a><span class="in"># Step 3: Use the propensity scores to calculate inverse probability of treatment weights</span></span>
<span id="cb15-171"><a href="#cb15-171" aria-hidden="true" tabindex="-1"></a><span class="in">nets_with_weights &lt;- augment(model_treatment_freq, nets,</span></span>
<span id="cb15-172"><a href="#cb15-172" aria-hidden="true" tabindex="-1"></a><span class="in">                             type.predict = "response") %&gt;%</span></span>
<span id="cb15-173"><a href="#cb15-173" aria-hidden="true" tabindex="-1"></a><span class="in">  rename(propensity = .fitted) %&gt;% </span></span>
<span id="cb15-174"><a href="#cb15-174" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(iptw = (net_num / propensity) + ((1 - net_num) / (1 - propensity)))</span></span>
<span id="cb15-175"><a href="#cb15-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-176"><a href="#cb15-176" aria-hidden="true" tabindex="-1"></a><span class="in"># Step 4: Use the IPTWs in a model that estimates the effect of treatment on outcome</span></span>
<span id="cb15-177"><a href="#cb15-177" aria-hidden="true" tabindex="-1"></a><span class="in">model_outcome_freq &lt;- lm(malaria_risk ~ net,</span></span>
<span id="cb15-178"><a href="#cb15-178" aria-hidden="true" tabindex="-1"></a><span class="in">                         data = nets_with_weights,</span></span>
<span id="cb15-179"><a href="#cb15-179" aria-hidden="true" tabindex="-1"></a><span class="in">                         weights = iptw)</span></span>
<span id="cb15-180"><a href="#cb15-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-181"><a href="#cb15-181" aria-hidden="true" tabindex="-1"></a><span class="in"># Coefficient for `net` should be -10ish</span></span>
<span id="cb15-182"><a href="#cb15-182" aria-hidden="true" tabindex="-1"></a><span class="in">tidy(model_outcome_freq)</span></span>
<span id="cb15-183"><a href="#cb15-183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-184"><a href="#cb15-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-185"><a href="#cb15-185" aria-hidden="true" tabindex="-1"></a>It worked! After going through this two-step process of (1) creating propensity scores and weights, and (2) using those weights to estimate the actual effect, we successfully closed the backdoor pathways that confounded the relationship between net use and malaria risk, ending up with an unbiased ATE. Neato.</span>
<span id="cb15-186"><a href="#cb15-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-187"><a href="#cb15-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-188"><a href="#cb15-188" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pseudo-populations</span></span>
<span id="cb15-189"><a href="#cb15-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-190"><a href="#cb15-190" aria-hidden="true" tabindex="-1"></a>Before looking at how to do this analysis Bayesianly, it's helpful to understand what these weights are actually doing behind the scenes. The point of these IPTWs is to create pseudo-populations of treated and untreated observations that are comparable across all the different levels of confounders. They're essentially a way to let us fake treatment and control groups so that we can interpret the results of outcome models causally.</span>
<span id="cb15-191"><a href="#cb15-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-192"><a href="#cb15-192" aria-hidden="true" tabindex="-1"></a>Visualizing the propensity scores for treated and untreated people can help show what's going on. Here are the distributions of propensity scores for these two groups: the treated group is in the top half in brown; the untreated group is in the bottom half in turquoise. (Thanks to <span class="co">[</span><span class="ot">Lucy D'Agostino McGowan for this really neat way of looking at weight distributions</span><span class="co">](https://livefreeordichotomize.com/2019/01/17/understanding-propensity-score-weighting/)</span>!)</span>
<span id="cb15-193"><a href="#cb15-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-194"><a href="#cb15-194" aria-hidden="true" tabindex="-1"></a><span class="in">```{r plot-propensity-hist}</span></span>
<span id="cb15-195"><a href="#cb15-195" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot() + </span></span>
<span id="cb15-196"><a href="#cb15-196" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_histogram(data = filter(nets_with_weights, net_num == 1), </span></span>
<span id="cb15-197"><a href="#cb15-197" aria-hidden="true" tabindex="-1"></a><span class="in">                 bins = 50, aes(x = propensity), </span></span>
<span id="cb15-198"><a href="#cb15-198" aria-hidden="true" tabindex="-1"></a><span class="in">                 fill = isfahan[2]) + </span></span>
<span id="cb15-199"><a href="#cb15-199" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_histogram(data = filter(nets_with_weights, net_num == 0), </span></span>
<span id="cb15-200"><a href="#cb15-200" aria-hidden="true" tabindex="-1"></a><span class="in">                 bins = 50, aes(x = propensity, y = -after_stat(count)),</span></span>
<span id="cb15-201"><a href="#cb15-201" aria-hidden="true" tabindex="-1"></a><span class="in">                 fill = isfahan[6]) +</span></span>
<span id="cb15-202"><a href="#cb15-202" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_hline(yintercept = 0) +</span></span>
<span id="cb15-203"><a href="#cb15-203" aria-hidden="true" tabindex="-1"></a><span class="in">  annotate(geom = "label", x = 0.1, y = 20, label = "Treated", </span></span>
<span id="cb15-204"><a href="#cb15-204" aria-hidden="true" tabindex="-1"></a><span class="in">           fill = isfahan[2], color = "white", hjust = 0) +</span></span>
<span id="cb15-205"><a href="#cb15-205" aria-hidden="true" tabindex="-1"></a><span class="in">  annotate(geom = "label", x = 0.1, y = -20, label = "Untreated", </span></span>
<span id="cb15-206"><a href="#cb15-206" aria-hidden="true" tabindex="-1"></a><span class="in">           fill = isfahan[6], color = "white", hjust = 0) +</span></span>
<span id="cb15-207"><a href="#cb15-207" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_y_continuous(label = abs) +</span></span>
<span id="cb15-208"><a href="#cb15-208" aria-hidden="true" tabindex="-1"></a><span class="in">  coord_cartesian(xlim = c(0.1, 0.8), ylim = c(-80, 100)) +</span></span>
<span id="cb15-209"><a href="#cb15-209" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(x = "Propensity", y = "Count")</span></span>
<span id="cb15-210"><a href="#cb15-210" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-211"><a href="#cb15-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-212"><a href="#cb15-212" aria-hidden="true" tabindex="-1"></a>We can learn a few different things from this plot. Fewer people received the treatment than didn't—there are more people in the untreated part of the graph. We can confirm this really quick:</span>
<span id="cb15-213"><a href="#cb15-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-214"><a href="#cb15-214" aria-hidden="true" tabindex="-1"></a><span class="in">```{r show-net-count}</span></span>
<span id="cb15-215"><a href="#cb15-215" aria-hidden="true" tabindex="-1"></a><span class="in">nets %&gt;% count(net)</span></span>
<span id="cb15-216"><a href="#cb15-216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-217"><a href="#cb15-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-218"><a href="#cb15-218" aria-hidden="true" tabindex="-1"></a>Yep. There are ≈400 more net-non-users than net-users.</span>
<span id="cb15-219"><a href="#cb15-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-220"><a href="#cb15-220" aria-hidden="true" tabindex="-1"></a>We can also see that those who did not receive treatment tend to have a lower probability of doing so—the bulk of the untreated distribution is clustered in the low end of propensity scores. This makes sense! If people have a low chance of using a mosquito net, there should be fewer people ultimately using a bed net. </span>
<span id="cb15-221"><a href="#cb15-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-222"><a href="#cb15-222" aria-hidden="true" tabindex="-1"></a>But these two groups—treated and untreated—aren't exactly comparable at this point. There are confounding factors that make people who didn't use nets less likely to use them. To make causal inferences about the effect of nets, we'd need to look at a "treatment" and a "control" group with similar characteristics and with similar probabilities of using nets.</span>
<span id="cb15-223"><a href="#cb15-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-224"><a href="#cb15-224" aria-hidden="true" tabindex="-1"></a>To get around this, we can create two pseudo-populations of treated and untreated people. We can give less statistical importance (or weight) to those who had a low probability of being treated and who subsequently weren't treated (since there are a ton of those people) and more statistical weight to those who had a high probability of being treated but weren't. Similarly, we can give more weight to treated people who had a low probability of being treated (that's surprising!) and less weight to treated people who had a high probability of being treated (that's not surprising!). If we scale each person by their weight, given the confounders of income, temperature, and health, we can create comparable treated and untreated populations:</span>
<span id="cb15-225"><a href="#cb15-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-226"><a href="#cb15-226" aria-hidden="true" tabindex="-1"></a><span class="in">```{r plot-iptw-pseudo-populations}</span></span>
<span id="cb15-227"><a href="#cb15-227" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot() + </span></span>
<span id="cb15-228"><a href="#cb15-228" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_histogram(data = filter(nets_with_weights, net_num == 1), </span></span>
<span id="cb15-229"><a href="#cb15-229" aria-hidden="true" tabindex="-1"></a><span class="in">                 bins = 50, aes(x = propensity, weight = iptw), </span></span>
<span id="cb15-230"><a href="#cb15-230" aria-hidden="true" tabindex="-1"></a><span class="in">                 fill = colorspace::lighten(isfahan[2], 0.35)) + </span></span>
<span id="cb15-231"><a href="#cb15-231" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_histogram(data = filter(nets_with_weights, net_num == 0), </span></span>
<span id="cb15-232"><a href="#cb15-232" aria-hidden="true" tabindex="-1"></a><span class="in">                 bins = 50, aes(x = propensity, weight = iptw, y = -after_stat(count)),</span></span>
<span id="cb15-233"><a href="#cb15-233" aria-hidden="true" tabindex="-1"></a><span class="in">                 fill = colorspace::lighten(isfahan[6], 0.35)) +</span></span>
<span id="cb15-234"><a href="#cb15-234" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_histogram(data = filter(nets_with_weights, net_num == 1), </span></span>
<span id="cb15-235"><a href="#cb15-235" aria-hidden="true" tabindex="-1"></a><span class="in">                 bins = 50, aes(x = propensity), </span></span>
<span id="cb15-236"><a href="#cb15-236" aria-hidden="true" tabindex="-1"></a><span class="in">                 fill = isfahan[2]) + </span></span>
<span id="cb15-237"><a href="#cb15-237" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_histogram(data = filter(nets_with_weights, net_num == 0), </span></span>
<span id="cb15-238"><a href="#cb15-238" aria-hidden="true" tabindex="-1"></a><span class="in">                 bins = 50, aes(x = propensity, y = -after_stat(count)),</span></span>
<span id="cb15-239"><a href="#cb15-239" aria-hidden="true" tabindex="-1"></a><span class="in">                 fill = isfahan[6]) +</span></span>
<span id="cb15-240"><a href="#cb15-240" aria-hidden="true" tabindex="-1"></a><span class="in">  annotate(geom = "label", x = 0.8, y = 70, label = "Treated (actual)", </span></span>
<span id="cb15-241"><a href="#cb15-241" aria-hidden="true" tabindex="-1"></a><span class="in">           fill = isfahan[2], color = "white", hjust = 1) +</span></span>
<span id="cb15-242"><a href="#cb15-242" aria-hidden="true" tabindex="-1"></a><span class="in">  annotate(geom = "label", x = 0.8, y = 90, label = "Treated (IPTW pseudo-population)", </span></span>
<span id="cb15-243"><a href="#cb15-243" aria-hidden="true" tabindex="-1"></a><span class="in">           fill = colorspace::lighten(isfahan[2], 0.35), color = "white", hjust = 1) +</span></span>
<span id="cb15-244"><a href="#cb15-244" aria-hidden="true" tabindex="-1"></a><span class="in">  annotate(geom = "label", x = 0.8, y = -60, label = "Untreated (actual)", </span></span>
<span id="cb15-245"><a href="#cb15-245" aria-hidden="true" tabindex="-1"></a><span class="in">           fill = isfahan[6], color = "white", hjust = 1) +</span></span>
<span id="cb15-246"><a href="#cb15-246" aria-hidden="true" tabindex="-1"></a><span class="in">  annotate(geom = "label", x = 0.8, y = -80, label = "Untreated (IPTW pseudo-population)", </span></span>
<span id="cb15-247"><a href="#cb15-247" aria-hidden="true" tabindex="-1"></a><span class="in">           fill = colorspace::lighten(isfahan[6], 0.35), color = "white", hjust = 1) +</span></span>
<span id="cb15-248"><a href="#cb15-248" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_hline(yintercept = 0, color = "white", linewidth = 0.25) +</span></span>
<span id="cb15-249"><a href="#cb15-249" aria-hidden="true" tabindex="-1"></a><span class="in">  scale_y_continuous(label = abs) +</span></span>
<span id="cb15-250"><a href="#cb15-250" aria-hidden="true" tabindex="-1"></a><span class="in">  coord_cartesian(xlim = c(0.1, 0.8), ylim = c(-80, 100)) +</span></span>
<span id="cb15-251"><a href="#cb15-251" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(x = "Propensity", y = "Count")</span></span>
<span id="cb15-252"><a href="#cb15-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-253"><a href="#cb15-253" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-254"><a href="#cb15-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-255"><a href="#cb15-255" aria-hidden="true" tabindex="-1"></a>This plot shows the original distributions for treated and untreated propensities in darker colors and overlays the weighted/adjusted propensities in lighter colors. These pseudo-populations mirror each other pretty well now! These two rescaled and reweighted groups are now much more equally sized and comparable: the low-propensity net users have a much higher weight, while high-propensity non-net users also get more weight. </span>
<span id="cb15-256"><a href="#cb15-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-257"><a href="#cb15-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-258"><a href="#cb15-258" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian inverse probability weighting</span></span>
<span id="cb15-259"><a href="#cb15-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-260"><a href="#cb15-260" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why even do this Bayesianly?</span></span>
<span id="cb15-261"><a href="#cb15-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-262"><a href="#cb15-262" aria-hidden="true" tabindex="-1"></a>We successfully found the causal effect of -10 malaria risk points using regular frequentist regression, so why am I trying to make life more complex and do this with Bayesian methods instead? Mostly because I'm not a fan of null hypothesis signficance testing (NHST), or the whole process of proposing a null hypothesis, generating a statistical test, finding a p-value, and seeing if the test/p-value/confidence interval provides enough evidence to reject the null hypothesis. With frequentist statistics we test for the probability of the data given a null hypothesis, or $P(\text{data} \mid H_0)$, while with Bayesian statistics, we get to test for the probability of a hypothesis given the data, or $P(H \mid \text{data})$. </span>
<span id="cb15-263"><a href="#cb15-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-264"><a href="#cb15-264" aria-hidden="true" tabindex="-1"></a>Testing a hypothesis directly with Bayesian inference is a lot more intuitive, with Bayesian <span class="co">[</span><span class="ot">credible intervals</span><span class="co">](https://evalf21.classes.andrewheiss.com/resource/bayes/#bayesian-credible-intervals)</span> and inferential approaches like measuring the probability that a parameter is greater/less than 0 (i.e. <span class="co">[</span><span class="ot">probability of direction</span><span class="co">](https://evalf21.classes.andrewheiss.com/resource/bayes/#probability-of-direction)</span>), or measuring the proportion of a posterior that falls within a null region of practical equivalence, or <span class="co">[</span><span class="ot">ROPE</span><span class="co">](https://evalf21.classes.andrewheiss.com/resource/bayes/#region-of-practical-equivalence-rope)</span>. <span class="co">[</span><span class="ot">See this page for an overview of all these methods</span><span class="co">](https://evalf21.classes.andrewheiss.com/resource/bayes/#)</span> and a comparison with frequentism, and check out <span class="co">[</span><span class="ot">this amazing (and free!) textbook on Bayesianism in general</span><span class="co">](https://www.bayesrulesbook.com/)</span>.</span>
<span id="cb15-265"><a href="#cb15-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-266"><a href="#cb15-266" aria-hidden="true" tabindex="-1"></a>Plus, <span class="co">[</span><span class="ot">the amazing **brms** package</span><span class="co">](https://paul-buerkner.github.io/brms/)</span> lets us make all sorts of powerful and complex models with <span class="co">[</span><span class="ot">multilevel nested intercepts and slopes</span><span class="co">](https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/)</span> and fancy families like <span class="co">[</span><span class="ot">beta</span><span class="co">](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/)</span>, <span class="co">[</span><span class="ot">zero-inflated beta</span><span class="co">](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/)</span>, hurdled lognormal, and so on.</span>
<span id="cb15-267"><a href="#cb15-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-268"><a href="#cb15-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-269"><a href="#cb15-269" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fundamental problem with Bayesian propensity scores</span></span>
<span id="cb15-270"><a href="#cb15-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-271"><a href="#cb15-271" aria-hidden="true" tabindex="-1"></a>However, there are serious mathematical and philosophical issues with using propensity scores (and IPTWs) in Bayesian models. Put simply, IPTWs aren't actually part of the model! They're a neat way to scale and shift the population into comparable pseudo-populations, but they're not part of the data-generating process for any element of a Bayesian model.</span>
<span id="cb15-272"><a href="#cb15-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-273"><a href="#cb15-273" aria-hidden="true" tabindex="-1"></a>Here's how @RobinsHernanWasserman:2015, @Zigler:2016, and @LiaoZigler:2020 explain it (but substantially simplified to the point of being a little bit wrong, but acceptably wrong). We can more formally define the average treatment effect (ATE) of mosquito nets on malaria risk using this estimand:</span>
<span id="cb15-274"><a href="#cb15-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-275"><a href="#cb15-275" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-276"><a href="#cb15-276" aria-hidden="true" tabindex="-1"></a>\Delta_{\text{ATE}} = E<span class="co">[</span><span class="ot"> \overbrace{E \left( Y_i \mid T_i = 1, X_i \right)}^{\substack{\text{Average outcome } Y \text{ when} \\ \text{treated, given confounders }X}} - \overbrace{E \left( Y_i \mid T_i = 0, X_i \right)}^{\substack{\text{Average outcome } Y \text{ when} \\ \text{not treated, given confounders }X}} </span><span class="co">]</span></span>
<span id="cb15-277"><a href="#cb15-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-278"><a href="#cb15-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-279"><a href="#cb15-279" aria-hidden="true" tabindex="-1"></a>There are three moving parts in the equation for the ATE here: the treatment effect is a function of the outcome $Y$ (malaria risk), the treatment $T$ (nets), and all other covariates $X$ (health, income, temperatures). In order to calculate this ATE, we need to create some sort of function that incorporates all three, or something like this that calculates $\Delta$ given $T$, $X$, and $Y$:</span>
<span id="cb15-280"><a href="#cb15-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-281"><a href="#cb15-281" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-282"><a href="#cb15-282" aria-hidden="true" tabindex="-1"></a>f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y}) </span>
<span id="cb15-283"><a href="#cb15-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-284"><a href="#cb15-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-285"><a href="#cb15-285" aria-hidden="true" tabindex="-1"></a>This works well with Bayesian methods. We don't know what $\Delta$ is, so we can use Bayes' theorem to estimate it given our existing data for $T$, $X$, and $Y$. To quote from @LiaoZigler:2020, </span>
<span id="cb15-286"><a href="#cb15-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-287"><a href="#cb15-287" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">T</span><span class="co">]</span><span class="at">raditional Bayeisan inference for $\Delta$ would follow from specification of a likelihood for $(\boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y})$ conditional on unknown parameters, $\theta$, a prior distribution for $\theta$, and some function relating the data and $\theta$ to the quantity $\Delta$.</span></span>
<span id="cb15-288"><a href="#cb15-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-289"><a href="#cb15-289" aria-hidden="true" tabindex="-1"></a>Note that the equation below is slightly wrong—ordinarily in Bayesian modeling we're interested in estimating an unknown $\theta$ parameter, so if we wanted to be super official we'd need to create "some function relating the data and $\theta$ to the quantity $\Delta$", but for the sake of simplicity and intuition we'll skip that part and pretend that $\Delta$ is the output of the model:</span>
<span id="cb15-290"><a href="#cb15-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-291"><a href="#cb15-291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-292"><a href="#cb15-292" aria-hidden="true" tabindex="-1"></a>\overbrace{P<span class="co">[</span><span class="ot">\Delta \mid (\boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y})</span><span class="co">]</span>}^{\substack{\text{Posterior estimate} <span class="sc">\\</span> \text{of } \Delta \text{, given data}}} \propto \overbrace{P<span class="co">[</span><span class="ot">(\boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y}) \mid \Delta</span><span class="co">]</span>}^{\substack{\text{Likelihood for existing} <span class="sc">\\</span> \text{data, given unknown }\Delta}} \times \overbrace{P<span class="co">[</span><span class="ot">\Delta</span><span class="co">]</span>}^{\substack{\text{Prior} <span class="sc">\\</span> \text{for }\Delta}}</span>
<span id="cb15-293"><a href="#cb15-293" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-294"><a href="#cb15-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-295"><a href="#cb15-295" aria-hidden="true" tabindex="-1"></a>But guess what's missing entirely from this equation?! Our weights! Propensity scores and inverse probability weights have no place in this kind of Bayesian estimation. They would theoretically show up in the likelihood part of the Bayesian equation, but in practice, weights aren't part of the data-generating process and thus aren't actually part of the likelihood. This is the key problem with Bayesian weights: these weights **are not part of the model** for calculating the ATE ($f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y})$). We can't set a prior on the weight parameter—there isn't even a weight parameter to work with in the likelihood. In order to be a true Bayesian, we can't use propensity scores and weighting. This is the argument that @RobinsHernanWasserman:2015 make: because propensity scores (and thus weights) aren't part of the likelihood, we can't do anything Bayesian with them.</span>
<span id="cb15-296"><a href="#cb15-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-297"><a href="#cb15-297" aria-hidden="true" tabindex="-1"></a>And that's disappointing because Bayesian inference is great! I find it far more intuitive (and fun) to make inferences with posterior distributions rather than work with null hypothesis significance testing.</span>
<span id="cb15-298"><a href="#cb15-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-299"><a href="#cb15-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-300"><a href="#cb15-300" aria-hidden="true" tabindex="-1"></a><span class="fu">### A legal way to use weights Bayesianly</span></span>
<span id="cb15-301"><a href="#cb15-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-302"><a href="#cb15-302" aria-hidden="true" tabindex="-1"></a>Fortunately, @RobinsHernanWasserman:2015 conclude by saying that there are possible compromises that we can use to work with weights in a Bayesian framework, and @LiaoZigler:2020 explore one of these compromises and propose a method of incorporating propensity scores into Bayesian estimation of causal effects. Their general approach is to think of the propensity score calculation as a new parameter $\nu$ (nu). Instead of calculating a single value of $\nu$ (i.e. a single set of propensity scores or weights) like we did with frequentist estimation, we incorporate a range of reasonable values of $\nu$ from the posterior distribution of the treatment/design model into the outcome model, which essentially lets us get rid of the $\nu$ term. Mathematically the approach looks like this:</span>
<span id="cb15-303"><a href="#cb15-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-304"><a href="#cb15-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-305"><a href="#cb15-305" aria-hidden="true" tabindex="-1"></a>\overbrace{f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y})}^{\substack{\text{Estimand for} <span class="sc">\\</span> \text{the ATE, without } \nu}} = \int_\nu \overbrace{f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y}, \nu)}^{\substack{\text{Outcome model} <span class="sc">\\</span> \text{with } \nu}}\ \overbrace{f(\nu \mid \boldsymbol{T}, \boldsymbol{X})}^{\substack{\text{Treatment model} <span class="sc">\\</span> \text{creating propensity} <span class="sc">\\</span> \text{scores with } T \text{ and } X}}\ \mathrm{d} \nu</span>
<span id="cb15-306"><a href="#cb15-306" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-307"><a href="#cb15-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-308"><a href="#cb15-308" aria-hidden="true" tabindex="-1"></a>The $f(\nu \mid \boldsymbol{T}, \boldsymbol{X})$ part of the equation is the treatment/design model and it doesn't have any information about the outcome $Y$ in it. In our running example, this is <span class="in">`net ~ confounders`</span>. The $f(\Delta \mid \boldsymbol{T}, \boldsymbol{X}, \boldsymbol{Y}, \nu)$ part of the equation is the outcome model and it incorporates $\nu$. This *isn't* the final estimand, which doesn't have the propensity scores ($\nu$) in it. We ultimately get rid of that $\nu$ term by marginalizing over the distribution for $\Delta$.</span>
<span id="cb15-309"><a href="#cb15-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-310"><a href="#cb15-310" aria-hidden="true" tabindex="-1"></a>To simplify this process more, here's the basic process for doing this:</span>
<span id="cb15-311"><a href="#cb15-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-312"><a href="#cb15-312" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Use a Bayesian model to estimate the likelihood of treatment and generate propensity scores ($\nu$). This is the treatment model (or design model) and is analogous to the logistic regression model <span class="in">`model_treatment_freq`</span> that we ran earlier. </span>
<span id="cb15-313"><a href="#cb15-313" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Generate $K$ samples of propensity scores based on the posterior distribution of propensity scores. This can be whatever number you want—often it's the number of posterior chains from the Bayesian model (like 2,000 or however many iterations you use).</span>
<span id="cb15-314"><a href="#cb15-314" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>For each of the $K$ samples, generate inverse probability weights and run an outcome model using those weights. This essentially means that we'll be running the outcome model <span class="in">`malaria_risk ~ net`</span> a bunch of times based on different weights each time.</span>
<span id="cb15-315"><a href="#cb15-315" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Combine the results from the outcome model to create the final $\nu$-free ATE.</span>
<span id="cb15-316"><a href="#cb15-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-317"><a href="#cb15-317" aria-hidden="true" tabindex="-1"></a>This process is similar to <span class="co">[</span><span class="ot">multiple imputation</span><span class="co">](https://www.andrewheiss.com/blog/2018/03/07/amelia-tidy-melding/)</span> or bootstrapping: run the same model a bunch of times on slightly different data and combine the results.</span>
<span id="cb15-318"><a href="#cb15-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-319"><a href="#cb15-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-320"><a href="#cb15-320" aria-hidden="true" tabindex="-1"></a>Here's what this looks like in practice. First we'll predict net usage based on the confounders of income, temperature, and health using **brms** (with default priors and settings):</span>
<span id="cb15-321"><a href="#cb15-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-322"><a href="#cb15-322" aria-hidden="true" tabindex="-1"></a><span class="in">```{r model-treatment-bayes, warning=FALSE, results="hide", cache=TRUE}</span></span>
<span id="cb15-323"><a href="#cb15-323" aria-hidden="true" tabindex="-1"></a><span class="in">model_treatment &lt;- brm(</span></span>
<span id="cb15-324"><a href="#cb15-324" aria-hidden="true" tabindex="-1"></a><span class="in">  bf(net ~ income + temperature + health,</span></span>
<span id="cb15-325"><a href="#cb15-325" aria-hidden="true" tabindex="-1"></a><span class="in">     decomp = "QR"),  # QR decomposition handles scaling and unscaling for us</span></span>
<span id="cb15-326"><a href="#cb15-326" aria-hidden="true" tabindex="-1"></a><span class="in">  family = bernoulli(),  # Logistic regression</span></span>
<span id="cb15-327"><a href="#cb15-327" aria-hidden="true" tabindex="-1"></a><span class="in">  data = nets,</span></span>
<span id="cb15-328"><a href="#cb15-328" aria-hidden="true" tabindex="-1"></a><span class="in">  chains = 4, cores = 4, iter = 1000,</span></span>
<span id="cb15-329"><a href="#cb15-329" aria-hidden="true" tabindex="-1"></a><span class="in">  seed = 1234, backend = "cmdstanr"</span></span>
<span id="cb15-330"><a href="#cb15-330" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb15-331"><a href="#cb15-331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-332"><a href="#cb15-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-333"><a href="#cb15-333" aria-hidden="true" tabindex="-1"></a>We'll then use <span class="in">`posterior_epred()`</span> to plug the original data into each of the posterior draws to calculate propensity scores for each draw. This will give us a lot of propensity scores: 2,000 probabilities for each of the <span class="in">`r scales::comma(nrow(nets))`</span> people in the data.</span>
<span id="cb15-334"><a href="#cb15-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-335"><a href="#cb15-335" aria-hidden="true" tabindex="-1"></a><span class="in">```{r calculate-propensity-scores}</span></span>
<span id="cb15-336"><a href="#cb15-336" aria-hidden="true" tabindex="-1"></a><span class="in"># Extract posterior predicted propensity scores</span></span>
<span id="cb15-337"><a href="#cb15-337" aria-hidden="true" tabindex="-1"></a><span class="in">pred_probs_chains &lt;- posterior_epred(model_treatment)</span></span>
<span id="cb15-338"><a href="#cb15-338" aria-hidden="true" tabindex="-1"></a><span class="in">dim(pred_probs_chains)</span></span>
<span id="cb15-339"><a href="#cb15-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-340"><a href="#cb15-340" aria-hidden="true" tabindex="-1"></a><span class="in"># Rows are chains; columns are individuals</span></span>
<span id="cb15-341"><a href="#cb15-341" aria-hidden="true" tabindex="-1"></a><span class="in">head(pred_probs_chains, c(5, 10))</span></span>
<span id="cb15-342"><a href="#cb15-342" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-343"><a href="#cb15-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-344"><a href="#cb15-344" aria-hidden="true" tabindex="-1"></a>Each column here is a person in the dataset; each row is a draw from the posterior distribution. Note how there's a lot of uncertainty in these propensity scores—we want to incorporate this uncertainty into our outcome model somehow. But as we've seen, there's no weight parameter in our model, so there's no way to directly add this uncertainty to the outcome model. Instead, we'll run the outcome model a bunch of times, or $K$ times. For now we'll set $K$ to 2,000—we'll create an outcome model for each of the posterior draws that we have.</span>
<span id="cb15-345"><a href="#cb15-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-346"><a href="#cb15-346" aria-hidden="true" tabindex="-1"></a>Doing this will require a little bit of **purrr** magic. We'll make a dataset with 2,000 ($K$) rows and put the propensity scores for all <span class="in">`r scales::comma(nrow(nets))`</span> people into their own cell to make it easier to keep track of these scores.</span>
<span id="cb15-347"><a href="#cb15-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-348"><a href="#cb15-348" aria-hidden="true" tabindex="-1"></a><span class="in">```{r nest-propensity-scores, message=FALSE}</span></span>
<span id="cb15-349"><a href="#cb15-349" aria-hidden="true" tabindex="-1"></a><span class="in"># Put each set of individual propensity scores into its own cell</span></span>
<span id="cb15-350"><a href="#cb15-350" aria-hidden="true" tabindex="-1"></a><span class="in">pred_probs_nested &lt;- pred_probs_chains %&gt;% </span></span>
<span id="cb15-351"><a href="#cb15-351" aria-hidden="true" tabindex="-1"></a><span class="in">  # Convert this matrix to a data frame</span></span>
<span id="cb15-352"><a href="#cb15-352" aria-hidden="true" tabindex="-1"></a><span class="in">  as_tibble(.name_repair = "unique") %&gt;% </span></span>
<span id="cb15-353"><a href="#cb15-353" aria-hidden="true" tabindex="-1"></a><span class="in">  # Add a column for the draw number</span></span>
<span id="cb15-354"><a href="#cb15-354" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(draw = 1:n()) %&gt;% </span></span>
<span id="cb15-355"><a href="#cb15-355" aria-hidden="true" tabindex="-1"></a><span class="in">  # Make this long so that each draw gets its own row</span></span>
<span id="cb15-356"><a href="#cb15-356" aria-hidden="true" tabindex="-1"></a><span class="in">  pivot_longer(-draw, names_to = "row", values_to = "prob") %&gt;% </span></span>
<span id="cb15-357"><a href="#cb15-357" aria-hidden="true" tabindex="-1"></a><span class="in">  # Clean up the draw number </span></span>
<span id="cb15-358"><a href="#cb15-358" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(row = as.numeric(str_remove(row, "..."))) %&gt;% </span></span>
<span id="cb15-359"><a href="#cb15-359" aria-hidden="true" tabindex="-1"></a><span class="in">  # Group by draw and nest all the scores in a cell</span></span>
<span id="cb15-360"><a href="#cb15-360" aria-hidden="true" tabindex="-1"></a><span class="in">  group_by(draw) %&gt;% </span></span>
<span id="cb15-361"><a href="#cb15-361" aria-hidden="true" tabindex="-1"></a><span class="in">  nest() %&gt;% </span></span>
<span id="cb15-362"><a href="#cb15-362" aria-hidden="true" tabindex="-1"></a><span class="in">  ungroup()</span></span>
<span id="cb15-363"><a href="#cb15-363" aria-hidden="true" tabindex="-1"></a><span class="in">pred_probs_nested</span></span>
<span id="cb15-364"><a href="#cb15-364" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-365"><a href="#cb15-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-366"><a href="#cb15-366" aria-hidden="true" tabindex="-1"></a>We'll then run an outcome model using each of these nested propensity scores. We'll take the scores, calculate weights, and run a basic frequentist <span class="in">`lm()`</span> model with those weights. We'll then use <span class="in">`tidy()`</span> from **broom** to extract different parts of the results.</span>
<span id="cb15-367"><a href="#cb15-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-368"><a href="#cb15-368" aria-hidden="true" tabindex="-1"></a><span class="in">```{r run-lots-of-outcome-models, cache=TRUE}</span></span>
<span id="cb15-369"><a href="#cb15-369" aria-hidden="true" tabindex="-1"></a><span class="in">outcome_models &lt;- pred_probs_nested %&gt;% </span></span>
<span id="cb15-370"><a href="#cb15-370" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(outcome_model = map(data, ~{</span></span>
<span id="cb15-371"><a href="#cb15-371" aria-hidden="true" tabindex="-1"></a><span class="in">    # Add this version of propensity scores to the original data and calculate</span></span>
<span id="cb15-372"><a href="#cb15-372" aria-hidden="true" tabindex="-1"></a><span class="in">    # weights. We could also do this prior to nesting everything.</span></span>
<span id="cb15-373"><a href="#cb15-373" aria-hidden="true" tabindex="-1"></a><span class="in">    df &lt;- bind_cols(nets, .) %&gt;% </span></span>
<span id="cb15-374"><a href="#cb15-374" aria-hidden="true" tabindex="-1"></a><span class="in">      mutate(iptw = (net_num / prob) + ((1 - net_num) / (1 - prob)))</span></span>
<span id="cb15-375"><a href="#cb15-375" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb15-376"><a href="#cb15-376" aria-hidden="true" tabindex="-1"></a><span class="in">    # Create outcome model with this iteration of weights</span></span>
<span id="cb15-377"><a href="#cb15-377" aria-hidden="true" tabindex="-1"></a><span class="in">    model &lt;- lm(malaria_risk ~ net, data = df, weights = iptw)</span></span>
<span id="cb15-378"><a href="#cb15-378" aria-hidden="true" tabindex="-1"></a><span class="in">  })) %&gt;% </span></span>
<span id="cb15-379"><a href="#cb15-379" aria-hidden="true" tabindex="-1"></a><span class="in">  # Extract results</span></span>
<span id="cb15-380"><a href="#cb15-380" aria-hidden="true" tabindex="-1"></a><span class="in">  mutate(tidied = map(outcome_model, ~tidy(.)),</span></span>
<span id="cb15-381"><a href="#cb15-381" aria-hidden="true" tabindex="-1"></a><span class="in">         ate = map_dbl(tidied, ~filter(., term == "netTRUE") %&gt;% pull(estimate)),</span></span>
<span id="cb15-382"><a href="#cb15-382" aria-hidden="true" tabindex="-1"></a><span class="in">         ate_se = map_dbl(tidied, ~filter(., term == "netTRUE") %&gt;% pull(std.error)))</span></span>
<span id="cb15-383"><a href="#cb15-383" aria-hidden="true" tabindex="-1"></a><span class="in">outcome_models</span></span>
<span id="cb15-384"><a href="#cb15-384" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-385"><a href="#cb15-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-386"><a href="#cb15-386" aria-hidden="true" tabindex="-1"></a>The <span class="in">`ate`</span> column here is the coefficient for <span class="in">`net`</span>, and we have 2,000 of them to work with now, each based on a different set of weights. If we average all these estimates, we can get one final estimate of the ATE that successfully incorporates the uncertainty from the treatment/design model. We can't take the direct average of the standard errors, but we can combine them using Rubin's rules. @Rubin:1987 outlines an set of rules for combining the results from multiply imputed datasets that reflects the averages and accounts for differences in standard errors (they're essentially a fancier, more robust way of averaging standard errors across multiple models). We didn't use multiply imputed datasets here, but the same principle applies—we used the same model on lots of slightly different datasets.</span>
<span id="cb15-387"><a href="#cb15-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-388"><a href="#cb15-388" aria-hidden="true" tabindex="-1"></a><span class="in">```{r combined-results}</span></span>
<span id="cb15-389"><a href="#cb15-389" aria-hidden="true" tabindex="-1"></a><span class="in"># Combined average treatment effect</span></span>
<span id="cb15-390"><a href="#cb15-390" aria-hidden="true" tabindex="-1"></a><span class="in">mean(outcome_models$ate)</span></span>
<span id="cb15-391"><a href="#cb15-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-392"><a href="#cb15-392" aria-hidden="true" tabindex="-1"></a><span class="in"># Combined standard errors (this is wrong)</span></span>
<span id="cb15-393"><a href="#cb15-393" aria-hidden="true" tabindex="-1"></a><span class="in">mean(outcome_models$ate_se)</span></span>
<span id="cb15-394"><a href="#cb15-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-395"><a href="#cb15-395" aria-hidden="true" tabindex="-1"></a><span class="in"># Combined standard errors with Rubin's rules (this is correct)</span></span>
<span id="cb15-396"><a href="#cb15-396" aria-hidden="true" tabindex="-1"></a><span class="in">rubin_se &lt;- function(ates, sigmas) {</span></span>
<span id="cb15-397"><a href="#cb15-397" aria-hidden="true" tabindex="-1"></a><span class="in">  sqrt(mean(sigmas^2) + var(ates))</span></span>
<span id="cb15-398"><a href="#cb15-398" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb15-399"><a href="#cb15-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-400"><a href="#cb15-400" aria-hidden="true" tabindex="-1"></a><span class="in">rubin_se(outcome_models$ate, outcome_models$ate_se)</span></span>
<span id="cb15-401"><a href="#cb15-401" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-402"><a href="#cb15-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-403"><a href="#cb15-403" aria-hidden="true" tabindex="-1"></a>The overall average treatment effect is still −10ish, like we found earlier, but now we have a bunch of extra uncertainty from the treatment model, which is neat. We can also visualize the distribution of the ATE, almost like a Bayesian posterior:</span>
<span id="cb15-404"><a href="#cb15-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-405"><a href="#cb15-405" aria-hidden="true" tabindex="-1"></a><span class="in">```{r visualize-ates}</span></span>
<span id="cb15-406"><a href="#cb15-406" aria-hidden="true" tabindex="-1"></a><span class="in">ggplot(outcome_models, aes(x = ate)) +</span></span>
<span id="cb15-407"><a href="#cb15-407" aria-hidden="true" tabindex="-1"></a><span class="in">  geom_density(fill = isfahan[4], color = NA) +</span></span>
<span id="cb15-408"><a href="#cb15-408" aria-hidden="true" tabindex="-1"></a><span class="in">  labs(x = "Average treatment effect of using a mosquito net", y = NULL)</span></span>
<span id="cb15-409"><a href="#cb15-409" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-410"><a href="#cb15-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-411"><a href="#cb15-411" aria-hidden="true" tabindex="-1"></a>We did it!</span>
<span id="cb15-412"><a href="#cb15-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-413"><a href="#cb15-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-414"><a href="#cb15-414" aria-hidden="true" tabindex="-1"></a><span class="fu">## Things I don't know yet but want to know!</span></span>
<span id="cb15-415"><a href="#cb15-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-416"><a href="#cb15-416" aria-hidden="true" tabindex="-1"></a>So, thanks to @LiaoZigler:2020, we have a fully Bayesian treatment/design model and we can incorporate the uncertainty from that model's propensity scores and weights into the outcome/analysis model. That's so neat!</span>
<span id="cb15-417"><a href="#cb15-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-418"><a href="#cb15-418" aria-hidden="true" tabindex="-1"></a>However, in their paper Liao and Zigler use a frequentist outcome model, like we just did here. While combining the ATEs from 2,000 different frequentist OLS-based models feels quasi-Bayesian, I don't know if we can legally talk about these results Bayesianly. Can we pretend that this distribution of ATEs is similar to a posterior distribution and use Bayesian inference rather than null hypothesis significance testing, or do we still need to talk about null hypotheses? These 2,000 models are essentially a mathematical transformation of the posterior, so maybe it's legal? But the Bayesian model is for predicting treatment, not the outcome, so maybe it's not legal? idk. (Turns out <span class="co">[</span><span class="ot">the answer is no</span><span class="co">](https://twitter.com/adamjnafa/status/1472269242334687236)</span>—there's no uncertainty in the outcome model here, since all the uncertainty comes from the treatment model. Ah! Well. Nevertheless.)</span>
<span id="cb15-419"><a href="#cb15-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-420"><a href="#cb15-420" aria-hidden="true" tabindex="-1"></a>We could technically run a Bayesian outcome model with <span class="in">`brm()`</span>, but we'd have to run it 2,000 times—one model per set of weights—and that would take literally forever and might melt my computer. There could be a way to only run a single outcome model once and use one set of weights for each of the iterations (i.e. use the first column of propensity scores for the first iteration of the outcome model, the second for the second, and so on), but that goes beyond my skills with **brms**. (**UPDATE**: <span class="co">[</span><span class="ot">Thanks to Jordan Nafa</span><span class="co">](https://twitter.com/adamjnafa)</span>, this is actually possible! <span class="co">[</span><span class="ot">See here!</span><span class="co">](https://www.andrewheiss.com/blog/2021/12/20/fully-bayesian-ate-iptw/)</span>)</span>
<span id="cb15-421"><a href="#cb15-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-422"><a href="#cb15-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-423"><a href="#cb15-423" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i> 2007–2024 Andrew Heiss</span> <span class="faux-block">All content licensed under<br><a href="https://creativecommons.org/licenses/by/4.0/"><i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i> <i class="fa-brands fa-creative-commons-by" aria-label="creative-commons-by"></i> Creative Commons CC BY 4.0</a></span></p>
</div>   
    <div class="nav-footer-center">
<p><span class="faux-block"><i class="fa-brands fa-orcid" aria-label="orcid"></i> <strong>ORCID</strong> <a href="https://orcid.org/0000-0002-3948-3914">0000-0002-3948-3914</a></span> <span class="faux-block"><i class="fa-solid fa-key" aria-label="key"></i> <a href="../../../../../pgp_ath.asc.txt">PGP public key</a>   <i class="fa-solid fa-fingerprint" aria-label="fingerprint"></i> Fingerprint:<br><span class="fingerprint">4AA2 FA83 A8B2 05A4 E30F<br> 610D 1382 6216 9178 36AB</span></span></p>
</div>
    <div class="nav-footer-right">
<p><span class="faux-block">Made with <i class="fa-brands fa-r-project" aria-label="r-project"></i> and <a href="https://quarto.org/">Quarto</a></span> <span class="faux-block"><a href="https://github.com/andrewheiss/ath-quarto">View the source at <i class="fa-brands fa-github" aria-label="github"></i> GitHub</a></span></p>
</div>
  </div>
</footer>


</body></html>