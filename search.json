[
  {
    "objectID": "blog/2018/12/28/tidytext-pos-arabic/index.html",
    "href": "blog/2018/12/28/tidytext-pos-arabic/index.html",
    "title": "Tidy text, parts of speech, and unique words in the Qur’an",
    "section": "",
    "text": "(See this notebook on GitHub)\nAs I showed in a previous blog post, the cleanNLP package is a phenomenal frontend for natural language processing in R. Rather than learn the exact syntax for NLP packages like spaCy or CoreNLP, you can use a consistent set of functions and let cleanNLP handle the API translation behind the scenes for you.\nPreviously, I used spaCy to tag the parts of speech in the Four Gospels to find the most distinctive nouns and verbs in the Gospel of John. Here, I’ll show a quick example of how to use CoreNLP to tag parts of speech in Arabic. CoreNLP is far far far slower than spaCy, but it can handle languages like Arabic and Chinese, which is pretty magical.\nHere we go!"
  },
  {
    "objectID": "blog/2018/12/28/tidytext-pos-arabic/index.html#load-packages-and-data",
    "href": "blog/2018/12/28/tidytext-pos-arabic/index.html#load-packages-and-data",
    "title": "Tidy text, parts of speech, and unique words in the Qur’an",
    "section": "Load packages and data",
    "text": "Load packages and data\nYou’ll need to install the new quRan package, which contains two Arabic versions of the Qur’an (with and without vowels) and two English translations (Yusuf Ali and Saheeh International). Install it with remotes::install_github(\"andrewheiss/quRan\") or devtools::install_github(\"andrewheiss/quRan\"). It’ll be on CRAN once they open up for submissions again in January.\nYou’ll also need to install rJava. Best of luck with this. It is the worst package in the world to install, especially on macOS. The only way I got it to work was to follow the instructions here (even though at the top of that page it says that recent versions of R/Java no longer require this fix—that is false). Here’s basically what you have to do:\n\nDownload and install the most recent Java SE 8 JDK from Oracle (I did Java SE 8u191)\nReinstall R\nDownload and run the fix-rJava.sh script listed at snaq.net\nInstall rJava with install.packages(\"rJava\") or install.packages(\"rJava\", type = \"source\", repos = \"http://cran.us.r-project.org\")\nHopefully it worked?\n\nFinally, you’ll need to install cleanNLP and use it to download the CoreNLP parser and all its accompanying languages. Here’s how you do that:\nlibrary(cleanNLP)\n\n# This should download everything\ncnlp_download_corenlp()\n\n# Check if Arabic is there\ndir(system.file(\"extdata\", package = \"cleanNLP\"))\nPhew. If you’ve done all that, we can get started officially. We’ll load these packages:\nlibrary(tidyverse)\nlibrary(cleanNLP)\nlibrary(quRan)\nlibrary(tidytext)\nlibrary(arabicStemR)  # Has a list of Arabic stopwords"
  },
  {
    "objectID": "blog/2018/12/28/tidytext-pos-arabic/index.html#part-of-speech-tagging",
    "href": "blog/2018/12/28/tidytext-pos-arabic/index.html#part-of-speech-tagging",
    "title": "Tidy text, parts of speech, and unique words in the Qur’an",
    "section": "Part-of-speech tagging",
    "text": "Part-of-speech tagging\nTo start, we’ll initialize the CoreNLP Arabic tagging engine (this takes a few seconds to run):\ncnlp_init_corenlp(language = \"ar\")\nThen, for the sake of speed, we’ll extract just the first surah of the Qur’an (the Fatihah) and tag it with cnlp_annotate(). We’ll use the version of the Qur’an without vowels because it seems to work better with CoreNLP. cnlp_annotate() does the heavy lifting of annotation and returns an object with the annotate class, which isn’t readily usable.\nfatiha &lt;- quran_ar_min %&gt;% \n  filter(surah_id == 1)\n\nfatiha_annotated &lt;- cnlp_annotate(fatiha,\n                                  text_name = \"text\", doc_name = \"ayah_title\")\n\nfatiha_terms &lt;- fatiha_annotated$token\n\nhead(fatiha_terms)\n\n## # A tibble: 6 x 8\n##   id      sid   tid word   lemma upos  pos     cid\n##   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n## 1 1:1       1     1 بسم    \"\"    NOUN  NNP       1\n## 2 1:1       1     2 الله   \"\"    NOUN  NNP       5\n## 3 1:1       1     3 الرحمن \"\"    \"\"    DTNNP    10\n## 4 1:1       1     4 الرحيم \"\"    \"\"    DTNNP    17\n## 5 1:2       1     1 الحمد  \"\"    \"\"    DTNNP     0\n## 6 1:2       1     2 لله    \"\"    NOUN  NN        6\nThe output we get here isn’t as fancy as what spaCy returns for English text. We don’t get lemmatized words (base unpluralized, unconjugated words), and the upos column isn’t very rich—it just pulls out verbs and nouns. The pos column, though, seems to have more information in it. This column uses codes from the Penn Treebank Project. For instance, “Allah” (الله) in the second row is marked as NNP, which is a singular proper noun, while “al-rahman” (the merciful; الرحمن) in the third row is marked as DTNNP, which is a determiner (DT; ال / “the”) + a singular proper noun (NNP; رحمن / “merciful”).\nNow that we know the tagging works, we can tag the entire Qur’an. Warning: CoreNLP is incredibly slow with this much text. It took 2,194 seconds to run this on my newish MacBook Pro (36 minutes!). To speed this up, you can download and load the pre-tagged text:\n\nquran_annotated.rds\n\n# THIS TAKES SO LONG\nquran_annotated &lt;- cnlp_annotate(quran_ar_min,\n                                 text_name = \"text\", doc_name = \"ayah_title\")\n\n# SAVE THIS SO YOU NEVER HAVE TO RUN IT AGAIN\nsaveRDS(quran_annotated, \"quran_annotated.rds\")\nquran_annotated &lt;- readRDS(\"data/quran_annotated.rds\")\n\n# Convert to data frame\nquran_terms &lt;- quran_annotated$token"
  },
  {
    "objectID": "blog/2018/12/28/tidytext-pos-arabic/index.html#most-common-nouns-and-verbs",
    "href": "blog/2018/12/28/tidytext-pos-arabic/index.html#most-common-nouns-and-verbs",
    "title": "Tidy text, parts of speech, and unique words in the Qur’an",
    "section": "Most common nouns and verbs",
    "text": "Most common nouns and verbs\nNow that we have all the parts of speech for the Qur’an, we can do some quick exploratory data analysis.\nFor instance, what are the most common nouns in the book? We need to do a little fancy filtering for this to work because the pos column marks all sorts of nouns: singular proper nouns (NNP), plural proper nouns (NNPS), regular nouns with a “the” (DTNN), etc. We need to find all the words with pos that contains NN. The str_detect() function from stringr makes this easy.\nAdditionally, we need to remove common words like في and لهم. The arabicStemR package has a list of Arabic stopwords, but it’s buried in a removeStopWords() function. We can extract that list into its own mini data frame, though.\nThe stopwords package also has a bunch of common Arabic words, accessible through stopwords::stopwords(language = \"ar\", source = \"misc\")), but it’s not as long of a list as arabicStemR::removeStopWords(), and it includes important Qur’anic words like يوم (day). So we’ll just use the arabicStemR words here.\n# In order to get the full list of Arabic stopwords, we have to feed\n# removeStopWorsd some sort of Arabic text, and then access the\n# `arabicStopwordList` from the resulting object. It's a roundabout approach,\n# but it works\narabic_stopwords &lt;- data_frame(word = removeStopWords(\"سلام\")$arabicStopwordList)\n(NOTE: It is currently not possible to correctly plot Arabic text (or any right-to-left text) in R on macOS, and it’s been an issue for a long time. I made all these graphs with the tidyverse Docker image. You can also use plotly::ggplotly())\ntop_nouns &lt;- quran_terms %&gt;% \n  filter(str_detect(pos, \"NN\")) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  # Get rid of tiny diacritic-like words\n  filter(nchar(word) &gt; 1) %&gt;%\n  # Get rid of stopwords\n  anti_join(arabic_stopwords, by = \"word\") %&gt;% \n  top_n(10, n) %&gt;% \n  mutate(word = fct_inorder(word))\n\nplot_top_nouns &lt;- ggplot(top_nouns, aes(x = fct_rev(word), y = n, fill = n &gt; 500)) +\n  geom_col() + \n  scale_fill_manual(values = c(\"#8F562B\", \"#276B42\"), guide = FALSE) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(y = \"Frequency\", x = NULL) +\n  coord_flip() +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank())\nplot_top_nouns\n\nThe most common noun by far is Allah, which makes sense, given that this is the Qur’an. The other common words are “earth,” “by Allah,” “day,” “your Lord,” “thing,” “heavens,” “people,” “book,” and “the Merciful.”\nWhat about the most common verbs? Here we select words that have VB in their pos:\ntop_verbs &lt;- quran_terms %&gt;% \n  filter(str_detect(pos, \"VB\")) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  # Get rid of tiny diacritic-like words\n  filter(nchar(word) &gt; 1) %&gt;%\n  # Get rid of stopwords\n  anti_join(arabic_stopwords, by = \"word\") %&gt;% \n  top_n(10, n) %&gt;% \n  mutate(word = fct_inorder(word))\n\nword_themes &lt;- data_frame(theme = c(\"Saying\", \"Other\", \"Saying\", \"Believing\", \n                                    \"Saying\", \"Other\", \"Other\", \"Believing\", \n                                    \"Other\", \"Other\"))\ntop_verbs_themes &lt;- top_verbs %&gt;% \n  bind_cols(word_themes) %&gt;% \n  mutate(theme = factor(theme, levels = c(\"Saying\", \"Believing\", \"Other\")))\n\nplot_top_verbs &lt;- ggplot(top_verbs_themes, \n                         aes(x = fct_rev(word), y = n, fill = theme)) +\n  geom_col() + \n  scale_fill_manual(values = c(\"#2BCAE0\", \"#9D2D2A\", \"#F5D085\"),\n                    name = NULL) +\n  labs(y = \"Frequency\", x = NULL) +\n  coord_flip() +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n        legend.position = \"bottom\",\n        legend.key.size = unit(0.65, \"lines\"))\nplot_top_verbs\n\nHere, the most common verbs fall into two general themes: “saying” and “believing”. The most common “saying” words are literally “he said” and “said”, while the most common “believing” words are “y’all believe” (imperative) and “they disbelieve.” The other words are either forms of “to be” or preposition-like things that the tagger thought were verbs."
  },
  {
    "objectID": "blog/2018/12/28/tidytext-pos-arabic/index.html#most-distinctive-meccan-and-medinan-nouns",
    "href": "blog/2018/12/28/tidytext-pos-arabic/index.html#most-distinctive-meccan-and-medinan-nouns",
    "title": "Tidy text, parts of speech, and unique words in the Qur’an",
    "section": "Most distinctive Meccan and Medinan nouns",
    "text": "Most distinctive Meccan and Medinan nouns\nThe Qur’an is a collection of revelations received by Mohammed in two different locations during two different time periods: Mecca and Medina. Scholars have categorized the different surahs by their likely provenance, and the quRan package helpfully includes a column marking if each surah is Meccan or Medinan.\nTypically, Meccan surahs are shorter than Medinan surahs and were revealed earlier during the development of Islam. As a check, lets see what the average surah length is across these two types of surahs:\nperiod_length &lt;- quran_ar_min %&gt;% \n  group_by(revelation_type, surah_id) %&gt;% \n  summarize(n_ayaat = n()) %&gt;% \n  group_by(revelation_type) %&gt;% \n  summarize(avg_ayaat = mean(n_ayaat))\nperiod_length\n\n## # A tibble: 2 x 2\n##   revelation_type avg_ayaat\n##   &lt;chr&gt;               &lt;dbl&gt;\n## 1 Meccan               53.6\n## 2 Medinan              58.0\nMeccan surahs are an average of 4.5 ayahs shorter than Medinan surahs. Cool.\nWhat are the most distinctive nouns and verbs in these two types of surahs? As in my previous post about John and the synoptic gospels, we can use the tf-idf score as a measure of word uniqueness. The higher this value, the more unique a word is in a document in a corpus.\nWe lost lots of the helpful columns from quran_ar_min when tagging the parts of speech, though. All we’re left with from the original data is the ayah_title column, which gives the surah and ayah of each word (e.g. 2:242). To bring the revelation type and other columns back in, we’ll make a smaller dataset and then join that to our tagged data:\ntype_lookup &lt;- quran_ar_min %&gt;% \n  select(ayah_title, surah_id, surah_title_ar, surah_title_en, ayah, revelation_type)\n\nquran_terms &lt;- quran_terms %&gt;% \n  left_join(type_lookup, by = c(\"id\" = \"ayah_title\"))\n\nglimpse(quran_terms)\n\n## Observations: 82,823\n## Variables: 13\n## $ id              &lt;chr&gt; \"1:1\", \"1:1\", \"1:1\", \"1:1\", \"1:2\", \"1:2\", \"1:2...\n## $ sid             &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\n## $ tid             &lt;int&gt; 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 1, 2, 3...\n## $ word            &lt;chr&gt; \"بسم\", \"الله\", \"الرحمن\", \"الرحيم\", \"الحمد\", \"ل...\n## $ lemma           &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"...\n## $ upos            &lt;chr&gt; \"NOUN\", \"NOUN\", \"\", \"\", \"\", \"NOUN\", \"NOUN\", \"\"...\n## $ pos             &lt;chr&gt; \"NNP\", \"NNP\", \"DTNNP\", \"DTNNP\", \"DTNNP\", \"NN\",...\n## $ cid             &lt;int&gt; 1, 5, 10, 17, 0, 6, 10, 13, 0, 7, 0, 5, 9, 0, ...\n## $ surah_id        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\n## $ surah_title_ar  &lt;fct&gt; الفاتحة, الفاتحة, الفاتحة, الفاتحة, الفاتحة, ا...\n## $ surah_title_en  &lt;fct&gt; Al-Faatiha, Al-Faatiha, Al-Faatiha, Al-Faatiha...\n## $ ayah            &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5...\n## $ revelation_type &lt;chr&gt; \"Meccan\", \"Meccan\", \"Meccan\", \"Meccan\", \"Mecca...\nNow we can use the bind_tf_idf() function from tidytext to calculate the tf-idf for each word in each type of revelation:\ntype_tf_idf &lt;- quran_terms %&gt;% \n  count(revelation_type, word, pos, sort = TRUE) %&gt;% \n  bind_tf_idf(word, revelation_type, n) %&gt;% \n  arrange(desc(tf_idf))\nNow we can find the uniquest nouns across the two types of surahs:\nuniquest_nouns &lt;- type_tf_idf %&gt;% \n  # Only look at nouns\n  filter(str_detect(pos, \"NN\")) %&gt;% \n  group_by(revelation_type) %&gt;% \n  top_n(10, tf_idf) %&gt;% \n  ungroup() %&gt;% \n  mutate(word = fct_inorder(word))\n\nplot_unique_nouns &lt;- ggplot(uniquest_nouns, \n                            aes(x = fct_rev(word), y = tf_idf, fill = revelation_type)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"#A02513\", \"#274E49\"), guide = FALSE) +\n  labs(x = NULL, y = \"tf-idf\",\n       title = \"Most unique nouns in the Meccan and Medinan surahs\") +\n  coord_flip() +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"),\n        strip.text = element_text(face = \"bold\"),\n        panel.grid.major.y = element_blank()) +\n  facet_wrap(~ revelation_type, scales = \"free_y\")\nplot_unique_nouns\n\nThis is so cool. The most unique and distinctive Meccan nouns are “Joseph,” “criminals,” “filling,” “wo,” “others,” “Thamoud,” “right,” “humans,” “liars,” and “hearing.” Most of these make sense: For instance, surah 12, “Yusuf”, is Meccan and is all about the story of Joseph in Egypt.\nThe most distinctive Medinan nouns, on the other hand, are “liars,” “his prophet,” “borders/limits,” “wisdom,” “Arabs,” “the Messiah,” “houses,” “killing/fighting,” “the Jews,” “the Hajj,” “hypocrites,” and “promised.”\nWe can also find the uniquest verbs—just use filter(str_detect(pos, \"VB\")) to select the verbs and do the same thing. I’ll leave that as an exercise to the reader.\nThus, we can do a lot of cool text analysis with Arabic using R!"
  },
  {
    "objectID": "blog/2017/08/26/quickly-play-with-polity-iv-and-oecd-data-and-see-the-danger-of-us-democracy/index.html",
    "href": "blog/2017/08/26/quickly-play-with-polity-iv-and-oecd-data-and-see-the-danger-of-us-democracy/index.html",
    "title": "Quickly play with Polity IV and OECD data (and see the danger of US democracy)",
    "section": "",
    "text": "The Polity IV Project released new data yesterday, with democratization scores for 169 countries up to 2016. I wanted to check if the ongoing erosion of US democratic institutions since the 2016 elections registered in the US’s Polity score, and, lo and behold, it did! We dropped from our solid, historically consistent 10 to an 8.\nBut is that bad? How does that compare to other advanced democracies, like countries in the OECD?\nWhat follows below shows how relatively easy it is to quickly and reproducibly grab the new data, graph it, and compare scores across countries. (This notebook is also in a GitHub repository.)\nBefore we start, we’ll load all the libraries we’ll need:\nlibrary(tidyverse)     # dplyr, ggplot, etc.\nlibrary(readxl)        # Read Excel files\nlibrary(forcats)       # Deal with factors\nlibrary(countrycode)   # Deal with country codes and names\nlibrary(rvest)         # Scrape websites\nlibrary(httr)          # Download stuff\nlibrary(ggrepel)       # Place non-overlapping labels on plots\nFirst, we have to download the new Polity data. We could navigate to the Polity IV data page and download the data manually, but that’s not scriptable. Instead, we can use GET() from the httr package (or httr::GET() for short) to download the file directly rather than hunting it down with a browser. After saving the Excel file to a temporary file, we use readxl::read_excel() to load and parse the data. The chain of functions following read_excel() (chained together with dplyr’s %&gt;% pipes) selects and renames the Correlates of War country code, year, and polity score; ensures that those columns are integers (rather than text or decimal-based numbers); and finally filters the data to 2001 and beyond.\npolity.url &lt;- \"http://www.systemicpeace.org/inscr/p4v2016.xls\"\n\n# Download Polity data to temporary file\nGET(polity.url, write_disk(polity.temp &lt;- tempfile(fileext = \".xls\")))\n\n## Response [http://www.systemicpeace.org/inscr/p4v2016.xls]\n## Date: 2017-08-27 01:23\n## Status: 200\n## Content-Type: application/vnd.ms-excel\n## Size: 4.29 MB\n## &lt;ON DISK&gt;  /var/folders/0h/6jl6g9317lv3k5w9h72m18kw0000gn/T//Rtmp0TEhay/file125c8561fd799.xls\n# Read and clean Polity data\npolity &lt;- read_excel(polity.temp) %&gt;%\n  select(cowcode = ccode, year, polity = polity2) %&gt;%\n  mutate_at(vars(year, cowcode, polity),\n            funs(as.integer)) %&gt;%\n  filter(year &gt;= 2000)\n\npolity %&gt;% glimpse()\n\n## Observations: 2,814\n## Variables: 3\n## $ cowcode &lt;int&gt; 700, 700, 700, 700, 700, 700, 700, 700, 700, 700, 700, 700, 700, 700, 700, 700, 700, 339, 339, 339,...\n## $ year    &lt;int&gt; 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 201...\n## $ polity  &lt;int&gt; -7, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, -1, -1, -1, 5, 5, 7, 7, 7, 9, 9, 9, 9, 9, 9...\nThe Polity project does not include information about OECD membership, so we have to download that on our own. It’s relatively easy to google “OECD member countries” and copy/paste the results from any of the resulting webpages into a CSV file, but that’s not reproducible or scriptable.\nInstead, we can use the rvest package to scrape data directly from the OECD’s most recent list of members. There are several ways to target HTML elements on a page, including both CSS selectors and XPath queries, each with advantages and disadvantages. Using CSS is fairly simple if you’re familiar with the structure of the HTML—if you want to select a table inside an &lt;article&gt; element in the main &lt;section id=\"content\"&gt; section, you can specify #content &gt; article &gt; table.\nThe syntax for XPath is easier to use, but is really hard to write by hand. Fortunately, you don’t have to do it manually. In Chrome, go to the OECD’s list, right click on the webpage, and select “Inspect” to open Chrome’s built in web inspector. Activate the inspection tool and find the table with the list of countries:\n\n\n\nSelect HTML table\n\n\nRight click on &lt;table align=\"center\" border=\"0\" style=\"width: 75%;\"&gt; in the inspector and choose “Copy” &gt; “Copy XPath” to copy an XPath query for that table to the clipboard:\n\n\n\nCopy XPath\n\n\nWe can then use that query (which should be something like //*[@id=\"webEditContent\"]/table[2]) in a dplyr chain that will load the webpage, select the HTML &lt;table&gt; element, and parse it into something R can read.\nThe dplyr chain has a few interesting quirks. read_html() will download the given URL as HTML, and html_nodes() will select the HTML element specified by the XPath query. html_table() will parse that element into an R object (here we specify fill = TRUE since not all the rows in the table have the same number of columns; filling the table adds additional empty columns to rows that lack them). bind_rows() and as_data_frame() convert the parsed HTML table into a data frame.\n# Pro tip: the HTML structure of the OECD page can change over time, and the\n# XPath query will inevitably break. To avoid this, use a snapshot of the\n# webpage from the Internet Archive instead of the current OECD page, since the\n# archived version won't change.\n\noecd.url &lt;- \"https://web.archive.org/web/20170821160714/https://www.oecd.org/about/membersandpartners/list-oecd-member-countries.htm\"\n\noecd.countries.raw &lt;- read_html(oecd.url) %&gt;%\n  # Use single quotes instead of double quotes since XPath uses \" in the query\n  html_nodes(xpath = '//*[@id=\"webEditContent\"]/table[2]') %&gt;%\n  html_table(fill = TRUE) %&gt;%\n  bind_rows() %&gt;% as_data_frame()\nBecause this table has extra empty columns, R doesn’t recognize header rows automatically and names each column X1, X2, and so on.\noecd.countries.raw %&gt;% head()\n\n## # A tibble: 6 x 4\n##      X1        X2                X3    X4\n##   &lt;chr&gt;     &lt;chr&gt;             &lt;chr&gt; &lt;lgl&gt;\n## 1         Country              Date    NA\n## 2       AUSTRALIA       7 June 1971    NA\n## 3         AUSTRIA 29 September 1961    NA\n## 4         BELGIUM 13 September 1961    NA\n## 5          CANADA     10 April 1961    NA\n## 6           CHILE        7 May 2010    NA\nWe can select and rename the country and date columns and ignore the empty first and last columns using select():\noecd.countries.raw1 &lt;- oecd.countries.raw %&gt;%\n  select(Country = X2, Date = X3)\n\noecd.countries.raw1 %&gt;% head()\n\n## # A tibble: 6 x 2\n##     Country              Date\n##       &lt;chr&gt;             &lt;chr&gt;\n## 1   Country              Date\n## 2 AUSTRALIA       7 June 1971\n## 3   AUSTRIA 29 September 1961\n## 4   BELGIUM 13 September 1961\n## 5    CANADA     10 April 1961\n## 6     CHILE        7 May 2010\nBecause R didn’t recognize the header, it included the header as an actual row of data. Additionally, the webpage put a note about membership in the final row:\noecd.countries.raw1 %&gt;% tail()\n\n## # A tibble: 6 x 2\n##                              Country                               Date\n##                                &lt;chr&gt;                              &lt;chr&gt;\n## 1                             SWEDEN                  28 September 1961\n## 2                        SWITZERLAND                  28 September 1961\n## 3                             TURKEY                      2 August 1961\n## 4                     UNITED KINGDOM                         2 May 1961\n## 5                      UNITED STATES                      12 April 1961\n## 6 More on membership and enlargement More on membership and enlargement\nWe can use slice() to select all rows in between the first and last, starting from row 2 to row n() - 1:\noecd.countries.raw2 &lt;- oecd.countries.raw1 %&gt;%\n  slice(2:(n() - 1))\n\noecd.countries.raw2 %&gt;% head()\n\n## # A tibble: 6 x 2\n##          Country              Date\n##            &lt;chr&gt;             &lt;chr&gt;\n## 1      AUSTRALIA       7 June 1971\n## 2        AUSTRIA 29 September 1961\n## 3        BELGIUM 13 September 1961\n## 4         CANADA     10 April 1961\n## 5          CHILE        7 May 2010\n## 6 CZECH REPUBLIC  21 December 1995\noecd.countries.raw2 %&gt;% tail()\n\n## # A tibble: 6 x 2\n##          Country              Date\n##            &lt;chr&gt;             &lt;chr&gt;\n## 1          SPAIN     3 August 1961\n## 2         SWEDEN 28 September 1961\n## 3    SWITZERLAND 28 September 1961\n## 4         TURKEY     2 August 1961\n## 5 UNITED KINGDOM        2 May 1961\n## 6  UNITED STATES     12 April 1961\nFinally, we can use the countrycode package to convert the country names into Correlates of War (COW) codes:\noecd.countries &lt;- oecd.countries.raw2 %&gt;%\n  mutate(cowcode = countrycode(Country, \"country.name\", \"cown\"))\n\noecd.countries %&gt;% glimpse()\n\n## Observations: 35\n## Variables: 3\n## $ Country &lt;chr&gt; \"AUSTRALIA\", \"AUSTRIA\", \"BELGIUM\", \"CANADA\", \"CHILE\", \"CZECH REPUBLIC\", \"DENMARK\", \"ESTONIA\", \"FINL...\n## $ Date    &lt;chr&gt; \"7 June 1971\", \"29 September 1961\", \"13 September 1961\", \"10 April 1961\", \"7 May 2010\", \"21 Decembe...\n## $ cowcode &lt;int&gt; 900, 305, 211, 20, 155, 316, 390, 366, 375, 220, 255, 350, 310, 395, 205, 666, 325, 740, 732, 367, ...\nBefore we start plotting, we have to manipulate and filter the data a little bit more. First, we’ll create a data frame of just US Polity scores, selecting only countries with a COW code of 2 (which is the US)\nus.polity &lt;- polity %&gt;%\n  filter(cowcode == 2)\n\nus.polity %&gt;% head()\n\n## # A tibble: 6 x 3\n##   cowcode  year polity\n##     &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n## 1       2  2000     10\n## 2       2  2001     10\n## 3       2  2002     10\n## 4       2  2003     10\n## 5       2  2004     10\n## 6       2  2005     10\nWe then want to calculate the average Polity score for all OECD countries over time, excluding the US. We select all rows with a COW code in oecd.countries$cowcode using %in% in the filter query and then exclude the US with cowcode != 2. We then calculate the yearly mean with group_by() and summarise():\noecd.polity &lt;- polity %&gt;%\n  filter(cowcode %in% oecd.countries$cowcode,\n         cowcode != 2) %&gt;%\n  group_by(year) %&gt;%\n  summarise(polity = mean(polity, na.rm=TRUE))\n\noecd.polity %&gt;% head()\n\n## # A tibble: 6 x 2\n##    year   polity\n##   &lt;int&gt;    &lt;dbl&gt;\n## 1  2000 9.454545\n## 2  2001 9.454545\n## 3  2002 9.484848\n## 4  2003 9.484848\n## 5  2004 9.484848\n## 6  2005 9.484848\nIn the final plot, we want to include shaded regions showing Polity’s general classifications of regime type, where autocracies range from −10 to −6, anocracies range from −5 to 5, and democracies range from 6 to 10. We can use dplyr::tribble() to quickly create a small data frame with these ranges. The mutate() command at the end uses forcats::fct_inorder to change the democracy column into an ordered factor so the ranges are plotted in the correct order. Finally, to prevent gaps, I add/subtract 0.5 from the start and end values (i.e. since democracies end at 6 and anocracies start at 5, there would be an empty gap in the plot between 5 and 6).\npolity.breaks &lt;- tribble(\n  ~start, ~end, ~democracy,\n  10, 5.5, \"Democracy\",\n  5.5, -5.5, \"Anocracy\",\n  -5.5, -10, \"Autocracy\"\n) %&gt;%\n  mutate(democracy = fct_inorder(democracy, ordered = TRUE))\n\npolity.breaks\n\n## # A tibble: 3 x 3\n##   start   end democracy\n##   &lt;dbl&gt; &lt;dbl&gt;     &lt;ord&gt;\n## 1  10.0   5.5 Democracy\n## 2   5.5  -5.5  Anocracy\n## 3  -5.5 -10.0 Autocracy\nNow that we have all the cleaned up data frames, we can finally put it all together in one final plot. Check the heavily commented code below for an explanation of each layer\n# We start with an empty ggplot object since we're using so many separate data frames\nggplot() +\n  # First we include the shaded ranges for democracies, anocracies, and\n  # autocracies. Since we want the regions to go from edge to edge\n  # horizontally, we set xmin and xmax to ±Inf. ymin and ymax use the ranges we\n  # created in the polity.breaks data frame. We use democracy as the fill\n  # variable. The alpha value makes the layer 80% transparent.\n  geom_rect(data = polity.breaks, aes(xmin = -Inf, xmax = +Inf,\n                                      ymin = end, ymax = start,\n                                      fill = democracy),\n            alpha = 0.2) +\n  # We then add the line for the US polity score\n  geom_line(data = us.polity, aes(x = year, y = polity), size = 2, color = \"#00A1B0\") +\n  # And then the line for the average OECD polity score\n  geom_line(data = oecd.polity, aes(x = year, y = polity), size = 2, color = \"#EB6642\") +\n  # We add a label for the US line above the line at y = 11, using the same color\n  geom_text(aes(x = 2000, y = 11, label = \"United States\"),\n            hjust = 0, family = \"Open Sans Condensed Bold\", color = \"#00A1B0\") +\n  # We add a similar label for the OECD line at y = 8.5, again with the same color\n  geom_text(aes(x = 2000, y = 8.5, label = \"OECD average\"),\n            hjust = 0, family = \"Open Sans Condensed Bold\", color = \"#EB6642\") +\n  # All of the data happens in the democracy region, but we want to show the\n  # full range of Polity values, so we force the y axis to go from -10 to 11\n  coord_cartesian(ylim = c(11, -10)) +\n  # Remove the \"democracy\" title from the legend\n  guides(fill = guide_legend(title = NULL)) +\n  # Add titles and labels and captions\n  labs(x = NULL, y = \"Polity IV score\",\n       title = \"Democracy in the USA\", subtitle = \"I wonder what happened in 2016…\",\n       caption = \"Source: Polity IV Project\") +\n  # Use a light theme with Open Sans Light as the default font\n  theme_light(base_family = \"Open Sans Light\") +\n  # Move the legend to the bottom and make the title font bigger, bolder, and condenseder\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(family = \"Open Sans Condensed Bold\",\n                                  size = rel(1.6)))\n\n\n\nPolity in the US and OECD\n\n\nOnce we have this basic plot, we can extend it with more data. For instance, we can compare the US’s Polity score not only to the OECD average, but to specific countries:\nlots.of.countries &lt;- c(\"United States\", \"Hungary\", \"Turkey\", \"Poland\", \"Mexico\") %&gt;%\n  countrycode(\"country.name\", \"cown\")\n\nlots.of.countries.polity &lt;- polity %&gt;%\n  filter(cowcode %in% lots.of.countries) %&gt;%\n  mutate(country = countrycode(cowcode, \"cown\", \"country.name\"))\n  \nggplot() +\n  geom_rect(data = polity.breaks, aes(xmin = -Inf, xmax = +Inf,\n                                      ymin = end, ymax = start,\n                                      fill = democracy),\n            alpha = 0.2) +\n  # First we add the line for the average OECD polity score. This time we make it dashed\n  geom_line(data = oecd.polity, aes(x = year, y = polity),\n            size = 1, color = \"#EB6642\", linetype = \"dashed\") +\n  # We then add the lines for all the other countries\n  geom_line(data = lots.of.countries.polity,\n            aes(x = year, y = polity, color = country), size = 1) +\n  # To create the labels we only select the 2005 values. If we didn't, labels\n  # would appear on every year, and we'd essentially have a plot of repeated\n  # labels\n  geom_label_repel(data = filter(lots.of.countries.polity, year == 2005),\n                   aes(x = year, y = polity, color = country, label = country),\n                   family = \"Open Sans Condensed Bold\") +\n  # We add a label for the OECD line at y = 8.5, again with the same color\n  geom_label(aes(x = 2000, y = 8.5, label = \"OECD average\"),\n            hjust = 0, family = \"Open Sans Condensed Bold\", color = \"#EB6642\") +\n  coord_cartesian(ylim = c(11, -10)) +\n  # Turn off the legend for the color aesthetic\n  guides(fill = guide_legend(title = NULL),\n         color = FALSE) +\n  labs(x = NULL, y = \"Polity IV score\",\n       title = \"Democracy around the world\",\n       subtitle = \"Everyone's doing okay except the US (and Turkey)\",\n       caption = \"Source: Polity IV Project\") +\n  theme_light(base_family = \"Open Sans Light\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(family = \"Open Sans Condensed Bold\",\n                                  size = rel(1.6)))\n\n\n\nPolity in the US, OECD, and others\n\n\nAnd thus we can quickly and reproducibly see that democracy in the USA post-2016 is pretty precariously positioned. (But we’re not Turkey. Yet.)"
  },
  {
    "objectID": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html",
    "href": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html",
    "title": "Super basic practical guide to Docker and RStudio",
    "section": "",
    "text": "All the cool data science kids seem to be using Docker these days, and being able to instantly spin up a pre-built computer with a complete development or production environment is magic. The R community has also jumped on the Docker whale, and rOpenSci maintains dozens of pre-built Docker images. These images are well documented and there are helpful guides explaining how to get started.\nRead all that first. This post doesn’t explain how Docker works. Instead, it’s a quick super basic beginner’s guide about how I use these Docker images in real-world R development and research.\nContents:"
  },
  {
    "objectID": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#why-even-do-this",
    "href": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#why-even-do-this",
    "title": "Super basic practical guide to Docker and RStudio",
    "section": "Why even do this",
    "text": "Why even do this\nI’ve found two general reasons for running R in a Docker container:\n\nReproducibility and consistent development environments: Python virtual environments are awesome—anyone can install all the packages/libraries your script needs in a local Python installation. R doesn’t have virtual environments. R has packrat, which is incorporated into RStudio, but it’s a hassle and I hate using it and can never get it working right.\nInstead of using packrat, you can develop an R project within a Docker container. (This idea comes from a Twitter conversation with (noamross?).) Create a custom Dockerfile for your project where you install any additional packages your project needs (more on that below), and then develop your project in the browser-based RStudio from the container. (Or, be lazy like me and keep developing in your local R installation without containers and periodically check to make sure it all works in a pristine development environment in a container).\nAll someone needs to do to run your project is pull the Docker image for your project, which will already have all the packages and dependencies and extra files installed.\nIn an ideal world, you can create a Dockerfile for a specific project, develop everything in RStudio in the browser, and the distribute both the Dockerfile and the project repository so others can reproduce everything.\nOffloading computationally intensive stuff: I often build Bayesian models with Stan and rstanarm. Complicated Bayesian models take forever to run, though, because of long Monte Carlo Markov chains (it takes hours to run dozens of Bayesian random effects effects models with rstanarm::stan_glmer(), for instance). Rather than tie up my computer and make it so I can’t put it to sleep, I create a virtual server on DigitalOcean and run these intensive scripts in a Docker container there."
  },
  {
    "objectID": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#run-locally-with-a-gui",
    "href": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#run-locally-with-a-gui",
    "title": "Super basic practical guide to Docker and RStudio",
    "section": "Run locally with a GUI",
    "text": "Run locally with a GUI\nThe easiest possible way to play with Docker is to use Kitematic, Docker’s own cross-platform GUI. Kitematic automatically installs Docker and all its dependencies and it runs a virtual Linux machine in the background, which then runs Docker inside. With Kitematic you can search for public images and builds on Docker Hub and manage all your running containers without having to deal with the command line.\n\nDownload and install Kitematic\nSearch for a Docker image, like rocker/tidyverse and create it\n\n\n\nSearch for a container in Kitematic\n\n\nAccess a containerized RStudio installation at the URL shown in Kitematic (in this case 192.168.99.100:32769 (default username and password are both rstudio)\n\n\n\nRStudio running in a Docker container in Kitematic\n\n\nStop the container when you’re all done"
  },
  {
    "objectID": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#run-on-a-remote-server",
    "href": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#run-on-a-remote-server",
    "title": "Super basic practical guide to Docker and RStudio",
    "section": "Run on a remote server",
    "text": "Run on a remote server\nRunning a container remotely lets you offload computationally intensive tasks. There’s no fancy GUI for starting remote containers, but it’s easy enough.\n\nCreate a DigitalOcean droplet. Use one of the pre-built Docker one-click apps and choose some size for your server (number of CPUs and amount of RAM).\n\n\n\nCreate DigitalOcean droplet with Docker pre-installed\n\n\nOnce the server is live, log into it via ssh from your local machine: ssh root@ip_address_here\nGet the latest version of the Docker image of your choice by typing docker pull rocker/tidyverse\nRun that image docker run -d -p 8787:8787 rocker/tidyverse\nAccess the remote RStudio installation at ip_address_here:8787\nStop the container or destroy the DigitalOcean droplet when you’re done.\n\nBonus things:\n\nIf you select “User data” in the additional options section when creating the droplet, you can paste a cloud-config file that contains a list of directives to run when the droplet is initialized.\n\n\n\nUser data option in DigitalOcean\n\n\nYou can use this to automatically pull and run the Docker image when you create the droplet so there’s no need to even SSH into the new machine. Just wait a couple minutes for Docker to do its thing and you’ll have an RStudio installation waiting at ip_address_here:8787.\nHere’s a basic cloud-config file I use to automatically start the server and set a password:\n#cloud-config\nruncmd:\n  - docker run -d -p 8787:8787 -e PASSWORD=supersekrit rocker/tidyverse\nIf you upload your public SSH key to DigitalOcean you can automatically add it to the droplet when creating it, making it so you can log in via SSH without a password.\n\n\n\nEnable SSH key in DigitalOcean"
  },
  {
    "objectID": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#getting-stuff-in-and-out-of-the-container",
    "href": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#getting-stuff-in-and-out-of-the-container",
    "title": "Super basic practical guide to Docker and RStudio",
    "section": "Getting stuff in and out of the container",
    "text": "Getting stuff in and out of the container\nWhen you pull an image from Docker Hub, they’re typically empty and pristine. You have to add your own files and data. Also, Docker containers are ephemeral. Once you stop running a container, any data stored in it disappears—when you start the container again, it’ll be back in its pristine state.\nThere are multiple ways to get files in and out of a container though:\n\nLocally mounted volume: It’s possible to run a Docker container that mounts a local folder into the container’s file system, making local files accessible to the container. Setting this up in Kitematic is trivial—change the container’s “Volumes” setting to map the built-in kitematic folder to some local directory on your computer, and anything you save in the kitematic folder within the container will actually be saved on your computer. Setting this up from terminal (like on a remote computer) is trickier (Google “docker mount local directory”). However, since I create and delete remote servers willy nilly, I don’t ever really get around to mounting directories remotely.\n\n\n\nVolume mounting settings in Kitematic\n\n\nUpload and download with RStudio: You can use the “Upload” button in the “Files” panel in RStudio to get individual files or whole directories into the container. Use “More &gt; Export” to download files.\n\n\n\nUpload and download with RStudio\n\n\ngit: You can clone and pull a git repository using RStudio.\n\n\n\nGit push/pull in RStudio"
  },
  {
    "objectID": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#running-on-a-remote-server-without-rstudio",
    "href": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#running-on-a-remote-server-without-rstudio",
    "title": "Super basic practical guide to Docker and RStudio",
    "section": "Running on a remote server without RStudio",
    "text": "Running on a remote server without RStudio\nWhen running computationally intensive stuff, RStudio in the browser can sometimes cause problems. I often can’t get back into the session if it’s stuck running a script for hours, meaning I just have to trust that it’s working, which is awful.\nInstead, I run long scripts in the background without using RStudio and monitor them using a log file.\n\nLog in to the remote server with ssh.\nFind the name or the ID of the Docker container that’s running RStudio by running docker ps\nConnect to that container by running docker exec -it hash_of_container bash (like fcab1f00f1d1) or docker exec -it name_of_container bash. You’re now inside the container.\nNavigate to the script you want to run: cd home/rstudio/name_of_your_file.R\nRun the script. There are a billion different ways to do this, like with screen or tmux or dtach. I find it easiest to use nohup and redirect the output of the script to a log file. I run nohup Rscript name_of_your_file.R &gt; process_of_script.log 2&gt;&1 & to send all output to process_of_script.log.\nMonitor the script periodically. You can do this in RStudio by logging in and opening the log file (and because the script is running in the background, there won’t be any problems restoring an RStudio session), or by typing tail process_of_script.log from the terminal inside the container."
  },
  {
    "objectID": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#making-a-custom-dockerfile",
    "href": "blog/2017/04/27/super-basic-practical-guide-to-docker-and-rstudio/index.html#making-a-custom-dockerfile",
    "title": "Super basic practical guide to Docker and RStudio",
    "section": "Making a custom Dockerfile",
    "text": "Making a custom Dockerfile\nThe base rocker/tidyverse container has most of the packages I typically need for projects, but I also regularly use packages that it doesn’t have by default, like pander, ggstance, and countrycode. Because containers come in a pristine state, you have to install all additional packages by hand in a new container, which is tedious and not reproducible.\nTo fix this, you can create your own Dockerfile that is based on another Dockerfile. This is actually what all the rocker images do—they’re based on a basic rocker/r-base image and then add additional installation commands and directives (for instance, rocker/tidyverse is based on rocker/rstudio which is based on rocker/r-base). If you make your own Dockerfile based on your favorite rocker image, you can host it as an automated build on Docker Hub and then pull it just like any other image.\nMaking Dockerfiles is tricky and full of tons of options that are well covered elsewhere. Here are some examples I’ve made:\n\nSuper basic easy example: This is rocker/tidyverse with the CRAN versions of pander, stargazer, countrycode, and WDI and the development version of ggrepel.\ntidyverse_rstanarm: This installs Stan and rstanarm in the rocker/tidyverse image. Installing Stan from source is tricky and it took me hours to get the right incantation of commands, but it works!\ndocker-donors-ngo-restrictions: This is tidyverse_rstanarm with all the packages needed for a specific project. It does fancy stuff like installing custom fonts and some Python packages.\n\nYou can host these custom Dockerfiles on GitHub and connect them to Docker Hub as automated builds. Every time you push to the GitHub repository, Docker Hub will automatically rebuild the image so when people run docker pull repository/name, they’ll get the latest version. Magic."
  },
  {
    "objectID": "blog/2016/04/25/convert-logistic-regression-standard-errors-to-odds-ratios-with-r/index.html",
    "href": "blog/2016/04/25/convert-logistic-regression-standard-errors-to-odds-ratios-with-r/index.html",
    "title": "Convert logistic regression standard errors to odds ratios with R",
    "section": "",
    "text": "Converting logistic regression coefficients and standard errors into odds ratios is trivial in Stata: just add , or to the end of a logit command:\n. use \"http://www.ats.ucla.edu/stat/data/hsbdemo\", clear\n\n. logit honors i.female math read, or\n\nLogistic regression                             Number of obs     =        200\n                                                LR chi2(3)        =      80.87\n                                                Prob &gt; chi2       =     0.0000\nLog likelihood = -75.209827                     Pseudo R2         =     0.3496\n\n------------------------------------------------------------------------------\n      honors | Odds Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n      female |\n     female  |   3.173393   1.377573     2.66   0.008      1.35524    7.430728\n        math |   1.140779   .0370323     4.06   0.000     1.070458     1.21572\n        read |   1.078145    .029733     2.73   0.006     1.021417    1.138025\n       _cons |   1.99e-06   3.68e-06    -7.09   0.000     5.29e-08    .0000749\n------------------------------------------------------------------------------\nDoing the same thing in R is a little trickier. Calculating odds ratios for coefficients is trivial, and exp(coef(model)) gives the same results as Stata:\n# Load libraries\nlibrary(dplyr)  # Data frame manipulation\nlibrary(readr)  # Read CSVs nicely\nlibrary(broom)  # Convert models to data frames\n\n# Use treatment contrasts instead of polynomial contrasts for ordered factors\noptions(contrasts=rep(\"contr.treatment\", 2))\n\n# Load and clean data\ndf &lt;- read_csv(\"http://www.ats.ucla.edu/stat/data/hsbdemo.csv\") %&gt;%\n  mutate(honors = factor(honors, levels=c(\"not enrolled\", \"enrolled\")),\n         female = factor(female, levels=c(\"male\", \"female\"), ordered=TRUE))\n\n# Run model\nmodel &lt;- glm(honors ~ female + math + read, data=df, family=binomial(link=\"logit\"))\nsummary(model)\n#&gt;\n#&gt; Call:\n#&gt; glm(formula = honors ~ female + math + read, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt;\n#&gt; Deviance Residuals:\n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -2.0055  -0.6061  -0.2730   0.4844   2.3953  \n#&gt;\n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  -13.12749    1.85080  -7.093 1.31e-12 ***\n#&gt; femalefemale   1.15480    0.43409   2.660  0.00781 ** \n#&gt; math           0.13171    0.03246   4.058 4.96e-05 ***\n#&gt; read           0.07524    0.02758   2.728  0.00636 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt;\n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt;\n#&gt;     Null deviance: 231.29  on 199  degrees of freedom\n#&gt; Residual deviance: 150.42  on 196  degrees of freedom\n#&gt; AIC: 158.42\n#&gt;\n#&gt; Number of Fisher Scoring iterations: 5\n\n# Exponentiate coefficients\nexp(coef(model))\n#&gt;  (Intercept) femalefemale         math         read \n#&gt; 1.989771e-06 3.173393e+00 1.140779e+00 1.078145e+00\n\n# Exponentiate standard errors\n# WRONG\nses &lt;- sqrt(diag(vcov(model)))\nexp(ses)\n#&gt;  (Intercept) femalefemale         math         read \n#&gt;     6.364894     1.543557     1.032994     1.027961\nCalculating the odds-ratio adjusted standard errors is less trivial—exp(ses) does not work. This is because of the underlying math behind logistic regression (and all other models that use odds ratios, hazard ratios, etc.). Instead of exponentiating, the standard errors have to be calculated with calculus (Taylor series) or simulation (bootstrapping). Stata uses the Taylor series-based delta method, which is fairly easy to implement in R (see Example 2).\nEssentially, you can calculate the odds ratio-adjusted standard error with \\(\\sqrt{\\text{gradient} \\times \\text{coefficient variance} \\times \\text{gradient}}\\), and since the first derivative/gradient of \\(e^x\\) is just \\(e^x\\), in this case the adjusted standard error is simply \\(\\sqrt{e^{\\text{coefficient}} \\times \\text{coefficient variance} \\times e^{\\text{coefficient}}}\\) or \\(\\sqrt{(e^{\\text{coefficient}})^2 \\times \\text{coefficient variance}}\\)\nDoing this in R is easy, especially with broom::tidy():\nmodel.df &lt;- tidy(model)  # Convert model to dataframe for easy manipulation\nmodel.df\n#&gt;           term     estimate  std.error statistic      p.value\n#&gt; 1  (Intercept) -13.12749111 1.85079765 -7.092883 1.313465e-12\n#&gt; 2 femalefemale   1.15480121 0.43408932  2.660285 7.807461e-03\n#&gt; 3         math   0.13171175 0.03246105  4.057532 4.959406e-05\n#&gt; 4         read   0.07524236 0.02757725  2.728422 6.363817e-03\n\nmodel.df %&gt;% \n  mutate(or = exp(estimate),  # Odds ratio/gradient\n         var.diag = diag(vcov(model)),  # Variance of each coefficient\n         or.se = sqrt(or^2 * var.diag))  # Odds-ratio adjusted \n#&gt;           term     estimate  std.error statistic      p.value           or\n#&gt; 1  (Intercept) -13.12749111 1.85079765 -7.092883 1.313465e-12 1.989771e-06\n#&gt; 2 femalefemale   1.15480121 0.43408932  2.660285 7.807461e-03 3.173393e+00\n#&gt; 3         math   0.13171175 0.03246105  4.057532 4.959406e-05 1.140779e+00\n#&gt; 4         read   0.07524236 0.02757725  2.728422 6.363817e-03 1.078145e+00\n#&gt;       var.diag        or.se\n#&gt; 1 3.4254519469 3.682663e-06\n#&gt; 2 0.1884335381 1.377536e+00\n#&gt; 3 0.0010537198 3.703090e-02\n#&gt; 4 0.0007605045 2.973228e-02\nThis can all be wrapped up into a simple function:\nget.or.se &lt;- function(model) {\n  broom::tidy(model) %&gt;%\n    mutate(or = exp(estimate),\n           var.diag = diag(vcov(model)),\n           or.se = sqrt(or^2 * var.diag)) %&gt;%\n    select(or.se) %&gt;% unlist %&gt;% unname\n}\n\nget.or.se(model)\n#&gt; [1] 3.682663e-06 1.377536e+00 3.703090e-02 2.973228e-02\nSame results in both programs!\n\n\n\nSame!"
  },
  {
    "objectID": "blog/2016/02/11/fauxcasts/index.html",
    "href": "blog/2016/02/11/fauxcasts/index.html",
    "title": "Fauxcasts: Use a podcast app to listen to audiobooks",
    "section": "",
    "text": "Our public library has a huge collection of books on CD, including a bunch of well dramatized versions of Shakespeare plays that I’ve always meant to read. However, out of all the music-listening devices I own, only two have optical drives: the ancient Mac Mini we use for our media center and our minivan. It’s trivial to rip the audiobooks to MP3 on the Mac Mini, but I’ve never been completely happy with the options for listening to them.\nI listen to to pretty much all spoken word media on my iPhone or iPad through either Overcast (for podcasts) or the Audible app (for audiobooks). Both of these apps allow for faster playback (up to 2.5 or 3x), and Overcast uses the fantastic Smart Speed feature to cut out the silence in between sentences.1 Apple’s native Music app has none of these extra features, which means I have to listen to CD-based audiobooks at 1x speed like a caveman.\n1 Since Overcast came out in July 2014, Smart Speed has saved me an extra 42 hours beyond speed adjustments, which is kind of ridiculously awesome.In a perfect world I’d be able fly through these CD-based audiobooks in Overcast.\nBehold that perfect world: Fauxcasts. I wrote up a little script that generates a podcast RSS feed from a folder of MP3 files. After running the script, I just have to upload the folder and feed to a server, subscribe to my fake podcast, and listen away.\n\n\n\nMacbeth as a podcast\n\n\nGet Fauxcasts from GitHub and make your own temporary podcasts."
  },
  {
    "objectID": "blog/2013/05/28/toggle-bluetooth-menu-applescript/index.html",
    "href": "blog/2013/05/28/toggle-bluetooth-menu-applescript/index.html",
    "title": "Toggle the Bluetooth menu item with AppleScript",
    "section": "",
    "text": "I like to keep my menubar as uncluttered as possible, so I keep as many items hidden as possible—especially system programs like Time Machine, the displays menu, or the volume menu.\n\n\n\nClean menu bar\n\n\nI also keep the Bluetooth menu turned off. However, when the battery runs low on either my keyboard or mouse, the Bluetooth menu item comes back, and it doesn’t turn back off after the batteries get replaced. The only way to turn it off is to go to the Bluetooth panel in System Preferences and disable the menu item manually. It’s a tiny chore, but a chore nonetheless. One that can be automated!12\n1 See this fantastic graph originally posted by Bruno Oliviera.2 And by spending 15 minutes figuring out the hidden Applescript API for System Preferences to automate a task that takes up 30 seconds every month, I totally saved time in the long run.Save this to an AppleScript application (or an Automator application) and run to (kludgingly) toggle the Bluetooth menu item.\ntell application \"System Preferences\"\n  activate\n  set the current pane to pane \"Bluetooth\"\n  delay 1\nend tell\n\ntell application \"System Events\"\n  tell process \"System Preferences\"\n    set toggleBluetooth to the checkbox \"Show Bluetooth in menu bar\" of the window \"Bluetooth\"\n    click toggleBluetooth\n  end tell\nend tell\n\ntell application \"System Preferences\"\n  quit\nend tell"
  },
  {
    "objectID": "blog/2012/07/02/gutenberg-ipsum/index.html",
    "href": "blog/2012/07/02/gutenberg-ipsum/index.html",
    "title": "Gutenberg ipsum",
    "section": "",
    "text": "I’ve decided to try to be more like Ben Crowder, Tod Robbins, and Brett Terpstra and release my code tinkerings into the public more often.\nSince upgrading to TextExpander 4 a couple weeks ago, I’ve decided to delve into it a lot more. I recently converted from Typinator, where I only really used a few expansions for typing transliterated Arabic. After belatedly stumbling upon a couple posts by Brett and Dr. Drang, I found that TextExpander can be used for some pretty fun stuff. Like random Lorem Ipsum-like text based on n-gram algorithms!\nI really liked Dr. Drang’s concept of using text from Project Gutenberg to build completely random—yet mostly sensible—dummy text, since the standard “Lorem ipsum” looks really repetitive and boring. Isn’t this Brothers Karamazov-esque paragraph a lot better?\n\nMitya uttered his wild speech. He turned quickly on his way. “Do you suppose gentlemen, that doesn’t care for an escort, for?” I am gone? You won’t be frightened and cry out: ‘I want to join the choir and shout nastiness into both ears,’ while he ran to his bedroom, lay down and I walked along both of us. Dear ones, why do you wait.\n\nOr this Homeric sounding stuff?\n\nAnd of bounding steeds: Seven captives next a little space Then rush d amid the bright band: great Ithacus, before, First of mortals! for the gods Then swift pursued her urged, and crown’d Then sunk unpitied to the dire alarms; Both breathing slaughter, follow’d and Patroclus loved remains defend. Beneath.\n\nI made a couple minor modifications to Dr. Drang’s original Perl script, clunkily removing paired characters like quotes and parentheses and allowing new corpus files to be specified with the command line, which makes it easier to repeat the code for multiple TextExpander snippets.\nHere’s how to get it working:\n\nCopy the script below and save it somewhere on your computer (mine is in ~/bin/gutenberg_ipsum/).\n\n#!/usr/bin/perl -w\n# Modified from Dr. Drang's original script at http://www.leancrew.com/all-this/2011/02/dissociated-darwin/\n\nuse Games::Dissociate;\n\n# Choose the corpus file\nif ($#ARGV == -1) {\n  $corpus = \"totc.txt\";\n} else {\n  $corpus = $ARGV[0];\n}\n\n# Slurp in the given corpus as a single string.\nopen(my $fh, \"$ENV{HOME}/bin/gutenberg_ipsum/words/\" . $corpus) or die \"Can't open\";\n{local $/; $corpus = &lt;$fh&gt;;}\n\n# Dissociate the corpus, using word pairs, and return 15-50 pairs.\n$length = int(15 + rand(35));\n$dis = dissociate($corpus, -2, $length);\n\n# Remove quotes and other paired characters, since there might be some that are unmatched\n# But this is an incredibly clunky fix. If I had more time/better Perl chops, I'd probably build some algorithm to find unmatched quotes or parentheses and insert them randomly in the text. But that's hard :)\n$dis =~ s/[\\\"\\[\\]\\_\\(\\)]//gm;\n\n# Capitalize the first word and end it with a period.\n$dis =~ s/^(.)/\\u$1/;\n$dis =~ s/[.);:?'\", -]+$/./;\n\nprint $dis;\n\nGo to Project Gutenberg and download the full text for some book (or create some other corpus), stripping out the legal text, table of contents, and anything else you don’t want the script to use to generate random text. Save that file in some folder on your computer (mine is in ~/bin/gutenberg_ipsum/words/).\nModify line 7 of the script to default to your newly downloaded and saved corpus (mine is totc.txt, for A Tale of Two Cities).\nModify line 14 to point to wherever you saved your script file and corpus file(s).\nMake the script executable (chmod +x gutenberg_ipsum.pl at the terminal).\nInstall Games::Dissociate. You can either follow Dr. Drang’s instructions or use Perl’s CPAN shell:\n\nType sudo perl -MCPAN -e shell at a terminal and hit enter for each of the configuration options if they haven’t been preset previously\nType install Games::Dissociate\nOnce everything has installed, type q to exit the Perl shell\n\nTest your script by running it from the terminal: gutenberg_ipsum.pl. You should get some fun random filler text.\nUse other corpus texts by passing their filenames as arguments: gutenberg_ipsum.pl alice.txt for Alice in Wonderland, for example. Just make sure a corpus with that name lives in your corpus directory.\n\nOnce the script is installed and running, It’s trivial to make it work with TextExpander. Make a new Shell Script snippet with this code:\n#!/usr/bin/env bash\n~/bin/gutenberg_ipsum/gutenberg_ipsum.pl princessofmars.txt\nMake as many snippets as you want—one for each of your corpus files. All you need to change in each script is the argument for the file. Replace princessofmars.txt with the name of whatever corpus you want that snippet to use.\nIn the end you can create a good collection of different automatic random filler text from any corpus you want.\n\n\n\nGutenberg ipsum snippets\n\n\nIf TextExpander 4 allowed its new Fill Ins to work as variables in a shell script, this whole collection could be a lot simpler—it could just be one dropdown menu of all the corpus files in the words folder. But since that’s not possible, this will have to do.\nAll the credit to this collection of snippets goes to Dr. Drang and Brett Terpstra. You should read their stuff—really."
  },
  {
    "objectID": "blog/2011/06/25/world-ready-composer-not-perfect/index.html",
    "href": "blog/2011/06/25/world-ready-composer-not-perfect/index.html",
    "title": "World-Ready Composer not Perfect",
    "section": "",
    "text": "Although Adobe has included the world-ready composer in InDesign CS4 and 5, like I said in my previous post, it’s not documented or supported at all. It’s still buggy and unfinished, unfortunately.\nHere’s a little example of how buggy it really is. I typeset the same sentence in Arabic in different fonts I have installed on my computer. As you can see, most of the fonts work flawlessly (yay!), with two exceptions. Traditional Arabic can’t display short vowels—they break up the connecting letters—and Geeza Pro is a sad, sad little font.\nDownload PDF of sample\n\n\n\nArabic typographic samples\n\n\nI don’t know if this is a problem with the fonts themselves or with the composer. Traditional Arabic was specially designed for Office 2007 to be a high quality Arabic font, and it comes with thousands of extra glyphs for every position possible, so it would seem like it should be able to display correctly. Just kidding. Arabic Typesetting is the special Office 2007 font and it works just fine (it has a nice Naskh-y feel to it). Traditional Arabic has been included with Windows since Windows 2000 and apparently isn’t very well made (?). Or maybe the composer is just buggy (?).\nGeeza Pro, on the other hand, definitely has a problem with the font itself. One dead forum (formerly at http://forum.redlers.com/viewtopic.php?f=1&t=2180) says that the version of Geeza Pro in Snow Leopard is faulty and doesn’t connect. Hopefully Lion fixes that…"
  },
  {
    "objectID": "blog/2011/06/19/fake-cloud-app-dropbox/index.html",
    "href": "blog/2011/06/19/fake-cloud-app-dropbox/index.html",
    "title": "Fake CloudApp with Dropbox and Quicksilver",
    "section": "",
    "text": "Recently a friend showed me CloudApp, a fantastic little app that sits in your menu bar and lets you share files almost instantly. With a quick keystroke you upload a file to their servers and get a public URL to share with anyone. Neato.\nHowever, with a basic (free) CloudApp account you are limited in the size and number of uploads you can make each month. I’m also really picky about what goes in my menu bar, and I didn’t really like having an extra app running just for when I need to upload something.\nMy current workflow for uploading and sending files is to drag a file into my Dropbox public folder, navigate to it, right click it, and copy its public Dropbox URL. I like this because Dropbox is already running on my computer (no need for extra menu bar apps) and because I feel like I have more control over the fate of my file than with CloudApp (regardless of Dropbox’s recent legal issues). Plus there aren’t any arbitrary upload or storage space restrictions.\nThe one downside to this workflow is that it’s long and convoluted and lacks the magic instantaneousness of CloudApp. So I decided to make my own magical CloudApp-esque instant upload + public URL system with programs I’m already running: Dropbox and Quicksilver.\nHere’s how to do it:\n\nDownload the “Post to Dropbox” OS X service.\nUnzip and open the Copy to Dropbox service in Automator.\nOptional: If you don’t want to clutter your public folder with random files, you can change the “Copy Finder Items” action in Automator to point to a subfolder in your public folder. Mine is set to “~/Dropbox/Public/Uploaded,” so I can periodically go and clear out any out of date uploads, like one-off screenshots and the like.\nIn the Run Shell Script section, replace &lt;DROPBOX-ID&gt; with your Dropbox user ID (the one from your public URLs, not your e-mail address).\nIf you want to use the Move to Dropbox service as well (I use both), open it and type your user ID there as well.\nMove both services to ~/Library/Services. You can now access them from the system wide services menu. Right click on any file, go to “Services” and easily move or copy any file to your public Dropbox folder and get the URL for that file placed automatically in your clipboard.\n\nWhile these services eliminate most of the clicking and dragging I was doing before, they still require three clicks (select the file, right click to navigate to the services menu, click on the service). Luckily Quicksilver simplifies the process even more. You could probably use other Quicksilver-esque programs like Alfred and the like to get the universal service keyboard shortcuts.\n\nEnable the “Services Menu Module” in Quicksliver’s plug-ins sheet in its preferences (⌘ + ,).\n\nGo to Preferences and make sure “Enable advanced features” is checked.\nGo to Catalog &gt; Quicksilver and check “Proxy Objects”\nCreate a new trigger to pipe the current selection through each of the services. I use ⌃⌥⌘U for Copy to Dropbox and ⌃⌥⇧⌘U (essentially mashing down the whole corner of my keyboard :) ) for Move to Dropbox. \nSelect a file, press your new keyboard shortcut, and voila—the file is instantly copied to Dropbox and you have a URL ready to paste somewhere. Pure magic :)\n\nOne of the only differences between this and CloudApp is that there’s no response after you use the service. There’s nothing to let you know that it actually happened. However, you can easily add a sound or Growl notification to the Automator service if you want some sort of response.\nSo there it is. Magic CloudApp functionality using Dropbox and Quicksilver!"
  },
  {
    "objectID": "blog/2011/01/27/mona-prince-on-egyptian-revolution/index.html",
    "href": "blog/2011/01/27/mona-prince-on-egyptian-revolution/index.html",
    "title": "Mona Prince on #jan25 Egyptian Protests",
    "section": "",
    "text": "Egyptian novelist and professor Mona Prince just sent me this personal report on her experience with the #jan25 protests. I’m reposting it here in its entirety.\n\na personal testimony on police brutality during protests: jan 26 2011 down town cairo—mona prince\nround 6.30 pm down town cairo, i joined the demonstration on qasr el nil street along with some friends and other people whom i don’t know. the protesters were marching peacefully and politely and decently from one street to another evading the security forces who were some how at a loss at how to stop us, especially when we took to shawarbi street, we were at the back, and the police officers behind and front talking in their walki talki, we could hear them saying they don’t know how to besiege us. we kept on moving and more people were joining us from the streets and shops, young men and women. we were chanting the requests of egyptian people ” leave ” and ” egyptian people want the regime out “. we didn’t attack any body, or destroy any shops or cars… we were moving in the middle of streets without blocking the traffic, just slowing it down until we reached sherif street and we were heading towards 26 july street, then the security forces appeared from behind and front. we started running, and security police in civilian clothes started grabbing randomly many young men and women. i saw them grab and beat a young innocent man, pushed him to the ground and kept kicking. i protested against the beating up, and kept screaming at them to stop acting like animals, 4 or 5 huge giant men grabbed me from my hair and said” well join him you bitch ” and slapped me on the face , beat me up, kicked me, and cornered me next to the young man and kept hitting me on my head, arm, shoulder, back , stepping on my head with their shoes until i bled from my mouth and could not speak any more on the ground. they kept their shoes on my head, kicked me every few seconds,calling me the dirtiest names ever the they dragged me all along the street to 26 july and threw me in one of those microbuses without number plates. they have also dragged the other young man who completely fainted and others too and threw us all in the microbus. and while pushing me inside they were trying to pull off my clothes and sexually harassed me,one grabbed my breasts , another held me my waste, and another grabbed my bottom. i tried to call friends discreetly, but they saw me, so they pull me out of the car and said ” get off ” but 3 of them were blocking the door, and they grabbed the mobile from me, then threw me to the asphalt road. despite the pain, i will go on protesting mona prince egyptian writer and university professor cairo january 27 2011\n\nشهادة عن وحشية بلطجية الامن في المظاهرات- وسط القاهرة ٢٦ يناير ٢٠١١ مني برنس  في حوالي السادسة و نصف مساء يوم الاربعاء ٢٦ يناير ٢٠١١، انضممت انا و أصدقاء و آخرين لا اعرفهم الى مسيرة سلمية احتجاجية في منطقة وسط المدينة، في شارع قصر النيل. و كان المتظاهرون يسيرون بكل ادب من شارع الي اخر في محاولة للهرب و تجنب قوات الامن المركزي الذين بدوا في حيرة و ارتباك حول كيفية محاصرتنا و تفريقنا، خاصة بعدما توجهنا الى شارع الشواربي. كنت انا و المجموعة التي معي في الخلف، و من امامنا و خلفنا ضباط الامن يتكلمون في اجهزتهم اللاسلكية و سمعناهم يقولون ما مفاده انهم لا يعرفون كيف يحاصروننا. استمررنا في السير بشجاعة و دون خوف، و اضم الينا الكثير من الشباب و البنات من الشوارع الجانبية و المحلات، و كنا نردد ” ارحل ” و ” الشعب يريد اسقاط النظام ” لم نعتدي على منشآت او سيارات و لم نهاجم احدا. كنا نسير و سط الشوارع دون أن نمنع مرور السيارات و ان كنا أبطأنا سيرها باتجاه شارع ٢٦ يوليو. ثم ظهرت قوات الامن المركزي من الامام و الخلف ر بدأت تجري نحونا بقوة، فجرينا متفرقين محاولين الاحتماء بالمحلات. و فجأة بدأ بلطجية الامن و وزارة الداخلية الذين يرتدون ملابس مدنية في القبض العشوائي علي الشباب و البنات و ضربهم بقوة. رأيتهم يقبضون على شاب كان يسير أمامي و لم يفعل اي شيء سوى الهتاف مثلنا، دفعوه بزخذيتهم و القوا به الي الأرض و انهالوا عليه ضربا و ركلا و لكما حتى اغمي عليه. اعترضت على ما يفعلون و صرخت فيهم الا يتصرفوا مثل الحيوانات و انهم ليسوا رجال و ليسوا بني آدمين، فاندفع نحوي ٤ او ٥ رجال ضخام الحجم و جروني من شعري، ضربوني علي و جهي، أخذوا يضربونني بعنف و يركلونني ثم القوا بي الى الآرض بجانب الشاب الذي فقد وعيه و هم يقولون ” طب يللا حصليه يا بنت القحبة “. فرد اخر” مرة قحبة ” و استمروا في ضربي و ركلي بضعة دقائق و يدوسون علي رأسي و جسدي بأحذيتهم حتى سال الدم من فمي و هم يسبونني بأقذر السباب و الصفات، الى ان سكت و لم استطع الكلام. ثم جرجروني و هم ما يزالرن يركلونني و يسبونني الى شارع ٢٦ يوليو ثم القوا في واحد من تلك الميكروباصات التي لا تحمل لوحة ارقام. و فعلوا نفس الشيء مع شباب اخرين و القوا بهم الى داخل نفس السيارة، من بينهم الشاب الذي كنت ادافع عنه و فقد وعيه. و في اثناء محاولتهم حشري داخل السيارة، كانوا يتحرشون بي جنسيا، و يحاولون تعريتي واحد مسكني من صدري، و الحر من وسطي و شخص اخر مسكني من اسفل ظهري ثم دفعوني الى الداخل. كان هناك نحو ٥ او ٦ و ربما اكثر شباب داخل السيارة. حاولت الاتصال بأصدقاء بشكل هامس و بعيد عن اعين الامن لكنهم رأوومي. سحبوني من داخل الميكروباص و امروني بالنزول و هم يسبونني و في نفس الوقت يسدون باب السيارة ” بتكلمي مين يا بنت ….” و خطفوا مني التلفون المحمول بالقوة ثم شدوني خارج السيارة و القوا بي الي الاسفلت.\nرغم الألم سوف استمر في الاحتجاج منى برنس كاتبة مصرية و استاذة بالجامعة القاهرة ٢٧ يناير ٢٠١١"
  },
  {
    "objectID": "blog/2010/02/28/queen-rania-at-auc/index.html",
    "href": "blog/2010/02/28/queen-rania-at-auc/index.html",
    "title": "Queen Rania at AUC",
    "section": "",
    "text": "This morning AUC’s John D. Gerhart Center for Civic Engagement hosted a lecture by Queen Rania al-Abdullah of Jordan. Queen Rania is famous for being intensely involved in the public sphere and, according to the venerable Wikipedia, is considered one of the world’s most powerful women. She’s involved with a ton of foundations and NGOs that cover a wide range of goals, from advocating for improvement in girls’ education and employment, promoting dialogue between the US and the Arab world, and calling for what became the buzzword of today’s lecture: civic engagement. She uses technology to promote her agenda of social improvement and is active on YouTube, Twitter, and Facebook. Basically a beautiful, famous, powerful world leader.\nQueen Rania focused primarily on the need for native grassroots movements in the Arab world, stating that civic engagement is an essential element of societal reform—changes so desperately needed in this region, where poverty and corruption are rampant. She stated that it is the responsibility of all citizens to take an interest in developing and improving their own country. Calling on Arabs to “look up” and think of others, she cited the example of one of AUC’s homegrown NGOs, Alashanek Ya Balady, which is heavily involved in promoting sustainable development in Egypt’s poorest neighborhoods. The efforts of AYB and dozens of other organizations constitute a laudable homegrown effort to reform Egyptian society, and they have had lots of success so far.\nThe queen then went on to decry the current state of civic laziness in the Arab world, saying that many Arabs confuse the responsibilities of “citizenship” with “sitizenship,” opting to sit still, sit back, and complain rather than take their own initiative to change society. This is not because Arabs are disconnected, emotionless people—far from it. “We’re all passionate about food, family, football, and Filistin,” she said, but those passions need to be refocused on bringing about change instead of complacently complaining about the backwardness of Arab governments and societies.\nShe developed this theme by responding to a series of preselected questions that followed her main motivational speech, declaring that universities and academia are responsible for remaining engaged with society and not remain isolated in academia. Too often the educated class remains cynically aloof from the rest of the world and fails to contribute real change or instill the culture of civic engagement in their pupils. She also noted the role of Islam in developing this culture of engagement, saying that Islam inherently advocates civic reform and grassroots movements. She called on the audience to “recapture those compassionate values of Islam” that have been hijacked by Western media and use the power of faith to help improve society.\nEssentially, she concluded, if we want to see any reform and improvement in the region, the onus is on citizens. Change in the Middle East cannot come from governments, since they are inherently inefficient. Real change can only come about with a change in attitude and an increase in engagement. Regular citizens are the driving factor for this change.\nIn the end it was a pretty motivating and inspiring speech, and I agree with most of what she said. Real, lasting change in the Middle East will need to come from some type of bottom-up popular movement, especially when the reforms so desperately needed reduce the power of the ruling class. However, repressive governments throughout the Arab world (including Rania’s Jordan) severely limit what progressive movements can do, and Middle Eastern dictatorships last for decades.\nFor example, Egypt’s current president, Hosni Mubarak, has been in power for almost 30 years. He ran the country uncontestedly until 2005, when, after pressure from George W. Bush, Mubarak amended the constitution to allow candidates from parties other than the ruling National Democratic Party (NDP) to run in presidential elections. The first “free and fair” elections were held that year. Despite a somewhat large grassroots political opposition movement, Kifaya, Mubarak won handily and his opponent, Ayman Nour, was arrested and sentenced to prison for five years. Today Kifaya, a native grassroots movement, is detoothed and powerless. Good thing civic engagement worked…\nThe next Egyptian presidential elections are slated for 2011, but since Mubarak is getting on in years (he’s 81!), he’s probably not going to run. Instead, it seems that his son, Gamal Mubarak, will inherit his father’s place. Sure, he’ll go through the farce of the election process, but the NDP politicos and thugs will pretty much guarantee his victory.\nUnderstandably, there is a growing anti-Gamal movement in Egypt, and it’s even homegrown and grassrooty. It got a huge boost last week when Mohamed ElBaradei, former head of the IAEA and winner of a Nobel Prize, returned to Cairo after a decades-long pseudo-exile. He has announced that he would consider running for president in 2011 if circumstances allowed for it. He already has a large base of support and even an official-ish campaign website. American media is picking up on him, too. He has a lot of international clout and could present a real threat to the Mubarak dynasty. He might have the potential of becoming the Egyptian Obama—“Yes we can!”\nBut (and this is a pretty big but…), he can’t even legally run—he’s not constitutionally permitted to run for president. For him to become president, Mubarak would first have to amend the constitution to open the field for more opposition parities. Then ElBaradei would actually have to win in elections run by the NDP. Yeah. Good luck there.\nWhile the possibility of an ElBaradei run has Mubarak somewhat scared, the cards are really stacked against him. This growing popular movement faces the impossible task of forcing a constitutional amendment. It’s a native movement, just like Queen Rania wants, but it is severely limited. Egyptians remain hopeful, but sadly, Gamal will most likely take over in 2011.\nWhile Jordan, as a monarchy, doesn’t face these issues of presidential elections, it has its own slew of problems with citizenship. Although Rania herself is a Palestinian, the Jordanian government regularly withdraws Jordanian nationality and rights from Palestinian refugees. How can native, grassroots Jordanian organizations reform when the government makes them stateless?\nIn her speech Rania exhorted AUCians: “Don’t wait for governments to change and reform. You must be the change you want to see.” Um. She’s in the Jordanian government. She’s the queen! She surely has some influence on the Jordanian political scene. Can’t she help the government change so that these popular groups can actually do something? Arab governments tolerate, even embrace, NGOs like AYB or the queen’s water projects in Jordan—they love this type of civic engagement. But heaven forbid these civically engaged movements threaten their despotic power, though.\nCan real governmental and societal change happen in the Middle East solely through “civic engagement.” No way. Arab governments must make changes if any popular movements want to make substantial change.\nSo, while Queen Rania’s speech was inspiring and motivational on the surface, it was full of kalaam fadi—empty words."
  },
  {
    "objectID": "blog/2009/08/18/itunes-plugin-for-flashbake/index.html",
    "href": "blog/2009/08/18/itunes-plugin-for-flashbake/index.html",
    "title": "iTunes plugin for Flashbake",
    "section": "",
    "text": "Flashbake is a fantastic script for nerdy writers (like me) that periodically commits changes to a Git repository and can optionally append various metadata to the commit message, allowing you to annotate the entire creative process.\nFlashbake includes several plugins for adding recent tweets, weather, the current time zone, and other random information. There’s even a plugin for the Banshee music player for Linux. There’s nothing for iTunes, however, which is unfortunate since I’m always listening to something when I write or code.\nSo I hacked together a little plugin for Flashbake that uses AppleScript to get the current track information from iTunes and add it to the commit message. It’s admittedly a “frankenscript” and only works on Mac OS X (since it relies on AppleScript), but it works great.\nYou can get it at GitHub. Enjoy!"
  },
  {
    "objectID": "blog/2009/07/30/alexandria-train-crash/index.html",
    "href": "blog/2009/07/30/alexandria-train-crash/index.html",
    "title": "Alexandria Train Crash",
    "section": "",
    "text": "Note\n\n\n\nSee update below (skip to update)\n\n\nAfter almost a year of being in Egypt, we finally decided to go up to Alexandria today. We took the 9:00 AM train from Cairo and the ride went smooth until we were just outside the final Mahatat Misr station, where we were delayed for over an hour until pulling up to a platform.\nImmediately after we got off the train we saw the reason for the delay. A train had rammed into the station, apparently at full speed, derailing the first three cars—the engine, the generator, and the first actual cabin. Part of the station itself was damaged, as was, ironically enough, a parked fire truck.\n\nClick to enlarge\nThe train crashed at around 8 AM and was empty. By 12:30 (when we got there), Egyptian Railway repair crews were lifting the damaged cabin off the tracks.\n\n\nBy 6:00 PM, when we arrived back at the station to return to Cairo, the repairmen were working on lifting the engine and had apparently already extricated the generator car.\n\nSince two platforms were out of commission, trains were shuffled around all day, causing major delays. Our train ride back to Cairo took over four hours instead of the usual two.\nAs far as I know, nobody was killed as the train was empty. I don’t know about injuries, since I arrived on the scene several hours after the actual accident.\nNeedless to say, it was exciting—in a macabre sort of way. I’ve read so much about Egyptian train crashes—I got to see the aftermath of one. :)\n\n\nI’ll post the full set of pictures tomorrow on Flickr.\n\n\n\nUpdate\n\nI posted our photos of the accident at Flickr. They are licensed under a Creative Commons Attribution license, so you can use them however you want.\nNancy has also blogged about the accident at our family blog.\nAdditionally, the online Egyptian news site Youm7 has posted an update (in Arabic). According to their report, the accident was caused by a brake failure as the train travelled from the nearby Sidi Gaber train station. It was indeed empty, fortunately, and there were no fatalities. Two of the crew members were injured in addition to a vendor—since there are snack booths at the end of each platform, he must have gotten smashed. There are conflicting statements as to the cause of the accident. Some claim neglect and disrepair; others sabotage. I’m leaning towards neglect—those trains are ancient."
  },
  {
    "objectID": "blog/2009/07/28/on-narrowing-and-redefining-research/index.html",
    "href": "blog/2009/07/28/on-narrowing-and-redefining-research/index.html",
    "title": "On narrowing and redefining research",
    "section": "",
    "text": "I’ve been in Egypt for almost a year now, studiously working towards my MA in Middle East Studies. The supposed capstone of my time here at AUC—my thesis—now looms ahead somewhat menacingly. I get to spend the next several months researching and writing what will end up being my largest research project to date. It’ll also set the foundation for my (hopefully) future PhD plans.\nThere’s only one problem: I don’t quite know what I’m writing it on.\nAt BYU I double majored in Middle East Studies/Arabic (MESA) and Italian—an odd mix of modern history, political science, and renaissance poetry. Most of my research focused on finding literary and historic connections between the Middle East and Italy. I looked at the role of Sufism in the birth of Catholic mysticism, especially with Jacopone da Todi and St. Francis of Assisi. I looked at the Young Ottomans and their reliance on Mazzini’s Giovine Italia ideology. I even presented a paper at a conference about the role of Mohammed in Inferno XXVIII in Dante’s Divine Comedy, connecting it to proto-orientalism.\nFun times :)\nAt the same time, though, there was an inherent conflict in my research interests. I wrote my MESA capstone paper on the media coverage of the 2006 Israel/Lebanon war, where no Italian literature was involved :). I love media—I’m obsessed with the news, the internet, blogs, Twitter; anything shiny, new, and exciting.\nI started my MA with the assumption that I’d have to choose one of these tracks—history or media. I dove headlong into the history track, writing a huge literature review on the history of the Italians in Egypt at the beginning of the 20th century. Large Italian communities in Cairo, Alexandria, and Ismailiyya sprang up after Napoleon’s 1798 invasion. Thousands of Italians were born and raised abroad in these cities, and many took part in the Egyptian nationalist movement, considering themselves more Egyptian than Italian, thus earning themselves the nickname mutamasirun (those who try to be Egyptian).\nI was excited about this research until a new shiny thing took my attention away—Twitter. I started using Twitter more or less full time in January, which then introduced me to the large Egyptian Twitter community. Many of the Egyptian twitterers are also activist bloggers who have been arrested multiple times. I began to see the power and potential of the internet in political reform and change in the Middle East and switched research gears to focus on the Middle Eastern blogosphere. It was fascinating stuff, but I felt like I had turned my back on history :) Further complicating things, I had an amazing history seminar last semester that resulted in some awesomely fun archival research. I had a blast writing the paper. The primeval dichotomy of history vs. media reared its ugly head again.\nSince I’m nearing the end of my MA, I’m looking at different PhD programs to go to once I’m done here. There are lots that look at bloggers and politics—even ones that look at bloggers and politics and the Middle East—but none do it with a historical approach (fancy that… blogs have been around for something like four years and it’s not history yet :) ). I sent out dozens of e-mails to different professors asking about graduate programs and my potential research. Most responses were confused; history != new media.\nHowever, one professor at Cornell responded positively. He studies media in modern Egyptian history, specifically in the time period of my Italian mutamasirun.\nI think I may have found a way to bridge the history/media studies gap. Now I just need to figure out exactly how to do it.\nAwesome. :)\nSo, sorry bloggers—I’ll keep following you as a tangential fascination, but my heart lies in history."
  },
  {
    "objectID": "blog/2009/06/19/pdftk-php-officially-released/index.html",
    "href": "blog/2009/06/19/pdftk-php-officially-released/index.html",
    "title": "pdftk-php Officially Released",
    "section": "",
    "text": "Wow. It’s been almost two years since I wrote a little tutorial on how to use LiveCycle, PHP, and MySQL together to make a web application that served dynamic PDF forms. Since then it has become the number one page on this site. I still get a substantial number of comments a week here on the blog and via e-mail—many of those comments are stuck in my inbox, sent to me before I rebuilt my site on WordPress and enabled commenting.\nUnfortunately, though, I wrote that tutorial as my first foray into the world of PHP/MySQL web development and had little idea of what I was really doing. Since then, however, I’ve done a fair amount of real-world web design and development, and even implemented this pdftk form system into a live, public-facing application.\nIn the interim, I’ve refined my system and released it as an open source PHP class, named pdftk-php. The project is hosted at GitHub, a brilliant hosting service for collaborative projects, which uses Git, the best version control software I’ve ever used. Anyone can check out, or clone, the project, make any edits to the core set of classes, and merge those with the main project branch—it is now a true community project. If you don’t want to contribute, you can still download it from GitHub as a .zip or .tar file.\nIncluded in the project is a (hopefully) extensively documented example application that you can set up on your own server.\nI’m working on writing a new step-by-step tutorial on how to set everything up, akin to the old one. In theory, the new pdftk-php should work with PDFs created in any program, not just LiveCycle. In the meantime, download pdftk-php, try it out, and report any bugs here on the blog or directly at GitHub (where I hope to keep everything related to pdftk-php from now on). Fork the project and contribute if you feel like it, too!\nThanks and good luck!"
  },
  {
    "objectID": "blog/2009/05/01/google-profile-business-cards/index.html",
    "href": "blog/2009/05/01/google-profile-business-cards/index.html",
    "title": "Google Profile Business Cards",
    "section": "",
    "text": "Look what I just ordered, for free:\n\n\n\nGoogle Profile Business Card\n\n\nAwesome :)\nGoogle is giving packs of 25 of these away for free to promote their new Google Profiles service. I think the profile idea is great—it makes finding people online so much easier and gives you more control over what Google says about you.\nToo bad I won’t get these until August when my mother-in-law comes to visit."
  },
  {
    "objectID": "blog/2009/03/18/libya-obsolete-paradigms-and-rip-van-winkle/index.html",
    "href": "blog/2009/03/18/libya-obsolete-paradigms-and-rip-van-winkle/index.html",
    "title": "Libya, obsolete paradigms, and Rip Van Winkle",
    "section": "",
    "text": "I’m still experimenting with what I’m going to blog about here. I’m thinking of sticking with an odd mixture of technology and academia, which might work out well since I’m considering doing my upcoming thesis on technology in the Middle East, specifically blogging. So, here’s a purely academic post to start things off.\nOn Tuesday Dr. Lisa Anderson, AUC’s Provost and former professor at Columbia University, came to speak at a faculty seminar at the Middle East Studies Center. She’s a political scientist by training and started her academic career by studying Libya. Her talk focused on the role of traditional social studies disciplines in the study of the Middle East.\nGeneral social studies separated into more specialized disciplines in the early 1900s as progressive governments in the US and Europe became increasingly interested in understanding the dynamics of the social “here and now.” New disciplines of social studies emerged with the explicit purpose of improving governance, public administration, and economy. Economists researched and theorized how to stabilize and grow a capitalist economy; political scientists looked at the processes that created a stable democracy; sociologists tackled the dynamics of the changing progressive societies. Social scientists not explicitly concerned with the issues of “here and now” also emerged; those not interested in “here” entered anthropology while those not interested in “now” created the discipline of history.\nOver the past century, scholarship in these disciplines has been rather entrenched along this foundational paradigm: research to improve capitalism and democracy. Because of this disciplinary emphasis and paradigm of US/Western-style economics and political systems, any research on other systems of government or economies are always done in terms of the “Western” social science paradigm.\nFor example, every class I’ve taken on Middle East politics always focuses on the processes of democratization and the persistence of authoritarianism. Elections (fair or not), development of civil society, the emergence of political parties—these are signs of “political development” for a political scientist. Economists look at how liberalized a state’s economy is—how open for investment and primed for capitalism it is. In a way, strict disciplinary social studies of societies outside the US and Europe seem to place other countries on a linear progressive timeline; studies of Egyptian politics reveal how democratized Egypt is or isn’t.\nNot all countries—not even Egypt—fit into this standard disciplinary framework. Did Nasser really care if his Arab Socialist revolution fit into a future program of democratization? No way. He didn’t even think of it. Arab political scientists don’t generally look at it that way.\nThis theme of trying to fit the Middle Eastern peg in the academic square is well pronounced in Libya, which, because international sanctions for the past few decades, has been pretty isolated. According to Dr. Anderson, the case of Libya shows the limits of traditional Western social sciences.\nMuammar Qaddafi, the crazy Libyan dictator surrounded by female bodyguards and who made up his country’s name, has ruled Libya for 40 years. He’s been operating with a unique paradigm of political and economic theory that doesn’t jive with the standard Western view. He published a three volume exposition of his political and social views in 1975 called The Green Book. Qaddafi’s view of Libya’s political paradigm is completely different from Western disciplinary political science. For Qaddafi, the ideal political system is more of a radical, romantic, Rousseauian world where each individual represents themselves and has an innate skepticism of the state. His theory of the Libyan economy makes no mention of the market, the main protagonist of “standard” economics. Sociologically, the individual in Qaddafi’s Libya is not the fundamental unit of analysis—the family or tribe is. In practice there are no political parties or parliaments in Libya; each community runs itself with town hall-esque meetings. His social theory seems to be working for the most part.\nIn the 80s, because of different international incidents involving Libya, including the bombing of Pan Am Flight 103 over Lockerbie, America and most of the international community imposed strict sanctions on Libya, aimed at isolating and punishing them. All relations between America and Libya were effectively destroyed and Libya entered what she called a “Rip Van Winkle” era; life continued in total isolation in Libya under Qaddafi’s political and social theory.\nIn 2004 (about 20 years later…just like Rip Van Winkle, oddly enough) the sanctions eased and American and the rest of the world began diplomatic and academic contacts again. Because of the sanctions, Libya had missed out on the techno-political revolution of the internet. There were no banks; nobody knew what a credit card was. Dr. Anderson visited several universities in 2004 after this long isolation and found a globe in the center of the reference area of the library—North Asia was still labeled as the USSR.\nDon’t go under international sanctions on the eve of a technological revolution. It’s not a good idea. At all.\nAs the conflict and tension between Libya and the rest of the world began to thaw, American social scientists got excited. They could finally observe Libya’s progress in democratization and economic liberalization; Libya was on the path. Once the sanctions ended, Libya started assimilating into the new world order. It officially apologized for its limited involvement in the Lockerbie crash. It disbanded all attempts at a nuclear weapons program. It was given a rotating seat on the UN security council. Condoleezza Rice even visited and on the eve of George Bush’s presidency, in January 2009, the US and Libya exchanged ambassadors.\nHowever, despite all this apparent liberalization and openness in Libyan politics and the subsequent disciplinary excitement in political science, Libyan experts like Lisa Anderson see a different reality. Qaddafi is following his own trajectory, totally outside the traditional paradigm. He’s not progressing towards democracy—he’s already got a pseduodemocratic Era of the Masses political and economic system and is happy with it. He’s realized that in order to continue with his social revolution, he has to be somewhat involved with the world, and so he presents a facade of integration.\nFor example, when the Libyans ended their WMD program in 2006 and turned over all their nuclear material, all the machinery was still boxed up in crates. American politicians and political scientists applauded the IAEA for catching and stopping Libya before they could unpack anything. A widespread rumor/theory in both Libya and in academic and political circles, though, claims that Libya never had a program for WMD development. Their infrastructure and level of development couldn’t have handled such a large project. According to this theory, Libya bought the materials from North Korea so they could have something to turn over to the UN and the USA. So, they announced their weapons program, were condemned by the international community, bought some nuclear machinery from Korea, turned themselves in, reduced international sanctions, and improved their reputation in the world.\nThe same theory applies to the Lockerbie apology. Two Libyans were partially responsible for the bombing, so the majority of the blame was placed on Libya, despite several other claims of responsibility from other international non-state actors. Most Libyans today fully believe that Libya had no real connection to the bombing, yet Libya settled on a large payout with the families of the victims and offered a full apology. Following the apology sanctions were further lightened.\nLibya was merely paying cynical lip service to the absurd US-led world order. As Dr. Anderson stated:\n\nIf you’re playing the game that cynically, are you really playing the game? Libya is just playing the game; they think it’s just nonsense.\n\nIf you’re studying a country that considers the “standard” world political, economic, and social system as an absurd game, can you really fully understand it if you study it using disciplines created by that system for the explicit purpose of improving the system? Standard Western political science is still stuck looking at the “here and now” of the early 1900s. Libya doesn’t fit the social science mold at all—traditional social sciences therefore fall flat on their faces.\nI’ve found that the same principle can be applied to other countries in the Middle East. When Egypt held elections in 2005, President Mubarak allowed a second candidate to run for the first time ever. Disciplinary political scientists praised his move as an important step towards liberal democratization. Mubarak won by a landslide through rigged elections and imprisoned his opposing candidate, Ayman Nour.\nGreat big step towards democracy, or false expectations rooted in a skewed and incorrect social science paradigm?"
  },
  {
    "objectID": "blog/2009/03/08/new-site-launched/index.html",
    "href": "blog/2009/03/08/new-site-launched/index.html",
    "title": "New site launched",
    "section": "",
    "text": "Two years ago I decided that it would be a good idea to start a personal website. I was always disappointed in the Google results for my name, so I figured having my own web space and domain would help boost my ratings. Yes. I initially started this site for the sole purpose of egogoogling. So much has changed since then…\nMy first website was ugly. I had just started my foray into web design and barely knew anything—my CSS was rudimentary at best and I used scattered PHP includes to make a pseudo dynamic site. It was a mess.\n\n\n\nScreenshot of old site\n\n\nUgh. It was pretty embarrassing. I should have just killed it, but I couldn’t because of one tutorial I posted. That one post drives more than 80% of the traffic to this site, and I get almost daily e-mails with questions about it. My old site couldn’t handle the community that built up around it.\nNow that I know a thing or two more about design, I’m finally revealing the newest incarnation of AndrewHeiss.com—a fully fledged, Wordpress-based site, with a blog, portfolio, and my general online identity hub.\nThe design isn’t finished yet; there are a few last tweaks that need to get done. Let me know about any issues, problems, or suggestions in the comments.\nOh, I’m still the top result for Andrew Heiss. Not that it’s hard…"
  },
  {
    "objectID": "blog/2007/09/17/using-arabic-in-indesign-without-indesign-me/index.html",
    "href": "blog/2007/09/17/using-arabic-in-indesign-without-indesign-me/index.html",
    "title": "Using Arabic in InDesign without InDesign ME",
    "section": "",
    "text": "About a year ago I discovered to my dismay that using Arabic in InDesign was entirely impossible.  I wanted to make a type of dictionary for my Arabic 101 students, using an Excel spreadsheet full of Arabic words. When I placed any Arabic text, though, this happened:\n\n\n\nMessed up Arabic text\n\n\nWhile Microsoft and Apple have great right-to-left (RTL) language support built in, Adobe doesn’t. InDesign and Illustrator cannot handle RTL text. Adobe has, however, outsourced their code to WinSoft, who develops the Creative Suite ME (Middle Eastern edition), which does have excellent RTL support, especially through the use of their Tasmeem typesetting framework, recently highlighted in Saudi Aramco Magazine. However, I don’t want to buy the ME version for minimal Arabic use.\n\nTyping backwards\nThe only way around this is to type the text in backwards: if you want the word alkitaab, you would have to type baatikla and InDesign should show it correctly.\nThere’s one big caveat though—Arabic letters have different forms depending on where they show up in the word (initial, medial, final, or isolated).\nThis typing-backwards, faux-RTL works great for Hebrew since almost every letter has the same shape no matter where they are in the word. In fact, InDesignSecrets.com mentioned a script that will take pasted Hebrew characters and reverse them automatically.\nUnfortunately, Arabic is more complex. There is a clunky solution—hunt and peck with the glyphs panel. This workaround is not useful for large amounts of Arabic text. If you want to design something with a substantial amount of Arabic, buy InDesign ME. (if you’re desperate, I guess you could do an entire book like this. It would just take several months to get the text done :) ).\n\n\nTyping with the glyphs panel\nThe glyphs panel is a great and often underused panel in InDesign. It’s generally used for finding and inserting dingbat characters or other non-standard glyphs in a font. You can even save your most commonly used glyphs for easy access:\n\n\n\nCustom glyphs\n\n\nYou can even type with the glyphs panel, which is how we get Arabic working in InDesign. This method also works for Illustrator and any other Adobe program with a glyphs panel.\nTo activate the panel, go to Window &gt; Type & Tables &gt; Glyphs. Choose an Arabic font from the list in the bottom left corner of the panel to load that font into the panel. I like working with the Arabic Typesetting font that comes with Office 2007 because of the dozens of alternate glyphs and ligatures that are available. Microsoft has an excellent collection of Arabic fonts as well.\nYou should see normal Roman characters in the panel. Scroll down until you get the Arabic glyphs. Double click on a letter to insert it at your cursor.\n\n\n\nArabic glyph panel\n\n\nHere’s where the magic starts. Many of the glyphs will have a black triangle in the bottom right corner of the grid box. This means that there are alternate glyphs for that character—in this case, different positions for the letter. Click and hold one of the boxes with alternate glyphs and you’ll see all the different possibilities for that letter.\n\n\n\nAll glyph positions\n\n\nTo type a full Arabic word, insert the appropriately positioned letters in backwards order using the glyphs panel. Here’s a live example of this in action (sorry for the horrible quality):\n\n\n\n\nYou can insert alternate glyphs and ligatures too:\n\n\n\nAlternate glyphs and ligatures\n\n\nIf you use a decorative Arabic font, like Microsoft’s Diwani family (found in the arafonts.exe font package), you can use the decorative swashes as well. You can even change the font after inserting the letters to another Arabic font and maintain the letters.\nIn the end, you’ll have real Arabic text that can be manipulated just like normal InDesign text. It’s a clunky method, but it works, as seen here.\nThis could all probably be automated with a script of sorts, but I’m no programmer.\nIf anyone has comments or suggestions (or knows how to make a script for this), leave a comment below…"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Data Visualization with R \n                \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Fall 2024 (asynchronous online)\n                \n                \n                \n                 Spring 2025 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Comparative Public Administration \n                \n            \n            \n                PMAP 8441/4441 | \n                Georgia State University\n                \n            \n            Explore how the public and nonprofit sectors work around the world and learn how political institutions shape government and civil society\n\n            \n                \n                \n                 Fall 2024\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2025\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section",
    "href": "teaching/index.html#section",
    "title": "Teaching",
    "section": "",
    "text": "Data Visualization with R \n                \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Fall 2024 (asynchronous online)\n                \n                \n                \n                 Spring 2025 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Comparative Public Administration \n                \n            \n            \n                PMAP 8441/4441 | \n                Georgia State University\n                \n            \n            Explore how the public and nonprofit sectors work around the world and learn how political institutions shape government and civil society\n\n            \n                \n                \n                 Fall 2024\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2025\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-1",
    "href": "teaching/index.html#section-1",
    "title": "Teaching",
    "section": "2023–24",
    "text": "2023–24\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2024\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Data Visualization with R \n                \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Fall 2023 (asynchronous online)\n                \n                \n                \n                 Summer 2024 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Microeconomics for Public Policy \n                \n            \n            \n                PMAP 8141 | \n                Georgia State University\n                \n            \n            Learn how to understand, speak, and do economics in the public sector\n\n            \n                \n                \n                 Summer 2024 (asynchronous online)\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-2",
    "href": "teaching/index.html#section-2",
    "title": "Teaching",
    "section": "2022–23",
    "text": "2022–23\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2023\n                \n                \n                \n                 Fall 2022\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Comparative Public Administration \n                \n            \n            \n                PMAP 8441 | \n                Georgia State University\n                \n            \n            Explore how the public and nonprofit sectors work around the world and learn how political institutions shape government and civil society\n\n            \n                \n                \n                 Spring 2023\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Bayesian Statistics Readings \n                \n            \n            \n                PMAP 8911 | \n                Georgia State University\n                \n            \n            Independent readings course on Bayesian statistics with R and Stan\n\n            \n                \n                \n                 Spring 2023\n                \n                \n                \n                 Fall 2022\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Introduction to Nonprofits \n                \n            \n            \n                PMAP 3210 | \n                Georgia State University\n                \n            \n            Discover what nonprofit organizations are, how they work, and how they can improve communities (and the world!)\n\n            \n                \n                \n                 Fall 2022\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Data Visualization with R \n                \n            \n            \n                PMAP 8921 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Summer 2023 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Microeconomics for Public Policy \n                \n            \n            \n                PMAP 8141 | \n                Georgia State University\n                \n            \n            Learn how to understand, speak, and do economics in the public sector\n\n            \n                \n                \n                 Summer 2023 (asynchronous online)\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-3",
    "href": "teaching/index.html#section-3",
    "title": "Teaching",
    "section": "2021–22",
    "text": "2021–22\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2022\n                \n                \n                \n                 Fall 2021\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Introduction to Nonprofits \n                \n            \n            \n                PMAP 3210 | \n                Georgia State University\n                \n            \n            Discover what nonprofit organizations are, how they work, and how they can improve communities (and the world!)\n\n            \n                \n                \n                 Spring 2022\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Data Visualization with R \n                \n            \n            \n                PMAP 8921 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Summer 2022 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Microeconomics for Public Policy \n                \n            \n            \n                PMAP 8141 | \n                Georgia State University\n                \n            \n            Learn how to understand, speak, and do economics in the public sector\n\n            \n                \n                \n                 Summer 2022 (asynchronous online)\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-4",
    "href": "teaching/index.html#section-4",
    "title": "Teaching",
    "section": "2020–21",
    "text": "2020–21\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2021 (asynchronous online)\n                \n                \n                \n                 Fall 2020 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Data Visualization with R \n                \n            \n            \n                PMAP 8921 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Summer 2021 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Microeconomics for Public Policy \n                \n            \n            \n                PMAP 8141 | \n                Georgia State University\n                \n            \n            Learn how to understand, speak, and do economics in the public sector\n\n            \n                \n                \n                 Summer 2021 (asynchronous online)\n                \n                \n                \n                 Spring 2021 (asynchronous online)\n                \n                \n                \n                 Fall 2020 (asynchronous online)\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-5",
    "href": "teaching/index.html#section-5",
    "title": "Teaching",
    "section": "2019–20",
    "text": "2019–20\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Data Visualization with R \n                \n            \n            \n                PMAP 8921 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 May 2020 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2020\n                \n                \n                \n                 Fall 2019\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Microeconomics for Public Policy \n                \n            \n            \n                PMAP 8141 | \n                Georgia State University\n                \n            \n            Learn how to understand, speak, and do economics in the public sector\n\n            \n                \n                \n                 Summer 2020 (asynchronous online)\n                \n                \n                \n                 Fall 2019\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-6",
    "href": "teaching/index.html#section-6",
    "title": "Teaching",
    "section": "2017–19",
    "text": "2017–19\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Economy, Society, and Public Policy \n                \n            \n            \n                MPA 612 | \n                Brigham Young University\n                \n            \n            \n\n            \n                \n                \n                 Winter 2019\n                \n                \n                \n                 Winter 2019 (executive MPA)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Data Science and Statistics for Public Management \n                \n            \n            \n                MPA 630 | \n                Brigham Young University\n                \n            \n            \n\n            \n                \n                \n                 Fall 2018\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Data Visualization \n                \n            \n            \n                MPA 635 | \n                Brigham Young University\n                \n            \n            \n\n            \n                \n                \n                 Fall 2018\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Economic Decision Making for Managers \n                \n            \n            \n                MPA 612 | \n                Brigham Young University\n                \n            \n            \n\n            \n                \n                \n                 Winter 2018\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Telling Stories with Data \n                \n            \n            \n                Bus M 491R | \n                Brigham Young University\n                \n            \n            \n\n            \n                \n                \n                 Fall 2017\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Data Visualization \n                \n            \n            \n                MPA 635 | \n                Brigham Young University\n                \n            \n            \n\n            \n                \n                \n                 Fall 2017\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#section-7",
    "href": "teaching/index.html#section-7",
    "title": "Teaching",
    "section": "2007–15",
    "text": "2007–15\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Stats in Real Life \n                \n            \n            \n                PubPol 590 | \n                Duke University\n                \n                | TA, as PhD student\n                \n            \n            \n\n            \n                \n                 \n                 Fall 2014\n                \n                \n                 \n                 Spring 2014\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Policy Analysis \n                \n            \n            \n                PubPol 803/807 | \n                Duke University\n                \n                | TA, as PhD student\n                \n            \n            \n\n            \n                \n                 \n                 Fall 2015\n                \n                \n                 \n                 Fall 2013\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Statistical Analysis for Public Administrators \n                \n            \n            \n                PMGT 630 | \n                Brigham Young University\n                \n                | TA, as MPA student\n                \n            \n            \n\n            \n                \n                 \n                 Summer 2012\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                International Development Field Study \n                \n            \n            \n                PubPol 803/807 | \n                Brigham Young University\n                \n                | TA, as MPA student\n                \n            \n            \n\n            \n                \n                 \n                 Winter and Spring 2012\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Decision Modeling and Analysis \n                \n            \n            \n                PubPol 803/807 | \n                Brigham Young University\n                \n                | TA, as MPA student\n                \n            \n            \n\n            \n                \n                 \n                 Winter 2012\n                \n                \n                 \n                 Fall 2011\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Print Publishing \n                \n            \n            \n                CHum 260 | \n                Brigham Young University\n                \n                | Instructor, as undergraduate\n                \n            \n            \n\n            \n                \n                 \n                 Winter 2008\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                First-year Arabic \n                \n            \n            \n                Arabic 101 | \n                Brigham Young University\n                \n                | Instructor, as undergraduate\n                \n            \n            \n\n            \n                \n                \n                 Winter 2007\n                \n                \n                \n                 Fall 2006\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/working-papers/heiss-ye-china-ongos/index.html",
    "href": "research/working-papers/heiss-ye-china-ongos/index.html",
    "title": "The Implementation of China’s Overseas NGO Law and the Operating Space for International Civil Society",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-china-ongos/index.html#important-links",
    "href": "research/working-papers/heiss-ye-china-ongos/index.html#important-links",
    "title": "The Implementation of China’s Overseas NGO Law and the Operating Space for International Civil Society",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-china-ongos/index.html#abstract",
    "href": "research/working-papers/heiss-ye-china-ongos/index.html#abstract",
    "title": "The Implementation of China’s Overseas NGO Law and the Operating Space for International Civil Society",
    "section": "Abstract",
    "text": "Abstract\nChina’s 2017 Overseas NGO (ONGO) Law is part of a larger global trend of legal restrictions on international NGOs (INGOs). However, the effects of this global crackdown on INGOs have been difficult to measure since there is often divergence between the formal de jure regulations and the de facto implementation of these laws. The enforcement of these laws also depends on the services provided by these organizations. In particular, authoritarian regimes display conflicting attitudes towards claim-making (e.g. advocacy) INGOs versus service-providing (e.g. humanitarian assistance) INGOs. In this paper, we measure the effect of China’s ONGO law on INGO operations since 2017. We test how INGO operational flexibility under the ONGO law is influenced by organizations’ issue areas. We argue that service provision INGOs have greater flexibility and are allowed to work in a larger number of provinces than their claim-making counterparts. China provides an excellent setting for testing our claim—the 2017 ONGO law regulates both claim-making and service-providing NGOs, and ambiguity in the law gives ample room for arbitrary discretion by authorities. We test our argument using multilevel Bayesian models with administrative data from 635 registered representative offices of INGOs registered in China between 2017–2021. Our results speak to the broader literature on closing civic space and provide an empirical illustration of the practical effect of NGO restrictions on global civil society."
  },
  {
    "objectID": "research/working-papers/heiss-nafa-bayes-ipw/index.html",
    "href": "research/working-papers/heiss-nafa-bayes-ipw/index.html",
    "title": "Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-nafa-bayes-ipw/index.html#important-links",
    "href": "research/working-papers/heiss-nafa-bayes-ipw/index.html#important-links",
    "title": "Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-nafa-bayes-ipw/index.html#abstract",
    "href": "research/working-papers/heiss-nafa-bayes-ipw/index.html#abstract",
    "title": "Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science",
    "section": "Abstract",
    "text": "Abstract\nThe past two decades have been characterized by considerable progress in developing approaches to causal inference in situations where true experimental manipulation is either impractical or impossible. With few exceptions, however, commonly employed techniques in political science have developed largely within the frequentist framework (i.e., Blackwell and Glynn 2018; Imai and Kim 2019; Torres 2020). In this article, we argue that common approaches rest fundamentally upon assumptions that are difficult to defend in many areas of political research and highlight the benefits of quantifying uncertainty in the estimation of causal effects (Gill 1999; Gill and Heuberger 2020; Schrodt 2014; Western and Jackman 1994). Extending the approach to causal inference for cross-sectional time series and panel data under selection on observables introduced by Blackwell and Glynn (2018), we develop a two-step Bayesian approach to the estimation of marginal structural models. We demonstrate our proposed procedure in the context of parametric survival analysis and linear mixed effects models via a simulation study and two empirical examples. Finally, we provide flexible open-source software implementing the proposed method."
  },
  {
    "objectID": "research/working-papers/heiss-nafa-bayes-ipw/index.html#bibtex-citation",
    "href": "research/working-papers/heiss-nafa-bayes-ipw/index.html#bibtex-citation",
    "title": "Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@unpublished{HeissNafa:2022,\n    Author = {Andrew Heiss and A. Jordan Nafa},\n    Note = {Working paper},\n    Title = {Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science},\n    Year = {2022}}"
  },
  {
    "objectID": "research/working-papers/heiss-amicable-contempt/index.html",
    "href": "research/working-papers/heiss-amicable-contempt/index.html",
    "title": "Amicable Contempt: A Conceptual Framework for Understanding International NGO Behavior in the Era of Closing Civic Space",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-amicable-contempt/index.html#important-links",
    "href": "research/working-papers/heiss-amicable-contempt/index.html#important-links",
    "title": "Amicable Contempt: A Conceptual Framework for Understanding International NGO Behavior in the Era of Closing Civic Space",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-amicable-contempt/index.html#abstract",
    "href": "research/working-papers/heiss-amicable-contempt/index.html#abstract",
    "title": "Amicable Contempt: A Conceptual Framework for Understanding International NGO Behavior in the Era of Closing Civic Space",
    "section": "Abstract",
    "text": "Abstract\nOver the past decade, international NGOs (INGOs) have become increasingly active in authoritarian regimes as they respond to emergencies, assist with development, or advocate for human rights. Though these services and advocacy can challenge the legitimacy and power of the regime, many autocratic states permit INGO activities and INGOs continue to work in these countries despite the sometimes heavy restrictions on their activities. In my dissertation, I theorize that this relationship between INGOs and autocrats creates a state of amicable contempt, where each party is aware that the other threatens—yet sustains—their existence. Autocrats and INGOs engage in a dance of cost-benefit calculus, each trying to advance their own agenda without upsetting their counterpart. Regimes work to set the optimal level of INGO regulations, maximizing the practical and reputational benefits that INGOs provide and minimizing the potential destabilizing costs of INGO activities. Meanwhile, INGOs work to find the optimal mix of programming within a country that allows them to pursue their principled objectives within the boundaries the regime has set—affecting as much change and providing as many services as possible without risking expulsion from the country. I use evidence from a global survey of international NGOs to define each INGO-related element of the theory of amicable contempt. I find that INGOs are primarily motivated by their core vision and values, but that they have to balance the pursuit of their missions with instrumental concerns such as fundraising, time, staffing, and collaboration. These concerns both limit and enable INGO activities—without substantial instrumental resources and programmatic flexibility, organizations are unable to carry out their mission, while too much emphasis on resource concerns distracts organizations from their core programming and reduces their effectiveness."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html#important-links",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html#important-links",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html#abstract",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html#abstract",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "Abstract",
    "text": "Abstract\nIn an effort to combat the COVID-19 pandemic, most governments have imposed restrictions on movement, association, and other civic freedoms in the interest of public health. In many cases, such as New Zealand, these emergency measures have been temporary and respect for human rights has returned to normal. In many other instances, however, governments have used these restrictions to suppress opposition and more permanently restrict civic space. Systematically measuring the consequences of COVID restrictions, however, is a difficult task. We will examine two possible quantitative measures of the relationship between of COVID restrictions and civil society space. First, we will use the Variety of Democracy project’s newly released Pandemic Violations of Democratic Standards Index to explore if and how civil society restrictions predict pandemic backsliding. Second, while many countries sign international human rights treaties that ostensibly bind states to respect rights, several treaties allow for emergency derogations from these obligations. We will tabulate formal human rights treaty derogations due to the pandemic and explore whether these emergency measures led to lasting declines in associational and human rights. We hope that our exploration of these two measures will provide rich data-based descriptions of the relationship between COVID restrictions and civic space that will allow for more causal work in the future."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html#important-figure",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html#important-figure",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "Important figure",
    "text": "Important figure\nFigure 5: Derogation decisions across pandemic violations and pandemic backsliding\n\n\n\nFigure 5: Derogation decisions across pandemic violations and pandemic backsliding"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-derogations/index.html#bibtex-citation",
    "href": "research/working-papers/chaudhry-heiss-derogations/index.html#bibtex-citation",
    "title": "Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@unpublished{ChaudhryHeiss:2021,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Note = {Working paper},\n    Title = {Derogations and Democratic Backsliding: Exploring the Pandemic's Effects on Civic Spaces},\n    Year = {2021}}"
  },
  {
    "objectID": "research/reviews/heiss-voluntas-review-2014/index.html",
    "href": "research/reviews/heiss-voluntas-review-2014/index.html",
    "title": "Review of Manufacturing Civil Society: Principles, Practices, and Effects, ed. Taco Brandsen, Willem Trommel, and Bram Verschuere",
    "section": "",
    "text": "Add to Zotero \n\n@article{Heiss:2015,\n    author = {Andrew Heiss},\n    doi = {10.1007/s11266-014-9541-3},\n    journal = {Voluntas: International Journal of Voluntary and Nonprofit Organizations},\n    pages = {728--730},\n    titleaddon = {\\bibstring{reviewof} Taco Brandsen, Willem Trommel, and Bram Verschuere (eds.), \\mkbibemph{Manufacturing Civil Society: Principles, Practices, and Effects}},\n    volume = {26},\n    year = {2015}}"
  },
  {
    "objectID": "research/reviews/heiss-voluntas-review-2014/index.html#citation",
    "href": "research/reviews/heiss-voluntas-review-2014/index.html#citation",
    "title": "Review of Manufacturing Civil Society: Principles, Practices, and Effects, ed. Taco Brandsen, Willem Trommel, and Bram Verschuere",
    "section": "",
    "text": "Add to Zotero \n\n@article{Heiss:2015,\n    author = {Andrew Heiss},\n    doi = {10.1007/s11266-014-9541-3},\n    journal = {Voluntas: International Journal of Voluntary and Nonprofit Organizations},\n    pages = {728--730},\n    titleaddon = {\\bibstring{reviewof} Taco Brandsen, Willem Trommel, and Bram Verschuere (eds.), \\mkbibemph{Manufacturing Civil Society: Principles, Practices, and Effects}},\n    volume = {26},\n    year = {2015}}"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My research spans public policy, nonprofit management, political science, and international relations. I study human rights and international nonprofit management and I focus on authoritarian regulation of civil society and international NGO responses to administrative crackdown. I also research causal inference methods, particularly using panel data."
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Research",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n    \n        \n            \n                Audrey L. Comstock, Andrew Heiss, and Suparna Chaudhry, “Derogations, Democratic Backsliding, and International Human Rights During the COVID-19 Pandemic,” Journal of Human Rights (forthcoming), doi: 10.1080/14754835.2024.2446854\n            \n\n            \n            \n                \n                    \n                            Human rights\n                        \n                    \n                    \n                            International law\n                        \n                    \n                    \n                            Treaty derogations\n                        \n                    \n                    \n                            Democratic backsliding\n                        \n                    \n                    \n                            COVID-19\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Pandemic human rights.\n            \n                 / Backsliding states keep treaties—\n            \n                 / signal compliance.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Analysis notebook\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Replication Docker container\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Suparna Chaudhry, Audrey L. Comstock, and Andrew Heiss, “Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19,” International Interactions (2024): 1–23, doi: 10.1080/03050629.2024.2413965\n            \n\n            \n            \n                \n                    \n                            Human rights\n                        \n                    \n                    \n                            Treaty derogations\n                        \n                    \n                    \n                            Emergency policies\n                        \n                    \n                    \n                            COVID-19\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                COVID policies.\n            \n                 / Bad news for human rights? No.\n            \n                 / Used as intended.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Analysis notebook\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Replication Docker container\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Vincent Arel-Bundock, Noah Greifer, and Andrew Heiss, “How to Interpret Statistical Models Using {marginaleffects} in R and Python,” Journal of Statistical Software forthcoming (2024), doi: 10.18637/jss.v111.i09\n            \n\n            \n            \n                \n                    \n                            regression\n                        \n                    \n                    \n                            marginal effect\n                        \n                    \n                    \n                            slope\n                        \n                    \n                    \n                            prediction\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Raw model results?\n            \n                 / Stop! Hard to understand! Use\n            \n                 / {marginaleffects}.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “NGO Repression as a Predictor of Worsening Human Rights Abuses,” Journal of Human Rights 21, no. 2 (2022): 123–140, doi: 10.1080/14754835.2022.2030205\n            \n\n            \n            \n                \n                    \n                            Human rights\n                        \n                    \n                    \n                            INGOs\n                        \n                    \n                    \n                            Civil society\n                        \n                    \n                    \n                            Bayes\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Look at state actions\n            \n                 / —not formal NGO laws—\n            \n                 / to predict abuse.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code and data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Suparna Chaudhry, Marc Dotson, and Andrew Heiss, “Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy,” Global Policy 12, no. S5 (July 2021): 45–58, doi: 10.1111/1758-5899.12984\n            \n\n            \n            \n                \n                    \n                            Philanthropy\n                        \n                    \n                    \n                            Experiment\n                        \n                    \n                    \n                            Conjoint analysis\n                        \n                    \n                    \n                            Simulation\n                        \n                    \n                    \n                            Bayes\n                        \n                    \n                    \n                            INGOs\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                When seeing crackdown,\n            \n                 / people with low social trust\n            \n                 / are fairweather friends.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Steven L. Peck and Andrew Heiss, “Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?” Oikos 130, no. 9 (September 2021): 1425–1439, doi: 10.1111/oik.07621\n            \n\n            \n            \n                \n                    \n                            Data visualization\n                        \n                    \n                    \n                            Ecology\n                        \n                    \n                    \n                            Simulation\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Fitness, turnover,\n            \n                 / stability, evenness—\n            \n                 / all due to constraints.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences,” Nonprofit and Voluntary Sector Quarterly 50, no. 3 (June 2021): 481–505, doi: 10.1177/0899764020971045\n            \n\n            \n            \n                \n                    \n                            Philanthropy\n                        \n                    \n                    \n                            Experiment\n                        \n                    \n                    \n                            Bayes\n                        \n                    \n                    \n                            INGOs\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                In trouble abroad?\n            \n                 / Crackdown may be heuristic—\n            \n                 / those who give give more.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, “Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries,” Interest Groups and Advocacy 8, no. 3 (September 2019): 356–75, doi: 10.1057/s41309-019-00061-0\n            \n\n            \n            \n                \n                    \n                            INGOs\n                        \n                    \n                    \n                            Civil society\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n                    \n                            Mixed methods\n                        \n                    \n                    \n                            Survey\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Flexibility—\n            \n                 / what lets NGOs reshape\n            \n                 / host environment.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss and Judith G. Kelley, “Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments,” Journal of Politics 79, no. 2 (April 2017): 732–41, doi: 10.1086/691218.\n            \n\n            \n            \n                \n                    \n                            INGOs\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n                    \n                            Civil society\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Donors and targets:\n            \n                 / institutional constraints\n            \n                 / limit NGOs.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss and Judith G. Kelley, “From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts,” Journal of Human Trafficking 3, no. 3 (2017): 1500–1528, doi: 10.1080/23322705.2016.1199241.\n            \n\n            \n            \n                \n                    \n                            INGOs\n                        \n                    \n                    \n                            Survey\n                        \n                    \n                    \n                            Human trafficking\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Big sector survey—\n            \n                 / NGOs like U.S. work\n            \n                 / fighting trafficking.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss and Tana Johnson, “Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior,” International Studies Review 18, no. 3 (September 2016): 528–41, doi: 10.1093/isr/viv014.\n            \n\n            \n            \n                \n                    \n                            INGOs\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Study NGOs?\n            \n                 / View behavior; policy\n            \n                 / with a new framework.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Eva Witesman and Andrew Heiss, “Nonprofit Collaboration and the Resurrection of Market Failure: How a Resource-Sharing Environment Can Suppress Social Objectives,” Voluntas: International Journal of Voluntary and Nonprofit Organizations 28, no. 4 (August 2017): 1500–1528, doi: 10.1007/s11266-016-9684-5.\n            \n\n            \n            \n                \n                    \n                            Nonprofits\n                        \n                    \n                    \n                            Simulation\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                When money is scarce;\n            \n                 / when objectives are not shared—\n            \n                 / don’t collaborate.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, “The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution,” Journal of Third World Studies 28, no. 1 (Spring 2012): 155–171, no doi.\n            \n\n            \n            \n                \n                    \n                            Management\n                        \n                    \n                    \n                            Egypt\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Management theory\n            \n                 / meets political science:\n            \n                 / Mubarak messed up.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version (PDF)\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research",
    "section": "Working papers",
    "text": "Working papers\n\n\n\n    \n        \n            \n                Andrew Heiss, “Amicable Contempt: A Conceptual Framework for Understanding International NGO Behavior in the Era of Closing Civic Space”\n            \n\n            \n            \n                \n                    \n                            INGOs\n                        \n                    \n                    \n                            Civil society\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n                    \n                            Mixed methods\n                        \n                    \n                    \n                            Survey\n                        \n                    \n                    \n                            Theory\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Suparna Chaudhry, Marc Dotson, and Andrew Heiss, Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy”\n            \n\n            \n            \n                \n                    \n                            Philanthropy\n                        \n                    \n                    \n                            Experiment\n                        \n                    \n                    \n                            Conjoint analysis\n                        \n                    \n                    \n                            Bayes\n                        \n                    \n                    \n                            INGOs\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs”\n            \n\n            \n            \n                \n                    \n                            Civil society\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n                    \n                            Foreign aid\n                        \n                    \n                    \n                            Causal inference\n                        \n                    \n                    \n                            Panel data\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Analysis notebook\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, “‘Some State Officials Want Your Services’: International NGO Responses to Authoritarian Program Capture Regulation”\n            \n\n            \n            \n                \n                    \n                            INGOs\n                        \n                    \n                    \n                            Civil society\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n                    \n                            Mixed methods\n                        \n                    \n                    \n                            Survey\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss and Meng Ye, “The Implementation of China’s Overseas NGO Law and the Operating Space for International Civil Society”\n            \n\n            \n            \n                \n                    \n                            Civil society\n                        \n                    \n                    \n                            China\n                        \n                    \n                    \n                            INGOs\n                        \n                    \n                    \n                            Panel data\n                        \n                    \n                    \n                            Bayes\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, “‘We Changed Our Strategy… Without Losing Our Values, Vision and Mission’: Mission, Money, and the Practical Operating Environment for International NGOs”\n            \n\n            \n            \n                \n                    \n                            INGOs\n                        \n                    \n                    \n                            Civil society\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n                    \n                            Mixed methods\n                        \n                    \n                    \n                            Survey\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss and A. Jordan Nafa, “Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science”\n            \n\n            \n            \n                \n                    \n                            Causal inference\n                        \n                    \n                    \n                            Bayes\n                        \n                    \n                    \n                            Panel data\n                        \n                    \n                    \n                            Methods\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss and Meng Ye, “Clarifying Correlation and Causation: A Guide to Modern Quantitative Causal Inference in Nonprofit Studies”\n            \n\n            \n            \n                \n                    \n                            Causal inference\n                        \n                    \n                    \n                            Nonprofits\n                        \n                    \n                    \n                            Methods\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “Derogations and Democratic Backsliding: Exploring the Pandemic’s Effects on Civic Spaces”\n            \n\n            \n            \n                \n                    \n                            Human rights\n                        \n                    \n                    \n                            Civil society\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n                    \n                            COVID-19\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#book-chapters",
    "href": "research/index.html#book-chapters",
    "title": "Research",
    "section": "Book chapters",
    "text": "Book chapters\n\n\n\n    \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on NGOs,” chap. 2 in Beyond the Boomerang: New Patterns in Transcalar Advocacy, eds. Christopher L. Pallas and Elizabeth Bloodgood (Tuscaloosa, AL: University of Alabama Press, 2022).\n            \n\n            \n            \n                \n                    \n                            Civil society\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n                    \n                            Human rights\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Governments threatened—\n            \n                 / NGO regulations\n            \n                 / shift funding; missions.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint (PDF)\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint (HTML)\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Analysis notebook\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, “Causal Inference,” chap. 10 in R for Political Data Science: A Practical Guide, ed. Francisco Urdinez and Andrés Cruz (Boca Raton, Florida: Chapman and Hall / CRC, 2021), 235–274, doi: 10.1201/9781003010623-10.\n            \n\n            \n            \n                \n                    \n                            Causal inference\n                        \n                    \n                    \n                            Methods\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                DAGs, inference, R!\n            \n                 / Observational data?\n            \n                 / Tell causal stories!\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, “NGOs and Authoritarianism,” chap. 38 in Routledge Handbook of NGOs and International Relations, ed. Thomas Davies (London: Routledge, 2019).\n            \n\n            \n            \n                \n                    \n                            NGOs\n                        \n                    \n                    \n                            Civil society\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n                    \n                            Authoritarianism\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                It’s complicated.\n            \n                 / Dictators love NGOs,\n            \n                 / but also they don’t.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Tana Johnson and Andrew Heiss, “Liberal Institutionalism,” chap. 8 in International Organization and Global Governance, 2nd ed., ed. Thomas G. Weiss and Rorden Wilkinson (London: Routledge, 2018), 123–34, doi: 10.4324/9781315301914.\n            \n\n            \n            \n                \n                    \n                            International relations\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Stuck between “ism”s,\n            \n                 / liberal global theory\n            \n                 / has rich past; future.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#reviews",
    "href": "research/index.html#reviews",
    "title": "Research",
    "section": "Reviews",
    "text": "Reviews\n\n\n\n    \n        \n            \n                Andrew Heiss, review of Meghan Elizabeth Kallman, The Death of Idealism: Development and Anti-Politics in the Peace Corps, Contemporary Sociology, 50, no. 6 (November 2021): 486–88, doi: 10.1177/00943061211050046g.\n            \n\n            \n            \n                \n                    \n                            International development\n                        \n                    \n                    \n                            Foreign aid\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint (PDF)\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint (HTML)\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, review of Taco Brandsen, Willem Trommel, and Bram Verschuere (eds.), Manufacturing Civil Society: Principles, Practices, and Effects, Voluntas: International Journal of Voluntary and Nonprofit Organizations 26, no. 2 (April 2014): 728–30, doi: 10.1007/s11266-014-9541-3.\n            \n\n            \n            \n                \n                    \n                            Civil society\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#dormant-working-papers",
    "href": "research/index.html#dormant-working-papers",
    "title": "Research",
    "section": "Dormant working papers",
    "text": "Dormant working papers\n\n\n\n    \n        \n            \n                \"Sources of Advocacy: When Does the Media Give Voice to Egyptian Advocacy NGOs?\" (with Ken Rogerson, Duke University)\n            \n\n            \n            \n                \n                    \n                            Egypt\n                        \n                    \n                    \n                            Text analysis\n                        \n                    \n                    \n                            Topic modeling\n                        \n                    \n                    \n                            NGOs\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                \"Discovering Discourse: The Relationship between Media and NGOs in Egypt between 2011–13\" (with Ken Rogerson, Duke University)\n            \n\n            \n            \n                \n                    \n                            Egypt\n                        \n                    \n                    \n                            Text analysis\n                        \n                    \n                    \n                            Topic modeling\n                        \n                    \n                    \n                            NGOs\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#selected-seminar-papers",
    "href": "research/index.html#selected-seminar-papers",
    "title": "Research",
    "section": "Selected seminar papers",
    "text": "Selected seminar papers\n\n\n\n    \n        \n            \n                “Do Democracies Discourage NGO Cooperation?\"Replication and extension of Amanda Murdie. 2014. “Scrambling for Contact: The Determinants of Inter-NGO Cooperation in Non-Western Countries.\" Review of International Organizations 9, no. 3 (September): 309–31.\n            \n\n            \n            \n                \n                    \n                            INGOs\n                        \n                    \n                    \n                            NGOs\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                “Explaining Support for Undemocratic Leaders in Democracies in the Middle East”Replication and extension of Amaney Jamal and Mark Tessler. 2008. “Attitudes in the Arab World.\" Journal of Democracy 19, no. 1 (January): 97–110.\n            \n\n            \n            \n                \n                    \n                            Authoritarianism\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Raw output\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Poster\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#translations",
    "href": "research/index.html#translations",
    "title": "Research",
    "section": "Translations",
    "text": "Translations\n\n\n\n    \n        \n            \n                Abdel Samad, Hamid. 2011. \"Farewell, Heaven (Wadaʿan Ayatuha al-Samaaʾ / وداعا أيتها السماء).\" In The Literary Life of Cairo: One Hundred Years in the Heart of the City, edited by Samia Mehrez, translated by Andrew Heiss, 347–50. Cairo: American University in Cairo Press.\n            \n\n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Book\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Prince, Mona. 2011. \"Three Suitcases for Departure (Thalatha Haqaʾib lil-Safar / ثلاثة حقائب للسفر).\" In The Literary Life of Cairo: One Hundred Years in the Heart of the City, edited by Samia Mehrez, translated by Andrew Heiss, 212–13. Cairo: American University in Cairo Press.\n            \n\n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Book\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/chapters/heiss-ngo-ir-2019/index.html",
    "href": "research/chapters/heiss-ngo-ir-2019/index.html",
    "title": "NGOs and Authoritarianism",
    "section": "",
    "text": "Authoritarian restrictions on domestic and international civil society have increased over the past decade, but authoritarian states continue to allow—and even invite—NGOs to work in their countries. Though the services and advocacy provided by NGOs can challenge the legitimacy and power of authoritarian regimes, the majority of autocratic states allow NGO activities, and NGOs in turn continue to work in these countries in spite of the heavy legal restrictions and attempts to limit their activities. This chapter examines the theories about and the experiences of domestic and international NGOs working in authoritarian countries. The review is premised on the theory of authoritarian institutions: dictators delegate political authority to democratic-appearing institutions in order to remain in power and maintain stability. After providing a brief overview of authoritarian institutionalism and balancing, I discuss how domestic and international NGOs fit into authoritarian stability-seeking calculus. I then look at three forms of state–NGO relationships in the context of authoritarianism and explore how autocrats have addressed and regulated international NGOs in particular. Finally, I conclude with suggestions for future research on NGOs and their relationship with and role in authoritarian regimes."
  },
  {
    "objectID": "research/chapters/heiss-ngo-ir-2019/index.html#abstract",
    "href": "research/chapters/heiss-ngo-ir-2019/index.html#abstract",
    "title": "NGOs and Authoritarianism",
    "section": "",
    "text": "Authoritarian restrictions on domestic and international civil society have increased over the past decade, but authoritarian states continue to allow—and even invite—NGOs to work in their countries. Though the services and advocacy provided by NGOs can challenge the legitimacy and power of authoritarian regimes, the majority of autocratic states allow NGO activities, and NGOs in turn continue to work in these countries in spite of the heavy legal restrictions and attempts to limit their activities. This chapter examines the theories about and the experiences of domestic and international NGOs working in authoritarian countries. The review is premised on the theory of authoritarian institutions: dictators delegate political authority to democratic-appearing institutions in order to remain in power and maintain stability. After providing a brief overview of authoritarian institutionalism and balancing, I discuss how domestic and international NGOs fit into authoritarian stability-seeking calculus. I then look at three forms of state–NGO relationships in the context of authoritarianism and explore how autocrats have addressed and regulated international NGOs in particular. Finally, I conclude with suggestions for future research on NGOs and their relationship with and role in authoritarian regimes."
  },
  {
    "objectID": "research/chapters/heiss-ngo-ir-2019/index.html#figure",
    "href": "research/chapters/heiss-ngo-ir-2019/index.html#figure",
    "title": "NGOs and Authoritarianism",
    "section": "Figure",
    "text": "Figure\nFigure 2: Civil society repression and regulations\n\n\n\nFigure 2: Civil society repression and regulations"
  },
  {
    "objectID": "research/chapters/heiss-ngo-ir-2019/index.html#citation",
    "href": "research/chapters/heiss-ngo-ir-2019/index.html#citation",
    "title": "NGOs and Authoritarianism",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@incollection{Heiss:2019,\n    Address = {London},\n    Author = {Andrew Heiss},\n    Booktitle = {Routledge Handbook of {NGOs} and International Relations},\n    Editor = {Thomas Davies},\n    Publisher = {Routledge},\n    Title = {{NGOs} and Authoritarianism},\n    Chapter = {38},\n    Year = {2019}}"
  },
  {
    "objectID": "research/chapters/chaudhry-heiss-closing-space/index.html",
    "href": "research/chapters/chaudhry-heiss-closing-space/index.html",
    "title": "Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on NGOs",
    "section": "",
    "text": "Figure 1: 2017 CIVICUS Monitor civic space ratings\n\n\n\nFigure 1: 2017 CIVICUS Monitor civic space ratings"
  },
  {
    "objectID": "research/chapters/chaudhry-heiss-closing-space/index.html#important-figure",
    "href": "research/chapters/chaudhry-heiss-closing-space/index.html#important-figure",
    "title": "Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on NGOs",
    "section": "",
    "text": "Figure 1: 2017 CIVICUS Monitor civic space ratings\n\n\n\nFigure 1: 2017 CIVICUS Monitor civic space ratings"
  },
  {
    "objectID": "research/chapters/chaudhry-heiss-closing-space/index.html#citation",
    "href": "research/chapters/chaudhry-heiss-closing-space/index.html#citation",
    "title": "Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on NGOs",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@incollection{ChaudhryHeiss:2022,\n    Address = {Tuscaloosa, AL},\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Booktitle = {Beyond the Boomerang: New Patterns in Transcalar Advocacy},\n    Editor = {Christopher L. Pallas and Elizabeth Bloodgood},\n    Publisher = {University of Alabama Press},\n    Title = {Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on {NGOs}},\n    Chapter = {2},\n    Year = {2022}}"
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html",
    "href": "research/articles/peck-heiss-2021/index.html",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html#important-links",
    "href": "research/articles/peck-heiss-2021/index.html#important-links",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html#abstract",
    "href": "research/articles/peck-heiss-2021/index.html#abstract",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "Abstract",
    "text": "Abstract\nEcological theorists have generated several yet unresolved disputes that try to untangle the difficulty in understanding the nature of complex ecological communities. In this paper, we combine two recent theoretical approaches that used together suggest a promising way to consider how evolutionary and ecological processes may be used to frame a general theory of community ecology and its functional stability. First, we consider the theoretical proposal by Mark Vellend (2016) to focus on a small set of higher-level evolutionary and ecological processes that act on species within an ecological community. These processes provide a basis for ecological theory similar to the way in which theoretical population genetics has focused on a small set of mathematical descriptions to undergird its theory. Second, we explore ideas that might be applied to ecosystem functioning developed by Alvaro Moreno and Matteo Mossio’s (2015) work on how biologically autonomous systems emerge from closure of relevant constraints. To explore the possibility that combining these two ideas may provide a more general theoretical understanding of ecological communities, we have developed a stochastic, agent-based model, with agents representing species, that explores the potential of using evolutionary and ecological processes as a constraint on the flow of species through an ecosystem. We explore how these ideas help illuminate aspects of stability found in many ecological communities. These agent-based modeling results provide in-principle arguments that suggest that constraint closure, using evolutionary and ecological processes, explains general features of ecological communities. In particular, we find that our model suggests a perspective useful in explaining repeated patterns of stability in ecological evenness, species turnover, species richness, and in measures of fitness."
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html#important-figure",
    "href": "research/articles/peck-heiss-2021/index.html#important-figure",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "Important figure",
    "text": "Important figure\nFigure 4: Boxplots show ecological measures across parameter space for different numbers of constraints generated by 2000 iterations of the simulation model across all 64 possible permutations of the proposed constraints being present or absent, and with uniform random values of parameters pulled from their possible values in the model iterations. The whiskers in the box plot show the range of variation. The actual data used to construct the boxplots are shown as fine gray dots to the right of each box plot. (a) Effect of the number of constraints on landscape fitness. (b) Effect of the number of constraints on ecological evenness. (c) Effect of the number of constraints on the average cell species richness (number of species defined as functional groups) present in the cell-niche. (d) Effect of the number of constraints on the difference between turnover in mutualistic linked and unlinked species. The turnover rate is the number of new species created at each time step in each cell.\n\n\n\nFigure 4: Boxplots show ecological measures across parameter space for different numbers of constraints generated by 2000 iterations of the simulation model across all 64 possible permutations of the proposed constraints being present or absent"
  },
  {
    "objectID": "research/articles/peck-heiss-2021/index.html#citation",
    "href": "research/articles/peck-heiss-2021/index.html#citation",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{PeckHeiss:2021,\n    Author = {Steven L. Peck and Andrew Heiss},\n    Doi = {10.1111/oik.07621},\n    Journal = {Oikos},\n    Month = {9},\n    Number = {9},\n    Pages = {1425--1439},\n    Title = {Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?},\n    Volume = {130},\n    Year = {2021}}"
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html",
    "href": "research/articles/heiss-kelley-2017/index.html",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "",
    "text": "How do the preferences and behavior of both donor organizations and host countries affect the strategies, activities, and effectiveness of international NGOs (INGOs)? Recent books by Sarah Bush, Jessica Teets, and Amanda Murdie bring unique ideas and empirical evidence to illustrate different parts of this question. To discuss the arguments in each book, as well as explore incidental ties between the three, we suggest a simple framework for organizing and understanding the dual institutional constraints on INGOs. In this essay, we use this framework to identify how each book addresses these influences on INGOs and how, in some cases, INGOs can reverse the direction of influence."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html#abstract",
    "href": "research/articles/heiss-kelley-2017/index.html#abstract",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "",
    "text": "How do the preferences and behavior of both donor organizations and host countries affect the strategies, activities, and effectiveness of international NGOs (INGOs)? Recent books by Sarah Bush, Jessica Teets, and Amanda Murdie bring unique ideas and empirical evidence to illustrate different parts of this question. To discuss the arguments in each book, as well as explore incidental ties between the three, we suggest a simple framework for organizing and understanding the dual institutional constraints on INGOs. In this essay, we use this framework to identify how each book addresses these influences on INGOs and how, in some cases, INGOs can reverse the direction of influence."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html#figure",
    "href": "research/articles/heiss-kelley-2017/index.html#figure",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "Figure",
    "text": "Figure\nFigure 2: The dual environmental constraints confronting INGOs\n\n\n\nFigure 2: The dual environmental constraints confronting INGOs"
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html#books-reviewed",
    "href": "research/articles/heiss-kelley-2017/index.html#books-reviewed",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "Books reviewed",
    "text": "Books reviewed\n\nSarah Sunn Bush, The Taming of Democracy Assistance: Why Democracy Promotion Does Not Confront Dictators (Cambridge, UK: Cambridge University Press, 2015), doi: 10.1017/cbo9781107706934.\nJessica C. Teets, Civil Society under Authoritarianism: The China Model (New York: Cambridge University Press, 2014), doi: 10.1017/cbo9781139839396.\nAmanda Murdie, Help or Harm: The Human Security Effects of International NGOs (Stanford: Stanford University Press, 2014), doi: 10.2307/j.ctvqsdpq8."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017/index.html#citation",
    "href": "research/articles/heiss-kelley-2017/index.html#citation",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{HeissKelley:2017,\n    Author = {Andrew Heiss and Judith G. Kelley},\n    Doi = {10.1086/691218},\n    Journal = {Journal of Politics},\n    Month = {4},\n    Number = {2},\n    Pages = {732--41},\n    Title = {Between a Rock and a Hard Place: International {NGOs} and the Dual Pressures of Donors and Host Governments},\n    Volume = {79},\n    Year = {2017}}"
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html",
    "href": "research/articles/heiss-2019-taking-control/index.html",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository\nRead-only PDF"
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html#important-links",
    "href": "research/articles/heiss-2019-taking-control/index.html#important-links",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository\nRead-only PDF"
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html#abstract",
    "href": "research/articles/heiss-2019-taking-control/index.html#abstract",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "Abstract",
    "text": "Abstract\nA wave of legislative and regulatory crackdown on international nongovernmental organizations (INGOs) has constricted the legal environment for foreign advocacy groups interested in influencing domestic and global policy. Although the legal space for advocacy is shrinking, many INGOs have continued their work and found creative ways to adapt to these restrictions, sometimes even reshaping the regulatory environments of their target countries in their favor. In this article, I explore what enables INGOs to cope with and reshape their regulatory environments. I bridge international relations and interest group literatures to examine the interaction between INGO resource configurations and institutional arrangements. I argue that specific resource and managerial characteristics provide organizations with ‘programmatic flexibility’ that enables groups to adjust their strategies without changing their core mission. I illustrate and test this argument with case studies of Article 19 and AMERA International and demonstrate how organizations with high programmatic flexibility can navigate regulations and shape policy in their target country, while those without this flexibility are shut out of policy discussions and often the target country itself. I conclude by exploring how the interaction between internal characteristics and institutional environments shape and constrain the effects of interest groups in global governance."
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html#important-figures",
    "href": "research/articles/heiss-2019-taking-control/index.html#important-figures",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "Important figures",
    "text": "Important figures\nFigure 1: Relationship between institutional constraints, resource configurations, programmatic flexibility, and advocacy effects\n\n\n\nFigure 1: Relationship between institutional constraints, resource configurations, programmatic flexibility, and advocacy effects"
  },
  {
    "objectID": "research/articles/heiss-2019-taking-control/index.html#citation",
    "href": "research/articles/heiss-2019-taking-control/index.html#citation",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{Heiss:2019,\n    Author = {Andrew Heiss},\n    Doi = {10.1057/s41309-019-00061-0},\n    Journal = {Interest Groups and Advocacy},\n    Month = {9},\n    Number = {3},\n    Pages = {356--375},\n    Title = {Taking Control of Regulations: How International Advocacy {NGOs} Shape the Regulatory Environments of their Target Countries},\n    Volume = {8},\n    Year = {2019}}"
  },
  {
    "objectID": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html",
    "href": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html",
    "title": "Derogations, Democratic Backsliding, and International Human Rights During the COVID-19 Pandemic",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html#important-links",
    "href": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html#important-links",
    "title": "Derogations, Democratic Backsliding, and International Human Rights During the COVID-19 Pandemic",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html#abstract",
    "href": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html#abstract",
    "title": "Derogations, Democratic Backsliding, and International Human Rights During the COVID-19 Pandemic",
    "section": "Abstract",
    "text": "Abstract\nDid states misuse international legal emergency provisions during the COVID-19 pandemic to justify human rights abuse or did they follow international human rights law? Many governments restricted citizens’ freedom of movement, association, and assembly during the crisis, raising questions about states’ commitments to international human rights law. Some states used derogations to communicate temporary suspension of international legal provisions in a proportional and non-discriminatory manner, while others did not. We explore the dynamics of democratic backsliding and derogation use during the pandemic. We find that backsliding states were more likely to issue human rights treaty derogations. These derogations had mitigating effects once issued. Backsliding states that issued derogations were more likely to communicate restrictions and were less likely to issue abusive and discriminatory policy during the pandemic. Derogations helped temper abuse in states not experiencing backsliding. However, derogations did not always protect against abuse and media transparency in backsliding states. These results lend support to the use of flexibility mechanisms in international law and find that most states did not use emergency derogations to heighten human rights violations. The study contributes to the understanding of how international legal measures may help mitigate elements of democratic backsliding during times of crisis."
  },
  {
    "objectID": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html#important-figures",
    "href": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html#important-figures",
    "title": "Derogations, Democratic Backsliding, and International Human Rights During the COVID-19 Pandemic",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 6: Predicted probabilities of violating human rights across states with low and high risks of democratic backsliding and derogation status"
  },
  {
    "objectID": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html#citation",
    "href": "research/articles/comstock-heiss-chaudhry-derogations-backsliding/index.html#citation",
    "title": "Derogations, Democratic Backsliding, and International Human Rights During the COVID-19 Pandemic",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ComstockHeissChaudhry:2025,\n    Author = {Audrey L. Comstock and Andrew Heiss and Suparna Chaudhry},\n    Doi = {10.1080/14754835.2024.2446854},\n    Journal = {Journal of Human Rights},\n    Title = {Derogations, Democratic Backsliding, and International Human Rights During the COVID-19 Pandemic},\n    Year = {2025}}"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#important-links",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#important-links",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Important links",
    "text": "Important links\n\nPaper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository\nExperiment preregistration"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#media-coverage",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#media-coverage",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Media coverage",
    "text": "Media coverage\n\n“Donors grow more generous when they support nonprofits facing hostile environments abroad,” The Conversation, December 7, 2020"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#abstract",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#abstract",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Abstract",
    "text": "Abstract\nState restrictions on non-governmental organizations (NGOs) have become increasingly pervasive across the globe. While this crackdown has been shown to have a negative impact on public funding flows, we know little about how it impacts private philanthropy. How does information about crackdown abroad, as well as organizational attributes of nonprofits affect individual donors’ willingness to donate internationally? Using a survey experiment, we find that learning about repressive NGO environments increases generosity in that already-likely donors are willing to donate substantially more to legally besieged nonprofits. This generosity persists when mediated by two organizational-level heuristics: NGO issue areas and main funding sources. We discuss the implications of our results on how nonprofits can use different framing appeals to increase fundraising at a time when traditional public donor funding to such organizations is decreasing."
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#important-figures",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#important-figures",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Important figures",
    "text": "Important figures\nFigures 2 & 3: Difference in likelihood of donation across crackdown and no crackdown groups, conditioned by other experimental frames + difference in amount donated across crackdown and no crackdown groups, conditioned by other experimental frames\n\n\n\nFigures 2 & 3: Difference in likelihood of donation across crackdown and no crackdown groups, conditioned by other experimental frames + difference in amount donated across crackdown and no crackdown groups, conditioned by other experimental frames"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#citation",
    "href": "research/articles/chaudhry-heiss-ngos-philanthropy/index.html#citation",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryHeiss:2021,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Doi = {10.1177/0899764020971045},\n    Journal = {Nonprofit and Voluntary Sector Quarterly},\n    Month = {6},\n    Number = {3},\n    Pages = {481--505},\n    Title = {Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences},\n    Volume = {50},\n    Year = {2021}}"
  },
  {
    "objectID": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html",
    "href": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html",
    "title": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html#important-links",
    "href": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html#important-links",
    "title": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html#abstract",
    "href": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html#abstract",
    "title": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19",
    "section": "Abstract",
    "text": "Abstract\nThis research note asks whether states issuing pandemic-era human rights treaty derogations implemented emergency provisions as intended or used them to abuse human rights during a time of crisis. In an effort to combat the COVID-19 pandemic, many countries declared states of emergency and derogated (temporarily suspended) from their international human rights treaty obligations. Using data from the Varieties of Democracy PanDem dataset and the Oxford COVID-19 Government Response Tracker, we find that states that derogated from their international human rights obligations imposed emergency measures that were temporary and did not violate non-derogable rights. On the other hand, states that did not derogate were more likely impose discriminatory measures, enact emergency measures without time limits and violate non-derogable rights. Our results support the role that flexibility mechanisms such as derogations play in international law and show that states are being sincere about their intentions and not, generally, using these mechanisms to cover abusive behavior."
  },
  {
    "objectID": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html#important-figures",
    "href": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html#important-figures",
    "title": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR\n\n\n\n\n\nFigure 3a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR"
  },
  {
    "objectID": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html#citation",
    "href": "research/articles/chaudhry-comstock-heiss-pandemic-pass-2024/index.html#citation",
    "title": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryComstockHeiss:2024,\n    Author = {Suparna Chaudhry and Audrey L. Comstock and Andrew Heiss},\n    Doi = {10.1080/03050629.2024.2413965},\n    Journal = {International Interactions},\n    Pages = {1--23},\n    Title = {Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19},\n    Year = {2024}}"
  },
  {
    "objectID": "now/index.html",
    "href": "now/index.html",
    "title": "What I’m doing now",
    "section": "",
    "text": "As of February 19, 2025, I’m spending all my time on these things:\n\nStaying at home pretty much 24/7 (STILL) because of the COVID-19 pandemic\nRaising 6 kids (17.5, 15, 12.5, 9.5, 7.5, and 3) and trying to stay sane (family blog)\nLiving in Atlanta and working as an assistant professor in the Department of Public Management and Policy at the Andrew Young School of Policy Studies at Georgia State University\n\nTeaching data visualization, program evaluation, comparative public administration, nonprofit management, and microeconomics at the Andrew Young School of Policy Studies at Georgia State University\n\nWorking as a part time data science mentor for Posit Academy\n\nConverting my dissertation into multiple articles and sending them out to journals + continuing my research on authoritarianism and international NGOs\n\nWorking on several articles on NGO restrictions with Suparna Chaudhry and Marc Dotson\n\nReading some sort of religiously themed text every day (books)"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum Vitæ",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/2025/02/19/ggplot-histogram-legend/index.html",
    "href": "blog/2025/02/19/ggplot-histogram-legend/index.html",
    "title": "How to use a histogram as a legend in {ggplot2}",
    "section": "",
    "text": "On Bluesky the other day, I came across this neat post that suggested using a histogram as a plot legend to provide additional context for the data being shown:\nHere’s a closer comparison of those two maps (click to zoom):\nThis histogram legend is especially useful for choropleth maps where units like counties are sized differently, which can create an illusion of a different distribution. For instance, in that original post, larger dark blue areas stand out a lot visually—like in Alaska, New Mexico, Arizona, and Central California—and make it seem like unemployment is fairly high.\nBut looking at the histogram that’s not actually the case. Most counties have an unemployment rate around 3–6%. This illusion is happening because land isn’t unemployed—people are.\nI thought this was a cool approach, so I figured I’d try to replicate it with R. In the original post, the map was created with D3, the bar chart legend was created with Excel, and the two were combined with Figma. That process is a little too manual for me, but with the magic of R, {ggplot2}, and {patchwork}, we can create the same map completely programmatically.\nLet’s do it!"
  },
  {
    "objectID": "blog/2025/02/19/ggplot-histogram-legend/index.html#clean-and-join-data",
    "href": "blog/2025/02/19/ggplot-histogram-legend/index.html#clean-and-join-data",
    "title": "How to use a histogram as a legend in {ggplot2}",
    "section": "Clean and join data",
    "text": "Clean and join data\nFirst, let’s load some packages and tweak some theme settings:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(sf)\nlibrary(tigris)\nlibrary(patchwork)\n\n# Add some font settings to theme_void()\ntheme_fancy_map &lt;- function() {\n  theme_void(base_family = \"IBM Plex Sans\") +\n    theme(\n      plot.title = element_text(face = \"bold\", hjust = 0.13, size = rel(1.4)),\n      plot.subtitle = element_text(hjust = 0.13, size = rel(1.1)),\n      plot.caption = element_text(hjust = 0.13, size = rel(0.8), color = \"grey50\"),\n    )\n}\n\nBLS unemployment data\nNext, we can get 2016 unemployment data from the Bureau of Labor Statistics. BLS offers county-level data on annual average labor force participation here, both as plain text and Excel files. The plain text data is structured a little goofily (it’s not comma-separated; it’s a fixed width format where column headings span multiple lines), but the Excel version is in nice columns and is easier to work with. Though even then, we need to skip the first few rows, and the last few rows, and specify column names ourselves.\nDownload this first from the BLS:\n\nLabor force data by county, 2016 annual averages (XLS)\n\nFor the sake of mapping, we’ll truncate the unemployment rate at 9% and mark any counties with higher than 9% unemployment with 9.1 and modify the legend to show “&gt;9%”:\n\n# Load BLS data and clean it up\nbls_2016 &lt;- read_excel(\n  \"laucnty16.xlsx\",\n  skip = 5,\n  col_names = c(\n    \"laus_code\", \"STATEFP\", \"COUNTYFP\", \"county_name_state\",\n    \"year\", \"nothing\", \"labor_force\", \"employed\", \"unemployed\", \"unemp\"\n  )\n) |&gt; \n  # The last few rows in the Excel file aren't actually data, but extra notes,\n  # so drop those rows here since they don't have a state FIPS code\n  drop_na(STATEFP) |&gt; \n  mutate(\n    # Truncate the unemployment rate at 9\n    unemp_truncated = ifelse(unemp &gt; 9, 9.1, unemp),\n    # Find difference from Fed target of 4%\n    unemp_diff = unemp_truncated - 4\n  )\n\nbls_2016\n## # A tibble: 3,219 × 12\n##    laus_code       STATEFP COUNTYFP county_name_state   year  nothing labor_force employed unemployed unemp unemp_truncated unemp_diff\n##    &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;               &lt;chr&gt; &lt;lgl&gt;         &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n##  1 CN0100100000000 01      001      Autauga County, AL  2016  NA            25710    24395       1315   5.1             5.1        1.1\n##  2 CN0100300000000 01      003      Baldwin County, AL  2016  NA            89778    84972       4806   5.4             5.4        1.4\n##  3 CN0100500000000 01      005      Barbour County, AL  2016  NA             8334     7638        696   8.4             8.4        4.4\n##  4 CN0100700000000 01      007      Bibb County, AL     2016  NA             8539     7986        553   6.5             6.5        2.5\n##  5 CN0100900000000 01      009      Blount County, AL   2016  NA            24380    23061       1319   5.4             5.4        1.4\n##  6 CN0101100000000 01      011      Bullock County, AL  2016  NA             4785     4457        328   6.9             6.9        2.9\n##  7 CN0101300000000 01      013      Butler County, AL   2016  NA             9116     8484        632   6.9             6.9        2.9\n##  8 CN0101500000000 01      015      Calhoun County, AL  2016  NA            45450    42470       2980   6.6             6.6        2.6\n##  9 CN0101700000000 01      017      Chambers County, AL 2016  NA            14858    14044        814   5.5             5.5        1.5\n## 10 CN0101900000000 01      019      Cherokee County, AL 2016  NA            11241    10671        570   5.1             5.1        1.1\n## # ℹ 3,209 more rows\n\nCensus geographic data\nNext we’ll get geographic data from the US Census with {tigris}\n\n\n\n\n\n\nBackup data source\n\n\n\nAt the time of this writing, {tigris} is working. It wasn’t working a couple weeks ago as the wildly illegal Department of Government Efficiency rampaged through different federal agencies—including the US Census—and shut down the Census’s GIS APIs. But it seems to be working for now?\nIf it’s not working, IPUMS’s NHGIS project offers the same shapefiles.\n\n\nThe BLS data and the Census data each have columns with state and county FIPS codes which we can use to join the two datasets:\n\n# Get county and state shapefiles from Tigris\nus_counties &lt;- counties(year = 2016, cb = TRUE) |&gt; \n  filter(as.numeric(STATEFP) &lt;= 56) |&gt; \n  shift_geometry()  # Move AK and HI\n\nus_states &lt;- states(year = 2016, cb = TRUE) |&gt; \n  filter(as.numeric(STATEFP) &lt;= 56) |&gt; \n  shift_geometry()  # Move AK and HI\n\n# Join BLS data to the map\ncounties_with_unemp &lt;- us_counties |&gt;\n  left_join(bls_2016, by = join_by(STATEFP, COUNTYFP))\n\n# Check out the joined data\ncounties_with_unemp |&gt; \n  select(STATEFP, COUNTYFP, county_name_state, unemp_truncated, geometry)\n## Simple feature collection with 3142 features and 4 fields\n## Geometry type: GEOMETRY\n## Dimension:     XY\n## Bounding box:  xmin: -3112000 ymin: -1698000 xmax: 2258000 ymax: 1566000\n## Projected CRS: USA_Contiguous_Albers_Equal_Area_Conic\n## First 10 features:\n##    STATEFP COUNTYFP    county_name_state unemp_truncated                       geometry\n## 1       19      107    Keokuk County, IA             4.3 MULTIPOLYGON (((297173 4548...\n## 2       19      189 Winnebago County, IA             3.4 MULTIPOLYGON (((163347 6734...\n## 3       20      093    Kearny County, KS             3.1 MULTIPOLYGON (((-482328 605...\n## 4       20      123  Mitchell County, KS             3.3 MULTIPOLYGON (((-212918 197...\n## 5       20      187   Stanton County, KS             2.8 MULTIPOLYGON (((-528445 214...\n## 6       21      005  Anderson County, KY             4.0 MULTIPOLYGON (((940067 1094...\n## 7       21      029   Bullitt County, KY             4.1 MULTIPOLYGON (((873753 1022...\n## 8       21      049     Clark County, KY             4.7 MULTIPOLYGON (((1012432 106...\n## 9       21      059   Daviess County, KY             4.4 MULTIPOLYGON (((749702 5517...\n## 10      21      063   Elliott County, KY             9.1 MULTIPOLYGON (((1102886 138...\n\nThe map works!\n\nggplot() +\n  geom_sf(data = us_states, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  # Albers projection\n  coord_sf(crs = st_crs(\"ESRI:102003\"))\n\n\n\n\n\n\n\nMap adjustments\nWe need to make a couple little adjustments to the map first. In the original image on Bluesky, there’s extra space on the right side of the map to allow for the legend. We can change the plot window by adding 10% of the width of the map to the right.\nTechnically we don’t have to work with percents here; the data is currently using the Albers projection, which works in meters, so we could add something like 500,000 meters / 500 km to the left. But this is a more general solution and also works if the map data is in decimal degrees instead of meters.\nAlso, the far western Aleutian islands mess with the visual balance of the map (and they don’t appear because they’re so small), so we’ll also subtract 10% of the map from the left.\n\n# Get x-axis limits of the bounding box for the state data\nxlim_current &lt;- st_bbox(us_states)$xlim\n\n# Add 540ish km (or 10% of the US) to the bounds (thus shifting the window over)\nxlim_expanded &lt;- c(\n  xlim_current[1] + (0.1 * diff(xlim_current)), \n  xlim_current[2] + (0.1 * diff(xlim_current))\n)\n\nggplot() +\n  geom_sf(data = us_states, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:102003\"), xlim = xlim_expanded)\n\n\n\n\n\n\n\nExtract interior state borders\nBecause we’re using color = \"white\", linewidth = 0.25, every state gets a thin white border. This causes some issues though. All the states that share borders actually get a thicker border, since a state’s western border joins up with its neighbor’s eastern border. Also, all the coastlines and islands get borders, which diminishes the landmass—especially on a white background.\nLike, look at Alaska’s Aleutian Islands, or Hawai’i’s smaller islands, or Michigan’s Les Cheneaux Islands and Isle Royale, or California’s Channel Islands, or the Florida Keys, or North Carolina’s Outer Banks—they all basically disappear.\nTo fix this, we can use st_intersection() to identify the intersections of all the state shapes (see this and this for more details)\nNow all the islands and coastlines have much better definition and the borders between states are truly sized at 0.25:\n\ninterior_state_borders &lt;- st_intersection(us_states) |&gt;\n  filter(n.overlaps &gt; 1) |&gt; \n  # Remove weird points that st_intersection() adds\n  filter(!(st_geometry_type(geometry) %in% c(\"POINT\", \"MULTIPOINT\")))\n\nggplot() +\n  geom_sf(data = us_states, fill = \"#0074D9\", linewidth = 0) +\n  geom_sf(data = interior_state_borders, linewidth = 0.25, color = \"white\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\"), xlim = xlim_expanded)"
  },
  {
    "objectID": "blog/2025/02/19/ggplot-histogram-legend/index.html#map-with-horizontal-gradient-step-legend",
    "href": "blog/2025/02/19/ggplot-histogram-legend/index.html#map-with-horizontal-gradient-step-legend",
    "title": "How to use a histogram as a legend in {ggplot2}",
    "section": "Map with horizontal gradient step legend",
    "text": "Map with horizontal gradient step legend\nNow that we have cleaned and adjusted geographic and unemployment data, we can make a fancy map! Instead of building this sequentially, I’ve included all the code all at once, with lots of comments at each step.\nA few things to note:\n\nscale_fill_stepsn() lets you use distinct bins of color instead of a continuous gradient\n\nWe position the legend inside the plot with theme(legend.position = \"inside\", legend.position.inside = c(0.86, 0.32)). Those 0.86, 0.32 coordinates took a lot of tinkering to get! The units for legend.position.inside are based on percentages of the plot, so the legend appears where x is 86% across and 32% up. The position changes every time the plot dimensions change. To make life easier as I played with different values, I used {ggview} to specify and lock in exact dimensions of the plot:\nlibrary(ggview)\n\np &lt;- ggplot(...) +\n  geom_sf(...)\n\np + canvas(7, 5)\nI’m not using ggview::canvas() here in the post because I’m specifying figure dimensions with Quarto chunk options instead (fig-width: 7 and fig-height: 5).\n\n\nHere’s the map!\n\nggplot() +\n  # Add counties filled with unemployment levels\n  geom_sf(\n    data = counties_with_unemp, aes(fill = unemp_truncated), linewidth = 0\n  ) +\n  # Add interior state boundaries\n  geom_sf(\n    data = interior_state_borders, color = \"white\", linewidth = 0.25\n  ) +\n  # Show the unemployment legend as steps instead of a standard gradient\n  scale_fill_stepsn(\n    colours = scales::brewer_pal(palette = \"YlGnBu\")(9),\n    breaks = 1:10,\n    limits = c(1, 10),\n    # Change the label for &gt;9%\n    labels = case_match(\n      1:10,\n      1 ~ \"1%\",\n      10 ~ \"&gt;9%\",\n      .default = as.character(1:10)\n    )\n  ) +\n  # Yay labels\n  labs(\n    title = \"US unemployment rates\",\n    subtitle = \"2016 annual averages by county\",\n    caption = \"Source: US Bureau of Labor Statistics\",\n    fill = \"Unemployment rate\"\n  ) +\n  # Use Albers projection and new x-axis limits\n  coord_sf(crs = st_crs(\"ESRI:102003\"), xlim = xlim_expanded) +\n  # Theme adjustments\n  theme_fancy_map() +\n  theme(\n    legend.position = \"inside\",\n    legend.position.inside = c(0.86, 0.32),\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = rel(0.55)),\n    legend.title = element_text(hjust = 0.5, face = \"bold\", size = rel(0.7), margin = margin(t = 3)),\n    legend.title.position = \"bottom\",\n    legend.key.width = unit(1.55, \"lines\"),\n    legend.key.height = unit(0.7, \"lines\")\n  )"
  },
  {
    "objectID": "blog/2025/02/19/ggplot-histogram-legend/index.html#map-with-histogram-legend",
    "href": "blog/2025/02/19/ggplot-histogram-legend/index.html#map-with-histogram-legend",
    "title": "How to use a histogram as a legend in {ggplot2}",
    "section": "Map with histogram legend",
    "text": "Map with histogram legend\nWe can replace the step gradient legend with a histogram that is filled using the same colors as the step legend.\nThe easiest method that gives us the most control over the legend histogram is to create a separate plot object for the histogram and place it inside the map with {patchwork}’s inset_element().\nHere’s the histogram, again with comments at each step. Only one neat trick to note here:\n\n\ngeom_histogram automatically determines the bin width for the variable assigned to the x aesthetic. In order to fill each bar by bin-specific color, we need to access information about those newly created bins. We can do this with after_stat()—here we fill each bar using the already-calculated x bin categories with fill = after_stat(factor(x))\n\n\n\nhist_legend &lt;- ggplot(bls_2016, aes(x = unemp_truncated)) +\n  # Fill each histogram bar using the x axis category that ggplot creates\n  geom_histogram(\n    aes(fill = after_stat(factor(x))), \n    binwidth = 1, boundary = 0, color = \"white\"\n  ) +\n  # Fill with the same palette as the map\n  scale_fill_brewer(palette = \"YlGnBu\", guide = \"none\") +\n  # Modify the x-axis labels to use &gt;9%\n  scale_x_continuous(\n    breaks = 2:10, \n    labels = case_match(\n      2:10,\n      2 ~ \"2%\",\n      10 ~ \"&gt;9%\",\n      .default = as.character(2:10)\n    )\n  ) +\n  # Just one label to replicate the legend title\n  labs(x = \"Unemployment rate\") +\n  # Theme adjustments\n  theme_fancy_map() +\n  theme(\n    axis.text.x = element_text(size = rel(0.55)),\n    axis.title.x = element_text(size = rel(0.68), margin = margin(t = 3, b = 3), face = \"bold\")\n  )\nhist_legend\n\n\n\n\n\n\n\nNext, we’ll place that hist_legend plot inside a map with inset_element(). Like legend.position.inside = c(0.86, 0.32) in the previous map, the left = 0.75, bottom = 0.26, right = 0.98, top = 0.5 values here are percentages of the plot area and they’re fully dependent on the overall dimensions of the plot. Getting these exact numbers took a lot of manual adjusting, and ggview::canvas() was once again indispensable for keeping the plot dimensions constant.\n\nunemp_map &lt;- ggplot() +\n  # Add counties filled with unemployment levels\n  geom_sf(\n    data = counties_with_unemp, aes(fill = unemp_truncated), color = NA, linewidth = 0\n  ) +\n  # Add interior state boundaries\n  geom_sf(\n    data = interior_state_borders, color = \"white\", linewidth = 0.25, fill = NA\n  ) +\n  # Show the unemployment legend as steps instead of a standard gradient, but\n  # don't actually show the legend\n  scale_fill_stepsn(\n    colours = scales::brewer_pal(palette = \"YlGnBu\")(9),\n    breaks = 1:10, \n    guide = \"none\"\n  ) +\n  # Yay labels\n  labs(\n    title = \"US unemployment rates\",\n    subtitle = \"2016 annual averages by county\",\n    caption = \"Source: US Bureau of Labor Statistics\"\n  ) +\n  # Use Albers projection and new x-axis limits\n  coord_sf(crs = st_crs(\"ESRI:102003\"), xlim = xlim_expanded) +\n  # Theme stuff\n  theme_fancy_map()\n\n# Add the histogram to the map\ncombined_map_hist &lt;- unemp_map + \n  inset_element(hist_legend, left = 0.75, bottom = 0.26, right = 0.98, top = 0.45)\ncombined_map_hist"
  },
  {
    "objectID": "blog/2025/02/19/ggplot-histogram-legend/index.html#map-with-automatic-histogram-legend-with-legendry",
    "href": "blog/2025/02/19/ggplot-histogram-legend/index.html#map-with-automatic-histogram-legend-with-legendry",
    "title": "How to use a histogram as a legend in {ggplot2}",
    "section": "Map with automatic histogram legend with {legendry}",
    "text": "Map with automatic histogram legend with {legendry}\nFinally, the new {legendry} package makes it so we can create a custom histogram-based legend without needing to use {patchwork} with a separate histogram plot!\nIt doesn’t provide as much control over the resulting histogram. The gizmo_histogram() function uses base R’s hist() behind the scenes, so we have to specify bin widths and other settings in hist.arg as base R arguments, like breaks = 10 instead of ggplot’s binwidth = 10.\nNot all of hist()’s options seem to work here. For instance, I get a warning if I use border = \"white\" to add a white border around each bar (argument ‘border’ is not made use of), since that border option is disabled when using base R’s hist() with plot = FALSE:\n\nhist(counties_with_unemp$unemp_truncated, breaks = 10, border = \"white\", plot = FALSE)\n## Warning in hist.default(counties_with_unemp$unemp_truncated, breaks = 10, : argument 'border' is not made use of\n## $breaks\n##  [1]  1  2  3  4  5  6  7  8  9 10\n## \n## $counts\n## [1]  12 244 589 821 644 410 205  97 119\n## \n## $density\n## [1] 0.00382 0.07768 0.18752 0.26138 0.20503 0.13053 0.06527 0.03088 0.03789\n## \n## $mids\n## [1] 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5\n## \n## $xname\n## [1] \"counties_with_unemp$unemp_truncated\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n\nAlso, it’s currently filling each histogram bar with the full gradient, not the 9 distinct steps, and I can’t figure out how to define custom colors for each bar—and it might not even be possible since color settings aren’t picked up anyway because of {legendry}’s use of plot = FALSE 🤷‍♂️.\nBut despite these downsides, this automatic histogram legend with {legendry} is really neat!\n\nlibrary(legendry)\n\n# Create a custom histogram guide\nhistogram_guide &lt;- compose_sandwich(\n  middle = gizmo_histogram(just = 0, hist.arg = list(breaks = 10)),\n  text = \"axis_base\"\n)\n\nggplot() +\n  # Add counties filled with unemployment levels\n  geom_sf(\n    data = counties_with_unemp, aes(fill = unemp_truncated), color = NA, linewidth = 0\n  ) +\n  # Add interior state boundaries\n  geom_sf(\n    data = interior_state_borders, color = \"white\", linewidth = 0.25, fill = NA\n  ) +\n  # Show the unemployment legend with a custom histogram guide\n  scale_fill_stepsn(\n    colours = scales::brewer_pal(palette = \"YlGnBu\")(9),\n    breaks = 1:10,\n    limits = c(1, 10),\n    guide = histogram_guide,\n    # Change the label for &gt;9%\n    labels = case_match(\n      1:10,\n      1 ~ \"1%\",\n      10 ~ \"&gt;9%\",\n      .default = as.character(1:10)\n    )\n  ) +\n  # Yay labels\n  labs(\n    title = \"US unemployment rates\",\n    subtitle = \"2016 annual averages by county\",\n    caption = \"Source: US Bureau of Labor Statistics\",\n    fill = \"Unemployment rate\"\n  ) +\n  # Use Albers projection and new x-axis limits\n  coord_sf(crs = st_crs(\"ESRI:102003\"), xlim = xlim_expanded) +\n  # Theme stuff\n  theme_fancy_map() +\n  theme(\n    legend.position = \"inside\",\n    legend.position.inside = c(0.86, 0.32),\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = rel(0.55)),\n    legend.title = element_text(hjust = 0.5, face = \"bold\", size = rel(0.7), margin = margin(t = 3)),\n    legend.title.position = \"bottom\"\n  )"
  },
  {
    "objectID": "blog/2025/02/19/ggplot-histogram-legend/index.html#bonus-use-points-instead-of-choropleths",
    "href": "blog/2025/02/19/ggplot-histogram-legend/index.html#bonus-use-points-instead-of-choropleths",
    "title": "How to use a histogram as a legend in {ggplot2}",
    "section": "Bonus! Use points instead of choropleths",
    "text": "Bonus! Use points instead of choropleths\nWe’re still using choropleth maps here, which still isn’t ideal for showing the idea that “land isn’t unemployed”. One solution is to plot points that are sized by population. This is pretty straightforward with {sf}—we need to convert the county polygons into single points, which we can do with st_point_on_surface(). Then, after a bunch of tinkering with legend options, we’ll have this gorgeous map:\n\n# Convert the county polygons into single points\ncounties_with_unemp_points &lt;- counties_with_unemp |&gt; \n  st_point_on_surface()\n\nunemp_map_points &lt;- ggplot() +\n  # Use a gray background\n  geom_sf(data = us_states, fill = \"gray90\", linewidth = 0) +\n  geom_sf(data = interior_state_borders, linewidth = 0.25, color = \"white\") +\n  # Include semi-transparent points with shape 21 (so there's a border)\n  geom_sf(\n    data = counties_with_unemp_points, \n    aes(size = labor_force, fill = unemp_truncated), \n    pch = 21, color = \"white\", stroke = 0.25, alpha = 0.8\n  ) +\n  # Control the size of the points in the legend\n  scale_size_continuous(\n    range = c(1, 9), labels = scales::label_comma(), \n    breaks = c(10000, 100000, 1000000),\n    # Make the points black and not have a border\n    guide = guide_legend(override.aes = list(pch = 19, color = \"black\"))\n  ) +\n  # Show the unemployment legend as steps instead of a standard gradient, but\n  # don't actually show the legend\n  scale_fill_stepsn(\n    colours = scales::brewer_pal(palette = \"YlGnBu\")(9),\n    breaks = 1:10, \n    guide = \"none\"\n  ) +\n  # Labels\n  labs(\n    title = \"US unemployment rates\",\n    subtitle = \"2016 annual averages by county\",\n    caption = \"Source: US Bureau of Labor Statistics\",\n    fill = \"Unemployment rate\",\n    size = \"Labor force\"\n  ) +\n  # Albers\n  coord_sf(crs = st_crs(\"ESRI:102003\"), xlim = xlim_expanded) +\n  # Theme stuff\n  theme_fancy_map() +\n  theme(\n    legend.position = \"inside\",\n    legend.position.inside = c(0.837, 0.13),\n    legend.text = element_text(size = rel(0.55)),\n    legend.title = element_text(hjust = 0.5, face = \"bold\", size = rel(0.7), margin = margin(t = 3)),\n    legend.title.position = \"bottom\"\n  )\n\n# Add the histogram to the map\ncombined_map_hist_points &lt;- unemp_map_points + \n  inset_element(hist_legend, left = 0.75, bottom = 0.26, right = 0.98, top = 0.45)\ncombined_map_hist_points"
  },
  {
    "objectID": "blog/2025/02/19/ggplot-histogram-legend/index.html#bonus-2-use-a-diverging-color-scheme-nested-legend-circles",
    "href": "blog/2025/02/19/ggplot-histogram-legend/index.html#bonus-2-use-a-diverging-color-scheme-nested-legend-circles",
    "title": "How to use a histogram as a legend in {ggplot2}",
    "section": "Bonus #2! Use a diverging color scheme + nested legend circles",
    "text": "Bonus #2! Use a diverging color scheme + nested legend circles\nBut wait, there’s more! Based on discussions with really smart dataviz people on Bluesky in the wake of me posting about this blog post there, we can make two additional tweaks:\n\n\nWhile the different sizes for the points are neat, I’m not a fan of how big the vertical spacing is between the 10,000; 100,000; and 1,000,000. Unfortunately there’s no way to change it. Technically we can use legend.key.spacing.y in theme() to adjust it, but that doesn’t work as expected here because each of those legend entries is sized to match the largest point—i.e., the point for 1,000,000 is the biggest, so the legend entries for all the other values match its height, even if they don’t need all that space.\nTo fix this, we can use guide_circles() from {legendry} to show the different point sizes as coencentric circles, which is more compact (and just looks neat).\n\n\nInstead of showing a range of low → high values, we can color these counties based on a meaningful midpoint to help highlight which counties are doing great (low unemployment! good!) and which aren’t (high unemployment! bad!). That might not always necessarily be the best approach—showing the full range of actual values like in the original map is a way of just describing the range and doesn’t inherently imply good or bad. But in other plots where data might be more actionable, divergences from some central value would be much more helpful.\nIn the United States, the Federal Reserve has a unique dual mandate to use macroeconomic policies to target both inflation and unemployment (most other countries’ central banks only target inflation). The Fed typically aims for an inflation rate of 2% and an unemployment rate of 4ish%. So in this new map, we’ll center each county’s unemployment rate around 4% and show the percentage point deviations from that Fed target. Counties colored in darker red have higher unemployment rates than the target; counties colored in blue have lower rates than the target.\nWe can then imagine that we’re a policymaker interested in unemployment trends—we can look at the map and quickly identify areas that are doing poorly and doing well.\n\n\nUp at the beginning of the document where we loaded and cleaned the bls_2016 dataset, I’ve added a new variable that centers the unemployment rate at 4:\nmutate(unemp_diff = unemp_truncated - 4)\nWe can then use this to create a new histogram and new map colored with the “vik” palette from the {scico} package, which has lots of neat diverging palettes. We’ll also create a fancy circle-based legend with {legendry}. Here’s the fully annotated code and final map:\n\nlibrary(ggtext)  # For Markdown-based text in ggplot\nlibrary(scico)   # For perceptually uniform colors\n\n# Make new histogram legend\nhist_legend_diffs &lt;- ggplot(bls_2016, aes(x = unemp_diff)) +\n  # Fill each histogram bar using the x axis category that ggplot creates\n  # Use boundary = 0.5 to shift the bin ranges from things like 1-2 to 1.5-2.5\n  geom_histogram(\n    aes(fill = after_stat((x))), \n    binwidth = 1, boundary = 0.5, color = \"white\"\n  ) +\n  # Fill with the same palette as the map\n  # scale_fill_brewer(palette = \"YlGnBu\", guide = \"none\") +\n  scale_fill_scico(palette = \"vik\", midpoint = 0, guide = \"none\") +\n  # Modify the x-axis labels to show perentage point values and format them with\n  # markdown to get original unemployment values on separate lines\n  scale_x_continuous(\n    breaks = -2:5, \n    labels = case_match(\n      -2:5,\n      -2 ~ \"**−2 pp.**&lt;br&gt;(2%)\",\n      0 ~ \"**0**&lt;br&gt;(4% ±&lt;br&gt;0.5 pp.)\",\n      5 ~ \"**&gt;+4 pp.**&lt;br&gt;(&gt;9%)\",\n      .default = glue::glue(\n        \"**{x}**\", \n        x = scales::label_comma(\n          style_positive = \"plus\", style_negative = \"minus\"\n        )(-2:5))\n    )\n  ) +\n  # Just one label to replicate the legend title\n  labs(x = \"Difference from Fed target\") +\n  # Theme adjustments\n  theme_fancy_map() +\n  theme(\n    axis.text.x = element_markdown(size = rel(0.5), vjust = 1, lineheight = 1.3),\n    axis.title.x = element_text(size = rel(0.68), margin = margin(t = 3, b = 3), face = \"bold\")\n  )\n\nunemp_map_points_diffs &lt;- ggplot() +\n  # Use a lighter gray background\n  geom_sf(data = us_states, fill = \"gray95\", linewidth = 0) +\n  # Use slightly darker state borders\n  geom_sf(data = interior_state_borders, linewidth = 0.25, color = \"grey60\") +\n  # Include semi-transparent points with shape 21 (so there's a border)\n  geom_sf(\n    data = counties_with_unemp_points, \n    aes(size = labor_force, fill = unemp_diff), \n    shape = 21, color = \"white\", stroke = 0.25, alpha = 0.8\n  ) +\n  # Control the size of the points in the legend\n  scale_size_continuous(\n    range = c(1, 11), labels = scales::label_comma(), \n    breaks = c(100000, 1000000, 5000000),\n    # Make the points black and not have a border\n    guide = guide_circles(\n      text_position = \"right\",\n      override.aes = list(\n        fill = \"grey30\", alpha = 0.8\n      )\n    )\n  ) +\n  # This is tricky! We want to use the diverging vik palette but have it \n  # centered at 0. With scale_fill_scico(), there's a midpoint argument, like we \n  # used for the histogram. For generating regular lists of colors with scico(), \n  # though, there's no midpoint argument. Instead, we need to make a few \n  # specific adjustments: \n  #\n  # 1. Generate 11 possible colors, since there are 5 colors above the 0 \n  #    midpoint in the histogram and we need 5 parallel negative colors below 0 \n  #    (even though we're only using 2)\n  # 2. Set the limits of the legend to the symmetrical -5 to 5 range so that \n  #    it's centered at 0\n  # 3. Set the breaks to go asymmetrically from -2:5. But actually set them \n  #    from -2.5 to 4.5 since that matches the shifted histogram, which uses a \n  #    boundary of 0.5 instead of 0 (so the histogram bins cover ranges like \n  #    0.5 to 1.5 instead of 0 to 1)\n  scale_fill_stepsn(\n    colours = scico::scico(11, palette = \"vik\"),\n    limits = c(-5, 5),\n    breaks = seq(-2.5, 4.5, by = 1),\n    guide = \"none\"\n  ) +\n  # Labels\n  labs(\n    title = \"US unemployment (2016)\",\n    subtitle = \"Differences from the Federal Reserve's 4% target\",\n    caption = \"Source: US Bureau of Labor Statistics\",\n    fill = \"Unemployment rate\",\n    size = \"County labor force\"\n  ) +\n  # Albers\n  coord_sf(crs = st_crs(\"ESRI:102003\"), xlim = xlim_expanded) +\n  # Theme adjustments\n  theme_fancy_map() +\n  theme(\n    # {legendry} complains if there's no legend.margin setting; using \n    # theme_void() removes that setting and breaks the plot, so we specify \n    # some 0 values here\n    legend.margin = margin(0, 0, 0, 0, \"pt\"),\n    legendry.legend.key.margin = margin(0, 5, 0, 0, \"pt\"),\n    legend.ticks = element_line(colour = \"black\", linetype = \"22\"),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.87, 0.17),\n    legend.text = element_text(size = rel(0.55)),\n    legend.title = element_text(hjust = 0.5, face = \"bold\", size = rel(0.7), margin = margin(t = 3)),\n    plot.subtitle = element_text(hjust = 0.18),\n    legend.title.position = \"bottom\"\n  )\n\n# Add the histogram to the map\ncombined_map_hist_points_diffs &lt;- unemp_map_points_diffs + \n  inset_element(hist_legend_diffs, left = 0.75, bottom = 0.26, right = 0.98, top = 0.45)\ncombined_map_hist_points_diffs"
  },
  {
    "objectID": "blog/2025/02/10/usaid-ojs-maps/index.html",
    "href": "blog/2025/02/10/usaid-ojs-maps/index.html",
    "title": "Using USAID data to make fancy world maps with Observable Plot",
    "section": "",
    "text": "As part of Elon Musk’s weird Department of Government Efficiency’s unconstitutional rampage through the federal government, USAID’s ForeignAssistance.gov was taken offline on January 31, 2025. It reappeared on February 3, but it’s not clear how long it will be available, especially as USAID is gutted (despite court orders and injunctions to stop).\nI study civil society, human rights, and foreign aid and rely on USAID aid data for several of my research projects, so as a backup, I used Datasette to create a mirror website/API of the entire ForeignAssistance.gov dataset at https://foreignassistance-data.andrewheiss.com/. Everything as of December 19, 2024 is available there, both as a queryable SQL database and as downloadable CSV files.\nI also made a little frontend website with links to each individual dataset. As I built that website, I decided to try recreating the ForeignAssistance.gov dashboard, which had neat interactive maps and tables.\nSince Quarto has native support for Observable JS for interactive work, and since I’ve meant to really dig into Observable and figure out how to make more interactive graphs, I figured I’d play around with the rescued USAID data.\nSo in this post, I show what I learned about working with geographic data and making pretty maps with Observable Plot,"
  },
  {
    "objectID": "blog/2025/02/10/usaid-ojs-maps/index.html#working-with-map-data",
    "href": "blog/2025/02/10/usaid-ojs-maps/index.html#working-with-map-data",
    "title": "Using USAID data to make fancy world maps with Observable Plot",
    "section": "Working with map data",
    "text": "Working with map data\nGet map data\nObservable Plot uses the d3-geo module behind the scenes to parse and work with map data, and D3 typically works with data formatted as GeoJSON. There are tons of high quality geographic data sources online, like the US Census (they’ve been removing those in the past few weeks), IPUMS NHGIS, IPUMS IHGIS, and the Natural Earth project, and cities and states typically offer GIS data for public sector-related data. These data sources tend to be stored as shapefiles, which are a fairly complex (but standard) format for geographic data that involve multiple files.\nObservable Plot/D3 might be able to work with shapefiles directly, but it’s nowhere in the documentation. They seem to expect GeoJSON instead. We could hunt around online for GeoJSON data, but—even better—we can use the {sf} package in R to convert any shapefile-based data into GeoJSON by setting driver = \"GeoJSON\" in sf::st_write(). Here we’ll load two datasets from Natural Earth—(1) small scale low resolution 1:110m data for mapping the whole world and (2) medium scale 1:50m data for mapping specific regions and countries—and convert them to GeoJSON files.\n\nlibrary(sf)\nlibrary(rnaturalearth)\n\n# Get low resolution Natural Earth data as map units instead of countries because of France\nworld &lt;- ne_countries(scale = 110, type = \"map_units\")\n\n# Save as geojson for Observable Plot\nst_write(\n  obj = world, \n  dsn = \"ne_110m_admin_0_countries.geojson\", \n  driver = \"GeoJSON\"\n)\n\n# Save medium resolution geojson\nst_write(\n  obj = ne_countries(scale = 50, type = \"countries\"), \n  dsn = \"ne_50m_admin_0_countries.geojson\", \n  driver = \"GeoJSON\"\n)\n\n\n\n\n\n\n\nMaybe skip intermediate saving?\n\n\n\nWe could probably use Quarto’s special R-to-OJS function ojs_define() and make these R objects directly accessible to OJS without needing to save intermediate files:\n\nojs_define(world = ne_countries(scale = 110, type = \"map_units\"))\n\n…but geographic data is complex and I don’t know how things like Observable Plot’s Plot.geo() handle data that’s not read as GeoJSON. So to keep things simple, I ended up just saving these as GeoJSON. 🤷‍♂️\n\n\nMaps and projections with Observable Plot\nWe can load these into our document with OJS with FileAttachment():\n\nworld = FileAttachment(\"ne_110m_admin_0_countries.geojson\").json()\nworld_medium = FileAttachment(\"ne_50m_admin_0_countries.geojson\").json()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck out the structure of world. It’s a FeatureCollection with a slot named crs with the projection information and a slot named features with entries for each country. Each country Feature has a slot named properties with columns like name, iso_a3, formal_en, pop_est, and other details.\n\nworld\n\n\n\n\n\n\nTo plot it, we can use the Geo mark:\n\nPlot.plot({\n  marks: [\n    Plot.geo(world)\n  ]\n})\n\n\n\n\n\n\nTo make things look nicer throughout this post, we’ll define some nicer colors for countries and land and ocean from CARTOColors:\n\ncarto_prism = [\n  \"#5F4690\", \"#1D6996\", \"#38A6A5\", \"#0F8554\", \"#73AF48\", \"#EDAD08\", \n  \"#E17C05\", \"#CC503E\", \"#94346E\", \"#6F4070\", \"#994E95\", \"#666666\"\n]\n\n// From R:\n// clr_ocean &lt;- colorspace::lighten(\"#88CCEE\", 0.7)\nclr_ocean = \"#D9F0FF\"\n\n// From CARTOColors Peach 2\nclr_land = \"#facba6\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ll make the land be orange-ish, add some thin black borders around the countries, and include a blue background color with Plot.frame():\n\nPlot.plot({\n  marks: [\n    Plot.frame({ fill: clr_ocean }),  \n    Plot.geo(world, { \n      stroke: \"black\", \n      strokeWidth: 0.5, \n      fill: clr_land \n    }) \n  ]\n})\n\n\n\n\n\n\nBuilt-in projections\nTaking a round globe and smashing it on a two-dimensional surface always requires geometric shenanigans to get things flat. We can control how things get flattened by specifying the projection for the map. Here we’ll use the Equal Earth projection (invented in 2018 to show countries and continents at their true relative sizes to each other). Since projections contain relative height and width details, we need to specify a width for the plot now. I arbitrarily chose 1000 pixels here, which is the maximum width—it should autoshrink in smaller browser windows, and the height should be calculated automatically. Finally, instead of adding the background color with Plot.frame(), we can use Plot.sphere() to get a nicer background that uses the specified projection:\n\nPlot.plot({\n  projection: \"equal-earth\", \n  width: 1000, \n  marks: [\n    Plot.sphere({ fill: clr_ocean }), \n    Plot.geo(world, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\nThe Observable Plot library includes a bunch of common built-in projections:\n\n\n\nviewof projection = Inputs.select(\n  [\"equirectangular\", \"equal-earth\", \"mercator\", \"transverse-mercator\", \"azimuthal-equal-area\", \"gnomonic\"],\n  {value: \"azimuthal-equal-area\", label: \"Projection\"}\n)\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  projection: projection,\n  // width,\n  marks: [\n    Plot.sphere({ fill: clr_ocean }),\n    Plot.geo(world, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\nOther projections\nObservable Plot can support any other D3 projection too. There are a whole bunch of projections in the main d3-geo module, and there’s a separate d3-geo-projection module for dozens of others. My favorite global projection is Robinson (the foundation for Equal Earth), which lives in d3-geo-projection. To use it, we can import the module with require() and then access it with d3_geo_projection.geoRobinson():\n\nd3_geo_projection = require(\"d3-geo-projection\") \n\nPlot.plot({\n  projection: d3_geo_projection.geoRobinson(), \n  width: 1000,\n  height: 500, \n  marks: [\n    Plot.sphere({ fill: clr_ocean }),\n    Plot.geo(world, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering map data and adjusting projections\nRemoving elements\nNow that we have a nice projection, we can tweak the map a little. Antarctica is taking up a big proportion of the southern hemisphere, so we’ll filter it out. The world object that has all the map data keeps each country object inside a features slot:\n\nworld.features\n\n\n\n\n\n\nWe can filter it using Javascript’s .filter() function. To make sure that the resulting array keeps the geographic-ness of the data and is a FeatureCollection, we need to create a similarly structured object, with type and features slots:\n\nworld_sans_penguins = ({ \n  type: \"FeatureCollection\", \n  features: world.features.filter(d =&gt; d.properties.iso_a3 !== \"ATA\") \n}) \n\nPlot.plot({\n  projection: d3_geo_projection.geoRobinson(),\n  width: 1000,\n  height: 500,\n  marks: [\n    Plot.sphere({ fill: clr_ocean }),\n    Plot.geo(world_sans_penguins, { \n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat works and Antarctica is gone, as expected, but in reality the map didn’t actually change that much. Even if we stop using the sphere background and just fill the plot frame, we can see that the area where Antarctica was is still there, it’s just missing the land itself:\n\nPlot.plot({\n  projection: d3_geo_projection.geoRobinson(),\n  width: 1000,\n  height: 500,\n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }), \n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\nQuick and dirty cheating method: change the width or height\nOne quick and dirty solution is to mess with the dimensions and shrink the height. After some trial and error, 430 pixels looks good:\n\nPlot.plot({\n  projection: d3_geo_projection.geoRobinson(),\n  width: 1000,\n  height: 430, \n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\nWhile this works in this case, it’s not a universal solution. The only reason this works is because Antarctica happens to be at the bottom of the map. When you adjust the height of the plot area, the map itself is anchored to the top. Like, if we set the height to 215, we’ll get just the northern hemisphere:\n\nPlot.plot({\n  projection: d3_geo_projection.geoRobinson(),\n  width: 1000,\n  height: 215, \n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\nAs far as I can tell, there’s no way to anchor the map in any other position. If we filter the map data to only look at one continent, there’s no easy way to focus on just that continent by adjusting only the width or height options. Here’s Africa all by itself in a big empty plot area:\n\njust_africa = ({ \n    type: \"FeatureCollection\", \n    features: world.features.filter(d =&gt; d.properties.continent == \"Africa\") \n}) \n\nPlot.plot({\n  projection: d3_geo_projection.geoRobinson(),\n  width: 1000,\n  height: 430, \n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(just_africa, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we adjust the width or the height, the plot area will be resized with the map anchored in the top left corner so we’re left with just the northwestern part of Africa (and big empty areas where North America, South America, and Europe would be):\n\nPlot.plot({\n  projection: d3_geo_projection.geoRobinson(),\n  width: 550, \n  height: 215, \n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(just_africa, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\nThat’s not great, but there are better ways!\nBuilt-in projections and domain settings\nThe official Observable Plot method for fitting the plot window to a specific area of the map is to define a “domain” for one of the built-in projections to zoom in on specific areas. The documentation shows how to use special functions in d3-geo to create a circle around a point, but you can also pass a GeoJSON object and Plot will use its boundaries for the domain. The built-in projection options also let us control the outside margin of the domain with inset.\nHere’s the world map without Antarctica with the Equal Earth projection, with the projection resized to fit within the bounds of world_sans_penguins, with 10 pixels of padding around the landmass. Antarctica is gone now and the rest of the map is vertically centered within the plot area:\n\nPlot.plot({\n  projection: { \n    type: \"equal-earth\", \n    domain: world_sans_penguins, \n    inset: 10 \n  }, \n  width: 1000,\n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\nWe can see what’s happening behind the scenes if we add Plot.sphere() back in. The rounded globe area is still there, but it’s shifted down and out of the frame. We’re essentially panning around and zooming in on the Equal Earth projection:\n\nPlot.plot({\n  projection: {\n    type: \"equal-earth\",\n    domain: world_sans_penguins, \n    inset: 10\n  },\n  width: 1000,\n  marks: [\n    Plot.frame({ stroke: \"black\", strokeWidth: 1 }), \n    Plot.sphere({ fill: clr_ocean }), \n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\nPassing a GeoJSON object as the domain is really neat because it makes it straightforward to zoom in on specific areas. For instance, here’s the complete medium resolution world map zoomed in around the just_africa object, which keeps non-African countries in the Middle East and southern Europe:\n\nPlot.plot({\n  projection: {\n    type: \"equal-earth\",\n    domain: just_africa, \n    inset: 10\n  },\n  width: 600, \n  height: 600, \n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(world_medium, { \n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\nWe could also extract Africa from the medium resolution world map and plot only that continent, omitting the Middle East and Europe:\n\njust_africa_medium = ({ \n    type: \"FeatureCollection\", \n    features: world_medium.features.filter(d =&gt; d.properties.continent == \"Africa\") \n}) \n\nPlot.plot({\n  projection: {\n    type: \"equal-earth\",\n    domain: just_africa,\n    inset: 10\n  },\n  width: 600,\n  height: 600,\n  marks: [\n    // Use a white background since we don't want to make it look like  \n    // the Sinai peninsula has a coastline  \n    Plot.frame({ fill: \"white\", stroke: \"black\", strokeWidth: 1 }), \n    Plot.geo(just_africa_medium, {  \n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther projections and .fitExtent()\n\nUnfortunately, it’s a little bit trickier to set the domain and inset for projections that aren’t built in to Plot. We can’t do this:\nPlot.plot({\n  projection: {\n    type: d3_geo_projection.geoRobinson(),\n    domain: world_sans_penguins,\n    inset: 10\n  },\n  ...\n})\nInstead, we need to adjust the size of the projection window itself and build in the inset with d3-geo’s .fitExtent(). This function takes four arguments in an array like [[x1, y1], [x2, y2]], defining the top left and bottom right corners (in pixels) of a window that is centered in the middle of a given GeoJSON object. Here, for instance, we create a copy of the Robinson projection that has a window around just_africa with a top left corner at (30, 30) and a bottom right corner at (570, 570):\n\ninset_africa = 30\nafrica_map_width = 600\nafrica_map_height = 600\n\nafrica_robinson = d3_geo_projection.geoRobinson()\n  .fitExtent(\n    [[inset_africa, inset_africa],  // Top left\n     [africa_map_width - inset_africa, africa_map_height - inset_africa]],  // Bottom right \n    just_africa\n  )\n\nPlot.plot({\n  projection: africa_robinson,\n  width: africa_map_width,\n  height: africa_map_height,\n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the same approach with individual countries. For extra fun, we’ll fill these countries with distinct colors using Natural Earth’s mapcolor7 column, which assigns countries one of 7 different colors that don’t border other countries (so neighboring countries will never be the same color). We’ll also add some labels in the middle of each country.\n\negypt = world_medium.features.find(d =&gt; d.properties.name === \"Egypt\")\n\ninset_egypt = 75\negypt_map_width = 600\negypt_map_height = 600\n\nrobinson_egypt = d3_geo_projection.geoRobinson()\n  .fitExtent(\n    [[inset_egypt, inset_egypt],  // Top left\n     [egypt_map_width - inset_egypt, egypt_map_height - inset_egypt]],  // Bottom right\n    egypt\n  )\n\nPlot.plot({\n  projection: robinson_egypt,\n  width: egypt_map_width,\n  height: egypt_map_height,\n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(world_medium, Plot.centroid({\n      fill: d =&gt; d.properties.mapcolor7,\n      stroke: \"black\", \n      strokeWidth: 0.5\n    })),\n    Plot.geo(egypt, { stroke: \"yellow\", strokeWidth: 3 }),\n    Plot.tip(world_medium.features, Plot.centroid({\n      title: d =&gt; d.properties.name, \n      anchor: \"top\",\n      fontSize: 13,\n      fontWeight: \"bold\",\n      textPadding: 3\n    }))\n  ],\n  color: {\n    range: carto_prism\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe approach works for the whole world_sans_penguins object as well. This addresses our original problem—here’s a world map with the Robinson projection without Antarctica that fills the plot area correctly:\n\ninset_world = 10\nworld_map_width = 1000\nworld_map_height = 450\n\nworld_sans_penguins_robinson = d3_geo_projection.geoRobinson()\n  .fitExtent(\n    [[inset_world, inset_world],\n     [world_map_width - inset_world, world_map_height - inset_world]],\n    world_sans_penguins\n  )\n\nPlot.plot({\n  projection: world_sans_penguins_robinson,\n  width: world_map_width,\n  height: world_map_height,\n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArbitrary areas and .fitExtent()\n\nFor bonus fun, this approach also works for any arbitrary rectangles. For example, we can use OpenStreetMap’s neat Export tool to pick the top, bottom, left, and right edges of a box that focuses on Western Europe.\n\n\nRectangle around western Europe with OpenStreetMap’s Export tool\n\nWe can then use those coordinates to create a MultiPoint geometric feature/object, which essentially acts like a rectangular fake country/region that can be used as the domain or extent of the map:\n\ninset_europe = 10\neurope_map_width = 800\neurope_map_height = 800\n\neurope_box = ({\n  type: \"Feature\",\n  geometry: {\n    type: \"MultiPoint\",\n    coordinates: [\n      [-13, 35],  // [left/west, bottom/south] (or bottom left corner)\n      [21, 60]    // [right/east, top/north]   (or top right corner)\n    ]\n  }\n})\n\neurope_robinson = d3_geo_projection.geoRobinson()\n  .fitExtent(\n    [[inset_europe, inset_europe],\n     [europe_map_width - inset_europe, europe_map_height - inset_europe]], \n    europe_box\n  )\n\nPlot.plot({\n  projection: europe_robinson,\n  width: europe_map_width,\n  height: europe_map_height,\n  marks: [\n    Plot.frame({ fill: clr_ocean, stroke: \"black\", strokeWidth: 1 }),\n    Plot.geo(world_medium, Plot.centroid({\n      fill: d =&gt; d.properties.mapcolor9,\n      stroke: \"white\", \n      strokeWidth: 0.25\n    })),\n    Plot.tip(world.features, Plot.centroid({\n      title: d =&gt; d.properties.name, \n      anchor: \"bottom\",\n      fontSize: 13,\n      fontWeight: \"bold\",\n      textPadding: 3\n    }))\n  ],\n  color: {\n    range: carto_prism\n  }\n})"
  },
  {
    "objectID": "blog/2025/02/10/usaid-ojs-maps/index.html#working-with-usaid-data",
    "href": "blog/2025/02/10/usaid-ojs-maps/index.html#working-with-usaid-data",
    "title": "Using USAID data to make fancy world maps with Observable Plot",
    "section": "Working with USAID data",
    "text": "Working with USAID data\nGet USAID data\nTo make it easier to access and filter and manipulate things, I put the rescued data on a Datasette instance, which is nice front-end for an SQLite database. This makes it possible to run SQL queries directly in the browser and generate custom datasets without needing to load the full massive CSV files into R or Python or Stata or whatever.\nFor example, one of the rescued USAID datasets is named us_foreign_aid_country and it contains 22,000+ rows, with data on aid obligations, appropriations, and disbursements starting in 1999.\nIf we want to get a total of all constant USD aid obligations by country in 2023, omitting regional and world totals, we could do something like this with R and {dplyr}:\n\nlibrary(tidyverse)\n\n# Download the raw CSV and put it somewhere\nus_foreign_aid_country &lt;- read_csv(\"us_foreign_aid_country.csv\")\n\nus_foreign_aid_country |&gt;\n  filter(\n    `Fiscal Year` == 2023, \n    `Transaction Type Name` == \"Obligations\",\n    !str_detect(`Country Name`, \"Region\"),\n    `Country Name` != \"World\"\n  ) |&gt;\n  group_by(`Country Code`, `Country Name`, `Region Name`) |&gt;\n  summarize(total_constant_amount = sum(constant_amount)) |&gt;\n  arrange(desc(total_constant_amount))\n#&gt;  A tibble: 176 × 4\n#&gt;  Groups:   Country Code, Country Name [176]\n#&gt;   `Country Code` `Country Name`   `Region Name`                total_constant_amount\n#&gt;   &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;                                        &lt;dbl&gt;\n#&gt; 1 UKR            Ukraine          Europe and Eurasia                     17193710403\n#&gt; 2 ISR            Israel           Middle East and North Africa            3302860882\n#&gt; 3 JOR            Jordan           Middle East and North Africa            1686862605\n#&gt; 4 EGY            Egypt            Middle East and North Africa            1503609426\n#&gt; 5 ETH            Ethiopia         Sub-Saharan Africa                      1457374911\n#&gt; 6 SOM            Somalia          Sub-Saharan Africa                      1181033990\n#&gt; 7 NGA            Nigeria          Sub-Saharan Africa                      1019947490\n#&gt; 8 COD            Congo (Kinshasa) Sub-Saharan Africa                       990456757\n#&gt; 9 AFG            Afghanistan      South and Central Asia                   886536741\n#&gt; 0 KEN            Kenya            Sub-Saharan Africa                       846303488\n#&gt;  ℹ 166 more rows\n#&gt;  ℹ Use `print(n = ...)` to see more rows\n\nOr we could get that data extract directly from the database without needing to load the huge original CSV file. We can run an SQL query like this at the Datasette website:\nSELECT \"Country Code\", \"Country Name\", \"Region Name\", SUM(\"constant_amount\") AS total_constant_amount\n  FROM \"./us_foreign_aid_country\"\n  WHERE \n    \"Fiscal Year\" = '2023' \n    AND \"Transaction Type Name\" = 'Obligations' \n    AND \"Country Name\" NOT LIKE '%Region%' \n    AND \"Country Name\" != \"World\"\n  GROUP BY \"Country Code\", \"Country Name\", \"Region Name\"\n  ORDER BY total_constant_amount DESC;\n\n\nSQL query and results\n\nSince we’re working with interactive Observable Javascript, we can load that data directly into the browser instead of downloading intermediate CSV files. There’s a neat Datasette database client for Observable that lets us run SQL queries (there are lots of other clients too, if you want to connect to things like DuckDB, SQLite, MySQL, Snowflake, and so on).\n\nimport { DatasetteClient } from \"@ambassadors/datasette-client\"\n\naid_db = new DatasetteClient(\n  \"https://foreignassistance-data.andrewheiss.com/2025-02-03_foreign-assistance\"\n)\n\nrecipient_countries = await aid_db.sql`\n  SELECT \"Country Code\", \"Country Name\", \"Region Name\", SUM(\"constant_amount\") AS total_constant_amount\n  FROM \"./us_foreign_aid_country\"\n  WHERE \n    \"Fiscal Year\" = '2023' \n    AND \"Transaction Type Name\" = 'Obligations' \n    AND \"Country Name\" NOT LIKE '%Region%' \n    AND \"Country Name\" != \"World\"\n  GROUP BY \"Country Code\", \"Country Name\", \"Region Name\"\n  ORDER BY total_constant_amount DESC;\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThrough the magic of this Datasette client, we now have a pre-summarized dataset to work with!\n\n// I don't want to keep hitting the Datasette server with requests, so I'm \n// cheating and loading a CSV extract instead. It comes from this query: \n// https://foreignassistance-data.andrewheiss.com/2025-02-03_foreign-assistance.csv?sql=SELECT+%22Country+Code%22%2C+%22Country+Name%22%2C+%22Region+Name%22%2C+SUM%28%22constant_amount%22%29+AS+total_constant_amount%0D%0A++FROM+%22.%2Fus_foreign_aid_country%22%0D%0A++WHERE+%22Fiscal+Year%22+%3D+%272023%27+%0D%0A++++AND+%22Transaction+Type+Name%22+%3D+%27Obligations%27+%0D%0A++++AND+%22Country+Name%22+NOT+LIKE+%27%25Region%25%27+%0D%0A++++AND+%22Country+Name%22+%21%3D+%22World%22%0D%0A++GROUP+BY+%22Country+Code%22%2C+%22Country+Name%22%2C+%22Region+Name%22%0D%0A++ORDER+BY+total_constant_amount+DESC%3B&_size=max\nrecipient_countries = await FileAttachment(\"recipient_countries.csv\").csv({ typed: true })\n\n\n\n\n\n\n\nrecipient_countries"
  },
  {
    "objectID": "blog/2025/02/10/usaid-ojs-maps/index.html#connect-usaid-data-to-the-map-data",
    "href": "blog/2025/02/10/usaid-ojs-maps/index.html#connect-usaid-data-to-the-map-data",
    "title": "Using USAID data to make fancy world maps with Observable Plot",
    "section": "Connect USAID data to the map data",
    "text": "Connect USAID data to the map data\nFollowing Observable Plot’s choropleth tutorial, to show these totals on a map, we need to create a Map object,1 which is like a Python dictionary or an R data frame with two columns, where we have (1) a name that shares a name with something in the geographic data, like an ISO3 country code, and (2) a value with the thing we want to plot.\n1 This term is admittedly confusing because it has nothing to do with geographic maps and is instead related to functional programming.\ncountry_totals = new Map(recipient_countries.map(d =&gt; [d[\"Country Code\"], d.total_constant_amount]))\ncountry_totals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis lets us get specific totals with the .get() method. Here’s Ukraine, for example:\n\ncountry_totals.get(\"UKR\")\n\n\n\n\n\n\nWe can feed the ISO3 code of each country-level geographic shape into this country_totals object to extract the total amount of aid for each country. We’ll use the Antarctica-free Robinson projection we made earlier, and we’ll remove the ocean fill since we’ll ultimately make this interactive and hoverable:\n\nPlot.plot({\n  projection: world_sans_penguins_robinson,\n  width: world_map_width,\n  height: world_map_height,\n  marks: [\n    Plot.frame({ stroke: \"black\", strokeWidth: 1} ),\n    Plot.geo(world_sans_penguins, Plot.centroid({\n      fill: d =&gt; country_totals.get(d.properties.iso_a3)\n    }))\n  ]\n})\n\n\n\n\n\n\nImproving the map\nWe have a choropleth! But this is hardly publication worthy. We need to fix a bunch of issues with it.\nFirst, countries that don’t receive aid don’t appear in the map. Let’s add borders to all the countries:\n\nPlot.plot({\n  projection: world_sans_penguins_robinson,\n  width: world_map_width,\n  height: world_map_height,\n  marks: [\n    Plot.frame({ stroke: \"black\", strokeWidth: 1} ),\n    Plot.geo(world_sans_penguins, Plot.centroid({\n      fill: d =&gt; country_totals.get(d.properties.iso_a3)\n    })),\n    Plot.geo(world_sans_penguins, {  \n      stroke: \"black\",  \n      strokeWidth: 0.5  \n    })  \n  ]\n})\n\n\n\n\n\n\nThe coloring here is gross because of some huge outliers (Ukraine) that make most countries black/dark blue. There’s also no legend to show what these values are. We can address all of this by adjusting the legend options. We’ll log total aid, include the legend, add a nice label, and use a single-hue coloring scheme with gray for countries without aid:\n\nPlot.plot({\n  projection: world_sans_penguins_robinson,\n  width: world_map_width,\n  height: world_map_height,\n  marks: [\n    Plot.frame({ stroke: \"black\", strokeWidth: 1} ),\n    Plot.geo(world_sans_penguins, Plot.centroid({\n      fill: d =&gt; country_totals.get(d.properties.iso_a3)\n    })),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5\n    })\n  ],\n  color: {  \n    scheme: \"blues\",  \n    unknown: \"#f2f2f2\",  \n    type: \"log\",   \n    legend: true,  \n    label: \"Total obligations\",  \n  }  \n})\n\n\n\n\n\n\nNext, let’s make this interactive by turning on hovering tooltips:\n\nPlot.plot({\n  projection: world_sans_penguins_robinson,\n  width: world_map_width,\n  height: world_map_height,\n  marks: [\n    Plot.frame({ stroke: \"black\", strokeWidth: 1} ),\n    Plot.geo(world_sans_penguins, Plot.centroid({\n      fill: d =&gt; country_totals.get(d.properties.iso_a3),\n      tip: true \n    })),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5\n    })\n  ],\n  color: {\n    scheme: \"blues\",\n    unknown: \"#f2f2f2\",\n    type: \"log\", \n    legend: true,\n    label: \"Total obligations\",\n  }\n})\n\n\n\n\n\n\nThat’s so cool. Hover over Mexico and you’ll see “Total obligations 232,214,023”.\nWe can make this tooltip more informative by including the country name and formatting the amount to show dollars. Instead of using tip: true, we can add the country name as a channel (Observable Plot’s version of a ggplot aesthetic), and format the tip so that the country name comes first and the total amount is formatted with d3.format():\n\nPlot.plot({\n  projection: world_sans_penguins_robinson,\n  width: world_map_width,\n  height: world_map_height,\n  marks: [\n    Plot.frame({ stroke: \"black\", strokeWidth: 1} ),\n    Plot.geo(world_sans_penguins, Plot.centroid({\n      fill: d =&gt; country_totals.get(d.properties.iso_a3),\n      channels: { \n        Country: d =&gt; d.properties.name, \n      }, \n      tip: { \n        format: { \n          Country: true, \n          fill: d3.format(\"$,d\") \n        } \n      } \n    })),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5\n    })\n  ],\n  color: {\n    scheme: \"blues\",\n    unknown: \"#f2f2f2\",\n    type: \"log\", \n    legend: true,\n    label: \"Total obligations\",\n  }\n})\n\n\n\n\n\n\nNow hover over Mexico and you’ll see the country name and the amount of aid in dollars.\nFixing labelling issues\nWe have two final super minor issues to address.\nFirst hover over a country that didn’t receive aid, like the United States or Australia. The total reported aid displays as “$NaN”. That’s gross. It’d be nicer if it said something else, like “$0” or “No aid” or something more informative.\nTo fix this, we can make a little function that formats the given value as a dollar amount if it’s an actual value, and formats it as something else if it’s missing or not a number (like log(0)):\n\nfunction format_aid_total(value) {\n  return value ? d3.format(\"$,d\")(value) : \"No aid\";\n}\n\n\n\n\n\n\nThat works nicely:\n\nformat_aid_total(394023)\n\n\n\n\n\n\n\nformat_aid_total(NaN)\n\n\n\n\n\n\n\n\nThe other problem is in the legend, which uses a logarithmic scale and includes breaks for 10k, 1M, 100M, and 10G, representing $10,000, $1 million, $100 million, and $10 billion in aid.\nThe issue is the $10 billion, which is abbreviated with “G”.\nThis is happening because d3.format() uses SI (Système international d’unités, or International System of Units) values for its numeric formats, which means that it uses SI metric prefixes. Those legend breaks, therefore, actually technically mean this:\n\n10k: 10 kilodollars\n1M: 1 megadollar\n100M: 100 megadollars\n10G: 10 gigadollars\n\nlol, I should start talking about big dollar amounts with these values (“the 2022 US federal budget deficit was 1.4 teradollars”)\nThe first letters of many of these SI prefixes happen to line up with US-style large numbers:\n\nIn the US we already commonly use “k” for thousand\nThe initial “m” in “mega” aligns with “million”\nThe initial “t” in tera aligns with “trillion”\n\nBut “giga” doesn’t align with “billion”, hence the strange “G” here for dollar amounts.\nPeople have requested that d3-format include an option for switching the abbreviation from G to B, but the developers haven’t added it (and probably won’t). Instead, a common recommended fix is to replace all “G”s with “B”s:\n\nnumber_in_billions = 13840918291  // A big number I randomly typed\n\n// Billions of dollars instead of SI-style gigadollars\nd3.format(\"$.4s\")(number_in_billions).replace(\"G\", \"B\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add format_aid_total() and the .replace(\"G\", \"B\") tweak and fix the labels in our interactive map:\n\nPlot.plot({\n  projection: world_sans_penguins_robinson,\n  width: world_map_width,\n  height: world_map_height,\n  marks: [\n    Plot.frame({ stroke: \"black\", strokeWidth: 1} ),\n    Plot.geo(world_sans_penguins, Plot.centroid({\n      fill: d =&gt; country_totals.get(d.properties.iso_a3),\n      channels: {\n        Country: d =&gt; d.properties.name,\n      },\n      tip: {\n        format: {\n          Country: true,\n          fill: d =&gt; format_aid_total(d) \n        }\n      }\n    })),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.5\n    })\n  ],\n  color: {\n    scheme: \"blues\",\n    unknown: \"#f2f2f2\",\n    type: \"log\", \n    legend: true,\n    label: \"Total obligations\",\n    tickFormat: d =&gt; d3.format(\"$0.2s\")(d).replace(\"G\", \"B\") \n  }\n})\n\n\n\n\n\n\nSome final tweaks\nWe’re so close! Just a couple final incredibly minor changes:\n\nWe’ll boost the font size of the tooltip a little and increase the font size of the legend\nWe’ll switch from the built-in ColorBrewer blues palette to show how to use custom gradients, like CARTOColors’s PurpOr sequential palette\n\n\nPlot.plot({\n  projection: world_sans_penguins_robinson,\n  width: world_map_width,\n  height: world_map_height,\n  marks: [\n    Plot.frame({ stroke: \"black\", strokeWidth: 1} ),\n    Plot.geo(world_sans_penguins, Plot.centroid({\n      fill: d =&gt; country_totals.get(d.properties.iso_a3),\n      channels: {\n        Country: d =&gt; d.properties.name,\n      },\n      tip: {\n        fontSize: 12, \n        format: {\n          Country: true,\n          fill: d =&gt; format_aid_total(d)\n        }\n      }\n    })),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.15\n    })\n  ],\n  color: {\n    // scheme: \"blues\", \n    range: [\"#f9ddda\", \"#f2b9c4\", \"#e597b9\", \"#ce78b3\", \"#ad5fad\", \"#834ba0\", \"#573b88\"], \n    unknown: \"#f2f2f2\",\n    type: \"log\", \n    legend: true,\n    label: \"Total obligations\",\n    tickFormat: d =&gt; d3.format(\"$0.2s\")(d).replace(\"G\", \"B\"),\n    style: { \n      \"font-size\": \"14px\" \n    } \n  }\n})"
  },
  {
    "objectID": "blog/2025/02/10/usaid-ojs-maps/index.html#the-full-game-complete-final-code",
    "href": "blog/2025/02/10/usaid-ojs-maps/index.html#the-full-game-complete-final-code",
    "title": "Using USAID data to make fancy world maps with Observable Plot",
    "section": "The full game: Complete final code",
    "text": "The full game: Complete final code\nThat final interactive map looks great! We could be even fancier with it by adding dropdowns for dynamically grabbing data for different years or different types of amounts (appropriations, allocations, etc.), or even filter by specific regions or countries. But we won’t.\nThe different colors and data sources we’ve used are scattered throughout this post. To simplify things, here’s the complete code all in one location. (This chunk doesn’t actually run, since Observable gets mad if you create a new variable with the same name as one that already exists.)\n\nd3_geo = require(\"d3-geo\")\nd3_geo_projection = require(\"d3-geo-projection\")\n\n// ----------------------------------------------------------------------\n// Map stuff\n// ----------------------------------------------------------------------\nworld = FileAttachment(\"ne_110m_admin_0_countries.geojson\").json()\n\n// Antarctica's ISO3 code is ATA\nworld_sans_penguins = ({\n  type: \"FeatureCollection\",\n  features: world.features.filter(d =&gt; d.properties.iso_a3 !== \"ATA\")\n})\n\ninset_world = 10\nworld_map_width = 1000\nworld_map_height = 450\n\nworld_sans_penguins_robinson = d3_geo_projection.geoRobinson()\n  .fitExtent(\n    [[inset_world, inset_world],\n     [world_map_width - inset_world, world_map_height - inset_world]],\n    world_sans_penguins\n  )\n\n// ----------------------------------------------------------------------\n// Data stuff\n// ----------------------------------------------------------------------\nimport { DatasetteClient } from \"@ambassadors/datasette-client\"\n\naid_db = new DatasetteClient(\n  \"https://foreignassistance-data.andrewheiss.com/2025-02-03_foreign-assistance\"\n)\n\nrecipient_countries = await aid_db.sql`\n  SELECT \"Country Code\", \"Country Name\", \"Region Name\", SUM(\"constant_amount\") AS total_constant_amount\n  FROM \"./us_foreign_aid_country\"\n  WHERE \n    \"Fiscal Year\" = '2023' \n    AND \"Transaction Type Name\" = 'Obligations' \n    AND \"Country Name\" NOT LIKE '%Region%' \n    AND \"Country Name\" != \"World\"\n  GROUP BY \"Country Code\", \"Country Name\", \"Region Name\"\n  ORDER BY total_constant_amount DESC;\n`\n\ncountry_totals = new Map(recipient_countries.map(d =&gt; [d[\"Country Code\"], d.total_constant_amount]))\n\nfunction format_aid_total(value) {\n  return value ? d3.format(\"$,d\")(value) : \"No aid\";\n}\n\n// ----------------------------------------------------------------------\n// Plot stuff\n// ----------------------------------------------------------------------\nPlot.plot({\n  projection: world_sans_penguins_robinson,\n  width: world_map_width,\n  height: world_map_height,\n  marks: [\n    Plot.frame({ stroke: \"black\", strokeWidth: 1} ),\n    Plot.geo(world_sans_penguins, Plot.centroid({\n      fill: d =&gt; country_totals.get(d.properties.iso_a3),\n      channels: {\n        Country: d =&gt; d.properties.name,\n      },\n      tip: {\n        fontSize: 12,\n        format: {\n          Country: true,\n          fill: d =&gt; format_aid_total(d)\n        }\n      }\n    })),\n    Plot.geo(world_sans_penguins, {\n      stroke: \"black\",\n      strokeWidth: 0.15\n    })\n  ],\n  color: {\n    range: [\"#f9ddda\", \"#f2b9c4\", \"#e597b9\", \"#ce78b3\", \"#ad5fad\", \"#834ba0\", \"#573b88\"],\n    unknown: \"#f2f2f2\",\n    type: \"log\", \n    legend: true,\n    label: \"Total obligations\",\n    tickFormat: d =&gt; d3.format(\"$0.2s\")(d).replace(\"G\", \"B\"),\n    style: {\n      \"font-size\": \"14px\" \n    }\n  }\n})"
  },
  {
    "objectID": "blog/2024/12/04/apple-music-wrapped-r/index.html",
    "href": "blog/2024/12/04/apple-music-wrapped-r/index.html",
    "title": "Apple Music Wrapped with R",
    "section": "",
    "text": "’Tis the season for Spotify Wrapped stats and I love it, both for seeing what everyone listens to and because it’s such a cool way of presenting data. A few years ago on Twitter, Caitlin Hudon noted that\nAt its core, Spotify Wrapped is really just some grouped and summarized data—a PivotTable with some album cover art slapped on. And it’s fun and neat and everyone loves it!\nI’ve always been jealous of everyone’s annual Spotify Wrapped reports, but since I don’t use Spotify, I’ve never gotten to see my own details.\nBecause I’m an Elder Millennial and started listening to music in the days of Napster, I prefer to control my music files rather than stream it Spotify-style, so I get all my stuff from either the Amazon Music store or Bandcamp since they both provide DRM-free MP3s. I listen to everything in the used-to-be-iTunes Music app (not to be confused with Apple’s music streaming service, Apple Music), and I use iTunes Match to access my library across all my devices.1\niTunes/Music keeps track of some song metadata, like a count of the number of times a song has been played:\nAll that metadata is stored in a big ol’ gross XML file. In days of iTunes, you could find it at ~/Music/iTunes/iTunes Library.xml; with Apple Music, it’s hidden in ~/Music/Music/Music Library/Library.musicdb. The easiest way to access it is to export a copy of it from Music with File &gt; Library &gt; Export Library…. It has a bunch of neat details about each file in your library:\nIt keeps track of play count…\n…but unfortunately for Spotify Wrapped purposes, it overwrites the count and date information when you listen to a track—it doesn’t keep track of individual play counts. Here’s what the XML for “In Another Life” looked like before I listened to the track while writing this post:\nThat September 7th listen was erased from history once I hit play in December :(\nThat means it’s impossible to figure out how many times you listen to a track during a given time period—the play count only shows the most recent listen. With one XML export, you can’t find Spotify Wrapped-like details about listening habits in a single year.\nHowever, if you have two XML exports, you can!"
  },
  {
    "objectID": "blog/2024/12/04/apple-music-wrapped-r/index.html#calculating-2024-play-counts-with-r",
    "href": "blog/2024/12/04/apple-music-wrapped-r/index.html#calculating-2024-play-counts-with-r",
    "title": "Apple Music Wrapped with R",
    "section": "Calculating 2024 play counts with R",
    "text": "Calculating 2024 play counts with R\nI played the long game this year and exported a copy of my iTunes/Music library on the morning of January 1 and stored the XML file in a folder on my computer. I then exported a copy of the library as it stands today. With these two library files, I can subtract the play count from January 1 from the play count today and find how many times I listened to each track. It still doesn’t give me date information—there’s no way to see time trends like what I was listening to in March or whatever2—but it gives me good data to work with.\n2 If I were super on top of things and cared that much, I could set up a script to automatically export a copy of the library every day and then reverse engineer daily listening data, but that seems like an excessive amount of work.In the spirit of Caitlin’s tweet, I’m going to keep the analysis of this data as simple and straightforward as possible—just filtering, grouping, and summarizing.\nThe only bit of fancy R work comes at the beginning with parsing and cleaning the Apple Music XML files. The track information is deeply nested inside a bunch of XML layers and untangling all that requires some data wrangling. Fortunatley Simon Couch already did it in his 2022 analysis of his music, and he even made an accompanying package {wrapped} for doing it yourself. His package is designed to extract the play counts of all the music added in a given year, while I want the counts for all years, so I modified his wrap_library() function slightly to ignore the year argument and just parse everything. The modified function, now read_itunes_library() is below, for the morbidly curious:\n\nR code for read_itunes_library()# Copied with tiiiiny modifications from Simon Couch's {wrapped}:\n#\n# - https://www.simonpcouch.com/blog/2022-12-01-listening-2022/\n# - https://github.com/simonpcouch/wrapped/blob/main/R/wrap_library.R\n\nlibrary(tidyverse)\n\nread_itunes_library &lt;- function(path, year = 2022L) {\n  raw &lt;- xml2::read_xml(path)\n  \n  res &lt;- xml2::as_list(raw)\n  \n  res &lt;- purrr::pluck(res, \"plist\", \"dict\", \"dict\")\n  \n  res &lt;- res[names(res) != \"key\"]\n  \n  res &lt;- \n    tibble::enframe(res) %&gt;%\n    dplyr::rowwise() %&gt;%\n    dplyr::mutate(value = list(tibble::enframe(value))) %&gt;%\n    dplyr::ungroup() %&gt;%\n    dplyr::mutate(id = dplyr::row_number()) %&gt;%\n    dplyr::select(-name) %&gt;%\n    tidyr::unnest(value) %&gt;%\n    dplyr::mutate(\n      entry_id = (dplyr::row_number() + (dplyr::row_number() %% 2)) / 2\n    ) %&gt;%\n    dplyr::rowwise() %&gt;%\n    dplyr::mutate(value = dplyr::if_else(length(value) == 0L, list(list(NA)), list(value)),\n           value = unlist(value)) %&gt;%\n    dplyr::ungroup() %&gt;%\n    tidyr::pivot_wider(id_cols = c(id, entry_id), names_from = name, values_from = value, values_fn = list) %&gt;%\n    tidyr::pivot_longer(cols = 4:ncol(.), names_to = \"type\", values_drop_na = TRUE) %&gt;%\n    dplyr::select(-type) %&gt;%\n    tidyr::pivot_wider(id_cols = id, names_from = key, values_from = value) %&gt;%\n    janitor::clean_names() %&gt;%\n    dplyr::select(id, track_title = name, artist, album_artist, album, genre, total_time, date_added, skip_count, play_count) %&gt;%\n    dplyr::rowwise() %&gt;%\n    dplyr::mutate(dplyr::across(everything(), ~dplyr::if_else(is.null(.x), list(NA), list(.x)))) %&gt;%\n    dplyr::mutate(dplyr::across(everything(), unlist)) %&gt;%\n    dplyr::mutate(\n      date_added = strsplit(date_added, \"T\"),\n      date_added = date_added[1],\n      date_added = lubridate::ymd(date_added),\n      skip_count = as.numeric(skip_count),\n      play_count = as.numeric(play_count),\n      total_time = as.numeric(total_time)\n    ) %&gt;%\n    dplyr::ungroup() %&gt;%\n    # dplyr::filter(lubridate::year(date_added) %in% year) %&gt;%\n    dplyr::arrange(dplyr::desc(play_count))\n\n  res\n}\n\n\nlibrary(tidyverse)\n\n# Copy this function from the text earlier \nread_itunes_library &lt;- function(...) {...}\n\nmusic_january &lt;- read_itunes_library(\"Library_2024-01-01.xml\")\nmusic_december &lt;- read_itunes_library(\"Library_2024-12-04.xml\")\nHere’s what that data looks like:3\n3 My most recent Bandcamp purchases were the two Minecraft soundtracks (Volume Alpha and Volume Beta) for my Minecraft-obsessed kids, hence those tracks in the glimpse() output there.\nglimpse(music_december)\n## Rows: 11,691\n## Columns: 10\n## $ id           &lt;int&gt; 12138, 12135, 12136, 12137, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 1217…\n## $ track_title  &lt;chr&gt; \"Living Mice\", \"Door\", \"Subwoofer Lullaby\", \"Death\", \"Moog City\", \"Haggstrom\", \"Minecraft\", \"Oxygène\", \"Équinoxe\", \"Mice on Venus\", \"Dry Hands\", \"Wet Hands\", \"Clark\", \"Chris\", \"Thirteen\", \"Excuse\", \"Sweden\", \"Key\", \"Cat\", \"Dog\", \"Danny\", \"Beginning\", \"Droopy likes ricochet\", \"…\n## $ artist       &lt;chr&gt; \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418…\n## $ album_artist &lt;chr&gt; \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418…\n## $ album        &lt;chr&gt; \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha…\n## $ genre        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Classical\", \"Classical\", \"Classical\", \"Classical\", \"Classica…\n## $ total_time   &lt;dbl&gt; 177554, 111490, 208640, 41560, 160052, 204068, 254066, 65201, 114938, 281573, 68571, 90070, 191817, 87823, 176561, 124055, 215562, 65071, 186305, 145815, 254563, 102164, 96287, 116819, 603062, 296071, 332564, 170396, 180062, 254249, 378122, 185077, 361743, 239438, 244950, 3100…\n## $ date_added   &lt;date&gt; 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, …\n## $ skip_count   &lt;dbl&gt; NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, N…\n## $ play_count   &lt;dbl&gt; 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\nThere are columns for iTunes/Music’s internal track ID, a bunch of track metadata like title, artist, album, genre, and date added, and columns for the skip count and play count. Those are all columns Simon decided to include with his {wrapper} package—if you modify the read_itunes_library() function from earlier, you can keep any of the metadata that Music keeps track of.\nWith library data from both January and December loaded, I next combine them into one dataset with the total number of plays in 2024. This requires a tiny bit of data wrangling: I rename the play count column in the December data, join the January data to it, rename the January play count column, recode missing play counts as 0, and find the difference between play counts in December and January:\n\nmusic_2024 &lt;- music_december |&gt; \n  # Rename the column of December play counts\n  rename(play_count_end = play_count) |&gt;\n  # Merge in the play count column from the January 1 data\n  left_join(\n    music_january |&gt; select(id, play_count_start = play_count),\n    by = join_by(id)\n  ) |&gt; \n  # Tracks that were added in 2024 don't show up in music_january, so they appear \n  # in the merged data as NA. This recodes them as 0, which makes it so I can \n  # do math with them in the next step\n  replace_na(list(play_count_start = 0, play_count_end = 0)) |&gt; \n  # Calculate the difference between December and January play counts\n  mutate(play_count_2024 = play_count_end - play_count_start)\n\nLet’s see what the merged data looks like really quick:\n\nglimpse(music_2024)\n## Rows: 11,691\n## Columns: 12\n## $ id               &lt;int&gt; 12138, 12135, 12136, 12137, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, …\n## $ track_title      &lt;chr&gt; \"Living Mice\", \"Door\", \"Subwoofer Lullaby\", \"Death\", \"Moog City\", \"Haggstrom\", \"Minecraft\", \"Oxygène\", \"Équinoxe\", \"Mice on Venus\", \"Dry Hands\", \"Wet Hands\", \"Clark\", \"Chris\", \"Thirteen\", \"Excuse\", \"Sweden\", \"Key\", \"Cat\", \"Dog\", \"Danny\", \"Beginning\", \"Droopy likes ricochet…\n## $ artist           &lt;chr&gt; \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"…\n## $ album_artist     &lt;chr&gt; \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"C418\", \"…\n## $ album            &lt;chr&gt; \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume Alpha\", \"Minecraft - Volume A…\n## $ genre            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Classical\", \"Classical\", \"Classical\", \"Classical\", \"Clas…\n## $ total_time       &lt;dbl&gt; 177554, 111490, 208640, 41560, 160052, 204068, 254066, 65201, 114938, 281573, 68571, 90070, 191817, 87823, 176561, 124055, 215562, 65071, 186305, 145815, 254563, 102164, 96287, 116819, 603062, 296071, 332564, 170396, 180062, 254249, 378122, 185077, 361743, 239438, 244950, …\n## $ date_added       &lt;date&gt; 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-29, 2024-11-…\n## $ skip_count       &lt;dbl&gt; NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, N…\n## $ play_count_end   &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 17, 16, 15, 15, 1…\n## $ play_count_start &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ play_count_2024  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 17, 16, 15, 15, 1…\n\nThat new play_count_2024 column is the main thing I’m interested in—I can summarize it a bunch of different ways."
  },
  {
    "objectID": "blog/2024/12/04/apple-music-wrapped-r/index.html#minutes-listened",
    "href": "blog/2024/12/04/apple-music-wrapped-r/index.html#minutes-listened",
    "title": "Apple Music Wrapped with R",
    "section": "Minutes listened",
    "text": "Minutes listened\nThere’s a column for total_time that’s measured in milliseconds. I can multiply it by the play count and do some division to figure out a rough count of the total number of minutes listened. It’s not 100% accurate since it doesn’t account for partial listens, but it’s close enough.\n\nmusic_2024 |&gt; \n  mutate(time_plays = total_time * play_count_2024) |&gt; \n  summarize(total_ms = sum(time_plays)) |&gt; \n  mutate(total_minutes = total_ms / 1000 / 60)\n## # A tibble: 1 × 2\n##    total_ms total_minutes\n##       &lt;dbl&gt;         &lt;dbl&gt;\n## 1 891497777        14858."
  },
  {
    "objectID": "blog/2024/12/04/apple-music-wrapped-r/index.html#new-music",
    "href": "blog/2024/12/04/apple-music-wrapped-r/index.html#new-music",
    "title": "Apple Music Wrapped with R",
    "section": "New music",
    "text": "New music\nHere’s all the new music I added in 2024:\n\nadded_2024 &lt;- music_2024 |&gt; \n  mutate(year_added = year(date_added)) |&gt; \n  filter(year_added == 2024) |&gt; \n  distinct(album_artist, album, date_added)\n\nadded_2024 |&gt; \n  arrange(date_added) |&gt; \n  print(n = Inf)\n## # A tibble: 30 × 3\n##    album_artist       album                                                                                   date_added\n##    &lt;chr&gt;              &lt;chr&gt;                                                                                   &lt;date&gt;    \n##  1 boygenius          the record [Explicit]                                                                   2024-02-05\n##  2 Olivia Rodrigo     GUTS (spilled) [Explicit]                                                               2024-03-23\n##  3 The Avett Brothers The Avett Brothers                                                                      2024-03-23\n##  4 The Decemberists   As It Ever Was, So It Will Be Again                                                     2024-03-23\n##  5 Lauren Mayberry    Change Shapes                                                                           2024-03-23\n##  6 Lauren Mayberry    Shame                                                                                   2024-03-23\n##  7 Nils Frahm         Day                                                                                     2024-04-09\n##  8 Taylor Swift       THE TORTURED POETS DEPARTMENT: THE ANTHOLOGY [Explicit]                                 2024-04-19\n##  9 Taylor Swift       THE TORTURED POETS DEPARTMENT [Explicit]                                                2024-04-19\n## 10 The Avett Brothers The Avett Brothers                                                                      2024-05-17\n## 11 Lindsey Stirling   Duality                                                                                 2024-06-18\n## 12 The Decemberists   As It Ever Was, So It Will Be Again                                                     2024-06-18\n## 13 Bits & Hits        Lord of The Rings but it's lofi beats                                                   2024-07-28\n## 14 Bits & Hits        Zelda but it's lofi beats                                                               2024-07-28\n## 15 Bits & Hits        Minecraft but it's lofi beats                                                           2024-07-28\n## 16 Sabrina Carpenter  Short n' Sweet [Explicit]                                                               2024-08-05\n## 17 Chappell Roan      The Rise and Fall of a Midwest Princess [Explicit]                                      2024-08-13\n## 18 Sabrina Carpenter  Short n' Sweet [Explicit]                                                               2024-08-23\n## 19 Eydís Evensen      The Light                                                                               2024-09-25\n## 20 Bear McCreary      The Lord of the Rings: The Rings of Power (Season 2: Amazon Original Series Soundtrack) 2024-10-07\n## 21 Dua Lipa           Radical Optimism [Explicit]                                                             2024-10-27\n## 22 Laufey             Bewitched: The Goddess Edition [Explicit]                                               2024-11-05\n## 23 Laufey             Everything I Know About Love                                                            2024-11-05\n## 24 Laufey             Typical of Me EP                                                                        2024-11-05\n## 25 Laufey             Bewitched                                                                               2024-11-05\n## 26 Anna Lapwood       Images                                                                                  2024-11-11\n## 27 Anna Lapwood       Luna                                                                                    2024-11-11\n## 28 Chappell Roan      Good Luck, Babe!                                                                        2024-11-11\n## 29 C418               Minecraft - Volume Alpha                                                                2024-11-29\n## 30 C418               Minecraft - Volume Beta                                                                 2024-11-29"
  },
  {
    "objectID": "blog/2024/12/04/apple-music-wrapped-r/index.html#top-songs",
    "href": "blog/2024/12/04/apple-music-wrapped-r/index.html#top-songs",
    "title": "Apple Music Wrapped with R",
    "section": "Top songs",
    "text": "Top songs\nAnd here are the top songs:\n\ntop_played &lt;- music_2024 |&gt; \n  select(track_title, artist, play_count_2024) |&gt; \n  arrange(desc(play_count_2024))\n\ntop_played\n## # A tibble: 11,691 × 3\n##    track_title                                   artist                                       play_count_2024\n##    &lt;chr&gt;                                         &lt;chr&gt;                                                  &lt;dbl&gt;\n##  1 love is embarrassing [Explicit]               Olivia Rodrigo                                            42\n##  2 New Romantics (Taylor's Version)              Taylor Swift                                              35\n##  3 Dreaming of Light                             Eydís Evensen                                             33\n##  4 Espresso [Explicit]                           Sabrina Carpenter                                         32\n##  5 Bewitched                                     Laufey                                                    31\n##  6 Please Please Please [Explicit]               Sabrina Carpenter                                         30\n##  7 Anna's Theme                                  Eydís Evensen                                             26\n##  8 All You Had To Do Was Stay (Taylor's Version) Taylor Swift                                              26\n##  9 Shake It Off (Taylor's Version)               Taylor Swift                                              26\n## 10 The Light II                                  Eydís Evensen;Schola Cantorum Reykjavicensis              23\n## # ℹ 11,681 more rows"
  },
  {
    "objectID": "blog/2024/12/04/apple-music-wrapped-r/index.html#top-artists",
    "href": "blog/2024/12/04/apple-music-wrapped-r/index.html#top-artists",
    "title": "Apple Music Wrapped with R",
    "section": "Top artists",
    "text": "Top artists\nAnd the top artists:\n\ntop_artists &lt;- music_2024 |&gt; \n  group_by(artist) |&gt; \n  summarize(play_count = sum(play_count_2024)) |&gt; \n  arrange(desc(play_count))\n\ntop_artists\n## # A tibble: 1,851 × 2\n##    artist             play_count\n##    &lt;chr&gt;                   &lt;dbl&gt;\n##  1 Taylor Swift              484\n##  2 Laufey                    384\n##  3 Olivia Rodrigo            281\n##  4 Eydís Evensen             249\n##  5 Bear McCreary             234\n##  6 The Decemberists          214\n##  7 Hans Zimmer               188\n##  8 Sabrina Carpenter         151\n##  9 The Avett Brothers        149\n## 10 Nicholas Britell          148\n## # ℹ 1,841 more rows"
  },
  {
    "objectID": "blog/2024/12/04/apple-music-wrapped-r/index.html#top-albums",
    "href": "blog/2024/12/04/apple-music-wrapped-r/index.html#top-albums",
    "title": "Apple Music Wrapped with R",
    "section": "Top albums",
    "text": "Top albums\nAnd the top albums. This is a little trickier since Music doesn’t keep track of full album listens (and I don’t think Spotify does that either), so it’s a count of the number of tracks played in the album. That means the count is biased towards longer albums like 1989 (21 tracks) or the Rings of Power soundtrack (40 tracks). But it’s still a helpful overview:\n\ntop_albums &lt;- music_2024 |&gt; \n  group_by(album, artist) |&gt; \n  summarize(count_of_tracks_played_in_album = sum(play_count_2024)) |&gt; \n  arrange(desc(count_of_tracks_played_in_album))\ntop_albums\n## # A tibble: 2,434 × 3\n## # Groups:   album [992]\n##    album                                                                                     artist             count_of_tracks_played_in_album\n##    &lt;chr&gt;                                                                                     &lt;chr&gt;                                        &lt;dbl&gt;\n##  1 1989 (Taylor's Version)                                                                   Taylor Swift                                   399\n##  2 GUTS (spilled) [Explicit]                                                                 Olivia Rodrigo                                 336\n##  3 Bewitched: The Goddess Edition [Explicit]                                                 Laufey                                         244\n##  4 The Light                                                                                 Eydís Evensen                                  216\n##  5 Short n' Sweet [Explicit]                                                                 Sabrina Carpenter                              151\n##  6 The Lord of the Rings: The Rings of Power (Season One: Amazon Original Series Soundtrack) Bear McCreary                                  131\n##  7 Interstellar: Original Motion Picture Soundtrack (Deluxe Version)                         Hans Zimmer                                    108\n##  8 The Lord of the Rings: The Rings of Power (Season 2: Amazon Original Series Soundtrack)   Bear McCreary                                  103\n##  9 The Avett Brothers                                                                        The Avett Brothers                              93\n## 10 8th Wonder                                                                                The National Parks                              84\n## # ℹ 2,424 more rows"
  },
  {
    "objectID": "blog/2024/12/04/apple-music-wrapped-r/index.html#final-images",
    "href": "blog/2024/12/04/apple-music-wrapped-r/index.html#final-images",
    "title": "Apple Music Wrapped with R",
    "section": "Final images",
    "text": "Final images\nThis is all ugly console output, so finally, I whipped up a couple Wrapped-esque images in Illustrator with the statistics;\n\n\nTop artists and total mintues\n\n\n\nTop songs"
  },
  {
    "objectID": "blog/2024/07/08/fun-with-positron/index.html",
    "href": "blog/2024/07/08/fun-with-positron/index.html",
    "title": "Fun with Positron",
    "section": "",
    "text": "At the end of June 2024, Posit released a beta version of its next-generation IDE for data science: Positron. This follows Posit’s general vision for language-agnostic data analysis software: RStudio PBC renamed itself to Posit PBC in 2022 to help move away from a pure R focus, and Quarto is pan-lingual successor to R Markdown. Having the name of the main programming language in the title of things is out—providing more general tools is in.\nPositron is essentially a specialized version of Microsoft’s Visual Studio Code, and is a fork of the underlying Code - OSS that powers VS Code. I’m super excited about this—in my own work, I use RStudio for most things R-related and VS Code for everything else (Stan, Python, HTML, CSS, Lua, LaTeX, Typst, etc.). VS Code is phenomenal and I love using it. It’s the best way to edit files on a remote server. It’s the best way to interact with Docker containers and Docker Compose. GitHub Copilot Chat is fantastic.\nBut for me, it’s never quite been a replacement for RStudio. Every couple months, I play around with trying to use VS Code for R work full time, but the constellation of VS Code R extensions (like the R extension and Radian for the terminal) and general R support has never been what I want, and I always end up going back to RStudio. Which is fine! I adore RStudio too and have been using it since it first came out in beta in February 2011 (13 years!).\nPositron brings pretty much all the little R-related things that I love from RStudio and have missed in Visual Studio Code. The regular collection of VS Code’s R extensions and add-ons is no longer necessary, since Posit has created a custom R kernel—Ark—for any text editor or IDE with Jupyter support. It’s still a beta product and a little rough around the edges, but I’ve found that it really is the perfect blend of the best parts of RStudio and VS Code.\nBelow, following the example of Marc Dotson and Christopher Kenny, I want to highlight some of the neat new things Positron can do and share some of the settings, extensions, and other customizations I’ve been using for the past couple weeks.\n\nSome cool new things\nPositron brings RStudio’s best features to other languages like Python, like line-by-line code execution:\n\nLine-by-line execution in PythonLine-by-line execution in R\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n…and the variables panel (equivalent to the Environment panel in RStudio):\n\nVariables panel in PythonVariables panel in R\n\n\n\n\n\nVariables panel in Python\n\n\n\n\n\n\n\nVariables panel in R\n\n\n\n\n\nYou can also switch between different R and Python installations and versions. If you have rig installed, you can switch between different R versions, and Positron scans your computer at startup to find all the different Python virtual environments you have. I think eventually that little menu might also have better support for {renv} too. Here I can switch between R 4.4.0 and 4.3.3, as well as a bunch of random Python virtual environments and installations (lol python installations are the worst):\n\n\n\nVersion switcher\n\n\nPlus, since it’s just a fancy version of VS Code, Positron supports pretty much everything VS Code can do, including making complex layouts. Use the little menu in the top right corner to set up your workspace however you want:\n\n\n\nCustomize layout\n\n\nFor instance, here’s an example of a fully armed and operational layout I’ve been using for one project:\n\n\n\n\nFull ultrawide workspace\n\n\n\n\n\nMy settings\n\n\n\n\n\n\nHow to change settings in Positron / VS Code\n\n\n\n\n\nPositron (and VS Code in general) stores all its settings in a JSON file named settings.json that’s stored somewhere on your computer. On macOS it’s in ~/Library/Application Support/Positron/User/settings.json (see here for other operating systems). But you don’t need to ever remember that!\nWhen you open Positron’s settings (with ⌘, on a Mac; using… something… on Windows), Positron provides a nice frontend for searching, managing, and changing settings so you don’t need to edit raw JSON if you don’t want to.\n\n\n\n\n\n\nIf you click on the document button in the top right corner, you can open the actual settings.json file in the editor and make changes there. This is the easiest way to share settings with other people (like in this blog post) or with yourself (you can commit settings.json to a git repository, for instance).\n\n\n\n\n\n\n\n\n\nPositron settings pagePositron Configuring and customizing Positron involved basically copying most of my settings from VS Code’s settings.json into Positron’s settings.json. Here’s everything I have set up, with comments explaining stuff. A few things to note in particular:\n\nI set rstudio.keymap.enable to true to enable most of RStudio’s R-related keyboard shortcuts (like ⌘⌥I for a new chunk, ⌥- to insert &lt;-, etc.).\nI’m using GitHub’s Monaspace font because it looks neat and it has excellent font ligatures. I’ve enabled a bunch of different stylistic sets for the ligatures.\nI’m a big fan of the Monokai color theme and use it in RStudio and VS Code. It’s easy enough to set in Positron too, but for mysterious unknown reasons, it uses colors differently and is overly aggressive in what gets colorized. Compare this ggplot code across three different Monokais (VS Code, Positron, and RStudio). The Positron version is incredibly green and pink, while VS Code and RStudio use color more sparingly.\n\n\n\nMonokai highlighting in VS Code, Positron, and RStudio + Positron Dark\n\n\nSo for now, I’m using the Positron Dark theme instead, which does the best job of highlighting the things that RStudio did. It’s nice enough.\n\nHere’s my settings.json file. Adapt from it however you want. All these settings are also accessible in the GUI too.\n\n\n\nGUI for enabling RStudio keymapping\n\n\nThere are some extension-specific options at the bottom that I’ll explain below too.\n\n\nsettings.json\n\n{\n    // Positron-specific settings\n    // -------------------------------------------------------------------------\n    \"rstudio.keymap.enable\": true,\n    \"python.defaultInterpreterPath\": \"/opt/homebrew/bin/python\",\n\n\n    // Editor settings\n    // -------------------------------------------------------------------------\n    // Fonts\n    // Use GitHub's Monaspace (https://github.com/githubnext/monaspace) and enable ligatures\n    \"editor.fontFamily\": \"'Monaspace Argon Var'\",\n    \"editor.fontSize\": 12.5,\n    \"editor.fontLigatures\": \"'ss01', 'ss02', 'ss03', 'ss04', 'ss05', 'ss06', 'ss07', 'ss08', 'calt', 'dlig', 'liga'\",\n\n    // Theme\n    // Monakai would be nice, but it has issues in Positron\n    // \"workbench.colorTheme\": \"Monokai\",\n    \"workbench.colorTheme\": \"Default Dark Modern\",\n\n    // Use nicer icons\n    \"workbench.productIconTheme\": \"fluent-icons\",\n    \"workbench.iconTheme\": \"material-icon-theme\",\n\n    // Highlight modified/unsaved tabs\n    \"workbench.editor.highlightModifiedTabs\": true,\n\n    // Add some rulers\n    \"editor.rulers\": [\n        80,\n        100\n    ],\n\n    // Indent with two spaces, but only for R\n    \"[r]\": {\n        \"editor.tabSize\": 2\n    },\n\n    // Nicer handling of end-of-document newlines, via\n    // https://rfdonnelly.github.io/posts/sane-vscode-whitespace-settings/\n    \"files.insertFinalNewline\": true,\n    \"editor.renderFinalNewline\": \"dimmed\",\n    \"editor.renderWhitespace\": \"trailing\",\n    \"files.trimFinalNewlines\": true,\n    \"files.trimTrailingWhitespace\": true,\n\n    // Various editor settings\n    \"editor.formatOnPaste\": true,\n    \"editor.detectIndentation\": false,\n    \"editor.showFoldingControls\": \"always\",\n    \"window.newWindowDimensions\": \"inherit\",\n    \"editor.scrollBeyondLastLine\": false,\n    \"window.title\": \"${activeEditorFull}${separator}${rootName}\",\n    \"editor.tabSize\": 4,\n    \"editor.wordWrap\": \"on\",\n    \"editor.multiCursorModifier\": \"ctrlCmd\",\n    \"editor.snippetSuggestions\": \"top\",\n\n    // Hide things from the global search menu and watcher\n    \"files.exclude\": {\n        \"**/.Rhistory\": true,\n        \"**/.Rproj\": true,\n        \"**/.Rproj.user\": true,\n        \"**/renv/library\": true,\n        \"**/renv/local\": true,\n        \"**/renv/staging\": true\n    },\n    \"files.watcherExclude\": {\n        \"**/.Rproj/*\": true,\n        \"**/renv/library\": true,\n        \"**/renv/local\": true,\n        \"**/renv/staging\": true\n    },\n\n    // Sign git commits\n    \"git.enableCommitSigning\": true,\n\n\n    // Extension-specific settings\n    // -------------------------------------------------------------------------\n    // Markdown linting settings (idk if this stuff even works with Quarto though)\n    \"markdownlint.config\": {\n        \"default\": true,\n        \"MD012\": { \"maximum\": 2 },\n        \"MD025\": false,\n        \"MD041\": false\n    },\n\n    // Wrap at 80 columns with the \"Rewrap\" extension\n    \"rewrap.wrappingColumn\": 80,\n\n    // Hacky \"Open Remote - SSH\" settings\n    \"remote.SSH.serverDownloadUrlTemplate\": \"https://github.com/gitpod-io/openvscode-server/releases/download/openvscode-server-v${version}/openvscode-server-v${version}-${os}-${arch}.tar.gz\",\n    \"remote.SSH.experimental.serverBinaryName\": \"openvscode-server\",\n\n    // Don't phone home for the \"YAML\" extension\n    \"redhat.telemetry.enabled\": false,\n}\n\n\n\nMy keyboard shortcuts\n\n\n\n\n\n\nHow to change keyboard shortcuts in Positron / VS Code\n\n\n\n\n\nChanging keyboard shortcuts is just like changing settings. All the settings are stored in a JSON file (keybindings.json) located in a special folder on your computer, but you don’t have to work with raw JSON if you don’t want to.\nThe easiest way to get to the keyboard shortcut settings page is to open the Command Palette (⌘⇧P on macOS; ctrl + shift + p on Windows) and search for “Open Keyboard Shortcuts”:\n\n\n\n\n\n\nThis will give you a nice page for changing different settings. There are hundreds of possible shortcuts, but there’s a nice filtering system you can use to narrow things down.\n\n\n\n\n\n\nIf you click on the little document icon at the top, it will open the actual JSON file, just like with settings.json:\n\n\n\n\n\n\n\n\n\nAccessing keyboard shortcuts from the Command PaletteKeyboard shortcut editorKeyboard shortcuts as JSONEnabling Positron’s RStudio Keymap option with rstudio.keymap.enable takes care of like 90% of my keyboard customization needs. Years ago when I first switched to VS Code, I changed several of RStudio’s keyboard shortcuts to match VS Code’s like ⌘/ for toggling commented code instead of RStudio’s default ⌘⇧C. Positron uses ⌘/ by default for comment toggling too, but when you enable the RStudio Keymap option, that gets overridden with ⌘⇧C, so I disable that.\nRStudio also uses ⌘D for deleting a line, while VS Code uses it for adding text to a selection (i.e. if I select the word “the” in this document and then press ⌘D a bunch of times, it’ll add all those “the”s to the selection). The RStudio Keymap option adds ⌘D to delete the current line, so I disable that shortcut too to bring things back in line with standard VS Code.\nFinally, I use iTerm2 for macOS for my systemwide terminal, and I have it configured with a global hotkey ^` so I can access the terminal from everywhere. This conflicts with VS Code’s and Positron’s terminal toggling shortcut, which is the same, so I change it to be ^⇧`.\nHere’s my keybindings.json file. Like with settings.json, these are also accessible in the GUI.\n\n\n\nCustom keyboard shortcuts\n\n\n\n\nkeybindings.json\n\n[\n    {\n        \"key\": \"ctrl+alt+`\",\n        \"command\": \"workbench.action.terminal.new\",\n        \"when\": \"terminalProcessSupported || terminalWebExtensionContributedProfile\"\n    },\n    {\n        \"key\": \"ctrl+shift+`\",\n        \"command\": \"-workbench.action.terminal.new\",\n        \"when\": \"terminalProcessSupported || terminalWebExtensionContributedProfile\"\n    },\n    {\n        \"key\": \"ctrl+shift+`\",\n        \"command\": \"workbench.action.terminal.toggleTerminal\",\n        \"when\": \"terminal.active\"\n    },\n    {\n        \"key\": \"ctrl+`\",\n        \"command\": \"-workbench.action.terminal.toggleTerminal\",\n        \"when\": \"terminal.active\"\n    },\n    {\n        \"key\": \"shift+cmd+c\",\n        \"command\": \"-editor.action.commentLine\",\n        \"when\": \"config.rstudio.keymap.enable && editorTextFocus\"\n    },\n    {\n        \"key\": \"alt+cmd+q\",\n        \"command\": \"rewrap.rewrapComment\",\n        \"when\": \"editorTextFocus\"\n    },\n    {\n        \"key\": \"alt+q\",\n        \"command\": \"-rewrap.rewrapComment\",\n        \"when\": \"editorTextFocus\"\n    },\n    {\n        \"key\": \"cmd+d\",\n        \"command\": \"-editor.action.deleteLines\",\n        \"when\": \"config.rstudio.keymap.enable && editorTextFocus\"\n    }\n]\n\n\n\nMy extensions\n\n\n\n\n\n\nHow to install extensions in Positron / VS Code\n\n\n\n\n\nInstalling extensions in Positron / VS Code is super straightforward (see here). Click on the Extensions icon in the main Activity Bar, search for an extension, and click on “Install”. You can also disable or uninstall existing extensions from here.\n\n\n\n\n\n\n\n\n\nExtension page for StanOne of the best things about Positron is that it has access to most of VS Code’s extensions. Positron is not allowed to access Microsoft’s Visual Studio Extension Marketplace, but it can access (and is a major sponsor of) the alternative Open VSX Registry. With the exception of Microsoft’s extensions like GitHub Copilot, Dev Containers, and Remote - SSH, Open VSX had pretty much all the extensions that I already regularly use in VS Code.\nThe only minor VS Code extension I normally use that I couldn’t install in Positron was Stata Enhanced (Not that I even ever use Stata—I don’t have it installed on my computer and don’t have a license, but it’s nice to be able to open .do files and see syntax highlighting). Stata Enhanced isn’t listed at Open VSX, but I’ve opened an issue requesting that it gets listed.\nHere’s what I use:\n\nManaging other environments\n\nDocker: Manage Docker containers and volumes; right click on docker-compose.yml files to spin them up and shut them down; syntax highlighting for Dockerfiles and Docker Compose\nOpen Remote - SSH: Connect to remote servers with SSH. This is bundled with Positron and there’s no need to install anything.\n\nText editing\n\nRewrap: Automatically add line breaks in long comments or text (I have it set to wrap at 80 characters using ⌘⌥Q)\nBetter Comments: Add special syntax highlighting for some types of comments like TODO, ?, !, and so on\nShebang Snippets: Provides snippets for adding shebang directives (e.g. type #!python to get #!/usr/bin/env python)\n\nViewers and syntaxes\n\nExcel Viewer: View .xlsx files\nvscode-pdf: View PDFs\nRainbow CSV: Does neat syntax highlighting for CSV files (highlighting each column with specific colors)\nStan: Syntax highlighting for Stan\nYAML: Syntax highlighting for YAML\nLua: Syntax highlighting for Lua\nmarkdownlint: Linting and style suggestions for Markdown\n\nTheme stuff\n\nMaterial Icon Theme: Customize the icons associated with specific file types in the file explorer\nFluent Icons: Customize the icons in the general Positron app (primarily the icons in the Activity Bar, like Explorer, Search, Source Control, etc.)\n\n\n\nRemote connections with SSH\nOne of the best features of VS Code is its ability to connect to remote servers through SSH, but because that’s enabled with a special closed source Microsoft extension, it doesn’t work in Positron.\nThe Open Remote - SSH extension replicates Microsoft’s remote SSH extension, and it’s available at Open VSX. However, it doesn’t work with Positron immediately—you’ll get an error when connecting. This now works and there’s no need to install anything! (See this for a previous partial workaround.)\n\n\n\n\n\n\nAugust 2024 update!\n\n\n\nAs of August 2024, Positron now bundles an SSH extension that Just Works™. If you have R or Python installed on a remote server, you can connect to it and run code remotely and it’s all great and wonderful now.\n\n\n\n\n\nThings I still wish Positron could do\nPositron is still in beta and is undergoing rapid development, and that’s totally fine. Even though it’s not a finished product yet, it works really really well.\nThere are still some things I wish it could do though. Some of these will eventually be addressed; some can’t because of Microsoft.\n\nPackages panel: I love RStudio’s Packages panel. It’s so helpful for seeing which packages are currently installed, which versions are installed, updating existing packages, and installing new ones.\n\n\n\nRStudio’s Packages panel\n\n\nNothing like this exists for Positron right now, but there’s discussion about how to build something like it (that would also work for Python).\nPlot dimensions: (This will hopefully be addressed someday). In RStudio, when working with Quarto and R Markdown documents, inline images use the dimensions that you set in the chunk options, which makes it really easy to tinker with plot dimensions (i.e. changing from fig-width: 2.5 to fig-width: 2.75 to make sure labels fit in the plot area). Current, plots in Positron show up in the plots panel and use whatever dimensions that panel is set to use, either by manually resizing it or by using a dropdown menu with specific sizes:\n\n\n\nPositron’s plot panel\n\n\nIt would be cool if Positron’s plot panel could pick up the dimensions specified in a Quarto document and auto-resize to match. For now, I’ve just been using the “Custom Size” option. If I want to preview an image that’s 5 inches wide and 3.75 inches tall, I convert the ratio of width/height to pixels. It’s not exact—there are issues with different DPIs and retina screens—but it at least shows the correct proportion.\n\n\n\nCustom sizes in Positron’s plot panel\n\n\nRemote editing and execution with Open Remote - SSH: It would be incredible if (1) it were a lot easier to install Open Remote - SSH, and (2) it were possible to run code on remote servers. I think they’re working on supporting this.\nSimilar to this, but less important to me because I don’t use Docker containers this way, VS Code can work with Docker containers with the Dev Containers extension, similar to SSH, using Docker environments to run R/Python locally. They might be working on supporting this some day.\nGitHub Copilot Chat: Being able to chat with GitHub Copilot in VS Code is fantastic and it’s like only LLM thing I use. But it only works through a closed source extension by Microsoft and probably won’t ever work outside of VS Code proper.\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{heiss2024,\n  author = {Heiss, Andrew},\n  title = {Fun with {Positron}},\n  date = {2024-07-08},\n  url = {https://www.andrewheiss.com/blog/2024/07/08/fun-with-positron/},\n  doi = {10.59350/zs7da-17c67},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHeiss, Andrew. 2024. “Fun with Positron.” July 8, 2024. https://doi.org/10.59350/zs7da-17c67."
  },
  {
    "objectID": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html",
    "href": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html",
    "title": "Calculating birthday probabilities with R instead of math",
    "section": "",
    "text": "Even though I’ve been teaching R and statistical programming since 2017, and despite the fact that I do all sorts of heavily quantitative research, I’m really really bad at probability math.\nLike super bad.\nThe last time I truly had to do set theory and probability math was in my first PhD-level stats class in 2012. The professor cancelled classes after the first month and gave us all of October to re-teach ourselves calculus and probability theory (thank you Sal Khan), and then the rest of the class was pretty much all about pure set theory stuff. It was… not fun.\nBut I learned a valuable secret power from the class. During the final couple weeks of the course, the professor mentioned in passing that it’s possible to skip most of this probability math and instead use simulations to get the same answers. That one throwaway comment changed my whole approach to doing anything based on probabilities."
  },
  {
    "objectID": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#why-simulate",
    "href": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#why-simulate",
    "title": "Calculating birthday probabilities with R instead of math",
    "section": "Why simulate?",
    "text": "Why simulate?\nIn one problem set from November 20121, we had to answer this question using both actual probability math and R simulation:\n1 I wrote this in a .Rnw file! R Markdown wasn’t even a thing yet!\nAn urn contains 10 red balls, 10 blue balls, and 20 green balls. If 5 balls are selected at random without replacement, what is the probability that at least 1 ball of each color will be selected?\n\new probability math\nWe can find this probability by finding the probability of not selecting one or more of the colors in the draw and subtracting it from 1. We need to find the probability of selecting no red balls, no blue balls, and no green balls, and then subtract the probability of the overlapping situations (i.e. no red or blue balls, no red or green balls, and no blue or green balls).\nTo do this, we can use n-choose-k notation from combinatorics to represent the number of choices from a pool of possible combinations. This notation looks like this:\n\\[\n\\dbinom{n}{k} = \\frac{n!}{k! (n-k)!}\n\\]\nIf we’re selecting 5 balls from a pool of 40, we can say “40 choose 5”, or \\(\\binom{40}{5}\\). To calculate that, we get this gross mess:\n\\[\n\\begin{aligned}\n\\dbinom{40}{5} &= \\frac{40!}{5! (40-5)!} \\\\[10pt]\n&= \\frac{40!}{5!\\ 35!} \\\\[10pt]\n&= \\frac{40 \\times 39 \\times 38 \\times 37 \\times 36}{5!} \\qquad \\text{(cancel out the 35!)} \\\\[10pt]\n&= \\frac{78,960,960}{120} \\\\[10pt]\n&= 658,008\n\\end{aligned}\n\\]\nOr we can do it with R:\n\nchoose(40, 5)\n## [1] 658008\n\nSo with this binomial choose notation, we can calculate the official formal probability of drawing at least one red, blue, and green ball from this urn:\n\n\\[\n\\begin{aligned}\n\\text{Pr}(\\text{at least one red, blue, and green}) &= 1 - \\frac{\\text{Ways to get no red or no blue or no green}}{\\text{Ways to get 5 balls from 40}} \\\\[10pt]\n&= 1 - \\dfrac{\n    \\begin{array}{@{}c@{}}\n        (\\text{Ways to get no red}) + (\\text{Ways to get no blue}) + (\\text{Ways to get no green}) - \\\\\n        (\\text{Ways to get no red or blue}) - (\\text{Ways to get no red or green}) - (\\text{Ways to get no blue or green})\n    \\end{array}\n  }{\\text{Ways to get 5 balls from 40}} \\\\[10pt]\n&= 1 - \\dfrac{\n    \\dbinom{30}{5} + \\dbinom{30}{5} + \\dbinom{20}{5} - \\dbinom{20}{5} - \\dbinom{10}{5} - \\dbinom{10}{5}\n  }{\n    \\dbinom{40}{5}\n}\n\\end{aligned}\n\\]\n\nIf we really really wanted, we could then calculate all of that by hand, but ew.\nWe can just use R instead:\n\n# Ways to draw 5 balls without getting a specific color\nno_red &lt;- choose(30, 5)\nno_blue &lt;- choose(30, 5)\nno_green &lt;- choose(20, 5)\n\n# Ways to draw 5 balls without getting two specific colors\nno_red_blue &lt;- choose(20, 5)\nno_red_green &lt;- choose(10, 5)\nno_blue_green &lt;- choose(10, 5)\n\n# Ways to draw 5 balls in general\ntotal_ways &lt;- choose(40, 5)\n\n# Probability of drawing at least 1 of each color\nprob_real &lt;- 1 - (no_red + no_blue + no_green - no_red_blue - no_red_green - no_blue_green) / total_ways\nprob_real\n## [1] 0.5676\n\nGreat. There’s a 56.76% chance of drawing at least one of each color. We have an answer, but this was really hard, and I could only do it because I dug up my old problem sets from 2012.\nyay brute force simulation\nI really don’t like formal probability math. Fortunately there’s a way I find a heck of a lot easier to use. Brute force simulation.\nInstead of figuring out all these weird n-choose-k probabilities, we’ll use the power of computers to literally draw from a hypothetical urn over and over and over again until we come to the right answer.\nHere’s one way to do it:2\n2 Again, this is 2012-era R code; nowadays I’d forgo the loop and use something like purrr::map() or sapply().\n# Make this randomness consistent\nset.seed(12345)\n\n# Make an urn with balls in it\nurn &lt;- c(rep('red', 10), rep('blue', 10), rep('green', 20))\n\n# How many times we'll draw from the urn\nsimulations &lt;- 100000\n\ncount &lt;- 0\nfor (i in 1:simulations) {\n  # Pick 5 balls from the urn\n  draw &lt;- sample(urn, 5)\n  \n  # See if there's a red, blue, and green; if so, record it\n  if ('red' %in% draw && 'blue' %in% draw && 'green' %in% draw) {\n    count &lt;- count + 1\n  }\n}\n\n# Find the simulated probability\nprob_simulated &lt;- count / simulations\nprob_simulated\n## [1] 0.5681\n\nSweet. The simulation spat out 0.5681, which is shockingly close to 0.5676. If we boosted the number of simulations from 100,000 to something even higher,3 we’d eventually converge on the true answer.\n3 Going up to 2,000,000 got me to 0.5676.\nI use this simulation-based approach to anything mathy as much as I can. Personally, I find it far more intuitive to re-create the data generating process rather than think in set theory and combinatorics. In my program evaluation class, we do an in-class activity with the dice game Zilch where we figure out the probability of scoring something in a given dice roll. Instead of finding real probabilities, we just simulate thousands of dice rolls and mark if something was rolled. We essentially recreate the exact data generating process.\nThis approach is also the core of modern Bayesian statistics. Calculating complex integrals to find posterior distributions is too hard, so we can use Markov Chain Monte Carlo (MCMC) processes bounce around the plausible space for a posterior distribution until they settle on a stable value."
  },
  {
    "objectID": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#birthday-probabilities",
    "href": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#birthday-probabilities",
    "title": "Calculating birthday probabilities with R instead of math",
    "section": "Birthday probabilities",
    "text": "Birthday probabilities\nA couple days ago, I came across this post on Bluesky:\n\n\nPost by Karl Rohe (@karlrohe.bsky.social)\n\nThis is neat because it’s also the case in my household. We have “birthday season” from May to November, and have a dearth of birthdays from November to May. They’re all clustered in half the year. I’d never thought about how unlikely that was.\n\n\n\n\n\n\n\n\nThere’s probably some formal probability math that can answer Karl’s question precisely, but that’s hard. So instead, I decided to figure this out with my old friend—brute force simulation.\nThe data generating process is a little more complicated than just drawing balls from urns, and the cyclical nature of calendars adds an extra wrinkle to simulating everything, but it’s doable (and fun!), so I figured I’d share the details of the simulation process here. And with the recent release of {ggplot2} 3.5 and its new coord_radial() and legend placement settings and point-based text sizing and absolute plot-based positioning, I figured I’d make some pretty plots along the way.\nLet’s load some libraries, make a custom theme, and get started!\n\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(patchwork)\n\nclrs &lt;- MetBrewer::met.brewer(\"Demuth\")\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Montserrat\ntheme_calendar &lt;- function() {\n  theme_minimal(base_family = \"Montserrat\") +\n    theme(\n      axis.text.y = element_blank(),\n      axis.title = element_blank(),\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n}\n\nupdate_geom_defaults(\"text\", list(family = \"Montserrat\"))"
  },
  {
    "objectID": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#visualizing-birthday-distributions-and-spans",
    "href": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#visualizing-birthday-distributions-and-spans",
    "title": "Calculating birthday probabilities with R instead of math",
    "section": "Visualizing birthday distributions and spans",
    "text": "Visualizing birthday distributions and spans\nAll birthdays within 6-month span\nFirst, let’s work with a hypothetical household with four people in it with birthdays on January 4, March 10, April 28, and May 21. We’ll plot these on a radial plot and add a 6-month span starting at the first birthday:\n\n# All these happen within a 6-month span\nbirthdays_yes &lt;- c(\n  ymd(\"2024-01-04\"),\n  ymd(\"2024-03-10\"),\n  ymd(\"2024-04-28\"),\n  ymd(\"2024-05-21\")\n)\n\ntibble(x = birthdays_yes) |&gt; \n  ggplot(aes(x = x, y = \"\")) +\n  annotate(\n    geom = \"segment\", \n    x = birthdays_yes[1], xend = birthdays_yes[1] + months(6), y = \"\",\n    linewidth = 3, color = clrs[5]) +\n  geom_point(size = 5, fill = clrs[10], color = \"white\", pch = 21) +\n  annotate(\n    \"text\", \n    label = \"Yep\", fontface = \"bold\",\n    x = I(0.5), y = I(0),\n    size = 14, size.unit = \"pt\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\", date_labels = \"%B\",\n    limits = c(ymd(\"2024-01-01\"), ymd(\"2024-12-31\")),\n    expand = expansion(0, 0)\n  ) +\n  scale_y_discrete(expand = expansion(add = c(0, 1))) +\n  coord_radial(inner.radius = 0.8) +\n  theme_calendar()\n\n\n\n\n\n\n\nThe four birthdays all fit comfortably within the 6-month span. Neat.\nAll birthdays within 6-month span, but tricky\nNext, let’s change the May birthday to December 1. These four birthdays still all fit within a 6-month span, but it’s trickier to see because the calendar year resets in the middle. Earlier, we plotted the yellow span with annotate(), but if we do that now, it breaks and we get a warning. We can’t draw a line segment from December 1 to six months later:\n\nbirthdays_yes_but_tricky &lt;- c(\n  ymd(\"2024-01-04\"),\n  ymd(\"2024-03-10\"),\n  ymd(\"2024-04-28\"),\n  ymd(\"2024-12-01\")\n)\n\ntibble(x = birthdays_yes_but_tricky) |&gt; \n  ggplot(aes(x = x, y = \"\")) +\n  annotate(\n    geom = \"segment\",\n    x = birthdays_yes_but_tricky[4],\n    xend = birthdays_yes_but_tricky[4] + months(6),\n    y = \"\",\n    linewidth = 3, color = clrs[5]) +\n  geom_point(size = 5, fill = clrs[10], color = \"white\", pch = 21) +\n  annotate(\n    \"text\", \n    label = \"Yep\\n(but broken)\", fontface = \"bold\",\n    x = I(0.5), y = I(0),\n    size = 14, size.unit = \"pt\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\", date_labels = \"%B\",\n    limits = c(ymd(\"2024-01-01\"), ymd(\"2024-12-31\")),\n    expand = expansion(0, 0)\n  ) +\n  scale_y_discrete(expand = expansion(add = c(0, 1))) +\n  coord_radial(inner.radius = 0.8) +\n  theme_calendar()\n## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_segment()`).\n\n\n\n\n\n\n\nInstead, we can draw two line segments—one from December 1 to December 31, and one from January 1 to whatever six months from December 1 is. Since this plot represents all of 2024, we’ll force the continued time after January 1 to also be in 2024 (even though it’s technically 2025). Here I colored the segments a little differently to highlight the fact that they’re two separate lines:\n\ntibble(x = birthdays_yes_but_tricky) |&gt; \n  ggplot(aes(x = x, y = \"\")) +\n  annotate(\n    geom = \"segment\",\n    x = birthdays_yes_but_tricky[4], \n    xend = ymd(\"2024-12-31\"), \n    y = \"\",\n    linewidth = 3, color = clrs[5]) +\n  annotate(\n    geom = \"segment\",\n    x = ymd(\"2024-01-01\"), \n    xend = (ymd(\"2024-01-01\") + months(6)) - \n      (ymd(\"2024-12-31\") - birthdays_yes_but_tricky[4]), \n    y = \"\",\n    linewidth = 3, color = clrs[4]) +\n  geom_point(size = 5, fill = clrs[10], color = \"white\", pch = 21) +\n  annotate(\n    \"text\", \n    label = \"Yep\\n(but tricky)\", fontface = \"bold\",\n    x = I(0.5), y = I(0),\n    size = 14, size.unit = \"pt\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\", date_labels = \"%B\",\n    limits = c(ymd(\"2024-01-01\"), ymd(\"2024-12-31\")),\n    expand = expansion(0, 0)\n  ) +\n  scale_y_discrete(expand = expansion(add = c(0, 1))) +\n  coord_radial(inner.radius = 0.8) +\n  theme_calendar()\n\n\n\n\n\n\n\nWriting two separate annotate() layers feels repetitive, though, and it’s easy to make mistakes. So instead, we can make a little helper function that will create a data frame with the start and end date of a six-month span. If the span crosses December 31, it returns two spans; if not, it returns one span:\n\ncalc_date_arc &lt;- function(start_date) {\n  days_till_end &lt;- ymd(\"2024-12-31\") - start_date\n  \n  if (days_till_end &gt;= months(6)) {\n    x &lt;- start_date\n    xend &lt;- start_date + months(6)\n  } else {\n    x &lt;- c(start_date, ymd(\"2024-01-01\"))\n    xend &lt;- c(\n      start_date + days_till_end, \n      (ymd(\"2024-01-01\") + months(6)) - days_till_end\n    )\n  }\n  \n  return(tibble(x = x, xend = xend))\n}\n\nLet’s make sure it works. Six months from March 15 is September 15, which doesn’t cross into a new year, so we get just one start and end date:\n\ncalc_date_arc(ymd(\"2024-03-15\"))\n## # A tibble: 1 × 2\n##   x          xend      \n##   &lt;date&gt;     &lt;date&gt;    \n## 1 2024-03-15 2024-09-15\n\nSix months from November 15 is sometime in May, which means we do cross into a new year. We thus get two spans: (1) a segment from November to the end of December, and (2) a segment from January to May:\n\ncalc_date_arc(ymd(\"2024-11-15\"))\n## # A tibble: 2 × 2\n##   x          xend      \n##   &lt;date&gt;     &lt;date&gt;    \n## 1 2024-11-15 2024-12-31\n## 2 2024-01-01 2024-05-16\n\nPlotting with this function is a lot easier, since it returns a data frame. We don’t need to worry about using annotate() anymore and can instead map the x and xend aesthetics from the data to the plot:\n\ntibble(x = birthdays_yes_but_tricky) |&gt; \n  ggplot(aes(x = x, y = \"\")) +\n  geom_segment(\n    data = calc_date_arc(birthdays_yes_but_tricky[4]), aes(xend = xend),\n    linewidth = 3, color = clrs[5]\n  ) +\n  geom_point(size = 5, fill = clrs[10], color = \"white\", pch = 21) +\n  annotate(\n    \"text\", \n    label = \"Yep\\n(but tricky)\", fontface = \"bold\",\n    x = I(0.5), y = I(0),\n    size = 14, size.unit = \"pt\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\", date_labels = \"%B\",\n    limits = c(ymd(\"2024-01-01\"), ymd(\"2024-12-31\")),\n    expand = expansion(0, 0)\n  ) +\n  scale_y_discrete(expand = expansion(add = c(0, 1))) +\n  coord_radial(inner.radius = 0.8) +\n  theme_calendar()\n\n\n\n\n\n\n\nSome birthdays outside a 6-month span\nLet’s change the set of birthdays again so that one of them falls outside the six-month window. Regardless of where we start the span, we can’t collect all the points within a continuous six-month period:\n\nbirthdays_no &lt;- c(\n  ymd(\"2024-01-04\"),\n  ymd(\"2024-03-10\"),\n  ymd(\"2024-04-28\"),\n  ymd(\"2024-09-21\")\n)\n\np1 &lt;- tibble(x = birthdays_no) |&gt; \n  ggplot(aes(x = x, y = \"\")) +\n  geom_segment(\n    data = calc_date_arc(birthdays_no[1]), aes(xend = xend),\n    linewidth = 3, color = clrs[3]\n  ) +\n  geom_point(size = 5, fill = clrs[10], color = \"white\", pch = 21) +\n  annotate(\n    \"text\", \n    label = \"Nope\", fontface = \"bold\",\n    x = I(0.5), y = I(0),\n    size = 14, size.unit = \"pt\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\", date_labels = \"%B\",\n    limits = c(ymd(\"2024-01-01\"), ymd(\"2024-12-31\")),\n    expand = expansion(0, 0)\n  ) +\n  scale_y_discrete(expand = expansion(add = c(0, 1))) +\n  coord_radial(inner.radius = 0.8) +\n  theme_calendar()\n\np2 &lt;- tibble(x = birthdays_no) |&gt; \n  ggplot(aes(x = x, y = \"\")) +\n  geom_segment(\n    data = calc_date_arc(birthdays_no[4]), aes(xend = xend),\n    linewidth = 3, color = clrs[3]\n  ) +\n  geom_point(size = 5, fill = clrs[10], color = \"white\", pch = 21) +\n  annotate(\n    \"text\", \n    label = \"Still nope\", fontface = \"bold\",\n    x = I(0.5), y = I(0),\n    size = 14, size.unit = \"pt\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\", date_labels = \"%B\",\n    limits = c(ymd(\"2024-01-01\"), ymd(\"2024-12-31\")),\n    expand = expansion(0, 0)\n  ) +\n  scale_y_discrete(expand = expansion(add = c(0, 1))) +\n  coord_radial(inner.radius = 0.8) +\n  theme_calendar()\n\n(p1 | plot_spacer() | p2) + \n  plot_layout(widths = c(0.45, 0.1, 0.45))"
  },
  {
    "objectID": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#simulation-time",
    "href": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#simulation-time",
    "title": "Calculating birthday probabilities with R instead of math",
    "section": "Simulation time!",
    "text": "Simulation time!\nSo far, we’ve been working with a hypothetical household of 4, with arbitrarily chosen birthdays. For the simulation, we’ll need to work with randomly selected birthdays for households of varying sizes.\nCounting simulated birthdays and measuring spans\nBut before we build the full simulation, we need to build a way to programmatically detect if a set of dates fit within a six-month span, which—as we saw with the plotting—is surprisingly tricky because of the possible change in year.\nIf we didn’t need to contend with a change in year, we could convert all the birthdays to their corresponding day of the year, sort them, find the difference between the first and the last, and see if it’s less than 183 days (366/2; we’re working with a leap year). This would work great:\n\nbirthdays_yes &lt;- c(\n  ymd(\"2024-01-04\"),\n  ymd(\"2024-03-10\"),\n  ymd(\"2024-04-28\"),\n  ymd(\"2024-05-21\")\n)\n\nbirthdays_sorted &lt;- sort(yday(birthdays_yes))\nbirthdays_sorted\n## [1]   4  70 119 142\n\nmax(birthdays_sorted) - min(birthdays_sorted)\n## [1] 138\n\nBut time is a circle. If we look at a set of birthdays that crosses a new year, we can’t just look at max - min. Also, there’s no guarantee that the six-month span will go from the first to the last; it could go from the last to the first.\n\nbirthdays_yes_but_tricky &lt;- c(\n  ymd(\"2024-01-04\"),\n  ymd(\"2024-03-10\"),\n  ymd(\"2024-04-28\"),\n  ymd(\"2024-12-01\")\n)\n\nbirthdays_sorted &lt;- sort(yday(birthdays_yes_but_tricky))\nbirthdays_sorted\n## [1]   4  70 119 336\n\nmax(birthdays_sorted) - min(birthdays_sorted)\n## [1] 332\n\nSo we need to do something a little trickier to account for the year looping over. There are several potential ways to do this, and there’s no one right way. Here’s the approach I settled on.\nWe need to check the span between each possible window of dates. In a household of 4, this means finding the distance between the 4th (or last) date and the 1st date, which we did with max - min. But we also need to check on the distance between the 1st date in the next cycle (i.e. 2025) and the 2nd date (in 2024), and so on, or this:\n\nDistance between 4th and 1st:\n\n\nymd(\"2024-12-01\") - ymd(\"2024-01-04\") or 332 days\n\n\nDistance between (1st + 1 year) and 2nd:\n\n\nymd(\"2025-01-04\") - ymd(\"2024-03-10\") or 300 days\n\n\nDistance between (2nd + 1 year) and 3rd:\n\n\nymd(\"2025-03-10\") - ymd(\"2024-04-28\") or 316 days\n\n\nDistance between (3rd + 1 year) and 4th:\n\n\nymd(\"2025-04-28\") - ymd(\"2024-12-01\") or 148 days\n\n\n\nThat last one, from December 1 to April 28, is less than 180 days, which means that the dates fit within a six-month span. We saw this in the plot earlier too—if we start the span in December, it more than covers the remaining birthdays.\nOne easy way to look at dates in the next year is to double up the vector of birthday days-of-the-year, adding 366 to the first set, like this:\n\nbirthdays_doubled &lt;- c(\n  sort(yday(birthdays_yes_but_tricky)), \n  sort(yday(birthdays_yes_but_tricky)) + 366\n)\n\nbirthdays_doubled\n## [1]   4  70 119 336 370 436 485 702\n\nThe first four represent the regular real birthdays; the next four are the same values, just shifted up a year (so 4 and 370 are both January 1, etc.)\nWith this vector, we can now find differences between dates that cross years more easily:4\n4 Some of these differences aren’t the same as before (317 instead of 361; 149 instead of 148). This is because 2024 is a leap year and 2025 is not, and {lubridate} accounts for that. By adding 366, we’re pretending 2025 is also a leap year. But that’s okay, because we want to pretend that February 29 happens each year.\nbirthdays_doubled[4] - birthdays_doubled[1]\n## [1] 332\nbirthdays_doubled[5] - birthdays_doubled[2]\n## [1] 300\nbirthdays_doubled[6] - birthdays_doubled[3]\n## [1] 317\nbirthdays_doubled[7] - birthdays_doubled[4]\n## [1] 149\n\nOr instead of repeating lots of lines like that, we can auto-increment the different indices:\n\nn &lt;- length(birthdays_yes_but_tricky)\nmap_dbl(1:n, ~ birthdays_doubled[.x + n - 1] - birthdays_doubled[.x])\n## [1] 332 300 317 149\n\nIf any of those values are less than 183, the birthdays fit in a six-month span:\n\nany(map_lgl(1:n, ~ birthdays_doubled[.x + n - 1] - birthdays_doubled[.x] &lt;= 183))\n## [1] TRUE\n\nLet’s check it with the other two test sets of birthdays. Here’s the easy set of birthdays without any cross-year loops:\n\nbirthdays_yes &lt;- c(\n  ymd(\"2024-01-04\"),\n  ymd(\"2024-03-10\"),\n  ymd(\"2024-04-28\"),\n  ymd(\"2024-05-21\")\n)\n\nbirthdays_doubled &lt;- c(\n  sort(yday(birthdays_yes)), \n  sort(yday(birthdays_yes)) + 366\n)\n\nmap_dbl(1:n, ~ birthdays_doubled[.x + n - 1] - birthdays_doubled[.x])\n## [1] 138 300 317 343\nany(map_lgl(1:n, ~ birthdays_doubled[.x + n - 1] - birthdays_doubled[.x] &lt;= 183))\n## [1] TRUE\n\nAnd here’s the set we know doesn’t fit:\n\nbirthdays_no &lt;- c(\n  ymd(\"2024-01-04\"),\n  ymd(\"2024-03-10\"),\n  ymd(\"2024-04-28\"),\n  ymd(\"2024-09-21\")\n)\n\nbirthdays_doubled &lt;- c(\n  sort(yday(birthdays_no)), \n  sort(yday(birthdays_no)) + 366\n)\n\nmap_dbl(1:n, ~ birthdays_doubled[.x + n - 1] - birthdays_doubled[.x])\n## [1] 261 300 317 220\nany(map_lgl(1:n, ~ birthdays_doubled[.x + n - 1] - birthdays_doubled[.x] &lt;= 183))\n## [1] FALSE\n\nIt works!\nActual simulation\nNow that we have the ability to check if any set of dates fits within six months, we can generalize this to any household size. To make life a little easier, we’ll stop working with days of the year for. Leap years are tricky, and the results changed a little bit above if we added 366 or 365 to the repeated years. So instead, we’ll think about 360° in a circle—circles don’t suddenly have 361° every four years or anything weird like that.\nIn this simulation, we’ll generate n random numbers between 0 and 360 (where n is the household size we’re interested in). We’ll then do the doubling and sorting thing and check to see if the distance between any of the 4-number spans is less than 180.\n\nsimulate_prob &lt;- function(n, num_simulations = 1000) {\n  results &lt;- map_lgl(1:num_simulations, ~{\n    birthdays &lt;- runif(n, 0, 360)\n    birthdays_doubled &lt;- sort(c(birthdays, birthdays + 360))\n    any(map_lgl(1:n, ~ birthdays_doubled[.x + n - 1] - birthdays_doubled[.x] &lt;= 180))\n  })\n  mean(results)\n}\n\nHere’s the probability of seeing all the birthdays in a six-month span in a household of 4:\n\nwithr::with_seed(1234, {\n  simulate_prob(4)\n})\n## [1] 0.522\n\nAbout 50%!\nWhat about a household of 6, like in Karl’s original post?\n\nwithr::with_seed(1234, {\n  simulate_prob(6)\n})\n## [1] 0.189\n\nAbout 18%!\nWe can get this more precise and consistent by boosting the number of simulations:\n\nwithr::with_seed(1234, {\n  simulate_prob(6, 50000)\n})\n## [1] 0.1866\n\nNow that this function is working, we can use it to simulate a bunch of possible household sizes, like from 2 to 10:\n\nsimulated_households &lt;- tibble(household_size = 2:10) |&gt;\n  mutate(prob_in_arc = map_dbl(household_size, ~simulate_prob(.x, 10000))) |&gt; \n  mutate(nice_prob = scales::label_percent(accuracy = 0.1)(prob_in_arc))\n\nggplot(simulated_households, aes(x = factor(household_size), y = prob_in_arc)) +\n  geom_pointrange(aes(ymin = 0, ymax = prob_in_arc), color = clrs[2]) +\n  geom_text(aes(label = nice_prob), nudge_y = 0.07, size = 8, size.unit = \"pt\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  labs(\n    x = \"Household size\",\n    y = \"Probability\",\n    title = \"Probability that all birthdays occur within \\na single six-month span across household size\",\n    caption = \"10,000 simulations\"\n  ) +\n  theme_minimal(base_family = \"Montserrat\") +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    plot.caption = element_text(hjust = 0, color = \"grey50\"),\n    axis.title.x = element_text(hjust = 0),\n    axis.title.y = element_text(hjust = 1)\n  )"
  },
  {
    "objectID": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#but-birthdays-arent-uniformly-distributed",
    "href": "blog/2024/05/03/birthday-spans-simulation-sans-math/index.html#but-birthdays-arent-uniformly-distributed",
    "title": "Calculating birthday probabilities with R instead of math",
    "section": "But birthdays aren’t uniformly distributed!",
    "text": "But birthdays aren’t uniformly distributed!\nWe just answered Karl’s original question: “Suppose n points are uniformly distributed on a circle. What is the probability that they belong to a connected half circle”. There’s probably some official mathy combinatorial way to build a real formula to describe this pattern, but that’s too hard. Simulation gets us there.\nUneven birthday disributions\nBut in real life, birthdays aren’t actually normally distributed. There are some fascinating patterns in days of birth. Instead of drawing birthdays from a uniform distribution where every day is equally likely, let’s draw from the actual distribution. There’s no official probability-math way to do this—the only way to do this kind of calculation is with simulation.\nThe CDC and the Social Security Administration track the counts of daily births in the Unitd States. In 2016, FiveThirtyEight reported a story about patterns in daily birthrate frequencies and they posted their CSV files on GitHub, so we’ll load their data and figure out daily probabilities of birthdays.\n\nbirths_1994_1999 &lt;- read_csv(\n  \"https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_1994-2003_CDC_NCHS.csv\"\n) |&gt;\n  # Ignore anything after 2000\n  filter(year &lt; 2000)\n\nbirths_2000_2014 &lt;- read_csv(\n  \"https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_2000-2014_SSA.csv\"\n)\n\n\nbirths_combined &lt;- bind_rows(births_1994_1999, births_2000_2014) |&gt; \n  mutate(\n    full_date = make_date(year = 2024, month = month, day = date_of_month),\n    day_of_year = yday(full_date)\n  ) |&gt; \n  mutate(\n    month_cateogrical = month(full_date, label = TRUE, abbr = FALSE)\n  )\nglimpse(births_combined)\n## Rows: 7,670\n## Columns: 8\n## $ year              &lt;dbl&gt; 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 19…\n## $ month             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4,…\n## $ date_of_month     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n## $ day_of_week       &lt;dbl&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5,…\n## $ births            &lt;dbl&gt; 8096, 7772, 10142, 11248, 11053, 11406, 11251, 8653, 7910, 10498, 11706, 11567, 11212, 11570, 8660, 8123, 10567, 11541, 11257, 11682, 11811, 8833, 8310, 11125, 11981, 11514, 11702, 11666, 8988, 8096, 10765, 11755, 11483, 11523, 11677, 8991, 8309, 10984, 12152, 11515, 1162…\n## $ full_date         &lt;date&gt; 2024-01-01, 2024-01-02, 2024-01-03, 2024-01-04, 2024-01-05, 2024-01-06, 2024-01-07, 2024-01-08, 2024-01-09, 2024-01-10, 2024-01-11, 2024-01-12, 2024-01-13, 2024-01-14, 2024-01-15, 2024-01-16, 2024-01-17, 2024-01-18, 2024-01-19, 2024-01-20, 2024-01-21, 2024-01-22, 2024-01…\n## $ day_of_year       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 7…\n## $ month_cateogrical &lt;ord&gt; January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, January, Ja…\n\nBut first, just because this is one of my favorite graphs ever, let’s visualize the data! Here’s a heatmap showing the daily average births for 366 days:\n\navg_births_month_day &lt;- births_combined |&gt; \n  group_by(month_cateogrical, date_of_month) %&gt;% \n  summarize(avg_births = mean(births))\n\nggplot(\n  avg_births_month_day, \n  aes(x = factor(date_of_month), y = fct_rev(month_cateogrical), fill = avg_births)\n) +\n  geom_tile() +\n  scale_fill_viridis_c(\n    option = \"rocket\", labels = scales::label_comma(),\n    guide = guide_colorbar(barwidth = 20, barheight = 0.5, position = \"bottom\")\n  ) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Average births per day\",\n    subtitle = \"1994–2014\",\n    fill = \"Average births\"\n  ) +\n  coord_equal() +\n  theme_minimal(base_family = \"Montserrat\") +\n  theme(\n    legend.justification.bottom = \"left\",\n    legend.title.position = \"top\",\n    panel.grid = element_blank(),\n    axis.title.x = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\nThere are some really fascinating stories here!\n\nNobody wants to have babies during Christmas or New Year’s. Christmas Day, Christmas Eve, and New Year’s Day seem to have the lowest average births.\nNew Year’s Eve, Halloween, July 4, April 1,5 and the whole week of Thanksgiving6 also have really low averages.\nThe 13th of every month has slightly fewer births than average—the column at the 13th is really obvious here.\nThe days with the highest average counts are in mid-September, from the 9th to the 20th—except for September 11.\n\n5 No one wants joke babies?6 American Thanksgiving is the fourth Thursday of November, so the exact day of the month moves around each yearWith this data, we can calculate the daily probability of having a birthday:\n\nprob_per_day &lt;- births_combined |&gt; \n  group_by(day_of_year) |&gt; \n  summarize(total = sum(births)) |&gt; \n  mutate(prob = total / sum(total)) |&gt; \n  mutate(full_date = ymd(\"2024-01-01\") + days(day_of_year - 1)) |&gt; \n  mutate(yearless_date = format(full_date, \"%B %d\"))\nglimpse(prob_per_day)\n## Rows: 366\n## Columns: 5\n## $ day_of_year   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 7…\n## $ total         &lt;dbl&gt; 164362, 196481, 228259, 232354, 230826, 229776, 230233, 223766, 224116, 232521, 231414, 230499, 223803, 231343, 222374, 224055, 230038, 229465, 225387, 228138, 228190, 225098, 229163, 233021, 230897, 228394, 228012, 228365, 222831, 226669, 229472, 230364, 230984, 228433, 2298…\n## $ prob          &lt;dbl&gt; 0.0019176, 0.0022923, 0.0026631, 0.0027108, 0.0026930, 0.0026808, 0.0026861, 0.0026107, 0.0026147, 0.0027128, 0.0026999, 0.0026892, 0.0026111, 0.0026991, 0.0025944, 0.0026140, 0.0026838, 0.0026771, 0.0026296, 0.0026617, 0.0026623, 0.0026262, 0.0026736, 0.0027186, 0.0026938, 0…\n## $ full_date     &lt;date&gt; 2024-01-01, 2024-01-02, 2024-01-03, 2024-01-04, 2024-01-05, 2024-01-06, 2024-01-07, 2024-01-08, 2024-01-09, 2024-01-10, 2024-01-11, 2024-01-12, 2024-01-13, 2024-01-14, 2024-01-15, 2024-01-16, 2024-01-17, 2024-01-18, 2024-01-19, 2024-01-20, 2024-01-21, 2024-01-22, 2024-01-23,…\n## $ yearless_date &lt;chr&gt; \"January 01\", \"January 02\", \"January 03\", \"January 04\", \"January 05\", \"January 06\", \"January 07\", \"January 08\", \"January 09\", \"January 10\", \"January 11\", \"January 12\", \"January 13\", \"January 14\", \"January 15\", \"January 16\", \"January 17\", \"January 18\", \"January 19\", \"January 2…\n\nHere are the 5 most common days:7\n7 There’s me on September 19.\nprob_per_day |&gt; \n  select(yearless_date, prob) |&gt; \n  slice_max(order_by = prob, n = 5)\n## # A tibble: 5 × 2\n##   yearless_date    prob\n##   &lt;chr&gt;           &lt;dbl&gt;\n## 1 September 09  0.00302\n## 2 September 19  0.00301\n## 3 September 12  0.00301\n## 4 September 17  0.00299\n## 5 September 10  0.00299\n\nAnd the 5 least probable days—Leap Day, Christmas Day, New Year’s Day, Christmas Eve, and July 4th:\n\nprob_per_day |&gt; \n  select(yearless_date, prob) |&gt; \n  slice_min(order_by = prob, n = 5)\n## # A tibble: 5 × 2\n##   yearless_date     prob\n##   &lt;chr&gt;            &lt;dbl&gt;\n## 1 February 29   0.000613\n## 2 December 25   0.00162 \n## 3 January 01    0.00192 \n## 4 December 24   0.00199 \n## 5 July 04       0.00216\n\nUsing actual birthday probabilities\nInstead of drawing random numbers between 0 and 360 from a uniform distribution, we can draw day-of-the-year numbers. This is easy with sample(). Here’s a random 4-person household:\n\nwithr::with_seed(1234, {\n  sample(1:366, size = 4, replace = TRUE)\n})\n## [1] 284 336 101 111\n\nThat gives us a uniform probability distribution—all the numbers between 1 and 366 are equally likely. sample() has a prob argument that we can use to feed a vector of probabilities:\n\nwithr::with_seed(1234, {\n  sample(\n    prob_per_day$day_of_year, \n    size = 4, \n    replace = TRUE,\n    prob = prob_per_day$prob)\n})\n## [1] 284 101 111 133\n\nThese days of the year now match the actual distribution of birthdays in the United States. If we simulated thousands of birthdays, we’d get more in September, fewer on the 13th of each month, and far fewer around Thanksgiving and Christmas.\nWe can now update our simulation to use this more realistic distribution of birthdays:\n\nsimulate_prob_real &lt;- function(n, num_simulations = 1000) {\n  results &lt;- map_lgl(1:num_simulations, ~{\n    birthdays &lt;- sample(\n      prob_per_day$day_of_year, \n      size = n, \n      replace = TRUE, \n      prob = prob_per_day$prob\n    )\n    \n    birthdays_doubled &lt;- sort(c(birthdays, birthdays + 366))\n    \n    any(map_lgl(1:n, ~ birthdays_doubled[.x + n - 1] - birthdays_doubled[.x] &lt;= (366 / 2)))\n  })\n  mean(results)\n}\n\nHere’s the probability of having all the birthdays within the same six months for a household of 4:\n\nwithr::with_seed(1234, {\n  simulate_prob_real(4)\n})\n## [1] 0.495\n\nAnd 6:\n\nwithr::with_seed(1234, {\n  simulate_prob_real(6)\n})\n## [1] 0.202\n\nAnd here’s the probability across different household sizes:\n\nsims_real &lt;- tibble(household_size = 2:10) |&gt;\n  mutate(prob_in_arc = map_dbl(household_size, ~simulate_prob_real(.x, 10000))) |&gt; \n  mutate(nice_prob = scales::label_percent(accuracy = 0.1)(prob_in_arc))\n\nggplot(sims_real, aes(x = factor(household_size), y = prob_in_arc)) +\n  geom_pointrange(aes(ymin = 0, ymax = prob_in_arc), color = clrs[9]) +\n  geom_text(aes(label = nice_prob), nudge_y = 0.07, size = 8, size.unit = \"pt\") +\n  scale_y_continuous(labels = scales::label_percent()) +\n  labs(\n    x = \"Household size\",\n    y = \"Probability\",\n    title = \"Probability that all birthdays occur within a\\nsingle 6-month span across household size\",\n    subtitle = \"Based on average daily birth probabilities from 1994–2014\",\n    caption = \"10,000 simulations; daily probabilities from the CDC and SSA\"\n  ) +\n  theme_minimal(base_family = \"Montserrat\") +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    plot.caption = element_text(hjust = 0, color = \"grey50\"),\n    plot.subtitle = element_text(hjust = 0, color = \"grey50\"),\n    axis.title.x = element_text(hjust = 0),\n    axis.title.y = element_text(hjust = 1)\n  )\n\n\n\n\n\n\n\nIn the end, these are all roughly the same as the uniform birthday distribution, but it feels more accurate since the probabilities are based on real-life frequencies.\n\nBut most importantly, we didn’t have to do any math to get the right answer. Brute force simulation techniques got us there."
  },
  {
    "objectID": "blog/2024/03/21/demystifying-ate-att-atu/index.html",
    "href": "blog/2024/03/21/demystifying-ate-att-atu/index.html",
    "title": "Demystifying causal inference estimands: ATE, ATT, and ATU",
    "section": "",
    "text": "In my causal inference class, I spend just one week talking about the Rubin causal model and potential outcomes. This view of causality argues that for any kind of intervention (passing a new policy, participation in a nonprofit program, taking a specific kind of medicine, etc.), people will have one of two possible outcomes:\nThese two outcomes are potential outcomes. Both are plausible, but only one will happen in real life. These potential outcomes lead to a bunch of different causal estimands we might be interested in, like the average treatment effect.\nI give such short shrift to potential outcomes largely because the bulk of the class approaches the idea of causal inference through Judea Pearl-style DAGs instead of potential outcomes. It’s a strange arrangement that I’ve stumbled into: the potential outcomes approach is incredibly popular and widespread in social sciences (particularly in economics), while causal models and DAGs are more popular in fields like epidemiology. For unfathomable reasons, there’s a weird animosity between these two worlds. Judea Pearl regularly needles social scientists on Twitter for not using DAGs and clinging to potential outcomes, while Nobel-winning econometricians decry DAGs. It’s weird.1\nAs a social scientist myself, you’d think I’d have embraced the potential outcomes approach, but for whatever reason, it never stuck and it was always confusing to me. When I came across Judea Pearl’s The Book of Why (Pearl and Mackenzie 2020) a few years ago, I fell in love with the world of DAGs. They made sense—far more sense than the weird decompositional algebra behind average treatment effects, average treatment on the treated effects, average treatment on the untreated effects, and so on.\nI’m not the only social science convert to DAGs. The general social science methods textbook Counterfactuals and Causal Inference (Morgan and Winship 2014) started popularizing DAGs in 2007, two modern phenomenal econometrics textbooks—The Effect (Huntington-Klein 2021) and Causal Inference: The Mixtape (Cunningham 2021)—feature DAGs throughout (despite that discipline’s weird aversion to them), and the latest version of the fantastic Bayesian Statistical Rethinking (McElreath 2020) uses them extensively.\nDespite all these newer DAG-based approaches in social science, in my class, I never really revisit the potential outcomes framework after that one week. We do all sorts of causal effects estimation with DAG-based adjustment through matching and inverse probability weighting, and quasi-experimental design-based approaches like difference-in-differences, regression discontinuity, and instrumental variables, but beyond emphasizing the fact that methods like regression discontinuity and instrumental variables only return local average treatment effects, we don’t really ever talk about ATEs and ATTs and ATUs again.\nThis has always bugged me.\nBeyond introducing the idea that we can’t find individual-level causal effects without a time machine, thinking about potential outcomes is neat, I guess, but not exactly relevant to all the other methods we cover. I know that’s wrong! But that’s how my mental model of these estimands has worked. All these other methods give some sort of general average causal effect, but I’m never sure which exact flavor of causal effect it is (or if the exact flavor matters).\nBut a newer working paper by Greifer and Stuart (2023) has finally helped me realize why these different estimands matter and what the subtle differences between them are.\nSo in this post, I’ll extend the basic standard ATE/ATT/ATU example to reflect a more realistic, larger dataset, and I’ll use propensity score weighting to estimate each estimand. I’ll also follow Greifer and Stuart’s example and translate these estimands into policy-relevant English equivalents (they use medical terminology; I’m not that kind of doctor and I work with social science and policy interventions).\nBut first, I highly recommend reading through their paper really quick. It’s not too long and and it’s not too mathy—it’s succinct and accessible and a good primer for all these estimands.\n(And before we start, let’s load some R packages.)\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(gt)\nlibrary(broom)\nlibrary(marginaleffects)\nlibrary(WeightIt)\n\n# Define a nice color palette from {MoMAColors}\n# https://github.com/BlakeRMills/MoMAColors\nclrs &lt;- MoMAColors::moma.colors(\"ustwo\")\n\n# Download Mulish from https://fonts.google.com/specimen/Mulish\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Mulish\") +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      plot.title = element_text(face = \"bold\"),\n      axis.title = element_text(face = \"bold\"),\n      strip.text = element_text(face = \"bold\"),\n      strip.background = element_rect(fill = \"grey80\", color = NA),\n      legend.title = element_text(face = \"bold\")\n    )\n}\n\ntheme_set(theme_nice())\n\nupdate_geom_defaults(\"text\", list(family = \"Mulish\", fontface = \"plain\"))\nupdate_geom_defaults(\"label\", list(family = \"Mulish\", fontface = \"plain\"))\nupdate_geom_defaults(ggdag:::GeomDagText, list(family = \"Mulish\", fontface = \"plain\"))\nupdate_geom_defaults(ggtext::GeomRichText, list(family = \"Mulish\", fontface = \"plain\"))"
  },
  {
    "objectID": "blog/2024/03/21/demystifying-ate-att-atu/index.html#quick-crash-course-in-potential-outcomes",
    "href": "blog/2024/03/21/demystifying-ate-att-atu/index.html#quick-crash-course-in-potential-outcomes",
    "title": "Demystifying causal inference estimands: ATE, ATT, and ATU",
    "section": "Quick crash course in potential outcomes",
    "text": "Quick crash course in potential outcomes\nBefore getting into the subtle differences between the various potential outcomes-related estimands, it’s helpful to get a general sense for how these things work. So let’s take a super abbreviated crash course in the potential outcomes framework.\nEvery causal inference textbook ever written will include a table like Table 1 to illustrate potential outcomes. To make this idea a little more mathy, we’ll call the treatment or intervention \\(X\\), the outcome that would happen if treated \\(Y^1\\), and the outcome that would happen if not treated \\(Y^0\\).2 We’ll use \\(\\delta\\) for the difference between \\(Y^1\\) and \\(Y^0\\), or the individual-level causal effect.\n2 The notation for all this varies wildly across disciplines. Economists call the treatment \\(D\\) for mysterious reasons; epidemiologists will often call it \\(A\\); I’ve seen political science papers call it \\(T\\) (which at least makes more sense than \\(D\\) or \\(A\\), since “treatment” starts with T). In my class I call it \\(X\\), which follows what a lot of other people do (like this guide to “10 Strategies for Figuring Out if X Caused Y”).\nCodebasic_po &lt;- tribble(\n  ~id, ~age,    ~treated, ~outcome_1, ~outcome_0,\n  1,   \"Old\",   1,        80,         60,\n  2,   \"Old\",   1,        75,         70,\n  3,   \"Old\",   1,        85,         80,\n  4,   \"Old\",   0,        70,         60,\n  5,   \"Young\", 1,        75,         70,\n  6,   \"Young\", 0,        80,         80,\n  7,   \"Young\", 0,        90,         100,\n  8,   \"Young\", 0,        85,         80\n) |&gt; \n  mutate(\n    ice = outcome_1 - outcome_0,\n    outcome = ifelse(treated == 1, outcome_1, outcome_0)\n  )\n\nbasic_po |&gt; \n  select(\n    id, age, treated, outcome_1, outcome_0, ice, outcome\n  ) |&gt; \n  gt() |&gt; \n  sub_missing(missing_text = \"…\") |&gt;\n  fmt_number(\n    columns = c(starts_with(\"outcome\"), ice),\n    decimals = 0\n  ) |&gt; \n\n  # Column labels\n  cols_label(\n    id = \"ID\",\n    age = md(\"$Z_i$\"),\n    treated = md(\"$X_i$\"),\n    outcome_0 = md(\"$Y^0_i$\"),\n    outcome_1 = md(\"$Y^1_i$\"),\n    outcome = md(\"$Y_i$\"),\n    ice = md(\"$Y^1_i - Y^0_i$\")\n  ) |&gt;\n  \n  # Level 1 spanner labels\n  tab_spanner(\n    label = \"Age\", columns = age, \n    level = 1, id = \"level1_a_po\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Treated\", columns = treated, \n    level = 1, id = \"level1_b_po\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Potential outcomes\",\n    columns = c(outcome_1, outcome_0),\n    level = 1, id = \"level1_c_po\"\n  ) |&gt; \n  tab_spanner(\n    label = \"ICE or \\\\(\\\\delta_i\\\\)\", columns = ice, \n    level = 1, id = \"level1_d_po\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Outcome\", columns = outcome, \n    level = 1, id = \"level1_e_po\"\n  ) |&gt; \n  \n  # Level 2 spanner labels\n  tab_spanner(\n    label = \"Confounder\",\n    columns = age,\n    level = 2, id = \"level2_a_po\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Treatment\", columns = treated, \n    level = 2, id = \"level2_b_po\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Unobservable\",\n    columns = c(outcome_1, outcome_0, ice), \n    level = 2, id = \"level2_c_po\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Realized\", columns = outcome, \n    level = 2, id = \"level2_d_po\") |&gt; \n  \n  # Style stuff\n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_column_labels()\n  ) |&gt; \n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_body()\n  ) |&gt; \n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = list(\n      cells_column_spanners(spanners = starts_with(\"level1\")),\n      cells_column_labels(columns = \"id\")\n    )\n  ) |&gt; \n  tab_style(\n    style = cell_text(style = \"italic\"),\n    locations = cells_column_spanners(spanners = starts_with(\"level2\"))\n  ) |&gt; \n  \n  tab_style(\n    style = list(\n      cell_fill(color = clrs[4], alpha = 0.5)\n    ),\n    locations = cells_body(rows = treated == 1)\n  ) |&gt; \n  tab_footnote(\n    footnote = \"ICE = individual causal effect\",\n    locations = cells_column_spanners(spanners = \"level1_d_po\")\n  ) |&gt; \n  opt_footnote_marks(marks = \"standard\") |&gt; \n  opt_horizontal_padding(scale = 3) |&gt; \n  opt_table_font(font = \"Jost\")\n\n\nTable 1: Standard table showing potential outcomes, individual causal effects, and realized outcomes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfounder\nTreatment\nUnobservable\nRealized\n\n\nID\nAge\nTreated\nPotential outcomes\nICE or \\(\\delta_i\\)*\nOutcome\n\n\n\\(Z_i\\)\n\\(X_i\\)\n\\(Y^1_i\\)\n\\(Y^0_i\\)\n\\(Y^1_i - Y^0_i\\)\n\\(Y_i\\)\n\n\n\n\n1\nOld\n1\n80\n60\n20\n80\n\n\n2\nOld\n1\n75\n70\n5\n75\n\n\n3\nOld\n1\n85\n80\n5\n85\n\n\n4\nOld\n0\n70\n60\n10\n60\n\n\n5\nYoung\n1\n75\n70\n5\n75\n\n\n6\nYoung\n0\n80\n80\n0\n80\n\n\n7\nYoung\n0\n90\n100\n−10\n100\n\n\n8\nYoung\n0\n85\n80\n5\n80\n\n\n\n\n* ICE = individual causal effect\n\n\n\n\n\n\n\n\nIndividual-level causal effects (ICE or \\(\\delta\\))\nEach person in Table 1 has two potential outcomes. Person 1, for instance, would have an outcome of 80 (\\(Y^1\\)) if they receive the treatment \\(X\\), but only an outcome of 60 (\\(Y^0\\)) if they don’t. Their individual-level causal effect (\\(\\delta\\)) is 20. This represents the effect of the treatment for just that one person, and it’s only measurable with a time machine or some way to observe parallel universes.\nAverage treatment effects (ATE)\nIf we could compare all these people in Universe A and Universe B and measure their individual causal effects, we could calculate the average treatment effect (ATE), or the effect of the intervention across the whole population. Officially, the ATE is the average of the \\(\\delta\\) column, or the average of \\(Y^1 - Y^0\\):\n\\[\n\\begin{aligned}\n\\text{ATE} &= E[\\delta_i] & \\text{ or} \\\\\n\\text{ATE} &= E[Y^1_i - Y^0_i]\n\\end{aligned}\n\\]\nGiven the data in Table 1, the ATE is 5—it’s the average of the \\(\\delta\\) column:\n\\[\n\\text{ATE} = \\frac{20 + 5 + 5 + 5 + 10 + 0 + -10 + 5}{8} = 5\n\\]\nNeat.\nAverage treatment effect on the treated (ATT)\nThere are a couple other causal effects we can measure. We might want to know how big the effect is just for those who received the treatment. This is called the average treatment effect on the treated (ATT), and is the average of the individual causal effects just among those who were treated. Officially, it’s defined like this:\n\\[\n\\begin{aligned}\n\\text{ATT} &= E[\\delta_i \\mid X_i = 1] & \\text{or} \\\\\n\\text{ATT} &= E[Y^1_i - Y^0_i \\mid X_i = 1]\n\\end{aligned}\n\\]\nIn this case, that means we’re only looking at the average \\(\\delta\\) for rows 1, 2, 3, and 5 in Table 1:\n\\[\n\\text{ATT} = \\frac{20 + 5 + 5 + 5}{4} = 8.75\n\\]\nAverage treatment effect on the untreated (ATU)\nWe can also calculate the average treatment effect on the untreated (ATU; sometimes called the ATC (for effect on the control group)) by finding the average of the individual causal effects among those who were not treated:\n\\[\n\\begin{aligned}\n\\text{ATU} &= E[\\delta_i \\mid X_i = 0] & \\text{or} \\\\\n\\text{ATU} &= E[Y^1_i - Y^0_i \\mid X_i = 0]\n\\end{aligned}\n\\]\nHere, we’re only looking at the average \\(\\delta\\) for rows 4, 6, 7, and 8 in Table 1:\n\\[\n\\text{ATU} = \\frac{10 + 0 - 10 + 5}{4} = 1.25\n\\]\nSelection bias\nThere’s a neat relationship between the ATE, ATT, and ATU—the ATE is technically a weighted average of the ATT added to the weighted average of the ATU (here \\(\\pi\\) means “proportion”, not 3.1415):\n\\[\n\\text{ATE} = (\\pi_\\text{Treated} \\times \\text{ATT}) + (\\pi_\\text{Untreated} \\times \\text{ATU})\n\\]\nApplying this to Table 1, we can get the same ATE:\n\\[\n\\begin{aligned}\n\\text{ATE} &= (\\frac{4}{8} \\times 8.75) + (\\frac{4}{8} \\times 1.25) \\\\\n&= 4.375 + 0.625 \\\\\n&= 5\n\\end{aligned}\n\\]\nOne reason it’s helpful to decompose the ATE into these two parts like this is that it shows that there’s some systematic difference between the treated and untreated people. This difference is called selection bias. People who chose to be treated did so for reasons known only to them.3\n3 Not only do we need a time machine for good causal inference, we also need a machine that can read minds.We can see the selection bias in an alternative decomposition of the ATE:\n\\[\n\\text{ATE} = \\text{ATT} + \\text{Selection bias}\n\\]\nThe fact that the ATT (8.75) is bigger than the ATE (5) here is a sign the two groups are different. This intervention, whatever it is, has a big effect on the treated people who signed up for it, likely because they somehow knew that the intervention would be helpful for them. Those who were untreated have a really low ATU (1.25)—they likely didn’t sign up for the intervention because they knew that it wouldn’t do much for them.\nFinding the ATE from observational data\nThis is all well and good, but we don’t have time machines. This is the fundamental problem of causal inference—we have no idea what each person’s \\(\\delta\\) is. We actually only see this in real life:\n\nCodebasic_po |&gt; \n  select(\n    id, age, treated, outcome\n  ) |&gt; \n  gt() |&gt; \n  fmt_number(\n    columns = \"outcome\",\n    decimals = 0\n  ) |&gt; \n\n  # Column labels\n  cols_label(\n    id = \"ID\",\n    age = md(\"$Z_i$\"),\n    treated = md(\"$X_i$\"),\n    outcome = md(\"$Y_i$\")\n  ) |&gt;\n  \n  # Level 1 spanner labels\n  tab_spanner(\n    label = \"Age\", columns = age, \n    level = 1, id = \"level1_a_po_obs\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Treated\", columns = treated, \n    level = 1, id = \"level1_b_po_obs\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Outcome\", columns = outcome, \n    level = 1, id = \"level1_c_po_obs\"\n  ) |&gt; \n  \n  # Level 2 spanner labels\n  tab_spanner(\n    label = \"Confounder\",\n    columns = age,\n    level = 2, id = \"level2_a_po_obs\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Treatment\", columns = treated, \n    level = 2, id = \"level2_b_po_obs\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Realized\", columns = outcome, \n    level = 2, id = \"level2_c_po_obs\") |&gt; \n  \n  # Style stuff\n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_column_labels()\n  ) |&gt; \n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_body()\n  ) |&gt; \n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = list(\n      cells_column_spanners(spanners = starts_with(\"level1\")),\n      cells_column_labels(columns = \"id\")\n    )\n  ) |&gt; \n  tab_style(\n    style = cell_text(style = \"italic\"),\n    locations = cells_column_spanners(spanners = starts_with(\"level2\"))\n  ) |&gt; \n\n  tab_style(\n    style = list(\n      cell_fill(color = clrs[4], alpha = 0.5)\n    ),\n    locations = cells_body(rows = treated == 1)\n  ) |&gt; \n  opt_horizontal_padding(scale = 3) |&gt; \n  opt_table_font(font = \"Jost\")\n\n\nTable 2: Observed version of Table 1 showing only the realized outcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfounder\nTreatment\nRealized\n\n\nID\nAge\nTreated\nOutcome\n\n\n\\(Z_i\\)\n\\(X_i\\)\n\\(Y_i\\)\n\n\n\n\n1\nOld\n1\n80\n\n\n2\nOld\n1\n75\n\n\n3\nOld\n1\n85\n\n\n4\nOld\n0\n60\n\n\n5\nYoung\n1\n75\n\n\n6\nYoung\n0\n80\n\n\n7\nYoung\n0\n100\n\n\n8\nYoung\n0\n80\n\n\n\n\n\n\n\n\n\nWe could try to find the ATE by finding the average outcome among the treated (\\(\\bar{Y}_\\text{Treated}\\)) and subtracting it from the average outcome among the untreated (\\(\\bar{Y}_\\text{Untreated}\\)):\n\\[\n\\begin{aligned}\n\\text{ATE}_\\text{naive} &= \\bar{Y}_\\text{Treated} - \\bar{Y}_\\text{Untreated} \\\\\n&= \\frac{80 + 75 + 85 + 75}{4} - \\frac{60 + 80 + 100 + 80}{4} \\\\\n&= 78.75 - 80 \\\\\n&= -1.25\n\\end{aligned}\n\\]\nBut this is horribly wrong. Selection bias is distorting the causal effect here. Treated people sought out treatment because they knew it would be good for them. We can’t compare the two groups directly like this because of the systematic differences between them.\nWe need to somehow account for the fact that the two groups are different. We’ve been ignoring one column in this table all this time—age (\\(Z\\)). It looks like age is highly correlated with treatment status here. 75% of people who signed up for the treatment were old; 75% of people who didn’t sign up for the treatment were young.4 In DAG terms, age seems to be a confounder—it causes both the choice to receive treatment and the ultimate value of the outcome. Age opens up a backdoor relationship between treatment and outcome and messes up the causal effect. If we can somehow statistically account for age, we’ll be left with a more accurate estimate of the actual ATE.\n4 Maybe this treatment is a colonoscopy? idk—use your imagination.\nCodebasic_dag &lt;- dagify(\n  Y ~ X + Z,\n  X ~ Z,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(\n    x = c(X = 1, Y = 3, Z = 2),\n    y = c(X = 1, Y = 1, Z = 2)\n  )\n)\n\nbasic_dag_plot &lt;- basic_dag |&gt; \n  tidy_dagitty() |&gt; \n  mutate(var_type = case_when(\n    name == \"X\" ~ \"Exposure\",\n    name == \"Y\" ~ \"Outcome\",\n    str_detect(name, \"Z\") ~ \"Confounder\"\n  )) |&gt; \n  mutate(var_label = case_match(name,\n    \"X\" ~ \"Treatment\",\n    \"Y\" ~ \"Outcome\",\n    \"Z\" ~ \"Age\"\n  ))\n\nggplot(basic_dag_plot, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = var_type), size = 12) +\n  geom_richtext(\n    aes(label = name), fontface = \"bold\", color = \"white\",\n    fill = NA, label.color = NA,\n    label.padding = grid::unit(c(3, 0, 0, 0), \"pt\")\n  ) + \n  geom_dag_text(\n    data = filter(basic_dag_plot, var_type != \"Confounder\"), aes(label = var_label), \n    nudge_y = -0.25, color = \"black\", size = 11, size.unit = \"pt\"\n  ) +\n  geom_dag_text(\n    data = filter(basic_dag_plot, var_type == \"Confounder\"), aes(label = var_label), \n    nudge_y = 0.26, color = \"black\", size = 11, size.unit = \"pt\"\n  ) +\n  scale_color_manual(values = clrs[c(4, 1, 6)], guide = \"none\") +\n  theme_dag()\n\n\n\n\n\n\nFigure 1: DAG showing that age confounds the relationship between treatment and outcome\n\n\n\n\nThere are lots of ways to adjust for age here (there are whole textbooks about this; matching, inverse probability weighting, etc.). One simple way is to stratify by age and find the sum of weighted averages for old and young people, like this:\n\\[\n\\text{ATE} = (\\pi_\\text{Old} \\times \\text{Effect}_\\text{Old}) + (\\pi_\\text{Young} \\times \\text{Effect}_\\text{Young})\n\\]\nWith the data from the table, we get:\n\\[\n\\begin{aligned}\n\\text{Effect}_\\text{Stratified} &= (\\pi_\\text{Treated} \\times \\bar{Y}_\\text{Treated}) - (\\pi_\\text{Untreated} \\times \\bar{Y}_\\text{Untreated}) \\\\[20pt]\n\\text{Effect}_\\text{Old} &= \\frac{80 + 75 + 85}{3} - \\frac{60}{1} &&= 20 \\\\\n\\text{Effect}_\\text{Young} &= \\frac{75}{1} - \\frac{80 + 100 + 80}{3} &&= -11.667 \\\\[20pt]\n\\text{ATE} &= (\\frac{4}{8} \\times 20) + (\\frac{4}{8} \\times -11.667) &&= 4.1667\n\\end{aligned}\n\\]\n4.1667 isn’t quite the true ATE of 5, but it’s surprisingly close! If we had more than 8 rows of data, we’d get even closer (as long as age was really truly the only confounder)."
  },
  {
    "objectID": "blog/2024/03/21/demystifying-ate-att-atu/index.html#moving-beyond-the-ate",
    "href": "blog/2024/03/21/demystifying-ate-att-atu/index.html#moving-beyond-the-ate",
    "title": "Demystifying causal inference estimands: ATE, ATT, and ATU",
    "section": "Moving beyond the ATE",
    "text": "Moving beyond the ATE\nWalking through a table like this is a useful exercise. It illustrates how individual causal effects are unobservable. It shows how we could theoretically find the average treatment effect of some intervention. It highlights the role selection bias and confounding play in distorting causal effects. It points to ways of accounting for confounding through statistical adjustment.\nBut for me personally, that’s about all the utility I’ve gotten out of tables like this. One aesthetic reason is that these numbers are all really generic—people get an outcome of 80 or 60 or whatever, but 80 or 60 what? It’s left up to the reader’s imagination.\nMore importantly, though, I’ve always figured that the ATT and ATU that you calculate from these kinds of tables were extra peripheral estimands that you found on your way to calculating the ATE, and that all you should really care about is the ATE.\nBut that’s wrong!\nSo to rectify this, let’s explore the nuances of different flavors of treatment effects using a more involved potential outcomes dataset that’s more realistic, based on a hypothetical international development intervention designed to reduce the risk of malaria through the use of anti-mosquito bed nets.5 The data is observational and non-experimental—people choose to use the nets or not based on personal preferences. In this case, income and health confound the net → malaria risk relationship: income influences health, and both income and health influence the choice to use a net and overall malaria risk.\n5 I used a similar dataset back in this blog post, and I use similar data in my class too; this version is new and improved though, with more explicit confounding and differences between the ATE, ATT, and ATU built in.\nCodemosquito_dag &lt;- dagitty('\ndag {\n\"X\" [exposure,pos=\"1,1\"]\n\"Y\" [outcome,pos=\"4,1\"]\n\"Z&lt;sub&gt;1&lt;/sub&gt;\" [pos=\"2,2\"]\n\"Z&lt;sub&gt;2&lt;/sub&gt;\" [pos=\"3,2\"]\n\"Z&lt;sub&gt;2&lt;/sub&gt;\" -&gt; \"Y\"\n\"Z&lt;sub&gt;2&lt;/sub&gt;\" -&gt; \"X\"\n\"Z&lt;sub&gt;1&lt;/sub&gt;\" -&gt; \"Y\"\n\"Z&lt;sub&gt;1&lt;/sub&gt;\" -&gt; \"Z&lt;sub&gt;2&lt;/sub&gt;\"\n\"Z&lt;sub&gt;1&lt;/sub&gt;\" -&gt; \"X\"\n\"X\" -&gt; \"Y\"\n}')\n\ndag_plot &lt;- mosquito_dag |&gt; \n  tidy_dagitty() |&gt; \n  mutate(var_type = case_when(\n    name == \"X\" ~ \"Exposure\",\n    name == \"Y\" ~ \"Outcome\",\n    str_detect(name, \"Z\") ~ \"Confounder\"\n  )) |&gt; \n  mutate(var_label = case_match(name,\n    \"X\" ~ \"Mosquito net\",\n    \"Y\" ~ \"Malaria risk\",\n    \"Z&lt;sub&gt;1&lt;/sub&gt;\" ~ \"Income\",\n    \"Z&lt;sub&gt;2&lt;/sub&gt;\" ~ \"Health\"\n  ))\n\nggplot(dag_plot, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = var_type), size = 12) +\n  geom_richtext(\n    aes(label = name), fontface = \"bold\", color = \"white\",\n    fill = NA, label.color = NA,\n    label.padding = grid::unit(c(3, 0, 0, 0), \"pt\")\n  ) + \n  geom_dag_text(\n    data = filter(dag_plot, var_type != \"Confounder\"), aes(label = var_label), \n    nudge_y = -0.15, color = \"black\", size = 11, size.unit = \"pt\"\n  ) +\n  geom_dag_text(\n    data = filter(dag_plot, var_type == \"Confounder\"), aes(label = var_label), \n    nudge_y = 0.16, color = \"black\", size = 11, size.unit = \"pt\"\n  ) +\n  scale_color_manual(values = clrs[c(4, 1, 6)], guide = \"none\") +\n  theme_dag()\n\n\n\n\n\n\nFigure 2: DAG showing the relationship between mosquito net use and malaria risk, confounded by health and income\n\n\n\n\nIn this dataset, the true ATE is −15. Using a bed net causes a 15-unit decrease in malaria risk across the whole population on average.\nIn the interest of space, I’ve hidden the data generation code below. You can also download a CSV version of it here if you want to follow along but not run all the code:\n\n mosquito_nets_v2.csv\n\n\nCode for generating synthetic mosquito net datawithr::with_seed(1234, {\n  n &lt;- 1500\n  \n  nets &lt;- tibble(\n    # Generate exogenous variables\n    id = 1:n,\n    income = rnorm(n, 500, 100)\n  ) |&gt; \n    # Generate health, which depends on income\n    mutate(\n      health = (0.1 * income) + rnorm(n, 0, 10),\n      health = pmin(pmax(health, 0), 100)  # Force values to 0-100 range\n    ) |&gt; \n    # Generate net usage (treatment), which depends on health and income\n    mutate(\n      # Make people with high health and high income a lot more likely to\n      # self-select into treatment\n      income_high = income &gt; quantile(income, 0.8),\n      health_high = health &gt; quantile(health, 0.8),\n      \n      # Create latent propensity scores based on logit formula\n      net_prob = plogis(\n        -3 + (income / 300) + (health / 40) + \n          (0.9 * income_high) + (0.5 * health_high)),\n      \n      # Assign to treatment\n      net = rbinom(n, 1, \n        # Fancy little trick---increase the latent propensity score by 5\n        # percentage points for people already more likely (i.e. p &gt; 50%) to\n        # receive treatment and decrease the propensity score by 8 percentage\n        # points for those already less likely to receive treatment\n        prob = ifelse(\n          net_prob &gt; 0.5, \n          pmin(net_prob + 0.05, 0.98), \n          pmax(net_prob - 0.08, 0.02))\n        )\n    ) |&gt; \n    # Generate potential and actual outcomes\n    mutate(\n      # Individual outcomes in a world where individuals are not treated\n      malaria_risk_0 = 90 + (-0.2 * health) + (-0.05 * income) + rnorm(n, 0, 6),\n      malaria_risk_0 = pmin(pmax(malaria_risk_0, 0), 100),  # Force values to 0-100\n      \n      # Individual outcomes in a world where individuals are treated\n      # Basically risk_0 + a baseline treatment effect + extra treatment effect\n      # boosts because of income and health\n      malaria_risk_1 = malaria_risk_0 - \n        rnorm(n, 3, 1) - (0.015 * income) - (0.09 * health),\n      malaria_risk_1 = pmin(pmax(malaria_risk_1, 0), 100),  # Force values to 0-100\n      \n      # Round stuff\n      malaria_risk_0 = round(malaria_risk_0, 1),\n      malaria_risk_1 = round(malaria_risk_1, 1),\n      \n      ice = malaria_risk_1 - malaria_risk_0,\n      \n      # Actual realized outcome\n      malaria_risk = ifelse(net == 1, malaria_risk_1, malaria_risk_0)\n    ) |&gt; \n    # Round more stuff\n    mutate(across(c(income, health), ~round(., 0))) |&gt; \n    # Only keep some columns\n    select(\n      id, income, income_high, health, health_high, net,\n      starts_with(\"malaria\"), ice\n    )\n})\n\nwrite_csv(nets, \"mosquito_nets_v2.csv\")\n\n\nWe have four main variables in this data:\n\n\nNet use (Treatment): Binary 0/1, TRUE/FALSE variable indicating if the person uses a bed net.\n\nMalaria risk (Outcome): Scale from 0–100, with higher values representing greater risk.\n\nIncome (Confounder): Weekly income, measured in dollars. Higher income causes a higher probability of using a net; people above the 80th percentile get an extra boost in probability.\n\nHealth (Confounder): Health status, scale from 0–100, with higher values representing better health. Better health causes a higher probability of using a net; people above the 80th percentile get an extra boost in probability.\n\nTable 3 shows a small excerpt of the data, with people using nets highlighted in yellow to mimic standard example potential outcome tables like Table 1.\n\nCodeexcerpt &lt;- nets |&gt; \n  slice(c(3, 14, 15, 29, 4, 35, 37, 73)) |&gt; \n  arrange(id)\n\nexcerpt |&gt; \n  add_row(id = NA) |&gt; \n  select(\n    id, income, health, net, \n    malaria_risk_1, malaria_risk_0, ice, malaria_risk\n  ) |&gt; \n  gt() |&gt; \n  sub_missing(missing_text = \"…\") |&gt;\n  fmt_number(\n    columns = c(starts_with(\"malaria_risk\"), ice),\n    decimals = 1\n  ) |&gt; \n  fmt_number(columns = c(income, health), decimals = 0) |&gt; \n  \n  # Column labels\n  cols_label(\n    id = \"ID\",\n    income = md(\"$Z_{1_i}$\"),\n    health = md(\"$Z_{2_i}$\"),\n    net = md(\"$X_i$\"),\n    malaria_risk_0 = md(\"$Y^0_i$\"),\n    malaria_risk_1 = md(\"$Y^1_i$\"),\n    malaria_risk = md(\"$Y_i$\"),\n    ice = md(\"$Y^1_i - Y^0_i$\")\n  ) |&gt;\n  \n  # Level 1 spanner labels\n  tab_spanner(\n    label = \"Income\", columns = income, \n    level = 1, id = \"level1_a\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Health\", columns = health, \n    level = 1, id = \"level1_b\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Net use\", columns = net, \n    level = 1, id = \"level1_c\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Potential outcomes\",\n    columns = c(malaria_risk_1, malaria_risk_0),\n    level = 1, id = \"level1_d\"\n  ) |&gt; \n  tab_spanner(\n    label = \"ICE or \\\\(\\\\delta_i\\\\)\", columns = ice, \n    level = 1, id = \"level1_e\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Outcome\", columns = malaria_risk, \n    level = 1, id = \"level1_f\"\n  ) |&gt; \n  \n  # Level 2 spanner labels\n  tab_spanner(\n    label = \"Confounders\",\n    columns = c(income, health),\n    level = 2, id = \"level2_a\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Treatment\", columns = net, \n    level = 2, id = \"level2_b\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Unobservable\",\n    columns = c(malaria_risk_1, malaria_risk_0, ice), \n    level = 2, id = \"level2_c\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Realized\", columns = malaria_risk, \n    level = 2, id = \"level2_d\") |&gt; \n  \n  # Style stuff\n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_column_labels()\n  ) |&gt; \n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_body()\n  ) |&gt; \n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = list(\n      cells_column_spanners(spanners = starts_with(\"level1\")),\n      cells_column_labels(columns = \"id\")\n    )\n  ) |&gt; \n  tab_style(\n    style = cell_text(style = \"italic\"),\n    locations = cells_column_spanners(spanners = starts_with(\"level2\"))\n  ) |&gt; \n  \n  tab_style(\n    style = list(\n      cell_fill(color = clrs[4], alpha = 0.5)\n    ),\n    locations = cells_body(rows = net == 1)\n  ) |&gt; \n  tab_footnote(\n    footnote = \"ICE = individual causal effect\",\n    locations = cells_column_spanners(spanners = \"level1_e\")\n  ) |&gt; \n  opt_footnote_marks(marks = \"standard\") |&gt; \n  opt_horizontal_padding(scale = 3) |&gt; \n  opt_table_font(font = \"Jost\")\n\n\nTable 3: Potential outcomes, individual causal effects, and realized outcomes for synthetic mosquito net data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfounders\nTreatment\nUnobservable\nRealized\n\n\nID\nIncome\nHealth\nNet use\nPotential outcomes\nICE or \\(\\delta_i\\)*\nOutcome\n\n\n\\(Z_{1_i}\\)\n\\(Z_{2_i}\\)\n\\(X_i\\)\n\\(Y^1_i\\)\n\\(Y^0_i\\)\n\\(Y^1_i - Y^0_i\\)\n\\(Y_i\\)\n\n\n\n\n3\n608\n54\n1\n34.9\n52.7\n−17.8\n34.9\n\n\n4\n265\n21\n0\n73.0\n82.8\n−9.8\n82.8\n\n\n14\n506\n58\n1\n22.5\n39.4\n−16.9\n22.5\n\n\n15\n596\n62\n1\n43.0\n59.0\n−16.0\n43.0\n\n\n29\n498\n61\n1\n40.0\n55.6\n−15.6\n40.0\n\n\n35\n337\n42\n0\n57.6\n68.0\n−10.4\n68.0\n\n\n37\n282\n28\n0\n50.5\n59.4\n−8.9\n59.4\n\n\n73\n463\n37\n0\n47.9\n59.4\n−11.5\n59.4\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\n* ICE = individual causal effect\n\n\n\n\n\n\n\n\nLet’s use this data to explore and calculate three different estimands from Greifer and Stuart (2023): the ATE, the ATT, and the ATU. For each estimand, we’ll do this:\n\nGive a plain translation of what the estimand measures and explanation about why we would care about it\nCalculate the estimand using the true-but-unobservable individual causal effects (ICE), or \\(\\delta\\)\n\nCalculate the estimand using propensity score-based weighting and g-computation"
  },
  {
    "objectID": "blog/2024/03/21/demystifying-ate-att-atu/index.html#plain-translations-and-true-values",
    "href": "blog/2024/03/21/demystifying-ate-att-atu/index.html#plain-translations-and-true-values",
    "title": "Demystifying causal inference estimands: ATE, ATT, and ATU",
    "section": "Plain translations and true values",
    "text": "Plain translations and true values\nHere’s what the synthetic nets data looks like—we have columns for each of the columns in Table 3, as well as some indicators for whether income and health are above the 80th percentile:\n\nnets |&gt; glimpse()\n## Rows: 1,500\n## Columns: 10\n## $ id             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, …\n## $ income         &lt;dbl&gt; 379, 528, 608, 265, 543, 551, 443, 445, 444, 411, 452, 400, 422, 506, 596, 489, 449, 409, 416, 742, 513, 451, 456, 546, 431, 355, 557, 398, 498, 406, 610, 452, 429, 450, 337, 383, 282, 366, 471, 453, 645, 393, 414, 472, 401, 403, 389, 375, 448, 450, 319, 442, 389, 399, 484, …\n## $ income_high    &lt;lgl&gt; FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n## $ health         &lt;dbl&gt; 56, 39, 54, 21, 51, 51, 49, 32, 39, 36, 41, 34, 54, 58, 62, 35, 74, 50, 54, 55, 49, 53, 49, 61, 49, 31, 53, 31, 61, 21, 58, 53, 41, 59, 42, 30, 28, 37, 50, 40, 58, 37, 51, 58, 34, 41, 29, 30, 39, 32, 12, 52, 43, 32, 58, 74, 41, 54, 78, 40, 51, 64, 41, 37, 29, 61, 48, 56, 79,…\n## $ health_high    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n## $ net            &lt;int&gt; 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,…\n## $ malaria_risk_0 &lt;dbl&gt; 50.1, 59.4, 52.7, 82.8, 42.5, 54.0, 57.4, 68.8, 61.1, 54.9, 54.9, 66.6, 52.8, 39.4, 59.0, 60.7, 57.4, 54.9, 56.4, 29.2, 64.5, 61.4, 51.5, 52.0, 62.1, 60.1, 45.7, 67.7, 55.6, 70.3, 46.3, 53.9, 66.3, 58.5, 68.0, 51.7, 59.4, 61.6, 55.4, 64.7, 47.3, 59.0, 53.0, 68.9, 68.6, 64.5,…\n## $ malaria_risk_1 &lt;dbl&gt; 35.1, 42.9, 34.9, 73.0, 25.7, 38.9, 43.5, 56.1, 48.0, 41.1, 40.2, 53.0, 37.4, 22.5, 43.0, 46.6, 42.8, 41.1, 41.9, 10.8, 47.9, 46.6, 37.6, 34.8, 47.8, 47.7, 28.6, 54.4, 40.0, 60.2, 27.4, 39.7, 52.8, 42.6, 57.6, 39.8, 50.5, 50.6, 38.3, 52.9, 30.3, 48.5, 38.1, 54.4, 56.7, 51.2,…\n## $ malaria_risk   &lt;dbl&gt; 50.1, 59.4, 34.9, 82.8, 42.5, 38.9, 43.5, 56.1, 48.0, 41.1, 54.9, 66.6, 52.8, 22.5, 43.0, 60.7, 42.8, 54.9, 41.9, 29.2, 64.5, 61.4, 37.6, 52.0, 47.8, 60.1, 45.7, 67.7, 40.0, 70.3, 46.3, 39.7, 66.3, 42.6, 68.0, 39.8, 59.4, 61.6, 38.3, 64.7, 47.3, 59.0, 38.1, 54.4, 68.6, 51.2,…\n## $ ice            &lt;dbl&gt; -15.0, -16.5, -17.8, -9.8, -16.8, -15.1, -13.9, -12.7, -13.1, -13.8, -14.7, -13.6, -15.4, -16.9, -16.0, -14.1, -14.6, -13.8, -14.5, -18.4, -16.6, -14.8, -13.9, -17.2, -14.3, -12.4, -17.1, -13.3, -15.6, -10.1, -18.9, -14.2, -13.5, -15.9, -10.4, -11.9, -8.9, -11.0, -17.1, -11.…\n\nATE\nThe average treatment effect (ATE) is what I’ve always assumed people always want to find when doing causal inference. It represents the effect of using bed nets across the whole population, even if people wouldn’t ordinarily receive the treatment. It’s a really broad estimand, so it’s good for policies with a wide scope that might influence an entire population, like if a government is considering rolling out free or subsidized bed nets to everyone in the country.\nWe calculate this by taking the average of all individual causal effects:\n\\[\n\\begin{aligned}\n\\text{ATE} &= E[\\delta_i] & \\text{ or} \\\\\n\\text{ATE} &= E[Y^1_i - Y^0_i]\n\\end{aligned}\n\\]\nConceptually this is the same as imagining two hypothetical worlds: in Universe A we give everyone a bed net (even if they don’t need it), and in Universe B we give nobody a bed net. We then calculate the difference in individual causal effects between those worlds.\nIf we look at just the eight rows of the bed net data in Table 3, the ATE would be −13.36:\n\\[\n\\frac{(-17.8) + (-9.8) + (-16.9) + (-16) + (-15.6) + (-10.4) + (-8.9) + (-11.5)}{8} = -13.36\n\\]\nWe can calculate it with the full data by taking the average of the ICE column. It’s −15.0, which is what I baked into the data:\n\nnets |&gt; \n  # I included the alternative versions here just to show that it's \n  # the average of Y1 - average of Y0\n  summarize(\n    ATE = mean(ice),\n    ATE_alt1 = mean(malaria_risk_1) - mean(malaria_risk_0),\n    ATE_alt2 = mean(malaria_risk_1 - malaria_risk_0)\n  )\n## # A tibble: 1 × 3\n##     ATE ATE_alt1 ATE_alt2\n##   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n## 1 -15.0    -15.0    -15.0\n\nATT\nThe average treatment effect on the treated (ATT) is narrower than the ATE, but (surprisingly!) is often more useful and policy relevant! It represents the effect of using bed nets, but only for people who use bed nets. That feels weird, so here are a few other examples:\n\nCodetribble(\n  ~ATE, ~ATT,\n  \"Effect of mosquito bed nets for everyone in the country\", \"Effect of mosquito bed nets for people who use bed nets\",\n  \"Effect of military service for typical applicants to the military\", \"Effect of military service for typical soldiers\",\n  \"Effect of a job training program on all residents in a state\", \"Effect of a job training program on everyone who used it\",\n  \"Effect of a new cancer medicine on everyone in the world\", \"Effect of a new cancer medicine on people with cancer\"\n) |&gt; \n  gt() |&gt; \n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) |&gt; \n  tab_style(\n    style = cell_text(v_align = \"top\"),\n    locations = cells_body()\n  ) |&gt; \n  tab_footnote(\n    footnote = html(\"Example via &lt;a href='https://stats.stackexchange.com/a/455012/3025'&gt;this Cross Validated response&lt;/a&gt;\"),\n    locations = cells_body(columns = ATE, rows = 2)\n  ) |&gt; \n  opt_footnote_marks(marks = \"standard\") |&gt; \n  opt_horizontal_padding(scale = 3) |&gt; \n  opt_table_font(font = \"Jost\")\n\n\nTable 4: Different types of causal effects that can be found as ATEs and ATTs\n\n\n\n\n\n\nATE\nATT\n\n\n\nEffect of mosquito bed nets for everyone in the country\nEffect of mosquito bed nets for people who use bed nets\n\n\nEffect of military service for typical applicants to the military*\n\nEffect of military service for typical soldiers\n\n\nEffect of a job training program on all residents in a state\nEffect of a job training program on everyone who used it\n\n\nEffect of a new cancer medicine on everyone in the world\nEffect of a new cancer medicine on people with cancer\n\n\n\n\n* Example via this Cross Validated response\n\n\n\n\n\n\n\n\n\nThe ATT feels a lot more useful for policy purposes. In medicine, it feels wrong to judge the effectiveness of a new drug based on the effect it would have on every person; in program evaluation, it feels wrong to judge the effectiveness of a training program on everyone, even if they wouldn’t ever use it.\nBoth the ATE and the ATT are useful estimands for policy!6\n6 See Ho et al. (2007, 204) and Greifer and Stuart (2023).Conceptually, we can imagine two parallel worlds again: in Universe A we give everyone who’s currently using a bed net a net, and in Universe B we take away those nets. We then calculate the difference between those worlds. The ATT thus represents the effect of withholding nets from those who would have used them (Greifer and Stuart 2023, 7).\nBUT(!!!), thinking about the ATT feels wrong and weird—it feels like we’re abandoning the whole idea of a control group and only looking at the treated group—which is why I’ve always been confused about this estimand.\nAnd I’m not alone! In forums and Q&A sites, people always wonder the same thing. Take this comment to this response on Cross Validated, for instance:\n\nI’m a statistician, and I never have understood why someone would want to measure the ATT instead of the ATE. It’s important to have a control group to account for regression to the mean effects & other factors - you’re missing this when you use the ATT\n\nIt really does feel like we’re only looking at treated people. Like this official formula here:\n\\[\n\\begin{aligned}\n\\text{ATT} &= E[\\delta_i \\mid X_i = 1] & \\text{or} \\\\\n\\text{ATT} &= E[Y^1_i - Y^0_i \\mid X_i = 1]\n\\end{aligned}\n\\]\nThat conditional \\([\\dots \\mid X = 1]\\) part says that we’re only looking at treated people. And when we calculate it with hypothetical data, we really truly only look at the treated people. If we only look at the eight rows in Table 3, the ATT would be −16.58:\n\\[\n\\frac{(-17.8) + (-16.9) + (-16) + (-15.6)}{4} = -16.58\n\\]\nWe can calculate it with the full data by taking the average of the ICE column after filtering the data so that we only look at people using a net. It’s −16.3, higher than the ATE (implying selection bias).\n\nnets |&gt; \n  filter(net == 1) |&gt; \n  summarize(ATT = mean(ice))\n## # A tibble: 1 × 1\n##     ATT\n##   &lt;dbl&gt;\n## 1 -16.3\n\nThis is why the ATT is weird for me (and lots of other people). We’re no longer finding the difference between treated and untreated people; we’re looking only at treated people and somehow finding a causal effect, which feels wrong.\nI actually think that Table 3 here is making things more confusing for thinking about the ATT with observed data! Any causal effect is the difference between two numbers. Because we have time machine-based data, we’re finding the difference between what would have happened in Universe A and Universe B, just for treated people. In real life, though, where we don’t have a time machine, we can’t find that difference. We still need to compare treated people with untreated people! We’ll just do some extra statistical work to make the untreated people more comparable to the treated people.\nSo, with hypothetical data we compare treated people to their parallel selves and we ignore the untreated; with real data we compare treated people to untreated people (at least, in part; as we’ll see below, we use information from the untreated people to estimate possible average causal effects for just the treated people).\nATU\nThe average treatment effect for the untreated (ATU) is just like the ATT, but in reverse. It represents the effect of using bed nets for people who aren’t using them.\nLet’s go back to the two parallel worlds: in Universe A we give everyone who’s not currently using a bed net a net, and in Universe B we take away those nets. We then calculate the difference between those worlds. The ATU represents the effect of expanding treatment to those who did not receive it (Greifer and Stuart 2023, 7).\nWe calculate this with hypothetical data by taking the average of the individual causal effects for untreated people:\n\\[\n\\begin{aligned}\n\\text{ATU} &= E[\\delta_i \\mid X_i = 0] & \\text{or} \\\\\n\\text{ATU} &= E[Y^1_i - Y^0_i \\mid X_i = 0]\n\\end{aligned}\n\\]\nIf we look at just Table 3, the ATU would be −10.15:\n\\[\n\\frac{(-9.8) + (-10.4) + (-8.9) + (-11.5)}{4} = -10.15\n\\]\nWe can calculate it with the full data by taking the average of the ICE column after filtering the data so that we only look at people not using a net. It’s −13.6, lower than the ATE (again, implying selection bias).\n\nnets |&gt; \n  filter(net == 0) |&gt; \n  summarize(ATU = mean(ice))\n## # A tibble: 1 × 1\n##     ATU\n##   &lt;dbl&gt;\n## 1 -13.6\n\nJust like the ATT, with hypothetical data we compare untreated people to their parallel selves and we ignore the treated. With real data, we’ll compare untreated people to treated people, with some statistical shenanigans to make the two groups comparable and remove confounding and selection bias."
  },
  {
    "objectID": "blog/2024/03/21/demystifying-ate-att-atu/index.html#selection-bias-and-the-ate-att-and-atu",
    "href": "blog/2024/03/21/demystifying-ate-att-atu/index.html#selection-bias-and-the-ate-att-and-atu",
    "title": "Demystifying causal inference estimands: ATE, ATT, and ATU",
    "section": "Selection bias and the ATE, ATT, and ATU",
    "text": "Selection bias and the ATE, ATT, and ATU\nBefore we calculate these different treatment effects with the realized outcomes instead of the hypothetical potential outcomes, let’s look really quick at the practical difference between the true ATE, ATT, and ATU. All three estimands are useful for policymaking!\nThe ATE is −15, implying that mosquito nets cause a 15 point reduction in malaria risk for every person in the country. This includes people who live at high elevations where mosquitoes don’t live, people who live near mosquito-infested swamps, people who are rich enough to buy Bill Gates’s mosquito laser, and people who can’t afford a net but would really like to use one. If we worked in the Ministry of Health and wanted to know if we should make a new national program that gave everyone a free bed net, the overall reduction in risk is −15, which is probably pretty good!\nThe ATT is −16.29, which is bigger than the ATE. The effect of net usage is bigger for people who are already using the nets. This is because of underlying systematic reasons, or selection bias. Those using nets want to use them because they need them more or can access them more easily—they might live in areas more prone to mosquitoes, or they can afford to buy their own nets, or something else. They know themselves and understand some notion of their personal individual causal effect and seek out nets. If we removed access to their nets, it would have a strong effect.\nThe ATU is −13.63, which is smaller than the ATE. The effect of net usage is smaller for people who aren’t using the nets. Again, this is because of selection bias. Those not using nets are likely not using them for systematic reasons—they live far away from mosquitoes, they’ve received a future malaria vaccine, they have some other form of mosquito abatement, or something else. Because they can read their own minds, they know that mosquito net use won’t do much for them personally, so they don’t seek out nets. If we expanded access to nets to them, they wouldn’t benefit as much.\nAnd one final note about the ATE—as we saw before, it includes both the ATT and the ATU. Because it’s the average for everyone in the whole population, it’s the sum of the weighted averages of these two estimands:\n\\[\n\\text{ATE} = (\\pi_\\text{Net users} \\times \\text{ATT}) + (\\pi_\\text{Net non-users} \\times \\text{ATU})\n\\]\nWe can confirm this with the data:\n\neffect_types &lt;- nets |&gt; \n  group_by(net) |&gt; \n  summarize(\n    effect = mean(ice),\n    n = n()\n  ) |&gt; \n  mutate(\n    prop = n / sum(n),\n    weighted_effect = effect * prop\n  ) |&gt; \n  mutate(estimand = case_match(net, 0 ~ \"ATU\", 1 ~ \"ATT\"), .before = 1)\neffect_types\n## # A tibble: 2 × 6\n##   estimand   net effect     n  prop weighted_effect\n##   &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;           &lt;dbl&gt;\n## 1 ATU          0  -13.6   715 0.477           -6.50\n## 2 ATT          1  -16.3   785 0.523           -8.52\n\neffect_types |&gt; \n  summarize(ATE = sum(weighted_effect))\n## # A tibble: 1 × 1\n##     ATE\n##   &lt;dbl&gt;\n## 1 -15.0"
  },
  {
    "objectID": "blog/2024/03/21/demystifying-ate-att-atu/index.html#finding-these-estimands-with-non-potential-outcome-based-data",
    "href": "blog/2024/03/21/demystifying-ate-att-atu/index.html#finding-these-estimands-with-non-potential-outcome-based-data",
    "title": "Demystifying causal inference estimands: ATE, ATT, and ATU",
    "section": "Finding these estimands with non-potential-outcome-based data",
    "text": "Finding these estimands with non-potential-outcome-based data\nSo far we’ve calculated the ATE, ATT, and ATU with data based on a time machine. In real life, we can only see this:\n\nCodeexcerpt |&gt; \n  add_row(id = NA) |&gt; \n  select(\n    id, income, health, net, malaria_risk\n  ) |&gt; \n  gt() |&gt; \n  sub_missing(missing_text = \"…\") |&gt;\n  fmt_number(\n    columns = malaria_risk,\n    decimals = 1\n  ) |&gt; \n  fmt_number(columns = c(income, health), decimals = 0) |&gt; \n  \n  # Column labels\n  cols_label(\n    id = \"ID\",\n    income = md(\"$Z_{1_i}$\"),\n    health = md(\"$Z_{2_i}$\"),\n    net = md(\"$X_i$\"),\n    malaria_risk = md(\"$Y_i$\")\n  ) |&gt;\n  \n  # Level 1 spanner labels\n  tab_spanner(\n    label = \"Income\", columns = income, \n    level = 1, id = \"level1_a_obs\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Health\", columns = health, \n    level = 1, id = \"level1_b_obs\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Net use\", columns = net, \n    level = 1, id = \"level1_c_obs\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Outcome\", columns = malaria_risk, \n    level = 1, id = \"level1_d_obs\"\n  ) |&gt; \n  \n  # Level 2 spanner labels\n  tab_spanner(\n    label = \"Confounders\",\n    columns = c(income, health),\n    level = 2, id = \"level2_a_obs\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Treatment\", columns = net, \n    level = 2, id = \"level2_b_obs\"\n  ) |&gt; \n  tab_spanner(\n    label = \"Realized\", columns = malaria_risk, \n    level = 2, id = \"level2_c_obs\") |&gt; \n  \n  # Style stuff\n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_column_labels()\n  ) |&gt; \n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_body()\n  ) |&gt; \n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = list(\n      cells_column_spanners(spanners = starts_with(\"level1\")),\n      cells_column_labels(columns = \"id\")\n    )\n  ) |&gt; \n  tab_style(\n    style = cell_text(style = \"italic\"),\n    locations = cells_column_spanners(spanners = starts_with(\"level2\"))\n  ) |&gt; \n  \n  tab_style(\n    style = list(\n      cell_fill(color = clrs[4], alpha = 0.5)\n    ),\n    locations = cells_body(rows = net == 1)\n  ) |&gt; \n  opt_horizontal_padding(scale = 3) |&gt; \n  opt_table_font(font = \"Jost\")\n\n\nTable 5: Observed version of Table 3 showing only the realized outcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfounders\nTreatment\nRealized\n\n\nID\nIncome\nHealth\nNet use\nOutcome\n\n\n\\(Z_{1_i}\\)\n\\(Z_{2_i}\\)\n\\(X_i\\)\n\\(Y_i\\)\n\n\n\n\n3\n608\n54\n1\n34.9\n\n\n4\n265\n21\n0\n82.8\n\n\n14\n506\n58\n1\n22.5\n\n\n15\n596\n62\n1\n43.0\n\n\n29\n498\n61\n1\n40.0\n\n\n35\n337\n42\n0\n68.0\n\n\n37\n282\n28\n0\n59.4\n\n\n73\n463\n37\n0\n59.4\n\n\n…\n…\n…\n…\n…\n\n\n\n\n\n\n\n\n\n\\(\\delta_i\\) is gone; we only have \\(Y_i\\). Instead of comparing and finding the differences between individuals and their parallel selves, we need to find the differences between treated and untreated people while removing the selection bias and confounding caused by income and health.\nIn the initial example in Table 1, there was just one confounder (old vs. young), and we adjusted for it by stratifying by age, combining the weighted averages for old and young people. Basic stratification like that is a lot harder to do here. Both income and health are continuous—we can’t just find weighted averages of rich vs. poor and sick vs. healthy people. We’ve got to do something fancier to make comparable comparison groups, like matching or weighting.\nImportantly, we have to match or weight in a way that is appropriate to the estimand we care about. The matching procedure that we follow for finding the ATE can’t be used for finding the ATT; the weighting process that we use for the ATU is different from what we use for the ATE. This is the main argument in Greifer and Stuart (2023)—the choice of estimand determines the process of statistical adjustment.\nNoah Greifer has two incredible parallel guides to doing this adjustment with both weighting with {WeightIt} and matching with {MatchIt}. We’ll illustrate this with propensity score weighting, but the same principles apply to matching (and the code is almost identical!)\nCovering the exact mechanics of probability weighting goes beyond the scope of this blog post! Check out these resources to learn more:\n\nChapter 12 in Hernán and Robins (2024)\n\nSection 10.7 in Heiss (2021)\n\nLucy D’Agostino McGowan’s “Understanding propensity score weighting”\n\nNoah Greifer’s {WeightIt} vignette, “Estimating Effects After Weighting”\n\nMy guide to matching and inverse probability weighting from my causal inference class and the corresponding lecture video\n\n\nIn general, with weighting we use confounders to predict the probability of choosing treatment, and then use these propensity scores to reweight the treated and untreated groups so that they resemble each other, thus removing any influence that the confounders have on the choice to select into treatment. My mental model of weighting is that we create pseudo-populations of treated and untreated people that feel like what would happen if we had randomly assigned everyone to treatment or control conditions—it’s like creating fake treatment and control groups.\nATE\nTo find the ATE, we need to adjust both the treated and untreated groups so that they are comparable. The most common way to do this is to create inverse probability weights, where all treated people get a weight of \\(\\frac{1}{p_i}\\) and all untreated people get a weight of \\(\\frac{1}{1 - p_i}\\) (where \\(p_i\\) refers to each person’s propensity scores).\nPractically speaking, this gives more weight to more unusual and unexpected observations. For instance, someone with a high predicted probability (90%) of choosing to use a net who then uses a net isn’t really notable and will have a low weight (\\(\\frac{1}{0.9}\\), or 1.111). Someone else with a high probability (90% again) of choosing a net who doesn’t use a net will have a high weight (\\(\\frac{1}{1 - 0.9}\\), or 10). This makes it so that the should-have-probably-used-a-net person is more comparable to the corresponding untreated people.\nVisualization helps a lot with this intuition!\nFirst, we’ll use logistic regression to calculate the predicted probabilities of using a net. We’ll then create inverse probability weights with this formula, which uses the fact that net is stored as 0 or 1 to assign the correct treated vs. untreated weight:\n\\[\n\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}\n\\]\n\n# Logistic regression model to predict net usage\nmodel_treatment &lt;- glm(\n  net ~ income + health, \n  data = nets, \n  family = binomial(link = \"logit\")\n)\n\n# Plug original data into the model to generate predicted probabilities\n#\n# (Here I'm using broom::augment_columns(), but you can also use \n#  marginaleffects::predictions() or even base R's predict())\n# \n# Create a new column for the weights\nnets_with_weights &lt;- augment_columns(\n  model_treatment,\n  data = nets,\n  type.predict = \"response\"\n) |&gt; \n  rename(propensity = .fitted) |&gt; \n  mutate(wts_ate = (net / propensity) + ((1 - net) / (1 - propensity)))\n\n# See if it worked\nnets_with_weights |&gt; \n  select(id, income, health, net, malaria_risk, propensity, wts_ate)\n## # A tibble: 1,500 × 7\n##       id income health   net malaria_risk propensity wts_ate\n##    &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n##  1     1    379     56     0         50.1     0.393     1.65\n##  2     2    528     39     0         59.4     0.427     1.75\n##  3     3    608     54     1         34.9     0.760     1.32\n##  4     4    265     21     0         82.8     0.0368    1.04\n##  5     5    543     51     0         42.5     0.622     2.65\n##  6     6    551     51     1         38.9     0.636     1.57\n##  7     7    443     49     1         43.5     0.412     2.43\n##  8     8    445     32     1         56.1     0.213     4.69\n##  9     9    444     39     1         48       0.286     3.50\n## 10    10    411     36     1         41.1     0.209     4.78\n## # ℹ 1,490 more rows\n\nAlternatively, we can use the {WeightIt} package to do the same thing:\n\n# Get ATE-focused weights for the treatment model\nweights_ate &lt;- weightit(\n  net ~ income + health, \n  data = nets, \n  method = \"glm\", \n  estimand = \"ATE\"\n)\n\n# Add the weights as a column in the data\nnets_with_weights$wts_ate_automatic &lt;- weights_ate$weights\n\n# They're the same!\nnets_with_weights |&gt; \n  select(id, wts_ate, wts_ate_automatic) |&gt; \n  head()\n## # A tibble: 6 × 3\n##      id wts_ate wts_ate_automatic\n##   &lt;int&gt;   &lt;dbl&gt;             &lt;dbl&gt;\n## 1     1    1.65              1.65\n## 2     2    1.75              1.75\n## 3     3    1.32              1.32\n## 4     4    1.04              1.04\n## 5     5    2.65              2.65\n## 6     6    1.57              1.57\n\nLet’s copy Lucy D’Agostino McGowan’s approach and visualize the distribution of these propensity scores across treatment status:\n\nCodeplot_data_weights_ate &lt;- tibble(\n  propensity = weights_ate$ps,\n  weight = weights_ate$weights,\n  treatment = weights_ate$treat\n)\n\nggplot() + \n  geom_histogram(data = filter(plot_data_weights_ate, treatment == 1), \n    bins = 50, aes(x = propensity, fill = \"Treated people\")) + \n  geom_histogram(data = filter(plot_data_weights_ate, treatment == 0), \n    bins = 50, aes(x = propensity, y = -after_stat(count), fill = \"Untreated people\")) +\n  geom_hline(yintercept = 0, color = \"white\", linewidth = 0.25) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  scale_y_continuous(label = abs) +\n  scale_fill_manual(values = c(clrs[1], clrs[6]), guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"Propensity\", y = \"Count\", fill = NULL) +\n  theme(\n    legend.position = \"top\",\n    legend.key.size = unit(0.65, \"lines\")\n  )\n\n\n\n\n\n\nFigure 3: Mirrored histogram showing the distribution of propensity scores for treated and untreated people\n\n\n\n\nIn general, the people who used nets had a higher probability of using them, while the people who didn’t use nets had a lower probability of using them. That’s to be expected.\nBut there are some people who had a low probability of using nets who did use them, and some people who had a high probability of using nets who did not use them. That’s curious and weird. These people are arguably very similar (i.e. with similar income and health), but for whatever reason, they didn’t do what the model predicted they should do.\n\nCodeggplot() + \n  geom_histogram(data = filter(plot_data_weights_ate, treatment == 1), \n    bins = 50, aes(x = propensity, fill = \"Treated people\")) + \n  geom_histogram(data = filter(plot_data_weights_ate, treatment == 0), \n    bins = 50, aes(x = propensity, y = -after_stat(count), fill = \"Untreated people\")) +\n  annotate(\n    geom = \"rect\", xmin = 0, xmax = 0.55, ymin = -1, ymax = 23,\n    fill = alpha(clrs[4], 0.1), color = clrs[4], linewidth = 1\n  ) +\n  annotate(\n    geom = \"text\", x = 0.01, y = 21.5, \n    label = \"Treated people who were\\nunlikely to be treated.\\nWeird!\",\n    color = clrs[3], fontface = \"bold\", hjust = 0, vjust = 1, lineheight = 1\n  ) +\n  annotate(\n    geom = \"rect\", xmin = 0.58, xmax = 1, ymin = 1, ymax = -27.5,\n    fill = alpha(clrs[4], 0.1), color = clrs[4], linewidth = 1\n  ) +\n  annotate(\n    geom = \"text\", x = 0.99, y = -26, \n    label = \"Weird!\\nUntreated people who\\nwere likely to be treated.\",\n    color = clrs[3], fontface = \"bold\", hjust = 1, vjust = 0, lineheight = 1\n  ) +\n  geom_hline(yintercept = 0, color = \"white\", linewidth = 0.25) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  scale_y_continuous(label = abs) +\n  scale_fill_manual(values = c(clrs[1], clrs[6]), guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"Propensity\", y = \"Count\", fill = NULL) +\n  theme(\n    legend.position = \"top\",\n    legend.key.size = unit(0.65, \"lines\")\n  )\n\n\n\n\n\n\nFigure 4: Mirrored histogram showing “weird” parts of the population: treated people who were unlikely to be treated, and untreated people who were likely to be treated\n\n\n\n\nIf we scale everyone’s importance according to their inverse probability weights, we can give more importance to these “weird” untreated-but-should-have-been-treated and treated-but-shouldn’t-have-been-treated people, making them more balanced compared to their treated-and-should-have-been-treated and untreated-and-should-have-been-untreated counterparts. The fainter histogram here represents the adjusted and reweighted pseudo-populations of net users and non-net users. These two pseudo-populations are now free of the confounding coming from health and income and are arguably more comparable, acting more like randomized treatment and control groups.\n\nCodeggplot() + \n  geom_histogram(data = filter(plot_data_weights_ate, treatment == 1), \n    bins = 50, aes(x = propensity, weight = weight, fill = \"Treated pseudo-population\")) + \n  geom_histogram(data = filter(plot_data_weights_ate, treatment == 0), \n    bins = 50, aes(x = propensity, weight = weight, y = -after_stat(count), fill = \"Untreated psuedo-population\")) +\n  geom_histogram(data = filter(plot_data_weights_ate, treatment == 1), \n    bins = 50, aes(x = propensity, fill = \"Treated people\")) + \n  geom_histogram(data = filter(plot_data_weights_ate, treatment == 0), \n    bins = 50, aes(x = propensity, y = -after_stat(count), fill = \"Untreated people\")) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 60, label = \"More weight here\", \n    hjust = 0.5, fontface = \"bold\", color = clrs[3]\n  ) +\n  annotate(\n    geom = \"segment\", x = 0.48, xend = 0.17, y = 55, yend = 25, color = \"white\", \n    arrow = arrow(angle = 15, length = unit(0.5, \"lines\")), linewidth = 2.5\n  ) +\n  annotate(\n    geom = \"segment\", x = 0.48, xend = 0.17, y = 55, yend = 25, color = clrs[3], \n    arrow = arrow(angle = 15, length = unit(0.5, \"lines\")), linewidth = 0.5\n  ) +\n  annotate(\n    geom = \"segment\", x = 0.52, xend = 0.8, y = 55, yend = -20, color = \"white\", \n    arrow = arrow(angle = 15, length = unit(0.5, \"lines\")), linewidth = 2.5\n  ) +\n  annotate(\n    geom = \"segment\", x = 0.52, xend = 0.8, y = 55, yend = -20, color = clrs[3], \n    arrow = arrow(angle = 15, length = unit(0.5, \"lines\")), linewidth = 0.5\n  ) +\n  geom_hline(yintercept = 0, color = \"white\", linewidth = 0.25) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  scale_y_continuous(label = abs) +\n  scale_fill_manual(\n    values = c(clrs[1], colorspace::lighten(clrs[1], 0.5), clrs[6], colorspace::lighten(clrs[6], 0.65)), \n    guide = guide_legend(reverse = FALSE, nrow = 2)) +\n  labs(x = \"Propensity\", y = \"Count\", fill = NULL) +\n  theme(\n    legend.position = \"top\",\n    legend.key.size = unit(0.65, \"lines\")\n  )\n\n\n\n\n\n\nFigure 5: Mirrored histogram showing pseudo-populations of treated and untreated people that have been reweighted to be more comparable and unconfounded\n\n\n\n\nTo find the ATE using these weights, we can fit a regression model for the outcome. We’ll then use an approach called g-computation to find the marginal causal effect (i.e. the causal effect of flipping net from “no” to “yes”). G-computation is a neat approach that feels more like the original potential outcomes framework we looked at earlier. It involves a few steps:\n\nFit a regression model predicting the outcome, like lm(malaria_risk ~ net, weights = w_ate, ...)\n\nUse the model to generate a set of predictions where we pretend that every person used a net (i.e. set net = 1)\nUse the model to generate a set of predictions where we pretend that every person did not use a net (i.e. set net = 0)\nCalculate the difference in the two sets of predictions to create a sort of predicted individual causal effect, then find the average of that. This is the ATE.\n\nThat looks really complicated and involved and feels like a lot of coding work, but {marignaleffects} makes this incredibly easy (there’s even a chapter dedicated to it in the documentation). Basically, all we have to do is use avg_comparisons(model_outcome, variables = list(net = 0:1)), or even more simply avg_comparisons(model_outcome, variables = \"net\"). This will do all the work of setting everyone’s treatment to 0, to 1, and finding the difference. We can do fancier things with it too, like finding robust and/or clustered standard errors, or bootstrapping the standard errors.\n\n# Fit an outcome model using the inverse probability weights\nmodel_outcome_ate &lt;- lm(\n  malaria_risk ~ net, \n  data = nets_with_weights, \n  weights = wts_ate_automatic\n)\n\n# Automatic g-computation with {marginaleffects}! This sets the \"net\" column to\n# each possible value of net (0 and 1) for the whole dataset, then calculates\n# the difference between the two sets of predictions\navg_comparisons(model_outcome_ate, variables = \"net\")\n## \n##  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n##   net    1 - 0    -14.7      0.559 -26.2   &lt;0.001 500.6 -15.8  -13.6\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n## Type:  response\n\nBased on our inverse probability weighting approach, the ATE from observational data (i.e. not using the unknowable individual potential outcomes) is −14.7, which is super close to the true ATE of −15.0!\nATT\nThat ATE shows the effect of the net program for everyone, even people who have no need for a net. If we’re interested in making this a universal program, the ATE is useful. If we’re interested in what the program is currently doing for people using it, or what would happen if we expanded it to just those not using it, we’d need to find the ATT or ATU instead.\nWe can do this with matching and weighting, but we cannot use the same matching and weighting that we used to find the ATE. When reweighting the data for estimating the ATT, we should not do anything that messes with the weights of the treated group. Since we’re interested in the effect of the program on just the treated people, we need to create a pseudo-control group that looks like the treated group. There are lots of different ways to do this (and Greifer and Stuart (2023) cover them). One common way is to give a weight of 1 to all the treated people and a weight of \\(\\frac{p_i}{1 - p_i}\\) for all the untreated people.\nThe process is the same—fit a model that predicts if people use a net, use those predictions to generate weights, then fit an outcome model using those weights. With the ATT-specific weights, we’ll create an untreated pseudo-population that is statistically comparable (and unconfounded) in relation to the actually treated population.\n\n# Get ATT-focused weights for the treatment model\nweights_att &lt;- weightit(\n  net ~ income + health, \n  data = nets, \n  method = \"glm\", \n  estimand = \"ATT\"\n)\n\n# Add the weights as a column in the data\nnets_with_weights$wts_att &lt;- weights_att$weights\n\nOur fancy weighted histogram looks like this:\n\nCodeplot_data_weights_att &lt;- tibble(\n  propensity = weights_att$ps,\n  weight = weights_att$weights,\n  treatment = weights_att$treat\n)\n\nggplot() + \n  geom_histogram(data = filter(plot_data_weights_att, treatment == 0), \n    bins = 50, aes(x = propensity, weight = weight, y = -after_stat(count), fill = \"Untreated psuedo-population\")) +\n  geom_histogram(data = filter(plot_data_weights_att, treatment == 1), \n    bins = 50, aes(x = propensity, fill = \"Treated people\")) + \n  geom_histogram(data = filter(plot_data_weights_att, treatment == 0), \n    bins = 50, alpha = 0.5, \n    aes(x = propensity, y = -after_stat(count), fill = \"Untreated people\")) +\n  geom_hline(yintercept = 0, color = \"white\", linewidth = 0.25) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  scale_y_continuous(label = abs) +\n  scale_fill_manual(\n    values = c(clrs[1], \"grey50\", colorspace::lighten(clrs[6], 0.65)), \n    guide = guide_legend(reverse = FALSE, nrow = 2)) +\n  labs(x = \"Propensity\", y = \"Count\", fill = NULL) +\n  theme(\n    legend.position = \"top\",\n    legend.key.size = unit(0.65, \"lines\")\n  )\n\n\n\n\n\n\nFigure 6: Mirrored histogram comparing the distribution of treated people with the reweighted pseudo-population of untreated people; because this is for the ATT, treated people were not reweighted\n\n\n\n\nThe treated people are untouched—they all have a weight of 1, and we don’t mess with them. Untreated people with high probability of using a net are given more weight to make them look more like the treated people.\nFitting the outcome model is the same as finding the ATE—we’ll just use the special ATT weights:\n\n# Fit an outcome model using the ATT weights\nmodel_outcome_att &lt;- lm(\n  malaria_risk ~ net, \n  data = nets_with_weights, \n  weights = wts_att\n)\n\nThe main difference is that when we do the g-computation, we only look at treated people. Using only this part of the population, we’ll set all their values to 1, then set all their values to 0, and then find the difference.\nThis is the key to finding the effect of the program only in the treated part of the population! Remember how when we had access to everyone’s individual causal effects, we were able to find the ATT by looking at the average of all the treated peoples’ \\(\\delta\\) column? G-computation lets us essentially recreate that process. Here’s how it works:\n\n# Do g-computation *only* on treated observations\navg_comparisons(\n  model_outcome_att, \n  variables = \"net\", \n  newdata = filter(nets, net == 1)\n)\n## \n##  Term Contrast Estimate Std. Error     z Pr(&gt;|z|)     S 2.5 % 97.5 %\n##   net    1 - 0    -17.6      0.497 -35.4   &lt;0.001 907.4 -18.5  -16.6\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n## Type:  response\n\nThe estimated observational ATT is −17.6, which is (1) bigger than the ATE, as expected, and (2) close-ish to the true ATT of −16.3!\nATU\nIf we’re interested in what would happen if we expanded the program to people not using it, we can find the ATU. The process is the same as the ATT, just in reverse. We’ll create a pseudo-population of net users by giving a weight of 1 to all untreated people and giving a weight of \\(\\frac{1-p_i}{p_i}\\) to all treated people.\n\n# Get ATU-focused weights for the treatment model\nweights_atu &lt;- weightit(\n  net ~ income + health, \n  data = nets, \n  method = \"glm\", \n  estimand = \"ATC\"  # WeightIt calls this ATC instead of ATU\n)\n\n# Add the weights as a column in the data\nnets_with_weights$wts_atu &lt;- weights_atu$weights\n\nHere’s what that looks like as a weighted histogram:\n\nCodeplot_data_weights_atu &lt;- tibble(\n  propensity = weights_atu$ps,\n  weight = weights_atu$weights,\n  treatment = weights_atu$treat\n)\n\nggplot() + \n  geom_histogram(data = filter(plot_data_weights_atu, treatment == 1), \n    bins = 50, aes(x = propensity, weight = weight, fill = \"Treated pseudo-population\")) + \n  geom_histogram(data = filter(plot_data_weights_atu, treatment == 1), \n    bins = 50, alpha = 0.5, aes(x = propensity, fill = \"Treated people\")) + \n  geom_histogram(data = filter(plot_data_weights_atu, treatment == 0), \n    bins = 50, aes(x = propensity, y = -after_stat(count), fill = \"Untreated people\")) +\n  geom_hline(yintercept = 0, color = \"white\", linewidth = 0.25) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  scale_y_continuous(label = abs) +\n  scale_fill_manual(\n    values = c(\"grey50\", colorspace::lighten(clrs[1], 0.5), clrs[6], colorspace::lighten(clrs[6], 0.65)), \n    guide = guide_legend(reverse = FALSE, nrow = 2)) +\n  labs(x = \"Propensity\", y = \"Count\", fill = NULL) +\n  theme(\n    legend.position = \"top\",\n    legend.key.size = unit(0.65, \"lines\")\n  )\n\n\n\n\n\n\nFigure 7: Mirrored histogram comparing the distribution of untreated people with the reweighted pseudo-population of treated people; because this is for the ATU, untreated people were not reweighted\n\n\n\n\nThe low-probability net users get a lot more weight so that they’re comparable to the untreated group.\nThe outcome model process is the same as before—we just use the ATU-specific weights. When doing the g-computation, we only use untreated people.\n\n# Fit an outcome model using the ATU weights\nmodel_outcome_atu &lt;- lm(\n  malaria_risk ~ net, \n  data = nets_with_weights, \n  weights = wts_atu\n)\n\n# Do g-computation *only* on untreated observations\navg_comparisons(\n  model_outcome_atu, \n  variables = \"net\", \n  newdata = filter(nets, net == 0)\n)\n## \n##  Term Contrast Estimate Std. Error   z Pr(&gt;|z|)     S 2.5 % 97.5 %\n##   net    1 - 0    -11.7      0.511 -23   &lt;0.001 385.0 -12.7  -10.7\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n## Type:  response\n\nThe estimated observational ATU is −11.7, which is (1) smaller than the ATE, as expected, and (2) close-ish to the true ATU of −13.6!\nSummary\nFinally, let’s show these estimands all together, with some basic interpretation of what they all mean:\n\n\n\nTable 6: Summary of the three main estimands\n\n\n\n\n\n\nEstimand\nTrue value\nEstimated value\nInterpretation\nWhy care?\n\n\n\nATE\n−15.0\n−14.7\nUsing a mosquito net reduces the risk of malaria by 14.7 points, on average across all people in the country\nHelpful for understanding the effect of creating a widely applied policy or program, like rolling out subsidized mosquito nets for everyone in the country\n\n\nATT\n−16.3\n−17.6\nPeople who currently use mosquito nets see a malaria risk reduction of 17.6 points, on average\nHelpful for understanding the effect of net usage on people who actually use them; this shows what would happen if we withheld the program or took away everyone's nets\n\n\nATU\n−13.6\n−11.7\nPeople who don't currently use mosquito nets would see a malaria risk reduction of 11.7 points, on average\nHelpful for understanding the effect of net usage on people who don't use them right now; this shows what would happen if we expanded the program or gave nets to people without them\n\n\n\nHere, the fact that ATT &gt; ATE &gt; ATU implies selection bias; the program works more effectively among those who use it because they know it will help them"
  },
  {
    "objectID": "blog/2024/03/21/demystifying-ate-att-atu/index.html#what-about-other-methods",
    "href": "blog/2024/03/21/demystifying-ate-att-atu/index.html#what-about-other-methods",
    "title": "Demystifying causal inference estimands: ATE, ATT, and ATU",
    "section": "What about other methods?",
    "text": "What about other methods?\nAwesome. For me, at least, this all makes a lot more sense. The ATE is not the final goal of all causal inference methods. The ATT is also a completely good and valid and policy relevant estimand—often more important than the ATE. I’ve long been confused about what the ATT actually means, since it feels so weird—particularly in standard textbook potential outcomes tables. Finding the treatment effect just for the treated feels like we’re abandoning the idea of a control or comparison group, but we’re not! We’re using information from untreated people to achieve balance, then applying that information to only the treated people through g-computation to approximate unobservable individual-level causal effects. We can thus find more specific causal effects using observational, non-experimental data.\nWhile working through all this, I discovered that I’ve actually been teaching methods that estimate the ATT without even knowing it. One super common method in econometrics and social science in general7 is difference-in-differences. The causal effect you find from diff-in-diff research designs is the ATT, or the effect of an intervention on only the treated people. Diff-in-diff doesn’t provide a population-level ATE.\n7 Though not in epidemiology, which is odd since an epidemiologist essentially invented it (Caniglia and Murray 2020)!Other quasi-experimental methods like regression discontinuity and instrumental variables provide much more limited causal effects—they don’t show the ATE, or the ATT, or the ATU. Instead, they provide a local average treatment effect (LATE), or a causal effect for a much narrower segment of the population. For regression discontinuity, we find the ATE for people close to an arbitrary threshold that causes a cutoff in treatment status; for instrumental variables we find the ATE for people who comply with the program assignment."
  },
  {
    "objectID": "blog/2023/12/11/separate-bibliographies-quarto/index.html",
    "href": "blog/2023/12/11/separate-bibliographies-quarto/index.html",
    "title": "How to create separate bibliographies in a Quarto document",
    "section": "",
    "text": "tl;dr\n\n\n\nIf you want to skip the explanation and justification for why you might want separate bibliographies, you can skip down to the example section, or just go see some example files at GitHub."
  },
  {
    "objectID": "blog/2023/12/11/separate-bibliographies-quarto/index.html#why-use-separate-bibliographies",
    "href": "blog/2023/12/11/separate-bibliographies-quarto/index.html#why-use-separate-bibliographies",
    "title": "How to create separate bibliographies in a Quarto document",
    "section": "Why use separate bibliographies?",
    "text": "Why use separate bibliographies?\nIn academic articles, it’s common to have a supplemental appendix with extra tables, figures, robustness checks, additional math, proofs, and other details. Putting content in the appendix is important for providing additional evidence for the paper’s argument—and for placating reviewers who want to see a dozen more robustness checks. Also, journal articles have word count limits, and sticking stuff in the appendix is a helpful way to circumvent those limits, since the appendix doesn’t get typeset with the rest of the article and is instead posted to the journal’s website as a supplemental file.\nFor years, my approach to making an appendix has been to use two separate Markdown files (either plain Markdown, R Markdown, or Quarto Markdown):\n\nmanuscript.md: The actual paper\nappendix.md: The appendix\n\nThat’s nice and straightforward and easy. I get two separate documents in the end, like this project:\n\n\n\nMultiple outputs for separate documents\n\n\nUsing separate files also lets me find the word count for the manuscript without needing to account for the appendix (like with my Quarto word count extension).\nThe biggest downside to this system, though, is that it’s impossible to reference the appendix from the manuscript. A magical feature of pandoc (and Quarto in particular) is its ability to cross reference tables, figures, equations, sections, and other content in your document. I can write something like this:\nSee @fig-results and @tbl-details for the results.\n…which turns into this when rendering the document:\n\nSee Figure 1 and Table 1 for the results.\n\nThose numbers are automatically incremented, so it might say “Figure 4” if you have images earlier in the document. The numbers can have hyperlinks added to them so you can jump to the corresponding figure or table, and if you’re using rendering to HTML with Quarto, you also get a neat little preview popup when you hover over the link:\n\n\n\nPopup hover preview\n\n\nHowever, you cannot cross reference content that is in a separate file. For instance, if I’m writing in manuscript.qmd and I have a table in appendix.qmd named tbl-extra-details, if I write this:\nSee @fig-results and @tbl-details for the results, and @tbl-extra-details for complete details.\n…I’ll get this when rendering:\n\nSee Figure 1 and Table 1 for the results, and ?tbl-extra-details for complete details.\n\nMy only way of fixing this has been to manually type the number of the appendix table:\nSee @fig-results and @tbl-details for the results, and Table A1 for complete details.\n…but this is annoying because (1) if I rearrange any of the content in the appendix, that table might not be A1 anymore, and (2) there’s no neat automatic hyperlink to Table A1.\nThe manuscript and appendix documents can’t see each other, so there’s no way to make them talk to each other when generating cross references.\nQuarto has a good solution for this for longer documents: the Quarto book format. A Quarto book is a special kind of website where you can maintain separate .qmd files (like chapter1.qmd, chapter2.qmd, appendix.qmd, etc.), and when you render, the documents can see each other, so you can refer to content in other documents.\nThat’s great for books, and it could theoretically work for shorter documents like articles—make a single-chapter “book” where manuscript.qmd is the first chapter and appendix.qmd is the appendix. However, there are some issues when rendering to PDF—see “Approach 2” at this discussion here.\nAn easier solution would be to keep all the content in the same Markdown file and put the appendix stuff at the end under a heading like # Appendix. That way you don’t have to worry about getting multiple documents to see each other and cross-referencing works just fine. Quarto even has a neat feature where it can generate an end-of-the-document appendix for you. And my Quarto word count extension has the ability to count the words in the appendix separately from the rest of the document.\nIn one paper I have under review, the journal doesn’t consider the bibliography as part of the overall word count (YAY), so I was able to include the appendix in the same document and cross reference the figures and tables in the appendix from the main document. It was so nice to not worry about two documents. I wrote this:\nWe present the posterior distributions of the marginal means and AMCEs\nfor each of our experimental conditions in @fig-all-results and provide \nposterior medians, credible intervals, and other model diagnostics in \n@apptbl-all-results and @apptbl-coefs-orig.\n…and thanks to Quarto’s new ability to have custom cross reference types, I got this when rendering:\n\nWe present the posterior distributions of the marginal means and AMCEs for each of our experimental conditions in Figure 1 and provide posterior medians, credible intervals, and other model diagnostics in Table A5 and Table A8.\n\nHoly grail achieved!\nExcept not quite.\nIn another paper I wrapped up last week, the bibliography counted as part of the total word count, and I had citations in the appendix and they were getting added to the main document’s word count and inflating it. So I had to resort to my old process of using two files: one for the manuscript and one for the appendix. To cross reference stuff, I had to type the figure and table names manually. Boooo.\nFortunately smarter people than me have developed multibib, a pandoc extension that allows you to have multiple bibliographies and place them in separate locations in the document. That means I can theoretically create a main references list and a separate appendix references list, and any references I have in the appendix won’t get included in the main list and won’t add to the word count of the actual document.\nGetting the extension to work with Quarto requires a little bit of extra work, with some extra settings in the YAML metadata, but it works!\nHoly grail actually achieved!"
  },
  {
    "objectID": "blog/2023/12/11/separate-bibliographies-quarto/index.html#how-to-get-quarto-to-work-with-multiple-bibliographies",
    "href": "blog/2023/12/11/separate-bibliographies-quarto/index.html#how-to-get-quarto-to-work-with-multiple-bibliographies",
    "title": "How to create separate bibliographies in a Quarto document",
    "section": "How to get Quarto to work with multiple bibliographies",
    "text": "How to get Quarto to work with multiple bibliographies\nInstead of explaining each step in detail, I’ll just link to a complete example at GitHub. Go clone or download the project there to see everything.\n\n\n\n\n\n\nComplete example\n\n\n\nDownload the complete example from GitHub.\n\n\nIMPORTANT: This only works with Quarto 1.4, which is currently a pre-release. You can download it from GitHub.\nThe example project has a couple Quarto extensions included in it already, but if you’re making a project from scratch and you want to both use multiple bibliographies and count words, install the multibib and quarto-wordcount extensions from your terminal:\nquarto add pandoc-ext/multibib\nquarto add andrewheiss/quarto-wordcount\nHere’s what the final rendered document looks like:\n\n\n\n\nBibliography as list\nNotice how the YAML uses this to define the separate bibliography files:\nbibliography:\n  main: references.json\n  appendix: appendix.json\nThis makes Quarto mad (see this and this):\n\n\n\nYAML validation error\n\n\nThis is because Quarto enforces a strict schema for all its YAML settings and tries to make you input correct data for the different settings. It also makes it so that tab completion of the different settings works really well. It’s a Good Thing. Technically, Quarto wants the bibliography setting to be a string, or the name of a file with the references. It doesn’t want to work with a list of strings.\nTo get around this, we need to turn off the YAML validation. RStudio and Visual Studio Code will still warn that the syntax is wrong, but the document will render.\nAlternatively, there’s a fork of multibib that uses separate keys like this\nbibliography_main: blah.bib\nbibliography_appendix: blah.bib\n\n\nFenced divs with IDs\nTo get the reference lists to appear where you want them in your document, you have to include fenced divs with the id of each of the bibliographies. Put them wherever you want in your document and the corresponding reference list will go there.\nHere's the main references list:\n\n::: {#refs-main}\n:::\n\nHere's the appendix references list:\n\n::: {#refs-appendix}\n:::\n\n\nDisable citeproc\nPandoc uses citeproc to process citations and convert them to whatever style you’re using (Chicago, APA, MLA, etc). For the multiple bibliography extension to work, pandoc has to see the different bibliography entries before they’re converted into text—the extension does its magic sorting and dividing and then runs each of the separated reference lists through citeproc. So we need to turn it off, since multibib does it on its own.\nciteproc: false\n\n\nDefine custom cross reference types\nTo get number prefixes like “Figure A2” and “Table A3”, we need to define custom cross reference types:\ncrossref:\n  custom:\n    - kind: float\n      key: apptbl\n      latex-env: apptbl\n      reference-prefix: Table A\n      space-before-numbering: false\n      latex-list-of-description: Appendix Table\nWe can then use things like @apptbl-whatever and get automatic “A” prefixes.\n\n\nCounting words\nThis magically works with my word count extension! As long we specify it as a filter after the multibib filter, everything will work.\n# Use the multibib and word count extensions\nfilters:\n  - at: pre-render\n    path: \"_extensions/pandoc-ext/multibib/multibib.lua\"\n  - at: pre-render\n    path: \"_extensions/andrewheiss/wordcount/wordcount.lua\"\nWe’ll get this output in the terminal:\n83 in the main text + references, with 70 in the appendix\n---------------------------------------------------------\n26 words in text body\n57 words in reference section\n70 words in appendix section\nWe only have to make one modification to the word count Lua filter (wordcount.lua). Ordinarily, when rendering to HTML, the reference list appears in a div with the id refs, so my extension looks for a block with the refs ID:\nfunction is_ref_div (blk)\n   return (blk.t == \"Div\" and blk.identifier == \"refs\")\nend\nSince the main reference list now gets inserted into a div with the id refs-main, the script needs to change:\nfunction is_ref_div (blk)\n   return (blk.t == \"Div\" and blk.identifier == \"refs-main\")\nend\nIf I were better at Lua, I could probably figure out a way to make that reference div setting more robust or maybe even configurable (like a YAML key named wordcount-refs-id or something? idk). For now, I’m fine just manually changing the Lua script as needed."
  },
  {
    "objectID": "blog/2023/12/11/separate-bibliographies-quarto/index.html#caveats-and-downsides",
    "href": "blog/2023/12/11/separate-bibliographies-quarto/index.html#caveats-and-downsides",
    "title": "How to create separate bibliographies in a Quarto document",
    "section": "Caveats and downsides",
    "text": "Caveats and downsides\n\nExtra warnings I can’t turn off\nOne slightly annoying downside to this approach is that in the terminal output, every citation is seen as missing:\n[WARNING] Citeproc: citation Stan-2-26-1 not found\n[WARNING] Citeproc: citation rproject-4-3-1 not found\n[WARNING] Citeproc: citation Lovelace1842 not found\n[WARNING] Citeproc: citation Turing1936 not found\nThat’s probably due the ordering of the filter (see this)—it’s getting run at some incorrect point in the whole Quarto rendering process.\nI don’t know how to fix it though, so for now I’m just living with the warnings :(\nI’d love to figure this out someday though!\n\n\nManually separating the PDF\nIn the example Quarto file, I added a \\newpage before the appendix section so that the appendix would start on a new page. Most journal submission sites require that you upload separate documents for the main manuscript and the appendix.\nTo do this, I open the rendered PDF in Acrobat or macOS Preview and drag the appendix section out manually.\nI suppose I could automate it somehow with a command line tool like pdftk:\n# Extract pages 1-5\npdftk manuscript.pdf cat 1-5 output actual-paper.pdf\n\n# Extract pages 6-9\npdftk manuscript.pdf cat 6-9 output appendix.pdf\nBut programmatically figuring out those page ranges seems too hard and not worth the effort—following xkcd’s automation chart, it won’t save any time."
  },
  {
    "objectID": "blog/2023/08/15/matrix-multiply-logit-predict/index.html",
    "href": "blog/2023/08/15/matrix-multiply-logit-predict/index.html",
    "title": "Manually generate predicted values for logistic regression with matrix multiplication in R",
    "section": "",
    "text": "In a project I’m working on, I need to generate predictions from a logistic regression model. That’s typically a really straightforward task—we can just use predict() to plug a dataset of values into a model, which will spit out predictions either on the (gross, uninterpretable) log odds scale or on the (nice, interpretable) percentage-point scale.1\nHowever, in this project I cannot use predict()—I’m working with a big matrix of posterior coefficient draws from a Bayesian model fit with raw Stan code, so there’s no special predict() function that will work. Instead, I need to use matrix multiplication and manually multiply a matrix of new data with a vector of slopes from the model.\nI haven’t had to matrix multiply coefficients with data since my first PhD stats class back in 2012 and I’ve completely forgotten how.\nI created this little guide as a reference for myself, and I figured it’d probably be useful for others, so up on the blog it goes (see the introduction to this post for more about my philosophy of public work)."
  },
  {
    "objectID": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-1-logistic-regression-model-with-an-intercept-and-slopes",
    "href": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-1-logistic-regression-model-with-an-intercept-and-slopes",
    "title": "Manually generate predicted values for logistic regression with matrix multiplication in R",
    "section": "Example 1: Logistic regression model with an intercept and slopes",
    "text": "Example 1: Logistic regression model with an intercept and slopes\nHere’s a basic regression model where we predict if a penguin is a Gentoo based on its bill length and body mass.\n\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins |&gt; \n  tidyr::drop_na(sex) |&gt; \n  dplyr::mutate(is_gentoo = species == \"Gentoo\")\n\nmodel &lt;- glm(\n  is_gentoo ~ bill_length_mm + body_mass_g,\n  data = penguins,\n  family = binomial(link = \"logit\")\n)\n\nWe can generate predicted values across different three different values of bill length: 40 mm, 44 mm, and 48 mm, holding body mass constant at the average (4207 g):\n\ndata_to_plug_in &lt;- expand.grid(\n  bill_length_mm = c(40, 44, 48),\n  body_mass_g = mean(penguins$body_mass_g)\n)\ndata_to_plug_in\n##   bill_length_mm body_mass_g\n## 1             40        4207\n## 2             44        4207\n## 3             48        4207\n\nWe can feed this little dataset into the model using predict(), which can generate predictions as log odds (type = \"link\") or probabilities (type = \"response\"):\n\npredict(model, newdata = data_to_plug_in, type = \"link\")\n##      1      2      3 \n## -2.097 -1.741 -1.385\npredict(model, newdata = data_to_plug_in, type = \"response\")\n##      1      2      3 \n## 0.1094 0.1492 0.2002\n\nYay, nice and easy.\nHere’s how to do the same thing with manual matrix multiplication:\n\n# Get all the coefficients\n(coefs &lt;- coef(model))\n##    (Intercept) bill_length_mm    body_mass_g \n##     -32.401514       0.088906       0.006358\n\n# Split intercept and slope coefficients into separate objects\n(intercept &lt;- coefs[1])\n## (Intercept) \n##       -32.4\n(slopes &lt;- coefs[-1])\n## bill_length_mm    body_mass_g \n##       0.088906       0.006358\n\n# Convert the data frame of new data into a matrix\n(data_to_plug_in_mat &lt;- as.matrix(data_to_plug_in))\n##      bill_length_mm body_mass_g\n## [1,]             40        4207\n## [2,]             44        4207\n## [3,]             48        4207\n\n# Matrix multiply the new data with the slope coefficients, then add the intercept\n(log_odds &lt;- as.numeric((data_to_plug_in_mat %*% slopes) + intercept))\n## [1] -2.097 -1.741 -1.385\n\n# Convert to probability scale\nplogis(log_odds)\n## [1] 0.1094 0.1492 0.2002\n\nThe results are the same as predict():\n\npredict(model, newdata = data_to_plug_in, type = \"link\")\n##      1      2      3 \n## -2.097 -1.741 -1.385\nlog_odds\n## [1] -2.097 -1.741 -1.385\n\npredict(model, newdata = data_to_plug_in, type = \"response\")\n##      1      2      3 \n## 0.1094 0.1492 0.2002\nplogis(log_odds)\n## [1] 0.1094 0.1492 0.2002"
  },
  {
    "objectID": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-2-logistic-regression-model-with-a-categorical-predictor-and-no-intercept",
    "href": "blog/2023/08/15/matrix-multiply-logit-predict/index.html#example-2-logistic-regression-model-with-a-categorical-predictor-and-no-intercept",
    "title": "Manually generate predicted values for logistic regression with matrix multiplication in R",
    "section": "Example 2: Logistic regression model with a categorical predictor and no intercept",
    "text": "Example 2: Logistic regression model with a categorical predictor and no intercept\nThis gets a little more complex when working with categorical predictors, especially if you’ve omitted the intercept term. For instance, in the data I’m working with, we have a model that looks something like this, with 0 added as a term to suppress the intercept and give separate coefficients for each of the levels of sex:\n\nmodel_categorical &lt;- glm(\n  is_gentoo ~ 0 + sex + bill_length_mm + body_mass_g,\n  data = penguins,\n  family = binomial(link = \"logit\")\n)\ncoef(model_categorical)\n##      sexfemale        sexmale bill_length_mm    body_mass_g \n##      -75.68237      -89.88199        0.03387        0.01831\n\nWhen using predict(), we don’t have to do anything special with this intercept-free model. We can plug in a dataset with different variations of predictors:\n\ndata_to_plug_in_cat &lt;- expand.grid(\n  sex = c(\"female\", \"male\"),\n  bill_length_mm = c(40, 44, 48),\n  body_mass_g = mean(penguins$body_mass_g)\n)\ndata_to_plug_in_cat\n##      sex bill_length_mm body_mass_g\n## 1 female             40        4207\n## 2   male             40        4207\n## 3 female             44        4207\n## 4   male             44        4207\n## 5 female             48        4207\n## 6   male             48        4207\n\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"link\")\n##       1       2       3       4       5       6 \n##   2.707 -11.493   2.842 -11.357   2.978 -11.222\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"response\")\n##         1         2         3         4         5         6 \n## 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05\n\nIf we want to do this manually, we have to create a matrix version of data_to_plug_in_cat that has separate columns for sexfemale and sexmale. We can’t just use as.matrix(data_to_plug_in_cat), since that only has a single column for sex (and because that column contains text, it forces the rest of the matrix to be text, which makes it so we can’t do math with it anymore):\n\nas.matrix(data_to_plug_in_cat)\n##      sex      bill_length_mm body_mass_g\n## [1,] \"female\" \"40\"           \"4207\"     \n## [2,] \"male\"   \"40\"           \"4207\"     \n## [3,] \"female\" \"44\"           \"4207\"     \n## [4,] \"male\"   \"44\"           \"4207\"     \n## [5,] \"female\" \"48\"           \"4207\"     \n## [6,] \"male\"   \"48\"           \"4207\"\n\nInstead, we can use model.matrix() to create a design matrix—also called a dummy-encoded matrix2 or a one-hot encoded matrix—which makes columns of 0s and 1s for each of the levels of sex\n2 Though we should probably quit using the word “dummy” because of its ableist connotations—see Google’s developer documentation style guide for alternatives.\ndata_to_plug_in_cat_mat &lt;- model.matrix(\n  ~ 0 + ., data = data_to_plug_in_cat\n)\ndata_to_plug_in_cat_mat\n##   sexfemale sexmale bill_length_mm body_mass_g\n## 1         1       0             40        4207\n## 2         0       1             40        4207\n## 3         1       0             44        4207\n## 4         0       1             44        4207\n## 5         1       0             48        4207\n## 6         0       1             48        4207\n## attr(,\"assign\")\n## [1] 1 1 2 3\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$sex\n## [1] \"contr.treatment\"\n\nWe can now do math with this matrix. Since we don’t have an intercept term, we don’t need to create separate objects for the slopes and intercepts and can matrix multiply the new data matrix with the model coefficients:\n\n# Get all the coefficients\n(coefs_cat &lt;- coef(model_categorical))\n##      sexfemale        sexmale bill_length_mm    body_mass_g \n##      -75.68237      -89.88199        0.03387        0.01831\n\n# Matrix multiply the newdata with the slope coefficients\n(log_odds_cat &lt;- as.numeric(data_to_plug_in_cat_mat %*% coefs_cat))\n## [1]   2.707 -11.493   2.842 -11.357   2.978 -11.222\n\n# Convert to probability scale\nplogis(log_odds_cat)\n## [1] 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05\n\nThe results are the same!\n\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"link\")\n##       1       2       3       4       5       6 \n##   2.707 -11.493   2.842 -11.357   2.978 -11.222\nlog_odds_cat\n## [1]   2.707 -11.493   2.842 -11.357   2.978 -11.222\n\npredict(model_categorical, newdata = data_to_plug_in_cat, type = \"response\")\n##         1         2         3         4         5         6 \n## 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05\nplogis(log_odds_cat)\n## [1] 9.374e-01 1.020e-05 9.449e-01 1.169e-05 9.516e-01 1.338e-05"
  },
  {
    "objectID": "blog/2023/07/28/gradient-map-fills-r-sf/index.html",
    "href": "blog/2023/07/28/gradient-map-fills-r-sf/index.html",
    "title": "How to fill maps with density gradients with R, {ggplot2}, and {sf}",
    "section": "",
    "text": "The students in my summer data visualization class are finishing up their final projects this week and I’ve been answering a bunch of questions on our class Slack. Often these are relatively standard reminders of how to tinker with specific ggplot layers (chaning the colors of a legend, adding line breaks in labels, etc.), but today one student had a fascinating and tricky question that led me down a realy fun dataviz rabbit hole. She was making a map with hundreds of points representing specific locations of events. This led to overplotting—it’s really hard to stick hundreds of dots on a small map of a city and have it make any sense. To help fix this, she wanted to fill areas of the map by the count of events, making a filled gradient rather than a bunch of points. This is fairly straightforward with regular scatterplots, but working with geographic data adds some extra wrinkles to the process.\nSo let’s all go down this rabbit hole together (mostly so future-me can remember how to do this)!\n\n\n\n\n\n\nWho this post is for\n\n\n\nHere’s what I assume you know:\n\nYou’re familiar with R and the tidyverse (particularly {dplyr} and {ggplot2}).\nYou’re somewhat familiar with {sf} for working with geographic data. I have a whole tutorial here and a simplified one here and the {sf} documentation has a ton of helpful vignettes and blog posts, and there are also two free books about it: Spatial Data Science and Geocomputation with R. Also check this fantastic post out to learn more about the anatomy of a geometry column with {sf}.\n\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(spatstat)\nlibrary(tigris)\nlibrary(rnaturalearth)\nlibrary(patchwork)\n\ntheme_set(\n  theme_void(base_family = \"Roboto Slab\") +\n    theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n)\n\nFixing overplotted scatterplots\nOverplotting happens when there are too many data points in one place in a plot. For instance, here’s a scatterplot of carats and prices for 54,000 diamonds, using {ggplot2}’s built-in diamonds dataset:\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\nWoof. It’s just a blob of black points.\nTo fix overplotting, you can either restyle the points somehow so they’re not so crowded, or you can summarize the data and display it a slightly different way. There are lots of possible ways to fix this though—here’s a quick overview of some:\n\n\nSmaller points\nSemi-transparent points\n■-binned points\n⬢-binned points\nDensity countours\n\n\n\nWe can try shrinking the points down a bunch:\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point(size = 0.2) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can make the points partially transparent so that clusters of points are darker (with more points stacked on top of each other)\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point(alpha = 0.01) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can draw a grid across the x- and y-axes and count how many points fall inside each box, then fill each box by that count.\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  stat_bin2d() +\n  scale_fill_viridis_c() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can draw a hexagonal grid across the x- and y-axes and count how many points fall inside each hexagon, then fill each hexagon by that count.\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  stat_binhex() +\n  scale_fill_viridis_c() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can also find the combined density of points along both the x- and y-axes and plot the contours of those densities. Here, the brighter the area, the more concentrated the points:\n\nwithr::with_seed(4393, {\n  dsmall &lt;- diamonds[sample(nrow(diamonds), 1000), ]\n})\n\nggplot(dsmall, aes(x = carat, y = price)) +\n  geom_density2d_filled() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInitial overplotted map\nGeographic data, however, is a little trickier to work with. Fundamentally, putting points on a map is the same as making a scatterplot, with latitude on the x-axis and longitude on the y-axis. But maps are strange. Scatterplots are nice rectangles; maps have oddly shaped borders. Scatterplots are naturally flat; maps are curved chunks of a globe and have to be flattened and reprojected into two dimensions somehow. Scatterplots come from nice rectangular datasets; maps from from complex shapefiles.\nShrinking and transparentifying points with map points is the same as with regular points: play with the size and alpha arguments in geom_sf(). Making bins and gradients, however, takes a little more work (hence this rabbit hole).\nTo illustrate this, we’ll plot all 264 campgrounds in the state of Georgia. This doesn’t involve severe overplotting (though at the end I’ve included an example of dealing with 10,000 map points), but it’s useful for playing with these different techniques.\nThe data comes from Georgia’s GIS Clearinghouse, which is a miserable ancient website that requires a (free) login. I downloaded the GNIS Cultural Features dataset (last updated in 1996; direct link + documentation). Since it’s government data from the US Department of the Interior and ostensibly public domain, you can download the shapefile here:\n\ncultural.zip\n\n\n# We'll make all the shapefiles use ESRI:102118 (NAD 1927 Georgia Statewide\n# Albers: https://epsg.io/102118)\nga_crs &lt;- st_crs(\"ESRI:102118\")\n\n# Geographic data from Georgia\nga_cultural &lt;- read_sf(\"data/cultural/cultural.shp\") %&gt;% \n  # This shapefile uses EPSG:4326 (WGS 84), but that projection information\n  # isn't included in the shapefile for whatever reason, so we need to set it\n  st_set_crs(st_crs(\"EPSG:4326\"))\n\nga_campgrounds &lt;- ga_cultural %&gt;% \n  filter(DESCRIPTOR == \"CAMP/CAMPGROUND\") %&gt;% \n  st_transform(ga_crs)\n\nWe’ll also grab a state map of Georgia and a map of all Georgia counties from the US Census Bureau using the {tigris} package:\n\n# Geographic data from the US Census\noptions(tigris_use_cache = TRUE)\nSys.setenv(TIGRIS_CACHE_DIR = \"maps\")\n\nga_state &lt;- states(cb = TRUE, resolution = \"500k\", year = 2022) %&gt;% \n  filter(STUSPS == \"GA\") %&gt;% \n  st_transform(ga_crs)\n\nga_counties &lt;- counties(state = \"13\", cb = TRUE, resolution = \"500k\", year = 2022) %&gt;% \n  st_transform(ga_crs)\n\nAnd finally, to help illustrate maps aren’t mere scatterplots, we’ll add all of Georgia’s rivers and lakes to the maps we make. Lots of campgrounds are clustered around lakes, so this will also help us see some patterns in the data. We’ll get river and lake data from the Natural Earth project, which provides all sorts of physical map data like coastlines, reefs, islands, and so on. They provide shapefiles for large rivers and lakes globally (rivers_lake_centerlines and lakes) and smaller rivers and lakes in North America specifically (rivers_north_america and lakes_north_america).\n\n# See rnaturalearth::df_layers_physical for all possible names\n# Create a vector of the four datasets we want\nne_shapes_to_get &lt;- c(\n  \"rivers_lake_centerlines\", \"rivers_north_america\",\n  \"lakes\", \"lakes_north_america\"\n)\n\n# Loop through ne_shapes_to_get and download each shapefile and store it locally\nif (!file.exists(\"maps/ne_10m_lakes.shp\")) {\n  ne_shapes_to_get %&gt;%\n    walk(~ ne_download(\n      scale = 10, type = .x, category = \"physical\",\n      destdir = \"maps\", load = FALSE\n    ))\n}\n\n# Load each pre-downloaded shapefile and store it in a list\nne_data_list &lt;- ne_shapes_to_get %&gt;%\n  map(~ {\n    ne_load(\n      scale = 10, type = .x, category = \"physical\",\n      destdir = \"maps\", returnclass = \"sf\"\n    ) %&gt;%\n      st_transform(ga_crs)\n  }) %&gt;%\n  set_names(ne_shapes_to_get)\n\n# Load all the datasets in the list into the global environment\nlist2env(ne_data_list, envir = .GlobalEnv)\n## &lt;environment: R_GlobalEnv&gt;\n\nThese physical shapefiles from Natural Earth contain thousands of rivers and lakes, but we only want the ones that exist in or cross through Georgia. We can use the Georgia state shapefile we got from the Census (ga_state) as a sort of cookie cutter on each of these larger shapefiles to only keep the parts of rivers and lakes that fall within Georgia’s boundaries:\n\n# ↓ these give a bunch of (harmless?) warnings about spatially constant attributes\nrivers_global_ga &lt;- st_intersection(ga_state, rivers_lake_centerlines)\nrivers_na_ga &lt;- st_intersection(ga_state, rivers_north_america)\nlakes_global_ga &lt;- st_intersection(ga_state, lakes)\nlakes_na_ga &lt;- st_intersection(ga_state, lakes_north_america)\n\nHere’s what our initial map looks like, with fancy rivers and maps added. It looks nice and detailed, but there are a lot of points, and even shrinking them down to 0.5, there are a few overplotted clusters.\n\nplot_initial &lt;- ggplot() +\n  geom_sf(data = ga_state, fill = \"grey20\") +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  geom_sf(data = ga_campgrounds, size = 0.5, color = \"grey50\") +\n  # Technically this isn't necessary since all the layers already use 102118, but\n  # we'll add it just in case I forgot to do that to one of them\n  coord_sf(crs = ga_crs)\nplot_initial\n\n\n\n\n\n\n\nOption 1: Fill each county by the number of campgrounds\nOne way to address this overplotting is to create bins with counts of the campgrounds in each bin. US states have a natural kind of “bin”, since they’re subdivided into counties. Georgia has an inordinate number of counties, so we can count the number of campgrounds per county and fill each county by that count. We’ll join the campground data to the county data with st_join() (which is the geographic equivalent of left_join()) and then use some group_by() %&gt;% summarize() magic to find the number of locations per county.\n\n# st_join() adds extra rows for repeated counties and returns partially blank\n# rows for counties with no campgrounds. It would ordinarily be easy to use\n# `summarize(total = n())`, but this won't be entirely accurate since counties\n# without campgrounds still appear in the combined data and would get\n# incorrectly counted. So instead, we look at one of the columns from\n# ga_campgrounds (DESCRIPTOR). If it's NA, it means that the county it was\n# joined to didn't have any campgrounds, so we can ignore it when counting.\nga_counties_campgrounds &lt;- ga_counties %&gt;% \n  st_join(ga_campgrounds) %&gt;% \n  group_by(NAMELSAD) %&gt;% \n  summarize(total = sum(!is.na(DESCRIPTOR)))\n\nWe can plot this new ga_counties_campgrounds data and fill by total:\n\nplot_county &lt;- ggplot() +\n  geom_sf(data = ga_counties_campgrounds, aes(fill = total), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\", na.value = \"black\") +\n  coord_sf(crs = ga_crs)\nplot_county\n\n\n\n\n\n\n\nThis already helps. We can see a cluster of campgrounds in central Georgia around the Piedmont National Wildlife Refuge and the Oconee National Forest, and another cluster in the mountains of northeast Georgia in the Chattahoochee-Oconee National forests.\nOption 2: Create a grid and fill each grid box by the number of campgrounds\nCounties are oddly shaped, though, and not all states or cities have this many subdivisions to work with. So instead, we can create our own subdivisions. We can use st_make_grid() to divide the state area up into a grid—here we’ll use 400 boxes:\n\n# Spit the state area into a 20x20 grid\nga_grid &lt;- ga_state %&gt;% \n  st_make_grid(n = c(20, 20))\n\nggplot() +\n  geom_sf(data = ga_state) +\n  geom_sf(data = ga_grid, alpha = 0.3) +\n  theme_void()\n\n\n\n\n\n\n\nWe can then use st_intersection() to cut the Georgia map into pieces that fall in each of those grid boxes:\n\nga_grid_map &lt;- st_intersection(ga_state, ga_grid) %&gt;% \n  st_as_sf() %&gt;% \n  mutate(grid_id = 1:n())\n\nggplot() +\n  geom_sf(data = ga_grid_map) +\n  theme_void()\n\n\n\n\n\n\n\nNext we can join the campground data to these boxes just like we did with the counties, and we can use group_by() %&gt;% summarize() to get counts in each grid box:\n\ncampgrounds_per_grid_box &lt;- ga_grid_map %&gt;% \n  st_join(ga_campgrounds) %&gt;% \n  group_by(grid_id) %&gt;% \n  summarize(total = sum(!is.na(DESCRIPTOR)))\n\nFinally we can plot it:\n\nplot_grid &lt;- ggplot() +\n  geom_sf(data = campgrounds_per_grid_box, aes(fill = total), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\") +\n  coord_sf(crs = ga_crs)\nplot_grid\n\n\n\n\n\n\n\nThat feels more uniform than the counties and still highlights the clusters of campgrounds in central and northeast Georgia.\nOption 3: Fill with a gradient of the density of the number of campgrounds\nHowever, it is a little misleading. Technically there are more campgrounds in northeast Georgia than in central Georgia, but because of how (1) county boundaries happened to be drawn, and (2) how the gridlines happened to be drawn, the campgrounds in the northeast were spread across multiple counties/tiles while the campgrounds in central Georgia happened to mostly fall in one county/tile, so it looks like there are more down there.\nTo make the shading more accurate, we can turn to turn to calculus and imagine grid boxes that are infinitely small. We can calculate densities instead of binned or clustered subunits.\nDoing this with geographic data is tricky, though, and requires some extra math and an extra package to handle the fancy math: {spatstat}. (See this and this and this and this for some examples of using {spatstat}.)\nTo calculate the density of campground latitudes and longitudes, we need to first convert our geometry column to a spatial point pattern object (or a ppp object) that {spatstat} can work with. Like sf objects, a ppp object is a collection of geographic points, and it can have overall boundaries embedded in it, or what {spatstat} calls a “window”:\n\n# Convert the campground coordinates to a ppp object with a built-in window\nga_campgrounds_ppp &lt;- as.ppp(ga_campgrounds$geometry, W = as.owin(ga_state))\n\n# Check to see if it worked\nplot(ga_campgrounds_ppp)\n\n\n\n\n\n\n\nppp objects have a corresponding density() function that can calculate the joint densities of each point’s latitude and longitude coordinates. It also has a dimyx argument that controls the number of pixels of the density—higher numbers will create smoother and higher resolution images; smaller numbers will be chunkier and less granular. The resulting object is a pixel-based bitmap image with extra attributes that describe how to connect it back to latitude and longitude points. If we feed that image to stars::st_as_stars() (from the {stars} package), we’ll force the image to use that geographic data:\n\n# Create a stars object of the density of campground locations\ndensity_campgrounds_stars &lt;- stars::st_as_stars(density(ga_campgrounds_ppp, dimyx = 300))\n\n# Check to see what it looks like\nplot(density_campgrounds_stars)\n\n\n\n\n\n\n\n\n\n\n\n\n\n{spatstat} and projections\n\n\n\nIMPORTANTLY density.ppp() doesn’t work with all CRS systems. From my experimenting, it seems to only work with projections that use meters as their units, like Albers and NAD 83. It gave me an error anytime I tried working with decimal degrees (i.e. the −180° to 180° scale). I don’t know why :(. That’s why I’ve forced all the different geographic datasets in this post to use ESRI:102118 (Georgia Statewide Albers)—it uses meters and it works.\n\n\nWe can convert this {stars} object back to {sf} so it’s normal and plottable with geom_sf():\n\nga_campgrounds_density &lt;- st_as_sf(density_campgrounds_stars) %&gt;%\n  st_set_crs(ga_crs)\n\nWe can then plot it with ggplot() and geom_sf() like normal, filling by v, which is the column that stores the calculated density:\n\nplot_density &lt;- ggplot() +\n  geom_sf(data = ga_campgrounds_density, aes(fill = v), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\")\nplot_density\n\n\n\n\n\n\n\nooooh that’s so pretty already. Let’s add all the rivers and lakes:\n\nplot_density_fancy &lt;- ggplot() +\n  geom_sf(data = ga_campgrounds_density, aes(fill = v), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\") +\n  coord_sf(crs = ga_crs)\nplot_density_fancy\n\n\n\n\n\n\n\nAbsolutely stunning.\nWe can add the actual campground points back in too:\n\nplot_density_fancy_points &lt;- ggplot() +\n  geom_sf(data = ga_campgrounds_density, aes(fill = v), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = rivers_global_ga, linewidth = 0.3, color = \"white\") +\n  geom_sf(data = rivers_na_ga, linewidth = 0.1, color = \"white\") +\n  geom_sf(data = lakes_global_ga, fill = \"white\", color = NA) +\n  geom_sf(data = lakes_na_ga, fill = \"white\", color = NA) +\n  geom_sf(data = ga_campgrounds, size = 0.3, color = \"grey80\") +\n  scale_fill_viridis_c(option = \"magma\", guide = \"none\") +\n  coord_sf(crs = ga_crs)\nplot_density_fancy_points\n\n\n\n\n\n\n\nThat’s so so cool.\nComparison\nHere’s what all these options look like together. There’s no one single best option—it depends on what story you’re trying to tell, how the data is distributed, how crowded it is, and so on—but it’s cool that there are so many options!\n\nCodelayout &lt;- \"\n#####\n#A#BC\n#####\n#D#EF\n#####\nG####\n\"\n\n(plot_initial + labs(title = \"Overplotted\") + theme(plot.background = element_rect(fill = \"white\", color = NA))) +\n  (plot_county + labs(title = \"Filled by county\") + theme(plot.background = element_rect(fill = \"white\", color = NA))) +\n  plot_spacer() +\n  (plot_grid + labs(title = \"Filled by grid\") + theme(plot.background = element_rect(fill = \"white\", color = NA))) + \n  (plot_density_fancy + labs(title = \"Filled by density\") + theme(plot.background = element_rect(fill = \"white\", color = NA))) +\n  plot_spacer() + plot_spacer() +\n  plot_layout(design = layout, widths = c(0.02, 0.47, 0.02, 0.47, 0.02), heights = c(0.02, 0.47, 0.02, 0.47, 0.02)) +\n  plot_annotation(theme = theme(plot.background = element_rect(fill = \"grey95\", color = NA)))\n\n\n\n\n\n\n\nExtra bonus fun: 10,000+ churches\nFinally, for some extra fun, let’s plot something that’s actually overplotted—Georgia’s 10,000+ churches!\n\nga_churches &lt;- ga_cultural %&gt;% \n  filter(DESCRIPTOR == \"CHURCH\") %&gt;% \n  st_transform(st_crs(\"ESRI:102118\"))\n\nggplot() +\n  geom_sf(data = ga_state) +\n  geom_sf(data = ga_churches)\n\n\n\n\n\n\n\nlol this is basically just a wildly overplotted population map at this point. Let’s calculate the density of these locations and plot a gradient:\n\n# Convert the church coordinates to a ppp object with a built-in window\nga_churches_ppp &lt;- as.ppp(ga_churches$geometry, W = as.owin(ga_state))\n\n# Create a stars object (whatever that is) of the density of church locations\ndensity_churches_stars &lt;- stars::st_as_stars(density(ga_churches_ppp, dimyx = 300))\n\n# Convert the stars object to an sf object so it's normal and plottable again\nga_churches_density &lt;- st_as_sf(density_churches_stars) %&gt;%\n  st_set_crs(ga_crs)\n\n\nggplot() +\n  geom_sf(data = ga_churches_density, aes(fill = v), color = NA) +\n  geom_sf(data = ga_state, fill = NA, color = \"black\", linewidth = 0.25) +\n  geom_sf(data = ga_churches, size = 0.005, alpha = 0.3) +\n  scale_fill_viridis_c(option = \"rocket\", guide = \"none\") +\n  theme_void()\n\n\n\n\n\n\n\nCheck out that hugely bright spot in Atlanta!\n\n\n\n\nCitationBibTeX citation:@online{heiss2023,\n  author = {Heiss, Andrew},\n  title = {How to Fill Maps with Density Gradients with {R,}\n    \\{Ggplot2\\}, and \\{Sf\\}},\n  date = {2023-07-28},\n  url = {https://www.andrewheiss.com/blog/2023/07/28/gradient-map-fills-r-sf/},\n  doi = {10.59350/bsctw-0a955},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHeiss, Andrew. 2023. “How to Fill Maps with Density Gradients with\nR, {Ggplot2}, and {Sf}.” July 28, 2023. https://doi.org/10.59350/bsctw-0a955."
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "",
    "text": "As I explained in my previous blog post, in June 2023 I drove my family across the country in a 5,000-mile roundtrip odyssey from Atlanta, Georgia to just-south-of-Salt Lake City, Utah, and back again. Instead of taking a direct path through the middle the United States, we decided to hit as many tourist-y stops along the way, making a big circle along the southern US on the way out and along the northern US on the way back, stopping at a dozen really neat places, including:\nIn an effort to stay On Brand™, I wanted to keep a log of all our stops so I could go back and analyze the trip. How many miles would we drive? How much time would we spend in the car? How long would each gas stop and bathroom break be? Could I make some sort of map showing our journey?\nSo, at the first gas station we stopped at in Alabama, I started a new file in my phone’s Notes app and dutifully wrote down the arrival time, name and address of the gas station, and departure time. I had a nascent data file and was ready to use it through the trip.\nAbout an hour after leaving our second gas station stop just outside Mississippi, I realized that I forgot to write down anything about the second gas station stop. My data collection process died after a single entry.\nI was initially disappointed that I wouldn’t have any sort of data to work with at the end of the trip, but then I remembered that my phone was tracking every part of the whole trip through Google Maps. When we got to the first hotel, I searched Google to see if it was possible to export your location history from Google Maps and saw that it was, so I gave up on trying to keep my own log and outsourced all that to Google instead. I didn’t know what exactly Google was tracking, but I figured it would at least have the date, time, and location of where we went, so that was good enough.\nRight after we got home from our trip last week, I exported my data and was shocked by how much detail Google actually had. The exported JSON files contained 100,000+ location entries going all the back to 2013 (!!!). Parsing and sifting through and working with all this data initially seemed overwhelming (especially since Google doesn’t actually provide any formal documentation for these files!), but once I worked through a few tricky issues, it was surprisingly straightforward to make maps, calculate distances and times, and do all sorts of other fun things with the data.\nSo in this post I do three things:"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#settings",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#settings",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Settings",
    "text": "Settings\n\n\n\n\n\n\nComplete documentation for Settings.json\n\n\n\n\nFull documentation\n\n\n\nSettings.json is a small file that contains a bunch of device-specific settings and details, like the unique ID for your phone, the date and time when you added your phone to your account, and so on. It’s not super important for working with your data—consider it extra metadata."
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#records",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#records",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Records",
    "text": "Records\n\n\n\n\n\n\nComplete documentation for Records.json\n\n\n\n\nBasic overview\nFull documentation\n\n\n\nRecords.json is important. This contains an entry for every time Google Maps recorded your location somewhere. Here’s what a typical entry looks like (each entry is nested under a \"locations\" property):\n{\n  \"locations\": [{\n    \"latitudeE7\": 300489450,\n    \"longitudeE7\": -899629451,\n    \"accuracy\": 9,\n    \"velocity\": 0,\n    \"heading\": 329,\n    \"altitude\": 1,\n    \"verticalAccuracy\": 4,\n    \"deviceTag\": 0,\n    \"platformType\": \"IOS\",\n    \"serverTimestamp\": \"2023-06-04T01:05:58.248Z\",\n    \"deviceTimestamp\": \"2023-06-04T01:05:57.559Z\",\n    \"batteryCharging\": false,\n    \"formFactor\": \"PHONE\",\n    \"timestamp\": \"2023-06-04T00:52:57Z\"\n  }]\n}\n\nLocation details like latitudeE7 and longitudeE7, multiplied by 10,000,000 so there are no decimals\nAccuracy details like accuracy, or radius of the location measurement, in meters. Smaller numbers mean better precision.\nPosition details like heading (degrees east of north, from 0 to 359), velocity (meters per second), altitude (meters), and verticalAccuracy (accuracy of altitude in meters; smaller means better precision)\nDevice details like deviceTag (Google’s internal ID for your device; I changed mine to 0 in this post for privacy reasons), platformType and formFactor (type of device), batteryCharging (whether your device was charging at the time), and some timestamps for the device time, server time, and UTC time.\n\nThis is pretty straightforward to work with, since it’s essentially rectangular data, or data that can be thought of as rows and columns. Each location entry in the JSON file is a row; each property is a column (accuracy is 9, altitude is 1, and so on), like this:\n\nExample dataset from Google Location History JSON\n\n\n\n\n\n\n\n\n\nrowid\nlatitudeE7\nlongitudeE7\n…\nformFactor\ntimestamp\n\n\n\n54383\n300489450\n-899629451\n…\nPHONE\n2023-06-04T00:52:57Z\n\n\n54384\n…\n…\n…\n…\n…\n\n\n\nThe only data manipulation we need to do is modify the latitude and longitude columns. Both of those have been multiplied by 10,000,000 so that there are no decimal points in the coordinates (probably to avoid the weirdness inherent in floating point math). There’s actually a hint about this in the column names for latitudeE7 and longitudeE7—in R and most other programming languages, 1e7 is computer notation for \\(10^7\\). If we divide both those columns by 10,000,000, we’ll get standard-looking geographic coordinates:\n\n300489450 / 10000000\n## [1] 30.05\n-899629451 / 1e7  # We can write it as 1e7 too\n## [1] -89.96\n\nFor the sake of this post, the most important pieces of data here will be location (latitudeE7 and longitudeE7), time (timestamp), and elevation (altitude). The velocity data might be cool to do something with some day, but I won’t touch it for now."
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#semantic-location-history",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#semantic-location-history",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Semantic location history",
    "text": "Semantic location history\n\n\n\n\n\n\nComplete documentation for semantic location history files\n\n\n\n\nBasic overview\nFull documentation\n\n\n\nThese month-specific JSON files are super neat and full of really rich data. Each entry is one of two types of events: activity segments (activitySegment) and place visits (placeVisit).\nHere’s what the beginning of a typical activity segment entry looks like (with a lot of parts omitted for space—click here for a complete example entry):\n{\n  \"timelineObjects\": [\n    {\n      \"activitySegment\": {\n        \"startLocation\": {\n          \"latitudeE7\": 339703585,\n          \"longitudeE7\": -842416959,\n          \"sourceInfo\": {\n            \"deviceTag\": 0\n          }\n        },\n        \"endLocation\": {\n          \"latitudeE7\": 339786509,\n          \"longitudeE7\": -842006268,\n          \"sourceInfo\": {\n            \"deviceTag\": 0\n          }\n        },\n        \"duration\": {\n          \"startTimestamp\": \"2023-06-01T00:15:26.999Z\",\n          \"endTimestamp\": \"2023-06-01T00:28:47Z\"\n        },\n        \"distance\": 5483,\n        \"activityType\": \"IN_PASSENGER_VEHICLE\",\n        \"confidence\": \"HIGH\",\n        \"activities\": [\n          {\n            \"activityType\": \"IN_PASSENGER_VEHICLE\",\n            \"probability\": 91.4052665233612\n          },\n          {\n            \"activityType\": \"STILL\",\n            \"probability\": 6.64205327630043\n          },\n          ...\n        ],\n        ...\n      }\n    }\n  ]\n}\nAnd here’s the beginning typical place visit entry (click here for a complete example entry)\n{\n  \"timelineObjects\": [\n    {\n      \"placeVisit\": {\n        \"location\": {\n          \"latitudeE7\": 323281024,\n          \"longitudeE7\": -863373283,\n          \"placeId\": \"ChIJQ0NgzsqAjogRISqopW4DZMc\",\n          \"address\": \"1030 W South Blvd, Montgomery, AL 36105, USA\",\n          \"name\": \"Chevron\",\n          \"sourceInfo\": {\n            \"deviceTag\": 0\n          },\n          \"locationConfidence\": 89.41639,\n          \"calibratedProbability\": 80.56358\n        },\n        \"duration\": {\n          \"startTimestamp\": \"2023-06-03T14:27:23.001Z\",\n          \"endTimestamp\": \"2023-06-03T14:50:24Z\"\n        },\n        \"placeConfidence\": \"MEDIUM_CONFIDENCE\",\n        ...\n      }\n    }\n  ]\n}\nWorking with these gnarly JSON files is a lot trickier than working with Records.json because all this data is deeply nested within specific properties of the activity or place visit.\nTo help visualize all this nesting, here’s what a single placeVisit entry looks like (screenshot via this JSON tree visualizer), with the visit details + alternative places + child visits and their alternative locations and confidence levels + lots of other details:\n\n\n\nJSON tree for one placeVisit entry\n\n\nYikes.\nFor this post, I don’t care about all the alternative possible locations or possible modes of transportation or confidence levels or anything. I cleaned those up in my Google Timeline before exporting the data, so I’m pretty confident everything is relatively correct.\n\n\n\n\n\n\nMore on cleaning up your Google Timeline\n\n\n\n\n\nGoogle actually has a really nice web-based frontend for your whole location history at Google Timeline.\n\n\nMy Google Timeline from June 4, 2023\n\nBefore downloading my location data from this road trip, I checked my history for each day at Google Timeline to confirm that it was marking the right trips locations. It generally did a good job, but when an entry has lower confidence (the confidence entry in the JSON data), Google shows you its uncertainty. For instance, here it’s not sure about two stops:\n\n\nGoogle’s uncertainty about two locations\n\nThose are both correct, so I clicked on the check to confirm.\nOther times, it gets things wrong. Like here, it thought I was on a motorcycle for this leg of the trip:\n\n\nGoogle’s incorrect guess that I was motorcycling instead of driving\n\nThat’s obviously not correct, so I edited it to be “Driving” instead:\n\n\nSwitching motorcylcing to driving\n\nAnd other times, when cell reception is bad, it misses entire activities. Like here, where we went to Devil’s Tower where there’s no cell reception at all, Google saw that we left the hotel at 8:14 AM and somehow magically arrived at Devil’s Tower at 9:21 AM. It failed to record how we did it, so I clicked on “Add activity” and told Google I was driving.\n\n\nMissing activity\n\n\n\n\nHere’s all I’ll worry about here:\nFor placeVisits:\n\nLocation coordinates (location → latitudeE7 and location → longitudeE7)\nLocation ID (location → placeId): Google’s internal ID for the place\nLocation name (location → name)\nLocation address (location → address)\nStart and end times (duration → startTimestamp and duration → endTimestamp)\n\nFor activitySegments:\n\nDistance (distance)\nActivity type (activityType)\nStart and end coordinates (startLocation → (latitudeE7 & longitudeE7) and endLocation → (latitudeE7 & longitudeE7))\nStart and end times (duration → startTimestamp and duration → endTimestamp)"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#records.json",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#records.json",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Records.json",
    "text": "Records.json\nLoading and cleaning data\nLoading Records.json is easy. Because it is essentially rectangular data already, we can use simplifyVector = TRUE to have R convert the locations slot of the JSON file to a data frame automatically.\n\nall_locations_raw &lt;- read_json(\"data/Records.json\", simplifyVector = TRUE) %&gt;% \n  # Pull out the \"locations\" slot (this is the same as doing full_data$locations)\n  pluck(\"locations\") %&gt;% \n  # Make this a tibble just so it prints nicer here on the blog\n  as_tibble() \nall_locations_raw\n## # A tibble: 2,288 × 15\n##    latitudeE7 longitudeE7 accuracy velocity heading altitude verticalAccuracy deviceTag platformType serverTimestamp          batteryCharging formFactor timestamp                source deviceTimestamp\n##         &lt;int&gt;       &lt;int&gt;    &lt;int&gt;    &lt;int&gt;   &lt;int&gt;    &lt;int&gt;            &lt;int&gt;     &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                    &lt;lgl&gt;           &lt;chr&gt;      &lt;chr&gt;                    &lt;chr&gt;  &lt;chr&gt;          \n##  1  338465853  -843051758        4       30     205      271                3         0 IOS          2023-06-03T12:00:21.140Z TRUE            PHONE      2023-06-03T12:00:21.001Z &lt;NA&gt;   &lt;NA&gt;           \n##  2  338361607  -843273246       25       32     238      272                3         0 IOS          2023-06-03T12:01:41.112Z TRUE            PHONE      2023-06-03T12:01:41Z     &lt;NA&gt;   &lt;NA&gt;           \n##  3  338283154  -843438356        4       29     228      270                3         0 IOS          2023-06-03T12:02:36.183Z TRUE            PHONE      2023-06-03T12:02:36.001Z &lt;NA&gt;   &lt;NA&gt;           \n##  4  338185757  -843621145        4       30     203      259                3         0 IOS          2023-06-03T12:03:46.098Z TRUE            PHONE      2023-06-03T12:03:46Z     &lt;NA&gt;   &lt;NA&gt;           \n##  5  338086421  -843782498        4       30     233      277                3         0 IOS          2023-06-03T12:04:51.107Z TRUE            PHONE      2023-06-03T12:04:51.001Z &lt;NA&gt;   &lt;NA&gt;           \n##  6  337970326  -843935971        4       27     192      269                3         0 IOS          2023-06-03T12:05:56.167Z TRUE            PHONE      2023-06-03T12:05:55.999Z &lt;NA&gt;   &lt;NA&gt;           \n##  7  337805263  -843911732        4       27     180      282                3         0 IOS          2023-06-03T12:07:03.134Z TRUE            PHONE      2023-06-03T12:07:03.001Z &lt;NA&gt;   &lt;NA&gt;           \n##  8  337672676  -843890003        4       23     125      291                3         0 IOS          2023-06-03T12:08:08.143Z TRUE            PHONE      2023-06-03T12:08:08Z     &lt;NA&gt;   &lt;NA&gt;           \n##  9  337553940  -843786019        4       27     181      310                3         0 IOS          2023-06-03T12:09:16.194Z TRUE            PHONE      2023-06-03T12:09:16.001Z &lt;NA&gt;   &lt;NA&gt;           \n## 10  337437364  -843914546        4       26     207      295                3         0 IOS          2023-06-03T12:10:24.118Z TRUE            PHONE      2023-06-03T12:10:24Z     &lt;NA&gt;   &lt;NA&gt;           \n## # ℹ 2,278 more rows\n\nWe need to clean this data up a bit before using it:\n\nRight now the latitude and longitude coordinates are multiplied by 10,000,000, and they’re just numbers—R doesn’t know that they’re geographic coordinates. We need to divide them by \\(10^7\\) and then use them to create an {sf}-enabled geometry column using st_as_sf() from {sf}.\n\nRight now all the timestamps are in UTC (Greenwich Mean Time), not local time. Ordinarily converting UTC times to local time zones is really easy with with_tz() from {lubridate}:\n\n# One of the times from the trip\ntime_utc &lt;- ymd_hms(\"2023-06-03 14:27:23\")\ntime_utc\n## [1] \"2023-06-03 14:27:23 UTC\"\n\n# Convert it to US Central Daylight Time since we were in Alabama at the time\nwith_tz(time_utc, tzone = \"America/Chicago\")\n## [1] \"2023-06-03 09:27:23 CDT\"\n\nHowever, this was a big road trip and we covered every possible time zone in the continental US (technically not Pacific Daylight Time, but Arizona doesn’t use daylight saving time, so it’s the same time as Pacific time during the summer), so we can’t just convert all the timestamps to just one time zone. We need to figure out which US time zone each latitude/longitude point is in.\nI fretted a lot about the best way to do this! I eventually stumbled across a project that has shapefiles for all the world’s time zones and was about to start using that to look up which time zone each point fell inside, but then I discovered that there’s already an R package that does this (and it uses the same time zone boundary project behind the scenes): {lutz} (look up time zones). We just have to use tz_lookup() and it’ll identify the correct time zone for each point.\nBut there’s one more wrinkle! This is surprisingly difficult! You’d think we could just use something like mutate(local_time = with_tz(timestamp, tzone = \"whatever\") and make a local time column, but NOPE. R stores time zone details as an attribute for the entire column, so there’s no way to have a column with times across multiple time zones (see this issue here). Notice how the local_time column here is the same regardless of time zone:\n\nexample_thing &lt;- tribble(\n  ~timestamp, ~tz,\n  \"2023-06-01 00:00:00\", \"America/New_York\",\n  \"2023-06-01 01:00:00\", \"America/New_York\",\n  \"2023-06-01 00:00:00\", \"America/Phoenix\",\n  \"2023-06-01 01:00:00\", \"America/Phoenix\"\n) %&gt;% \n  mutate(timestamp = ymd_hms(timestamp))\n\n# The local times are all the same (New York) because columns can only have one time zone\nexample_thing %&gt;% \n  # Have to use rowwise() because with_tz() isn't vectorized (with good reason)\n  rowwise() %&gt;% \n  mutate(local_time = with_tz(timestamp, tzone = tz))\n## # A tibble: 4 × 3\n## # Rowwise: \n##   timestamp           tz               local_time         \n##   &lt;dttm&gt;              &lt;chr&gt;            &lt;dttm&gt;             \n## 1 2023-06-01 00:00:00 America/New_York 2023-05-31 20:00:00\n## 2 2023-06-01 01:00:00 America/New_York 2023-05-31 21:00:00\n## 3 2023-06-01 00:00:00 America/Phoenix  2023-05-31 20:00:00\n## 4 2023-06-01 01:00:00 America/Phoenix  2023-05-31 21:00:00\n\nSo as a workaround, we can group by time zone, which lets us work on a smaller subset of the data for each time zone, thus getting around the “one time zone per column” rule. However, that still doesn’t quite work, because when R is done with the grouping, it puts everything back together in one column, thus breaking the “one time zone per column” rule, and New York time takes over for the whole column again:\n\nexample_thing %&gt;% \n  group_by(tz) %&gt;% \n  mutate(local_time = with_tz(timestamp, tz))\n## # A tibble: 4 × 3\n## # Groups:   tz [2]\n##   timestamp           tz               local_time         \n##   &lt;dttm&gt;              &lt;chr&gt;            &lt;dttm&gt;             \n## 1 2023-06-01 00:00:00 America/New_York 2023-05-31 20:00:00\n## 2 2023-06-01 01:00:00 America/New_York 2023-05-31 21:00:00\n## 3 2023-06-01 00:00:00 America/Phoenix  2023-05-31 20:00:00\n## 4 2023-06-01 01:00:00 America/Phoenix  2023-05-31 21:00:00\n\nTo get around that, we can use force_tz() to convert each local time back to a fake version of UTC. Each timestamp all have the same time zone attribute (UTC), but the time will be US Eastern, Central, Mountain, or whatever. It’s the local time as if it were UTC. It works!\n\nexample_thing %&gt;% \n  group_by(tz) %&gt;% \n  mutate(local_time = force_tz(with_tz(timestamp, tz), \"UTC\"))\n## # A tibble: 4 × 3\n## # Groups:   tz [2]\n##   timestamp           tz               local_time         \n##   &lt;dttm&gt;              &lt;chr&gt;            &lt;dttm&gt;             \n## 1 2023-06-01 00:00:00 America/New_York 2023-05-31 20:00:00\n## 2 2023-06-01 01:00:00 America/New_York 2023-05-31 21:00:00\n## 3 2023-06-01 00:00:00 America/Phoenix  2023-05-31 17:00:00\n## 4 2023-06-01 01:00:00 America/Phoenix  2023-05-31 18:00:00\n\n\nFinally we’ll add some extra columns for the day, month, and year based on the local time. The file I’m working with here only contains a few days from our trip, but if you’re using your own data, you’ll likely have years of entries, so these columns help with filtering (i.e. you can use filter(year == 2017) to see your whole location history for that year.)\n\n\nall_locations &lt;- all_locations_raw %&gt;% \n  # Scale down the location data (divide any column that ends in E7 by 10000000)\n  mutate(across(ends_with(\"E7\"), ~ . / 1e7)) %&gt;% \n  # Create a geometry column with the coordinates\n  st_as_sf(coords = c(\"longitudeE7\", \"latitudeE7\"), crs = st_crs(\"EPSG:4326\")) %&gt;% \n  # Make a column with the time zone for each point\n  mutate(tz = tz_lookup(., method = \"accurate\")) %&gt;% \n  # Convert the timestamp to an actual UTC-based timestamp\n  mutate(timestamp = ymd_hms(timestamp, tz = \"UTC\")) %&gt;% \n  # Create a version of the timestamp in local time, but in UTC\n  group_by(tz) %&gt;% \n  mutate(timestamp_local = force_tz(with_tz(timestamp, tz), \"UTC\")) %&gt;% \n  ungroup() %&gt;% \n  # Add some helper columns for filtering, grouping, etc.\n  mutate(\n    year = year(timestamp_local),\n    month = month(timestamp_local),\n    day = day(timestamp_local)\n  ) %&gt;% \n  mutate(\n    day_month = strftime(timestamp_local, \"%B %e\"),\n    # With %e, there's a leading space for single-digit numbers, so we remove\n    # any double spaces and replace them with single spaces \n    # (e.g., \"June  3\" becomes \"June 3\")\n    day_month = str_replace(day_month, \"  \", \" \"),\n    day_month = fct_inorder(day_month)\n  )\nall_locations\n## Simple feature collection with 2288 features and 19 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -84.31 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 2,288 × 20\n##    accuracy velocity heading altitude verticalAccuracy deviceTag platformType serverTimestamp          batteryCharging formFactor timestamp           source deviceTimestamp       geometry tz               timestamp_local      year month   day day_month\n##  *    &lt;int&gt;    &lt;int&gt;   &lt;int&gt;    &lt;int&gt;            &lt;int&gt;     &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                    &lt;lgl&gt;           &lt;chr&gt;      &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt;              &lt;POINT [°]&gt; &lt;chr&gt;            &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;    \n##  1        4       30     205      271                3         0 IOS          2023-06-03T12:00:21.140Z TRUE            PHONE      2023-06-03 12:00:21 &lt;NA&gt;   &lt;NA&gt;            (-84.31 33.85) America/New_York 2023-06-03 08:00:21  2023     6     3 June 3   \n##  2       25       32     238      272                3         0 IOS          2023-06-03T12:01:41.112Z TRUE            PHONE      2023-06-03 12:01:41 &lt;NA&gt;   &lt;NA&gt;            (-84.33 33.84) America/New_York 2023-06-03 08:01:41  2023     6     3 June 3   \n##  3        4       29     228      270                3         0 IOS          2023-06-03T12:02:36.183Z TRUE            PHONE      2023-06-03 12:02:36 &lt;NA&gt;   &lt;NA&gt;            (-84.34 33.83) America/New_York 2023-06-03 08:02:36  2023     6     3 June 3   \n##  4        4       30     203      259                3         0 IOS          2023-06-03T12:03:46.098Z TRUE            PHONE      2023-06-03 12:03:46 &lt;NA&gt;   &lt;NA&gt;            (-84.36 33.82) America/New_York 2023-06-03 08:03:46  2023     6     3 June 3   \n##  5        4       30     233      277                3         0 IOS          2023-06-03T12:04:51.107Z TRUE            PHONE      2023-06-03 12:04:51 &lt;NA&gt;   &lt;NA&gt;            (-84.38 33.81) America/New_York 2023-06-03 08:04:51  2023     6     3 June 3   \n##  6        4       27     192      269                3         0 IOS          2023-06-03T12:05:56.167Z TRUE            PHONE      2023-06-03 12:05:55 &lt;NA&gt;   &lt;NA&gt;             (-84.39 33.8) America/New_York 2023-06-03 08:05:55  2023     6     3 June 3   \n##  7        4       27     180      282                3         0 IOS          2023-06-03T12:07:03.134Z TRUE            PHONE      2023-06-03 12:07:03 &lt;NA&gt;   &lt;NA&gt;            (-84.39 33.78) America/New_York 2023-06-03 08:07:03  2023     6     3 June 3   \n##  8        4       23     125      291                3         0 IOS          2023-06-03T12:08:08.143Z TRUE            PHONE      2023-06-03 12:08:08 &lt;NA&gt;   &lt;NA&gt;            (-84.39 33.77) America/New_York 2023-06-03 08:08:08  2023     6     3 June 3   \n##  9        4       27     181      310                3         0 IOS          2023-06-03T12:09:16.194Z TRUE            PHONE      2023-06-03 12:09:16 &lt;NA&gt;   &lt;NA&gt;            (-84.38 33.76) America/New_York 2023-06-03 08:09:16  2023     6     3 June 3   \n## 10        4       26     207      295                3         0 IOS          2023-06-03T12:10:24.118Z TRUE            PHONE      2023-06-03 12:10:24 &lt;NA&gt;   &lt;NA&gt;            (-84.39 33.74) America/New_York 2023-06-03 08:10:24  2023     6     3 June 3   \n## # ℹ 2,278 more rows\n\nOver the 4 days I’ve included in this JSON file, Google recorded my location 2,288 times(!!).\nBasic maps\nSince we converted the latitude and longitude coordinates into a geometry column, we can plot it really easily:\n\n# I used https://www.openstreetmap.org/export to make this window\ntrip_window &lt;- st_sfc(\n  st_point(c(-115.137, 24.567)),  # left (west), bottom (south)\n  st_point(c(-79.146, 37.475)),   # right (east), top (north)\n  crs = st_crs(\"EPSG:4326\")  # WGS 84\n) %&gt;% \n  st_coordinates()\ntrip_window\n##            X     Y\n## [1,] -115.14 24.57\n## [2,]  -79.15 37.48\n\nggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = all_locations) +\n  coord_sf(\n    xlim = trip_window[, \"X\"],\n    ylim = trip_window[, \"Y\"]) +\n  theme_roadtrip_map()\n\n\n\n\n\n\n\nThat looks like a solid thick black line, but it’s not. It’s actually 2,288 individual dots. We can confirm if we zoom in really close (this is the area around The Alamo in San Antonio, Texas):\n\nall_locations %&gt;% \n  leaflet() %&gt;% \n  addTiles() %&gt;% \n  addCircles() %&gt;% \n  # I got these coordinates from Google Maps\n  setView(lat = 29.425733271447523, lng = -98.48627553904525, zoom = 18)\n\n\n\n\n\n \nWe can convert all these points into a single path (or a LINESTRING simple geographic feature instead of lots of POINTs) so that they can connect, using a combination of st_combine() to concatenate all the points and st_cast() to switch their format from a bunch of points to an official LINESTRING:\n\nbig_long_combined_route &lt;- all_locations$geometry %&gt;% \n  st_combine() %&gt;% \n  st_cast(\"LINESTRING\")\nbig_long_combined_route\n## Geometry set for 1 feature \n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -84.31 ymax: 35.53\n## Geodetic CRS:  WGS 84\n\nNow instead of having two thousand+ points, it’s just one single LINESTRING feature. It shows up as a line now:\n\nggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = big_long_combined_route) +\n  coord_sf(\n    xlim = trip_window[, \"X\"],\n    ylim = trip_window[, \"Y\"]) +\n  theme_roadtrip_map()\n\n\n\n\n\n\n\nWe can confirm if we zoom in around the Alamo again too:\n\nbig_long_combined_route %&gt;% \n  leaflet() %&gt;% \n  addTiles() %&gt;% \n  addPolylines() %&gt;% \n  setView(lat = 29.425733271447523, lng = -98.48627553904525, zoom = 18)\n\n\n\n\n\n \nThe big_long_combined_route object we just made here isn’t a data frame anymore—it’s a list instead. It still works with geom_sf(), but because we collapsed all the points into one line, we lots lots of detail, like timestamps. We can keep some of those details if we use group_by() to separate lines per day (or hour or whatever grouping variable we want). For instance, earlier we created a column called day. Let’s make a separate linestring for each day using group_by() and nest() and map():\n\n# Combine all the points in the day into a connected linestring\ndaily_routes &lt;- all_locations %&gt;% \n  group_by(day_month) %&gt;% \n  nest() %&gt;% \n  mutate(path = map(data, ~st_cast(st_combine(.), \"LINESTRING\"))) %&gt;% \n  unnest(path) %&gt;% \n  st_set_geometry(\"path\")\ndaily_routes\n## Simple feature collection with 4 features and 2 fields\n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -84.31 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 4 × 3\n## # Groups:   day_month [4]\n##   day_month data                                                                                                 path\n##   &lt;fct&gt;     &lt;list&gt;                                                                                   &lt;LINESTRING [°]&gt;\n## 1 June 3    &lt;sf [521 × 19]&gt; (-84.31 33.85, -84.33 33.84, -84.34 33.83, -84.36 33.82, -84.38 33.81, -84.39 33.8, -8...\n## 2 June 4    &lt;sf [596 × 19]&gt; (-89.96 30.05, -89.96 30.05, -89.96 30.05, -89.96 30.05, -89.96 30.05, -89.96 30.05, -...\n## 3 June 5    &lt;sf [509 × 19]&gt; (-98.44 29.4, -98.44 29.4, -98.44 29.4, -98.44 29.4, -98.44 29.4, -98.44 29.4, -98.44 ...\n## 4 June 6    &lt;sf [662 × 19]&gt; (-104.2 32.4, -104.2 32.4, -104.2 32.4, -104.2 32.4, -104.2 32.4, -104.2 32.4, -104.2 ...\n\nNow we have a data frame with a row per day, and a corresponding path per day too:\n\nggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = daily_routes, aes(color = day_month),\n    linewidth = 1) +\n  geom_label_repel(\n    data = daily_routes,\n    aes(label = day_month, fill = day_month, geometry = path),\n    stat = \"sf_coordinates\", seed = 12345,\n    color = \"white\", size = 3, segment.color = \"grey30\", \n    min.segment.length = 0, box.padding = 1,\n    show.legend = FALSE, family = \"Overpass ExtraBold\"\n  ) +\n  scale_color_manual(values = clrs[c(2, 4, 7, 9)], name = NULL, guide = \"none\") +\n  scale_fill_manual(values = clrs[c(2, 4, 7, 9)], name = NULL, guide = \"none\") +\n  coord_sf(\n    xlim = trip_window[, \"X\"],\n    ylim = trip_window[, \"Y\"]) +\n  theme_roadtrip_map()"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#semantic-location-history-1",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#semantic-location-history-1",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Semantic location history",
    "text": "Semantic location history\nWorking with Records.json is great and fairly straightforward (once we handle all the weird data cleaning issues and time zones!), but we can do even cooler things with the more detailed semantic location history data. As seen earlier, this data is far more complex, with all sorts of nested entries and predicted probabilities of modes of transportation or location stops, so we can’t use the simplifyVector to get a basic data frame. Instead, we need to pull out each of the elements we’re interested in and build our own data frame.\nAdditionally, the location history file includes both placeVisits and activitySegments in the same file, so to make life a little easier, we can filter the data after we read the file to only keep one of the types of events using map()\n\nplaceVisits\n\nplace_visits_raw &lt;- read_json(\n  \"data/Semantic Location History/2023/2023_JUNE_truncated.json\", \n  simplifyVector = FALSE\n) %&gt;% \n  # Extract the timelineObjects JSON element\n  pluck(\"timelineObjects\") %&gt;%\n  # Filter the list to only keep placeVisits\n  # { More verbose function-based approach: map(~ .x[[\"placeVisit\"]]) }\n  # Neat selection-based approach with just the name!\n  map(\"placeVisit\") %&gt;% \n  # Discard all the empty elements (i.e. the activitySegments)\n  compact()\n\nFor this post, I’m interested in looking at all the places we stopped for gas and bathroom breaks along the way. I’m not interested in hotel stops or tourist stops. I want to know how long we typically stopped for breaks. There are 24 entries in this list…\n\nlength(place_visits_raw)\n## [1] 24\n\n…but not all of them are gas or bathroom breaks, so I manually looked through the data and copied the location IDs of the stops that weren’t gas stops.\n\nnot_driving_stops &lt;- c(\n  \"ChIJnwDSTcsDnogRLyt_lqVprLY\",  # Hotel in New Orleans, LA\n  \"ChIJgcNDAhKmIIYRRA4mio_7VgE\",  # Parking in the French Quarter\n  \"ChIJv30_Xw-mIIYRpt26QbLBh58\",  # Louis Armstrong Park\n  \"ChIJ59n6fW8dnogRWi-5N6olcyU\",  # Chalmette Battlefield\n  \"ChIJ_7z4c1_2XIYR6b9p0NvEiVE\",  # Hotel in San Antonio, TX\n  \"ChIJX4k2TVVfXIYRIsTnhA-P-Rc\",  # The Alamo\n  \"ChIJAdu5Qad544YRhyJT8qzimi4\",  # Hotel in Carlsbad, NM\n  \"ChIJW9e4xBN544YRvbI7vfc91G4\",  # Carlsbad Caverns\n  \"ChIJERiZZMWOLYcRQbo78w80s34\"   # Hotel in Flagstaff, AZ\n)\n\nCurrently, place_visits_raw is a huge nested list with the complete place information from Google. All we care about is a small subset of that data, so we can take the list, extract the parts we need, and build a data frame with map() %&gt;% list_rbind() (the recommended replacement for {purrr}’s now-superseded map_df()). Like we did with Records.json, we’ll also figure out the time zone for each point and make all the timestamps be local.\n\n# Computer friendly timezones like America/New_York work for computers, but I\n# want to sometimes show them as US-standard abbreviations like EDT (Eastern\n# Daylight Time), so here's a little lookup table we can use to join to bigger\n# datasets for better abbreviations\ntz_abbreviations &lt;- tribble(\n  ~tz,                ~tz_abb,\n  \"America/New_York\", \"EDT\",\n  \"America/Chicago\",  \"CDT\",\n  \"America/Denver\",   \"MDT\",\n  \"America/Phoenix\",  \"MST\"\n)\n\nplace_visits &lt;- place_visits_raw %&gt;% \n  # Extract parts of the nested list\n  map(~{\n    tibble(\n      id = .x$location$placeId,\n      latitudeE7 = .x$location$latitudeE7 / 1e7,\n      longitudeE7 = .x$location$longitudeE7 / 1e7,\n      name = .x$location$name,\n      address = .x$location$address,\n      startTimestamp = ymd_hms(.x$duration$startTimestamp, tz = \"UTC\"),\n      endTimestamp = ymd_hms(.x$duration$endTimestamp, tz = \"UTC\")\n    )\n  }) %&gt;% \n  list_rbind() %&gt;% \n  # Calculate the duration of the stop\n  mutate(duration = endTimestamp - startTimestamp) %&gt;% \n  # Make an indicator for if the stop was a gas or bathroom break\n  mutate(driving_stop = !(id %in% not_driving_stops)) %&gt;% \n  # Make a geometry column\n  st_as_sf(coords = c(\"longitudeE7\", \"latitudeE7\"), crs = st_crs(\"EPSG:4326\")) %&gt;% \n  # Make a column with the time zone for each point\n  mutate(tz = tz_lookup(., method = \"accurate\")) %&gt;% \n  # Create a version of the timestamp in local time, but in UTC\n  group_by(tz) %&gt;% \n  mutate(\n    startTimestamp_local = force_tz(with_tz(startTimestamp, tz), \"UTC\"),\n    endTimestamp_local = force_tz(with_tz(endTimestamp, tz), \"UTC\")\n  ) %&gt;% \n  ungroup() %&gt;% \n  # Add a column for direction\n  # In the real data, I have values for \"There\" (the trip from Atlanta to Utah)\n  # and \"Back again\" (the trip from Utah to Atlanta)\n  mutate(direction = \"There\") %&gt;% \n  # Add some helper columns for filtering, grouping, etc.\n  mutate(\n    year = year(startTimestamp_local),\n    month = month(startTimestamp_local),\n    day = day(startTimestamp_local)\n  ) %&gt;% \n  mutate(\n    day_month = strftime(startTimestamp_local, \"%B %e\"),\n    # With %e, there's a leading space for single-digit numbers, so we remove\n    # any double spaces and replace them with single spaces \n    # (e.g., \"June  3\" becomes \"June 3\")\n    day_month = str_replace(day_month, \"  \", \" \"),\n    day_month = fct_inorder(day_month)\n  ) %&gt;% \n  # Bring in abbreviated time zones\n  left_join(tz_abbreviations, by = join_by(tz))\nplace_visits\n## Simple feature collection with 24 features and 16 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -86.34 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 24 × 17\n##    id                          name                            address                                            startTimestamp      endTimestamp        duration    driving_stop       geometry tz              startTimestamp_local endTimestamp_local  direction  year month   day day_month tz_abb\n##    &lt;chr&gt;                       &lt;chr&gt;                           &lt;chr&gt;                                              &lt;dttm&gt;              &lt;dttm&gt;              &lt;drtn&gt;      &lt;lgl&gt;           &lt;POINT [°]&gt; &lt;chr&gt;           &lt;dttm&gt;               &lt;dttm&gt;              &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt; \n##  1 ChIJQ0NgzsqAjogRISqopW4DZMc Chevron                         1030 W South Blvd, Montgomery, AL 36105, USA       2023-06-03 14:27:23 2023-06-03 14:50:24  23.02 mins TRUE         (-86.34 32.33) America/Chicago 2023-06-03 09:27:23  2023-06-03 09:50:24 There      2023     6     3 June 3    CDT   \n##  2 ChIJMU2weFNKmogRNvJ9RoXN_Vk Walmart Supercenter             5245 Rangeline Service Rd, Mobile, AL 36619, USA   2023-06-03 17:12:34 2023-06-03 17:59:38  47.07 mins TRUE         (-88.16 30.59) America/Chicago 2023-06-03 12:12:34  2023-06-03 12:59:38 There      2023     6     3 June 3    CDT   \n##  3 ChIJnwDSTcsDnogRLyt_lqVprLY Comfort Suites New Orleans East 7051 Bullard Ave, New Orleans, LA 70128, USA       2023-06-03 19:45:22 2023-06-03 20:52:17  66.90 mins FALSE        (-89.96 30.05) America/Chicago 2023-06-03 14:45:22  2023-06-03 15:52:17 There      2023     6     3 June 3    CDT   \n##  4 ChIJgcNDAhKmIIYRRA4mio_7VgE P149 - Chartres St Garage       537 Chartres St, New Orleans, LA 70130, USA        2023-06-03 21:08:30 2023-06-03 22:31:08  82.64 mins FALSE        (-90.06 29.96) America/Chicago 2023-06-03 16:08:30  2023-06-03 17:31:08 There      2023     6     3 June 3    CDT   \n##  5 ChIJv30_Xw-mIIYRpt26QbLBh58 Louis Armstrong Park            701 N Rampart St, New Orleans, LA 70116, USA       2023-06-03 22:44:22 2023-06-03 22:59:51  15.48 mins FALSE        (-90.07 29.96) America/Chicago 2023-06-03 17:44:22  2023-06-03 17:59:51 There      2023     6     3 June 3    CDT   \n##  6 ChIJgcNDAhKmIIYRRA4mio_7VgE P149 - Chartres St Garage       537 Chartres St, New Orleans, LA 70130, USA        2023-06-03 23:10:10 2023-06-03 23:19:40   9.50 mins FALSE        (-90.06 29.96) America/Chicago 2023-06-03 18:10:10  2023-06-03 18:19:40 There      2023     6     3 June 3    CDT   \n##  7 ChIJnwDSTcsDnogRLyt_lqVprLY Comfort Suites New Orleans East 7051 Bullard Ave, New Orleans, LA 70128, USA       2023-06-03 23:37:50 2023-06-04 14:01:02 863.20 mins FALSE        (-89.96 30.05) America/Chicago 2023-06-03 18:37:50  2023-06-04 09:01:02 There      2023     6     3 June 3    CDT   \n##  8 ChIJ59n6fW8dnogRWi-5N6olcyU Chalmette Battlefield           1 Battlefield Rd, Chalmette, LA 70043, USA         2023-06-04 14:24:06 2023-06-04 15:54:20  90.25 mins FALSE        (-89.99 29.94) America/Chicago 2023-06-04 09:24:06  2023-06-04 10:54:20 There      2023     6     4 June 4    CDT   \n##  9 ChIJTZPVqVh9JIYRx10_jt-c4LE Exxon                           2939 Grand Point Hwy, Breaux Bridge, LA 70517, USA 2023-06-04 17:54:09 2023-06-04 18:31:35  37.45 mins TRUE         (-91.83 30.32) America/Chicago 2023-06-04 12:54:09  2023-06-04 13:31:35 There      2023     6     4 June 4    CDT   \n## 10 ChIJFWLuzkT3O4YRBX2lqNQDF4w Exxon                           1410 Gum Cove Rd, Vinton, LA 70668, USA            2023-06-04 20:02:26 2023-06-04 20:26:48  24.37 mins TRUE         (-93.57 30.19) America/Chicago 2023-06-04 15:02:26  2023-06-04 15:26:48 There      2023     6     4 June 4    CDT   \n## # ℹ 14 more rows\n\n\nactivitySegments\nWe’ll go through the same data loading and cleaning process for the activitySegments. It’s a little more complicated because we have pairs of timestamps and locations (start/end times, start/end locations), and the times and locations can be in different time zones, so we need to do the group_by(tz) trick twice with group_by(tz_start) and group_by(tz_end). We also need to get two geometry columns, which requires a little data trickery, as you’ll see below:\n\nactivity_segments_raw &lt;- read_json(\n  \"data/Semantic Location History/2023/2023_JUNE_truncated.json\", \n  simplifyVector = FALSE\n) %&gt;% \n  # Extract the timelineObjects JSON element\n  pluck(\"timelineObjects\") %&gt;%\n  # Filter the list to only keep activitySegments\n  map(\"activitySegment\") %&gt;%\n  # Discard all the empty elements (i.e. the placeVisits)\n  compact()\n\nactivity_segments_not_clean &lt;- activity_segments_raw %&gt;% \n  # Extract parts of the nested list\n  map(~{\n    tibble(\n      distance_m = .x$distance,\n      activity_type = .x$activityType,\n      start_latitudeE7 = .x$startLocation$latitudeE7 / 1e7,\n      start_longitudeE7 = .x$startLocation$longitudeE7 / 1e7,\n      end_latitudeE7 = .x$endLocation$latitudeE7 / 1e7,\n      end_longitudeE7 = .x$endLocation$longitudeE7 / 1e7,\n      startTimestamp = ymd_hms(.x$duration$startTimestamp, tz = \"UTC\"),\n      endTimestamp = ymd_hms(.x$duration$endTimestamp, tz = \"UTC\")\n    )\n  }) %&gt;% \n  list_rbind()\n\n# ↑ that needs to be a separate data frame so that we can refer to it to make a\n# geometry column for the end latitude/longitude\nactivity_segments &lt;- activity_segments_not_clean %&gt;% \n  # Calculate the duration and distance and speed of the segment\n  mutate(duration = endTimestamp - startTimestamp) %&gt;% \n  mutate(distance_miles = meters_to_miles(distance_m)) %&gt;% \n  mutate(\n    hours = as.numeric(duration) / 60,\n    avg_mph = distance_miles / hours\n  ) %&gt;% \n  # Make two geometry columns\n  st_as_sf(coords = c(\"start_longitudeE7\", \"start_latitudeE7\"), crs = st_crs(\"EPSG:4326\")) %&gt;% \n  rename(\"geometry_start\" = \"geometry\") %&gt;% \n  mutate(geometry_end = st_geometry(\n    st_as_sf(\n      activity_segments_not_clean, \n      coords = c(\"end_longitudeE7\", \"end_latitudeE7\"), \n      crs = st_crs(\"EPSG:4326\"))\n    )\n  ) %&gt;% \n  select(-end_longitudeE7, -end_latitudeE7) %&gt;% \n  # Make a column with the time zone for each point\n  mutate(tz_start = tz_lookup(geometry_start, method = \"accurate\")) %&gt;% \n  mutate(tz_end = tz_lookup(geometry_end, method = \"accurate\")) %&gt;% \n  # Create a version of the timestamps in local time, but in UTC\n  group_by(tz_start) %&gt;% \n  mutate(startTimestamp_local = force_tz(with_tz(startTimestamp, tz_start), \"UTC\")) %&gt;% \n  ungroup() %&gt;% \n  group_by(tz_end) %&gt;% \n  mutate(endTimestamp_local = force_tz(with_tz(endTimestamp, tz_end), \"UTC\")) %&gt;% \n  ungroup() %&gt;% \n  # Add some helper columns for filtering, grouping, etc.\n  mutate(\n    year = year(startTimestamp_local),\n    month = month(startTimestamp_local),\n    day = day(startTimestamp_local)\n  ) %&gt;% \n  mutate(\n    day_month = strftime(startTimestamp_local, \"%B %e\"),\n    # With %e, there's a leading space for single-digit numbers, so we remove\n    # any double spaces and replace them with single spaces \n    # (e.g., \"June  3\" becomes \"June 3\")\n    day_month = str_replace(day_month, \"  \", \" \"),\n    day_month = fct_inorder(day_month)\n  ) %&gt;% \n  # Bring in abbreviated time zones for both the start and end time zones\n  left_join(\n    rename(tz_abbreviations, \"tz_start_abb\" = \"tz_abb\"), \n    by = join_by(tz_start == tz)\n  ) %&gt;% \n  left_join(\n    rename(tz_abbreviations, \"tz_end_abb\" = \"tz_abb\"),\n    by = join_by(tz_end == tz)\n  ) %&gt;% \n  # Create an id column so we can better reference individual activities \n  # Make it a character so it can combine with the place visit id column\n  mutate(id = as.character(1:n()))\nactivity_segments\n## Simple feature collection with 23 features and 19 fields\n## Active geometry column: geometry_start\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -108.7 ymin: 29.4 xmax: -86.34 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 23 × 21\n##    distance_m activity_type        startTimestamp      endTimestamp        duration    distance_miles hours avg_mph geometry_start   geometry_end tz_start        tz_end          startTimestamp_local endTimestamp_local   year month   day day_month tz_start_abb tz_end_abb id   \n##  *      &lt;int&gt; &lt;chr&gt;                &lt;dttm&gt;              &lt;dttm&gt;              &lt;drtn&gt;               &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;POINT [°]&gt;    &lt;POINT [°]&gt; &lt;chr&gt;           &lt;chr&gt;           &lt;dttm&gt;               &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;\n##  1     272317 IN_PASSENGER_VEHICLE 2023-06-03 14:50:24 2023-06-03 17:12:34 142.17 mins        169.    2.37    71.4  (-86.34 32.33) (-88.16 30.59) America/Chicago America/Chicago 2023-06-03 09:50:24  2023-06-03 12:12:34  2023     6     3 June 3    CDT          CDT        1    \n##  2     195400 IN_PASSENGER_VEHICLE 2023-06-03 17:59:38 2023-06-03 19:45:22 105.75 mins        121.    1.76    68.9   (-88.16 30.6) (-89.96 30.05) America/Chicago America/Chicago 2023-06-03 12:59:38  2023-06-03 14:45:22  2023     6     3 June 3    CDT          CDT        2    \n##  3      15368 IN_PASSENGER_VEHICLE 2023-06-03 20:52:17 2023-06-03 21:08:30  16.22 mins          9.55  0.270   35.3  (-89.96 30.05) (-90.07 29.96) America/Chicago America/Chicago 2023-06-03 15:52:17  2023-06-03 16:08:30  2023     6     3 June 3    CDT          CDT        3    \n##  4       1026 WALKING              2023-06-03 22:31:08 2023-06-03 22:44:22  13.23 mins          0.638 0.221    2.89 (-90.06 29.96) (-90.07 29.96) America/Chicago America/Chicago 2023-06-03 17:31:08  2023-06-03 17:44:22  2023     6     3 June 3    CDT          CDT        4    \n##  5       1280 WALKING              2023-06-03 22:59:51 2023-06-03 23:10:10  10.32 mins          0.795 0.172    4.63 (-90.07 29.96) (-90.06 29.96) America/Chicago America/Chicago 2023-06-03 17:59:51  2023-06-03 18:10:10  2023     6     3 June 3    CDT          CDT        5    \n##  6      14611 IN_PASSENGER_VEHICLE 2023-06-03 23:19:40 2023-06-03 23:37:50  18.17 mins          9.08  0.303   30.0  (-90.06 29.96) (-89.96 30.05) America/Chicago America/Chicago 2023-06-03 18:19:40  2023-06-03 18:37:50  2023     6     3 June 3    CDT          CDT        6    \n##  7      14258 IN_PASSENGER_VEHICLE 2023-06-04 14:01:02 2023-06-04 14:24:06  23.07 mins          8.86  0.384   23.0  (-89.96 30.05) (-89.99 29.94) America/Chicago America/Chicago 2023-06-04 09:01:02  2023-06-04 09:24:06  2023     6     4 June 4    CDT          CDT        7    \n##  8     199631 IN_PASSENGER_VEHICLE 2023-06-04 15:54:20 2023-06-04 17:54:09 119.80 mins        124.    2.00    62.1  (-89.99 29.94) (-91.83 30.32) America/Chicago America/Chicago 2023-06-04 10:54:20  2023-06-04 12:54:09  2023     6     4 June 4    CDT          CDT        8    \n##  9     169541 IN_PASSENGER_VEHICLE 2023-06-04 18:31:35 2023-06-04 20:02:26  90.83 mins        105.    1.51    69.6  (-91.83 30.32) (-93.57 30.19) America/Chicago America/Chicago 2023-06-04 13:31:35  2023-06-04 15:02:26  2023     6     4 June 4    CDT          CDT        9    \n## 10      71377 IN_PASSENGER_VEHICLE 2023-06-04 20:26:48 2023-06-04 21:06:44  39.93 mins         44.4   0.666   66.6  (-93.57 30.19) (-94.21 29.99) America/Chicago America/Chicago 2023-06-04 15:26:48  2023-06-04 16:06:44  2023     6     4 June 4    CDT          CDT        10   \n## # ℹ 13 more rows\n\nBoth combined\nAll these placeVisit and activitySegment entries were originally in the same JSON file since they actually fit together nicely—activity segments lead to place visits, which are then followed by more activity segments (i.e. you drive to a place, do stuff at that place, and drive to a different place, and so on). Because the two event types are structured so differently, we had to split them up and load and clean them separately. But it can be helpful to put them back together so there’s a consistent timeline—it’ll help with making plots below, and it creates a neat log of the whole trip.\nWe’ll use bind_rows() to combine the two and then sort. It’s kind of an ugly dataset, with lots of missing data in every other row since each type of entry has slightly different columns in it, but some columns are consistent throughout, like duration and the different timestamps, so it’ll be helpful.\n\nall_stops_activities &lt;- bind_rows(\n  list(visit = place_visits, segment = activity_segments),\n  .id = \"type\"\n) %&gt;% \n  arrange(startTimestamp)\nall_stops_activities\n## Simple feature collection with 47 features and 26 fields\n## Active geometry column: geometry (with 23 geometries empty)\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -111.6 ymin: 29.4 xmax: -86.34 ymax: 35.53\n## Geodetic CRS:  WGS 84\n## # A tibble: 47 × 29\n##    type    id             name  address startTimestamp      endTimestamp        duration driving_stop       geometry tz              startTimestamp_local endTimestamp_local  direction  year month   day day_month tz_abb distance_m activity_type  distance_miles  hours avg_mph geometry_start\n##    &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;              &lt;dttm&gt;              &lt;drtn&gt;   &lt;lgl&gt;           &lt;POINT [°]&gt; &lt;chr&gt;           &lt;dttm&gt;               &lt;dttm&gt;              &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;POINT [°]&gt;\n##  1 visit   ChIJQ0NgzsqAj… Chev… 1030 W… 2023-06-03 14:27:23 2023-06-03 14:50:24  23.02 … TRUE         (-86.34 32.33) America/Chicago 2023-06-03 09:27:23  2023-06-03 09:50:24 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n##  2 segment 1              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 14:50:24 2023-06-03 17:12:34 142.17 … NA                    EMPTY &lt;NA&gt;            2023-06-03 09:50:24  2023-06-03 12:12:34 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;       272317 IN_PASSENGER_…        169.     2.37    71.4  (-86.34 32.33)\n##  3 visit   ChIJMU2weFNKm… Walm… 5245 R… 2023-06-03 17:12:34 2023-06-03 17:59:38  47.07 … TRUE         (-88.16 30.59) America/Chicago 2023-06-03 12:12:34  2023-06-03 12:59:38 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n##  4 segment 2              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 17:59:38 2023-06-03 19:45:22 105.75 … NA                    EMPTY &lt;NA&gt;            2023-06-03 12:59:38  2023-06-03 14:45:22 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;       195400 IN_PASSENGER_…        121.     1.76    68.9   (-88.16 30.6)\n##  5 visit   ChIJnwDSTcsDn… Comf… 7051 B… 2023-06-03 19:45:22 2023-06-03 20:52:17  66.90 … FALSE        (-89.96 30.05) America/Chicago 2023-06-03 14:45:22  2023-06-03 15:52:17 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n##  6 segment 3              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 20:52:17 2023-06-03 21:08:30  16.22 … NA                    EMPTY &lt;NA&gt;            2023-06-03 15:52:17  2023-06-03 16:08:30 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;        15368 IN_PASSENGER_…          9.55   0.270   35.3  (-89.96 30.05)\n##  7 visit   ChIJgcNDAhKmI… P149… 537 Ch… 2023-06-03 21:08:30 2023-06-03 22:31:08  82.64 … FALSE        (-90.06 29.96) America/Chicago 2023-06-03 16:08:30  2023-06-03 17:31:08 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n##  8 segment 4              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 22:31:08 2023-06-03 22:44:22  13.23 … NA                    EMPTY &lt;NA&gt;            2023-06-03 17:31:08  2023-06-03 17:44:22 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;         1026 WALKING                 0.638  0.221    2.89 (-90.06 29.96)\n##  9 visit   ChIJv30_Xw-mI… Loui… 701 N … 2023-06-03 22:44:22 2023-06-03 22:59:51  15.48 … FALSE        (-90.07 29.96) America/Chicago 2023-06-03 17:44:22  2023-06-03 17:59:51 There      2023     6     3 June 3    CDT            NA &lt;NA&gt;                   NA     NA       NA             EMPTY\n## 10 segment 5              &lt;NA&gt;  &lt;NA&gt;    2023-06-03 22:59:51 2023-06-03 23:10:10  10.32 … NA                    EMPTY &lt;NA&gt;            2023-06-03 17:59:51  2023-06-03 18:10:10 &lt;NA&gt;       2023     6     3 June 3    &lt;NA&gt;         1280 WALKING                 0.795  0.172    4.63 (-90.07 29.96)\n## # ℹ 37 more rows\n## # ℹ 5 more variables: geometry_end &lt;POINT [°]&gt;, tz_start &lt;chr&gt;, tz_end &lt;chr&gt;, tz_start_abb &lt;chr&gt;, tz_end_abb &lt;chr&gt;"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#maps",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#maps",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Maps",
    "text": "Maps\n\nCode# Find all the states that daily_routes crosses\nstates_crossed_through &lt;- st_intersection(\n  st_transform(lower_48, st_crs(daily_routes)),\n  daily_routes\n)\n## Warning: attribute variables are assumed to be spatially constant throughout all geometries\n\n# Create a column that flags if the state is crossed through\nlower_48_highlighted &lt;- lower_48 %&gt;% \n  mutate(visited = NAME %in% unique(states_crossed_through$NAME))\n\n\nIn total, we crossed through 17 different states over the course of our big circular 13-day drive. We made 76 different stops along the way, all marked with points in Figure 1:\n\nCode# Make a map!\nggplot() +\n  geom_sf(data = lower_48_highlighted, aes(fill = visited)) +\n  scale_fill_manual(values = c(\"grey98\", \"grey90\"), guide = \"none\") +\n  geom_sf(data = daily_routes, linewidth = 0.75) +\n  geom_sf(data = place_visits, size = 1.5) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  labs(caption = \"Each point is a stop\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip_map()\n\n\n\n\n\n\nFigure 1: Basic map of our mega road trip\n\n\n\n\nIn Figure 2 we can color each leg of the trip a different color to see how the driving was divided up over time:\n\nCode# Use the same colors for each direction\ncolors_there &lt;- rcartocolor::carto_pal(7, \"SunsetDark\")[c(1, 3, 2, 4, 6, 5, 7)]\ncolors_back_again &lt;- rcartocolor::carto_pal(6, \"SunsetDark\")[c(1, 3, 2, 4, 6, 5)]\n\n# CARTO's Sunset Dark is neat, but the first yellow color is a little too light\n# to see comfortably, so we replace it with the yellow from the CARTO Prism\n# palette\ncolors_there[1] &lt;- clrs[6]\ncolors_back_again[1] &lt;- clrs[6]\n\n# Make a map!\nggplot() +\n  geom_sf(data = lower_48_highlighted, aes(fill = visited)) +\n  scale_fill_manual(values = c(\"grey98\", \"grey90\"), guide = \"none\") +\n  # Reset fill scale so that the labels can be filled\n  new_scale_fill() +\n  geom_sf(data = daily_routes, aes(color = day_month), linewidth = 0.75) +\n  scale_color_manual(values = c(colors_there, colors_back_again), guide = \"none\") +\n  geom_sf(data = place_visits, aes(color = day_month), size = 1.5) +\n  geom_label_repel(\n    data = daily_routes,\n    aes(label = day_month, fill = day_month, geometry = path),\n    stat = \"sf_coordinates\", seed = 1234,\n    size = 3, segment.color = \"grey30\", min.segment.length = 0,\n    show.legend = FALSE, family = \"Overpass ExtraBold\", color = \"white\"\n  ) +\n  scale_fill_manual(values = c(colors_there, colors_back_again), guide = \"none\") +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  labs(caption = \"Each point is a stop\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip_map()\n\n\n\n\n\n\nFigure 2: Map of our mega road trip, by day"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#distance-and-time",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#distance-and-time",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Distance and time",
    "text": "Distance and time\nTable 1 answers a bunch of questions about distance and time:\n\n\nHow many miles did we drive each day? Roughly 500ish miles per day. The way out was longer than the way back (2,832 miles there vs. 2,390 miles back). In total we drove 5,222 miles (!!!). This is shockingly close to the 5,260 miles that OpenStreetMap mapped out for me in this previous blog post, even with some changes to the route (i.e. we didn’t end up going through Minnesota and went through Nebraska instead)\n\nHow long did we spend driving each day? Roughly 8.5 hours per day. The way out was 5 hours longer than the way back (though that’s a little inflated because June 8 doesn’t really count, as we were just driving around Capitol Reef National Park). In total we drove for 91 hours and 47 minutes,\n\nWhat was the earliest time we started each day and the latest time we arrived each evening? The earliest departure was 7:30 AM when we left Atlanta on June 3; the latest arrivals were when we got to Sioux Falls, South Dakota at 10:42 PM on June 22 and when we got to Flagstaff, Arizona on June 6 at 10:35 PM.\n\n\nCodetime_distance_day &lt;- activity_segments %&gt;% \n  st_drop_geometry() %&gt;% \n  filter(activity_type == \"IN_PASSENGER_VEHICLE\") %&gt;% \n  group_by(direction, day_month) %&gt;% \n  summarize(\n    total_distance = sum(distance_miles),\n    total_time = sum(duration),\n    start_time = min(startTimestamp_local),\n    start_tz = tz_start_abb[which.min(startTimestamp_local)],\n    end_time = max(endTimestamp_local),\n    end_tz = tz_end_abb[which.max(endTimestamp_local)]\n  ) %&gt;% \n  mutate(\n    nice_start_time = paste(strftime(start_time, format = \"%I:%M %p\", tz = \"UTC\"), start_tz),\n    nice_end_time = paste(strftime(end_time, format = \"%I:%M %p\", tz = \"UTC\"), end_tz)\n  ) %&gt;% \n  # Remove leading zeros from hour\n  mutate(across(c(nice_start_time, nice_end_time), ~str_replace(.x, \"^0\", \"\")))\n\ntime_distance_day %&gt;% \n  select(direction, day_month, total_distance, total_time, nice_start_time, nice_end_time) %&gt;% \n  group_by(direction) %&gt;%\n  gt() %&gt;% \n  summary_rows(\n    groups = everything(),\n    columns = c(total_distance, total_time),\n    fns = list(Subtotal = ~ sum(.)),\n    fmt = list(\n      ~fmt_number(., columns = total_distance, decimals = 0),\n      ~fmt(., columns = total_time, fns = fmt_difftime)\n    ),\n  ) %&gt;% \n  grand_summary_rows(\n    columns = c(total_distance, total_time),\n    fns = list(Total = ~ sum(.)),\n    fmt = list(\n      ~fmt_number(., columns = total_distance, decimals = 0),\n      ~fmt(., columns = total_time, fns = fmt_difftime)\n    ),\n    missing_text = \"\"\n  ) %&gt;% \n  tab_footnote(\n    footnote = \"This wasn't really a travel day; we spent the day hiking around Capitol Reef National Park and hanging out at my aunt's cabin in Grover, Utah.\",\n    locations = cells_body(\n      columns = day_month,\n      rows = day_month == \"June 8\"\n    )\n  ) %&gt;% \n  cols_label(\n    day_month = \"Day\",\n    total_distance = \"Miles\",\n    total_time = \"Driving time\",\n    nice_start_time = \"Start time\",\n    nice_end_time = \"End time\"\n  ) %&gt;% \n  cols_align(\n    columns = total_time,\n    align = \"left\"\n  ) %&gt;% \n  fmt_number(columns = total_distance, decimals = 0) %&gt;% \n  fmt(columns = total_time, fns = fmt_difftime) %&gt;% \n  tab_style(\n    locations = cells_column_labels(),\n    style = list(\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_row_groups(),\n    style = list(\n      cell_text(weight = \"bold\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = list(cells_summary(), cells_stub_summary()),\n    style = list(\n      cell_fill(color = \"grey95\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = list(\n      cells_grand_summary(), cells_stub_grand_summary(), \n      cells_column_labels(), cells_stubhead()),\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"grey80\")\n    )\n  ) %&gt;% \n  opt_horizontal_padding(scale = 2) %&gt;% \n  opt_table_font(font = \"Jost\") %&gt;% \n  tab_options(\n    row_group.as_column = TRUE,\n    footnotes.marks = \"standard\"\n  )\n\n\nTable 1: Daily distance and time details\n\n\n\n\n\n\n\nDay\nMiles\nDriving time\nStart time\nEnd time\n\n\n\nThere\nJune 3\n492\n7 hours 26 minutes\n7:34 AM EDT\n6:37 PM CDT\n\n\nJune 4\n544\n8 hours 16 minutes\n9:01 AM CDT\n8:30 PM CDT\n\n\nJune 5\n442\n6 hours 56 minutes\n9:06 AM CDT\n6:10 PM MDT\n\n\nJune 6\n626\n10 hours 5 minutes\n8:24 AM MDT\n10:35 PM MST\n\n\nJune 7\n438\n8 hours 31 minutes\n8:40 AM MST\n8:46 PM MDT\n\n\nJune 8*\n\n63\n2 hours 18 minutes\n9:41 AM MDT\n7:42 PM MDT\n\n\nJune 9\n228\n4 hours 41 minutes\n9:04 AM MDT\n3:06 PM MDT\n\n\nSubtotal\n\n2,832\n48 hours 14 minutes\n\n\n\n\nBack again\nJune 20\n290\n4 hours 50 minutes\n8:58 AM MDT\n9:17 PM MDT\n\n\nJune 21\n408\n11 hours 36 minutes\n8:16 AM MDT\n10:04 PM MDT\n\n\nJune 22\n525\n8 hours 27 minutes\n8:14 AM MDT\n10:42 PM CDT\n\n\nJune 23\n459\n7 hours 27 minutes\n8:11 AM CDT\n8:22 PM CDT\n\n\nJune 24\n446\n7 hours 9 minutes\n9:09 AM CDT\n9:16 PM CDT\n\n\nJune 25\n262\n4 hours 1 minutes\n7:49 AM CDT\n12:51 PM EDT\n\n\nSubtotal\n\n2,390\n43 hours 33 minutes\n\n\n\n\nTotal\n\n5,222\n91 hours 47 minutes\n\n\n\n\n\n\n* This wasn't really a travel day; we spent the day hiking around Capitol Reef National Park and hanging out at my aunt's cabin in Grover, Utah.\n\n\n\n\n\n\n\n\nFigure 3 shows a timeline of each day’s events.1 We typically started each day with some sort of sightseeing adventure (we didn’t do this on the first day, since we left our house that day—instead, we ended that day with a walk through the French Quarter in New Orleans). Our longest touristy adventure was technically Capitol Reef National Park, since we spent the whole day there, but if we don’t count that and instead look at the along-the-way stops, our longest adventures were Yellowstone, Nauvoo, and Carlsbad Caverns.\n1 All the times are in local time, which causes some weirdness when crossing time zones. Instead of somehow expanding or shrinking these timelines during stretches where we gained or lost time because of time zone changes, I just put a dotted border around those stretches to point them out.\nCode# Little dataset with names for the tourist stops we made\n# Gotta do some goofy things here because I don't want things double labeled though\n#\n# - Some IDs, like ChIJgcNDAhKmIIYRRA4mio_7VgE (a parking lot near the \n#   French Quarter) show up twice as placeVisits, but there should only be one \n#   label, so down below, after merging this into the timeline data, I use \n#   `group_by(visit_label) %&gt;% slice(1)` to only keep the first visit for labeling\n# - Yellowstone, though, looks goofy if the first segment is labeled, so I want \n#   to label the second. That messes with the grouping/slicing approach, so I \n#   add a trailing space to the Yellowstone visits I don't want labeled \n#   (i.e. \"Yellowstone \") and then filter those out after grouping/slicing\n# - Nauvoo happened on two different days, but grouping/slicing only keeps the \n#   first. So I make the second day's visits use a different label with a space \n#   at the beginning and end of the label (i.e. \" Nauvoo \")\nsightseeing &lt;- tribble(\n  ~id,                           ~visit_label,\n  \"ChIJgcNDAhKmIIYRRA4mio_7VgE\", \"The French\\nQuarter\",\n  \"ChIJv30_Xw-mIIYRpt26QbLBh58\", \"The French\\nQuarter\",  # Technically Louis Armstrong Park\n  \"ChIJ59n6fW8dnogRWi-5N6olcyU\", \"Chalmette\\nBattlefield\",\n  \"ChIJX4k2TVVfXIYRIsTnhA-P-Rc\", \"The Alamo\",\n  \"ChIJW9e4xBN544YRvbI7vfc91G4\", \"Carlsbad\\nCaverns\",\n  \"ChIJrSLbJJIQM4cR4l5HTswDY8k\", \"The Grand\\nCanyon\",\n  \"ChIJU6LnB_8ASocRB_9PSFPsO94\", \"Capitol Reef\\nNational Park\",\n  \"ChIJIw-BhQkZSocRncIWG0YMLJU\", \"Capitol Reef\\nNational Park\",  # Technically the visitor center\n  \"ChIJaSHejn29SYcR-YzTt_DNlTg\", \"Goblin\\nValley\",\n  \"ChIJVQ4oZOP4VFMREjDKbf7bHIE\", \"My sister's\\nhouse\",\n  \"ChIJp4yR8asLVFMRJJExTuHrYEs\", \"Rexburg\",  # Technically Porter Park\n  \"ChIJz6VI3AMLVFMREvSOw9M0VR4\", \"Rexburg\",  # Technically BYU-I\n  \"51\",                          \"Yellowstone\",  # Technically the drive to Old Faithful; Google missed this\n  \"ChIJ3zGqpb65UVMR0rTSaqVZ5kc\", \"Yellowstone \",\n  \"ChIJXy5ZgRvtUVMRoSJoWid8Owg\", \"Yellowstone \",  # Technically Old Faithful\n  \"ChIJOT5U8z8GM1MResed1BOdJKk\", \"Devil's\\nTower\",\n  \"ChIJ39Y-tdg1fYcRQcZcBb499do\", \"Mount\\nRushmore\",\n  \"ChIJg_2MNnKRk4cRQGXbuvgqba4\", \"Winter\\nQuarters\",\n  \"ChIJh53YJHIm54cRmpf8_ZA3CVw\", \"Nauvoo\",  # Technically the visitors center\n  \"ChIJDUPPu3Im54cRKj6BG8UkOko\", \"Nauvoo\",  # Technically the temple\n  \"ChIJg0abVHYm54cR85yQbfLjt2o\", \" Nauvoo \",  # Technically the family living center\n  \"ChIJm7cRetkl54cR-lEKk-eZnXA\", \" Nauvoo \",  # Technically the Smith family cemetery\n  \"ChIJZ6tHUwsm54cRbmWsF639PjY\", \" Nauvoo \"   # Technically Carthage Jail\n)\n\n\ntimeline_data &lt;- all_stops_activities %&gt;% \n  # Get rid of the geometry column since we're not doing anything map-related\n  st_drop_geometry() %&gt;% \n  # Indicate if there was a time zone change during the activity\n  mutate(\n    tz_change = ifelse(is.na(tz_start) | tz_start_abb == tz_end_abb, NA, \"Time zone change\")\n  ) %&gt;% \n  # Find the midpoint between the start and end times\n  mutate(time_mid = startTimestamp_local - ((startTimestamp_local - endTimestamp_local) / 2)) %&gt;% \n  # Add column for the name of the tourist stop\n  left_join(sightseeing, by = join_by(id)) %&gt;% \n  # Add column for more detailed type of stop\n  mutate(\n    stop_type = case_when(\n      type == \"visit\" & !is.na(visit_label) ~ \"Sightseeing\",\n      type == \"visit\" & is.na(visit_label) ~ \"Gas/bathroom/hotel stop\",\n      type == \"segment\" & id == \"51\" ~ \"Sightseeing\",\n      type == \"segment\" ~ \"Traveling\"\n    )\n  ) %&gt;%\n  # After finishing at Capitol Reef we returned to my aunt's cabin in Grover to\n  # relax. I had to run back to the nearest town with 5G internet (Torrey) to\n  # respond to e-mails and Slack messages (since I was teaching two online\n  # classes throughout this whole road trip!), and that doesn't technically\n  # count as part of the road trip, so we need to remove everything after 5 PM\n  # on that day\n  filter(!(day == 8 & hour(endTimestamp_local) &gt; 17)) %&gt;% \n  # After arriving at the hotel in Rexburg, we made a late-night milkshake run\n  # after the kids were in bed, which Google picked up. That doesn't technically\n  # count as part of the road trip, so we need to remove everything after 6 PM\n  # on that day\n  filter(!(day == 20 & hour(endTimestamp_local) &gt; 18)) %&gt;% \n  # Set the timestamps for the final stop of the day (i.e. hotels) to NA and\n  # then remove those rows for plotting\n  group_by(day) %&gt;% \n  mutate(\n    across(c(startTimestamp_local, endTimestamp_local), \n      ~if_else(row_number() == n(), NA, .x))\n  ) %&gt;% \n  drop_na(startTimestamp_local)\n\n# Force all the timestamps to be on the same day for the sake of plotting\nday(timeline_data$startTimestamp_local) &lt;- 1\nday(timeline_data$endTimestamp_local) &lt;- 1\nday(timeline_data$time_mid) &lt;- 1\n\n# Extract just the labels for plotting\nsightseeing_labels &lt;- timeline_data %&gt;% \n  filter(stop_type == \"Sightseeing\") %&gt;% \n  group_by(visit_label) %&gt;% \n  slice(1) %&gt;% \n  filter(visit_label != \"Yellowstone \")\n\n# Timeline plot\nggplot(timeline_data) +\n  geom_rect(\n    aes(\n      xmin = startTimestamp_local, xmax = endTimestamp_local, \n      ymin = 0, ymax = 1, \n      fill = stop_type, color = tz_change, linetype = tz_change, linewidth = tz_change\n    )\n  ) +\n  geom_label(\n    data = sightseeing_labels,\n    aes(x = time_mid, y = 0.5, label = visit_label),\n    fill = colorspace::lighten(clrs[8], 0.1), color = \"white\", lineheight = 1,\n    inherit.aes = FALSE\n  ) +\n  # Phew fancy aesthetic and scale work here\n  scale_x_datetime(date_breaks = \"3 hours\", date_labels = \"%I:%M %p\") +\n  scale_y_continuous(expand = expansion(0, 0)) +\n  scale_fill_manual(values = c(clrs[6], clrs[8], clrs[2]), guide = guide_legend(order = 1)) +\n  scale_color_manual(values = c(\"black\", \"white\")) +\n  scale_linewidth_discrete(range = c(0.5, 0)) +\n  scale_linetype_manual(\n    values = c(\"21\"), breaks = \"Time zone change\",\n    guide = guide_legend(override.aes = list(fill = \"white\", color = \"black\", linewidth = 0.5), order = 10)\n  ) +\n  guides(color = \"none\", linewidth = \"none\") +\n  facet_wrap(vars(day_month), ncol = 2, dir = \"v\") +\n  labs(x = NULL, y = NULL, linetype = NULL, fill = NULL) +\n  theme_roadtrip() +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\nFigure 3: Timeline of our grand road trip"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#driving-and-breaks",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#driving-and-breaks",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Driving and breaks",
    "text": "Driving and breaks\nAs seen in Figure 3, we had a pretty good rhythm of driving and taking gas/bathroom breaks—we broke up long stretches of driving with stops for gas and bathrooms.\nTable 2 shows the top three longest and shortest stretches of driving. The longest stretch was on the final day—we drove 4 hours from Nashville to Atlanta with no bathroom breaks. We apparently just desperately wanted to get home! We did the same push-through-to-the-last-stop approach a few other times too. The third longest stretch was actually our first on the way home, leaving Spanish Fork, Utah to my sister’s house outside of Rexburg, Idaho, which we did in 3.5 hours, while the 2nd and 5th longest stretches involved pushing to our final destinations for the day (a hotel in Nauvoo, Illinois and my aunt’s cabin in Grover, Utah). The only long stretch that broke this patter was the fourth longest one after Carlsbad Caverns—we spent more than 3 hours on a little state road going all the way from southern New Mexico to a gas station at I-20.\nThe shortest stretches involved going from a main sightseeing stop to a gas station (Mount Rushmore to some Holiday gas station), going between sightseeing stops (the Smith Family Cemetery in Nauvoo to Carthage Jail), or going from a hotel to a sightseeing stop (our hotel in Carlsbad to Carlsbad Caverns National Park).\nMy favorite short stop is number 4 here, on June 4. We got gas in Louisiana somewhere and then had to stop at another gas station in Texas 39 minutes later for an emergency break for little kids who suddenly needed to use the bathroom again. After that particular stop, we instituted a “no drinking soda or juice in the car” rule for the kids and suddenly—like magic—their bladders worked a lot more slowly and we stopped making such frequent stops.\n\nCodedriving_data &lt;- all_stops_activities %&gt;% \n  # Get rid of the geometry column since we're not doing anything map-related\n  st_drop_geometry() %&gt;% \n  # Find the previous and next place visits\n  mutate(\n    origin_name = lag(name),\n    origin_address = lag(address),\n    destination_name = lead(name),\n    destination_address = lead(address)\n  ) %&gt;% \n  # lag() doesn't work for the first entry, so manually add it\n  mutate(\n    origin_name = ifelse(id == 1, \"Home\", origin_name),\n    origin_address = ifelse(id == 1, \"Atlanta, GA, USA\", origin_address)\n  ) %&gt;% \n  # Only look at driving activities\n  filter(activity_type == \"IN_PASSENGER_VEHICLE\") %&gt;% \n  # Get rid of empty, place-related columns\n  select(-c(name, address, driving_stop, tz, geometry_start, geometry_end))\n\n# Longest, shortest stretch of driving\ndriving_data %&gt;% \n  filter(distance_miles &gt; 10) %&gt;%\n  mutate(duration_rank = rank(-duration)) %&gt;% \n  mutate(top_bottom = case_when(\n    duration_rank %in% 1:5 ~ \"Longest stretches\",\n    duration_rank %in% seq(n() - 4, n(), by = 1) ~ \"Shortest stretches\"\n  )) %&gt;% \n  filter(!is.na(top_bottom)) %&gt;% \n  arrange(\n    top_bottom, \n    ifelse(top_bottom == \"Shortest stretches\", desc(duration_rank), duration_rank)\n  ) %&gt;% \n  # Format time stuff nicely\n  mutate(\n    nice_time = fmt_difftime(duration),\n    nice_start_time = paste(strftime(startTimestamp_local, format = \"%I:%M %p\", tz = \"UTC\"), tz_start_abb),\n    nice_end_time = paste(strftime(endTimestamp_local, format = \"%I:%M %p\", tz = \"UTC\"), tz_end_abb)\n  ) %&gt;% \n  # Remove leading zeros from hour\n  mutate(across(c(nice_start_time, nice_end_time), ~str_replace(.x, \"^0\", \"\"))) %&gt;% \n  # Make nice origin and destination columns\n  mutate(\n    origin_nice = glue('{origin_name}&lt;br&gt;&lt;span class=\"smaller-address\"&gt;{origin_address}&lt;/span&gt;'),\n    destination_nice = glue('{destination_name}&lt;br&gt;&lt;span class=\"smaller-address\"&gt;{destination_address}&lt;/span&gt;')\n  ) %&gt;% \n  select(\n    top_bottom, day_month, nice_time, nice_start_time, nice_end_time, \n    distance_miles, origin_nice, destination_nice\n  ) %&gt;% \n  group_by(top_bottom) %&gt;% \n  gt() %&gt;% \n  cols_label(\n    nice_time = \"Driving time\",\n    day_month = \"Day\",\n    nice_start_time = \"Start time\",\n    nice_end_time = \"End time\",\n    distance_miles = \"Miles\",\n    origin_nice = \"Origin\",\n    destination_nice = \"Destination\"\n  ) %&gt;% \n  tab_style(\n    locations = list(cells_column_labels(), cells_stubhead()),\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"grey80\")\n    )\n  ) %&gt;% \n  tab_style(\n    locations = cells_row_groups(),\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"grey95\")\n    )\n  ) %&gt;% \n  cols_align(\n    columns = c(day_month, origin_nice, destination_nice),\n    align = \"left\"\n  ) %&gt;% \n  fmt_markdown(columns = c(origin_nice, destination_nice)) %&gt;% \n  fmt_number(columns = distance_miles, decimals = 0) %&gt;% \n  tab_footnote(\n    footnote = \"Stretches less than 10 miles omitted (like if we had lunch somewhere and drove down the street to get gas).\"\n  ) %&gt;% \n  opt_css(\n    css = \"\n    .smaller-address {\n      font-size: 0.7em;\n    }\"\n  ) %&gt;% \n  opt_table_font(font = \"Jost\")\n\n\nTable 2: Top five longest and shortest stretches of driving during the trip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nDriving time\nStart time\nEnd time\nMiles\nOrigin\nDestination\n\n\n\nLongest stretches\n\n\nJune 25\n4 hours 1 minutes\n7:49 AM CDT\n12:51 PM EDT\n262\n\nQuality Inn2741 York Rd, Pleasant View, TN 37146, USA\n\n\nHomeAtlanta, GA, USA\n\n\n\nJune 23\n3 hours 35 minutes\n1:19 PM CDT\n4:54 PM CDT\n198\n\nMcDonald’s120 SE 7th St, Stuart, IA 50250, USA\n\n\nInn at Old Nauvoo1875 Mulholland St, Nauvoo, IL 62354, USA\n\n\n\nJune 20\n3 hours 31 minutes\n8:58 AM MDT\n12:30 PM MDT\n244\n\nMy aunt’s houseSpanish Fork, UT, USA\n\n\nMy sister’s houseShelley, ID, USA\n\n\n\nJune 6\n3 hours 9 minutes\n12:18 PM MDT\n3:28 PM MDT\n191\n\nCarlsbad Caverns National ParkCarlsbad, NM 88220, USA\n\n\nConoco1307 8th St, Vaughn, NM 88353, USA\n\n\n\nJune 7\n3 hours 7 minutes\n5:39 PM MDT\n8:46 PM MDT\n163\n\nGlazier’s Market264 S 100 E, Kanab, UT 84741, USA\n\n\nMy aunt’s cabinGrover, UT, USA\n\n\n\nShortest stretches\n\n\nJune 22\n21 minutes\n3:35 PM MDT\n3:56 PM MDT\n15\n\nMount Rushmore National Memorial13000 SD-244, Keystone, SD 57751, USA\n\n\nHoliday StationstoresCaregiver Cir, Rapid City, SD 57701, USA\n\n\n\nJune 24\n25 minutes\n12:20 PM CDT\n12:45 PM CDT\n18\n\nSmith Family CemeteryNauvoo, IL 62354, USA\n\n\nCarthage Jail310 Buchanan St, Carthage, IL 62321, USA\n\n\n\nJune 6\n33 minutes\n8:24 AM MDT\n8:57 AM MDT\n22\n\nStevens Inn1829 S Canal St, Carlsbad, NM 88220, USA\n\n\nCarlsbad Caverns National ParkCarlsbad, NM 88220, USA\n\n\n\nJune 4\n39 minutes\n3:26 PM CDT\n4:06 PM CDT\n44\n\nExxon1410 Gum Cove Rd, Vinton, LA 70668, USA\n\n\nLove’s Travel Stop7495 Smith Rd, Beaumont, TX 77713, USA\n\n\n\nJune 20\n42 minutes\n2:14 PM MDT\n2:56 PM MDT\n35\n\nMy sister’s houseShelley, ID, USA\n\n\nAmericInn by Wyndham Rexburg BYUI1098 Golden Beauty Dr, Rexburg, ID 83440, USA\n\n\n\n\nStretches less than 10 miles omitted (like if we had lunch somewhere and drove down the street to get gas).\n\n\n\n\n\n\n\n\nWhat about the stops? How long was a typical gas/bathroom break? Figure 4 shows the distribution of the duration of all the non-touristy stops, and it actually reveals a pretty neat trimodal distribution! We had three general types of stops:\n\n\nFast (10ish minutes): stops for switching drivers, getting gas right after a sightseeing visit, or letting one kid go to the bathroom\n\nStandard (25ish minutes): stops where we got gas and had everyone go to the bathroom\n\nMeal (40ish minutes): stops where we got gas, went to the bathroom, ate food (typically peanut butter sandwiches that we made in the hotel in the morning), and walked around to stretch\n\n\nCode# Cut duration into three categories\nplaces_hist_data &lt;- place_visits %&gt;% \n  filter(driving_stop == TRUE) %&gt;% \n  mutate(range = cut(as.numeric(duration), breaks = c(0, 15, 35, 50)))\n\nggplot(places_hist_data, aes(x = as.numeric(duration), fill = range)) +\n  geom_histogram(binwidth = 5, boundary = 0, color = \"white\") +\n  annotate(geom = \"text\", x = 10, y = 7.75, label = \"Fast stop\") +\n  annotate(geom = \"segment\", x = 6, xend = 14, y = 7.35, yend = 7.35) +\n  annotate(geom = \"text\", x = 25, y = 7.75, label = \"Standard gas + bathroom\") +\n  annotate(geom = \"segment\", x = 16, xend = 34, y = 7.35, yend = 7.35) +\n  annotate(geom = \"text\", x = 42.5, y = 7.75, label = \"Gas + bathroom + meal\") +\n  annotate(geom = \"segment\", x = 36, xend = 49, y = 7.35, yend = 7.35) +\n  scale_y_continuous(breaks = seq(0, 6, by = 2)) +\n  scale_fill_manual(values = clrs[c(7, 2, 1)], guide = \"none\") +\n  labs(x = \"Minutes\", y = \"Count\") +\n  theme_roadtrip() +\n  theme(panel.grid.major.x = element_blank())\n\n\n\n\n\n\nFigure 4: Distribution of non-sightseeing stops for the first four days of the trip"
  },
  {
    "objectID": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#elevation-over-time",
    "href": "blog/2023/07/03/using-google-location-history-with-r-roadtrip/index.html#elevation-over-time",
    "title": "Road trip analysis! How to use and play with Google Location History in R",
    "section": "Elevation over time",
    "text": "Elevation over time\nAnd finally, just for fun since Google tracked this data and we might as well do something neat with it, we can look at how much the elevation changed over the course of the trip. Atlanta is 1,000 feet above sea level, and Utah County is 4,500 feed above sea level, but the 3,500-foot increase in elevation was hardly uniform. We dropped down to below sea level in New Orleans, climbed up to above 4,000 feet in Carlsbad (it turns out that Carlsbad Caverns is on top of a big mountain), and then climbed rapidly up to the Grand Canyon and Southern Utah in general at 7,500ish feet until dropping down to 4,500 feet at our final destination in Utah.\nOn the way back, we stayed at around 4,500ish feet until Yellowstone, where we hit the highest elevation of the whole trip at 8,530 feet. We then gradually worked our way down toward sea level as we traveled further east, finally hitting 1,000ish feet at the Mississippi River at Nauvoo, Illinois, where we mostly stayed until getting home (with a little Appalachian spike near the end).\n\nCodeelevation_data &lt;- all_locations %&gt;% \n  mutate(elevation = meters_to_feet(altitude))\n\nelevation_stops &lt;- tribble(\n  ~id,                       ~direction,  ~nudge_direction, ~stop_label,\n  \"ChIJv30_Xw-mIIYRpt26QbLBh58\", \"There\", \"up\", \"New Orleans,\\nLouisiana\",\n  \"ChIJX4k2TVVfXIYRIsTnhA-P-Rc\", \"There\", \"down\", \"San Antonio,\\nTexas\",\n  \"ChIJW9e4xBN544YRvbI7vfc91G4\", \"There\", \"down\", \"Carlsbad Caverns,\\nNew Mexico\",\n  \"ChIJrSLbJJIQM4cR4l5HTswDY8k\", \"There\", \"up\", \"The Grand Canyon,\\nArizona\",\n  \"ChIJU6LnB_8ASocRB_9PSFPsO94\", \"There\", \"up\", \"Capitol Reef,\\nUtah\",\n  \"ChIJz6VI3AMLVFMREvSOw9M0VR4\", \"Back again\", \"down\", \"Rexburg,\\nIdaho\",\n  \"ChIJSWHsxv8JTlMR82z8b6wF_BM\", \"Back again\", \"up\", \"Yellowstone,\\nWyoming\",\n  \"ChIJ39Y-tdg1fYcRQcZcBb499do\", \"Back again\", \"up\", \"Mount Rushmore,\\nSouth Dakota\",\n  \"ChIJDUPPu3Im54cRKj6BG8UkOko\", \"Back again\", \"down\", \"Nauvoo,\\nIllinois\",\n  \"ChIJtUcJ-n36ZIgRhzY2PM19eWA\", \"Back again\", \"up\", \"Nashville,\\nTennessee\"\n)\n\nstops_to_show &lt;- place_visits %&gt;% \n  st_drop_geometry() %&gt;% \n  filter(id %in% elevation_stops$id) %&gt;% \n  left_join(elevation_stops, by = join_by(id)) %&gt;% \n  # Magical new join_by(closest(...)) syntax for inexact, approximate matching\n  # to get the closest elevation for the stop(!)\n  left_join(elevation_data, by = join_by(closest(startTimestamp_local &gt;= timestamp))) %&gt;% \n  # Get rid of duplicate ids\n  group_by(id) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\nggplot(elevation_data, aes(x = timestamp_local, y = elevation)) +\n  geom_line(linewidth = 0.3) +\n  geom_text_repel(\n    data = filter(stops_to_show, nudge_direction == \"up\"),\n    aes(x = startTimestamp_local, label = stop_label),\n    nudge_y = 2500, direction = \"y\", lineheight = 1, family = \"Overpass ExtraBold\",\n    color = clrs[8], segment.color = clrs[7], seed = 1234\n  ) +\n  geom_text_repel(\n    data = filter(stops_to_show, nudge_direction == \"down\"),\n    aes(x = startTimestamp_local, label = stop_label),\n    nudge_y = -1000, lineheight = 1, family = \"Overpass ExtraBold\",\n    color = clrs[8], segment.color = clrs[7], seed = 1234\n  ) +\n  scale_x_datetime(date_breaks = \"1 day\", date_labels = \"%B %e\") +\n  scale_y_continuous(\n    breaks = seq(0, 8000, by = 2000),\n    labels = label_comma(suffix = \" ft.\")\n  ) +\n  labs(x = NULL, y = \"Elevation\") +\n  facet_wrap(vars(direction), scales = \"free_x\") +\n  theme_roadtrip()\n\n\n\n\n\n\nFigure 5: Elevation over the course of the trip"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "",
    "text": "I’ve been working on converting a couple of my dissertation chapters into standalone articles, so I’ve been revisiting and improving my older R code. As part of my dissertation work, I ran a global survey of international NGOs to see how they adjust their programs and strategies when working under authoritarian legal restrictions in dictatorship. In the survey, I asked a bunch of questions with categorical responses (like “How would you characterize your organization’s relationship with your host-country government?”, with the possible responses “Positive,” “Negative”, and “Neither”).\nAnalyzing this kind of categorical data can be tricky. What I ended up doing (and what people typically do) is looking at the differences in proportions of responses (i.e. is there a substantial difference in the proportion reporting “Positive” or “Negative” in different subgroups?). But doing this requires a little bit of extra work because of how proportions work. We can’t just treat proportions like continuous values. Denominators matter and help determine the level of uncertainty in the proportions. We need to account for these disparate sample sizes underlying the proportions.\nAdditionally, it’s tempting to treat Likert scale responses1 (i.e. things like “strongly agree”, “agree”, “neutral”, etc.) as numbers (e.g. make “strongly agree” a 2, “agree” a 1, “neutral” a 0, and so on), and then find things like averages (e.g. “the average response is a 1.34”), but those summary statistics aren’t actually that accurate. What would a 1.34 mean? A little higher than agree? What does that even mean?\nWe need to treat these kinds of survey questions as the categories that they are, which requires a different set of analytical tools than just findings averages and running linear regressions. We instead need to use things like frequencies, proportions, contingency tables and crosstabs, and fancier regression like ordered logistic models.\nI’m a fan of Bayesian statistical inference—I find it way more intuitive and straightforward than frequentist null hypothesis testing. I first “converted” to Bayesianism back when I first analyzed my dissertation data in 2017 and used the newly-invented {rstanarm} package to calculate the difference in proportions of my various survey responses in Bayesian ways, based on a blog post and package for Bayesian proportion tests by Rasmus Bååth. He used JAGS, though, and I prefer to use Stan, hence my use of {rstanarm} back then.\nSince 2017, though, I’ve learned a lot more about Bayesianism. I’ve worked through both Richard McElreath’s Statistical Rethinking and the gloriously accessible Bayes Rules!, and I no longer use {rstanarm}. I instead use {brms} for everything (or raw Stan if I’m feeling extra fancy).\nSo, as a reference for myself while rewriting these chapters, and as a way to consolidate everything I’ve learned about Bayesian-flavored proportion tests, here’s a guide to thinking about differences in proportions in a principled Bayesian way. I explore two different questions (explained in detail below). For the first one I’ll be super pedagogical and long-winded, showing how to find differences in proportions with classical frequentist statistical tests, with different variations of Stan code, and with different variations of {brms} code. For the second one I’ll be less pedagogical and just show the code and results."
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#estimand",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#estimand",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Estimand",
    "text": "Estimand\nFor our first question, we want to know if there’s a substantial difference in the proportion of students who read comic books often in the United States and Mexico, or whether the difference between the ■two yellow cells is greater than zero:\n\nCodefancy_table %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = colorspace::lighten(\"#CD8A39\", 0.7)),\n      cell_text(color = \"#CD8A39\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = \"Often\", rows = c(1, 3))\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nRarely\nSometimes\nOften\n\n\n\nMexico\n\n\nComic books\n26.29%(9,897)\n45.53%(17,139)\n28.17%(10,605)\n\n\nNewspapers\n18.02%(6,812)\n32.73%(12,369)\n49.25%(18,613)\n\n\nUnited States\n\n\nComic books\n62.95%(3,237)\n26.86%(1,381)\n10.19%(524)\n\n\nNewspapers\n25.27%(1,307)\n37.22%(1,925)\n37.51%(1,940)\n\n\n\n\n\n\nTo be more formal about this, we’ll call this estimand \\(\\theta\\), which is the difference between the two proportions \\(\\pi\\):\n\\[\n\\theta = \\pi_\\text{Mexico, comic books, often} - \\pi_\\text{United States, comic books, often}\n\\]\nWhen visualizing this, we’ll use colors to help communicate the idea of between-group differences. We’ll get fancier with the colors in question 2, where we’ll look at three sets of pairwise differences, but here we’re just looking at a single pairwise difference (the difference between the US and Mexico), so we’ll use a bit of subtle and not-quite-correct color theory. In kindergarten we all learned the RYB color model, where primary pigments can be mixed to create secondary pigments. For instance\n\n■blue + ■yellow = ■green.\n\nIf we do some (wrong color-theory-wise) algebra, we can rearrange the formula so that\n\n■blue − ■green = ■yellow\n\nIf we make the United States ■blue and Mexico ■green, the ostensible color for their difference is ■yellow. This is TOTALLY WRONG and a lot more complicated according to actual color theory, but it’s a cute and subtle visual cue, so we’ll use it.\nThese primary colors are a little too bright for my taste though, so let’s artsty them up a bit. We’re looking at data about the US and Mexico, so we’ll use the Saguaro palette from the {NatParksPalettes} package, since Saguaro National Park is near the US-Mexico border and it has a nice blue, yellow, and green.\nWe’ll use ■yellow for the difference (θ) between the ■United States and ■Mexico.\n\n\n\n“Saguaro Fruit Ripens in June” by the US National Park Service\n\nCode# US/Mexico question colors\nclrs_saguaro &lt;- NatParksPalettes::natparks.pals(\"Saguaro\")\nclr_usa &lt;- clrs_saguaro[1]\nclr_mex &lt;- clrs_saguaro[6]\nclr_diff &lt;- clrs_saguaro[4]"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#classically",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#classically",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Classically",
    "text": "Classically\nIn classical frequentist statistics, there are lots of ways to test for the significance of a difference in proportions of counts, rates, proportions, and other categorical-related things, like chi-squared tests, Fisher’s exact test, or proportion tests. Each of these tests have special corresponding “flavors” that apply to specific conditions within the data being tested or the estimand being calculated (corrections for sample sizes, etc.). In standard stats classes, you memorize big flowcharts of possible statistical operations and select the correct one for the situation.\nSince we want to test the difference between two group proportions, we’ll use R’s prop.test(), which tests the null hypothesis that the proportions in some number of groups are the same:\n\\[\nH_0: \\theta = 0\n\\]\nOur job with null hypothesis significance testing is to calculate a test statistic (\\(\\chi^2\\) in this case) for \\(\\theta\\), determine the probability of seeing that statistic in a world where \\(\\theta\\) is actually 0, and infer whether the value we see could plausibly fit in a world where the null hypothesis is true.\nWe need to feed prop.test() either a matrix with a column of counts of successes (students who read comic books often) and failures (students who do not read comic books often) or two separate vectors: one of counts of successes (students who read comic books often) and one of counts of totals (all students). We’ll do it both ways for fun.\nFirst we’ll make a matrix of the counts of students from Mexico and the United States, with columns for the counts of those who read often and of those who don’t read often.\n\nCodeoften_matrix &lt;- reading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\") %&gt;% \n  mutate(n_not_often = total - n) %&gt;% \n  select(n_often = n, n_not_often) %&gt;% \n  as.matrix()\noften_matrix\n##      n_often n_not_often\n## [1,]   10605       27036\n## [2,]     524        4618\n\n\nNow we can feed that to prop.test():\n\nCodeprop.test(often_matrix)\n## \n##  2-sample test for equality of proportions with continuity correction\n## \n## data:  often_matrix\n## X-squared = 759, df = 1, p-value &lt;2e-16\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  0.170 0.189\n## sample estimates:\n## prop 1 prop 2 \n##  0.282  0.102\n\n\nThis gives us some helpful information. The “flavor” of the formal test is a “2-sample test for equality of proportions with continuity correction”, which is fine, I guess.\nWe have proportions that are the same as what we have in the highlighted cells in the contingency table (28.17% / 10.19%) and we have 95% confidence interval for the difference. Oddly, R doesn’t show the actual difference in these results, but we can see that difference if we use model_parameters() from the {parameters} package (which is the apparent successor to broom::tidy()?). Here we can see that the difference in proportions is 18 percentage points:\n\nCodeprop.test(often_matrix) %&gt;% \n  model_parameters()\n## 2-sample test for equality of proportions\n## \n## Proportion      | Difference |       95% CI | Chi2(1) |      p\n## --------------------------------------------------------------\n## 28.17% / 10.19% |     17.98% | [0.17, 0.19] |  759.26 | &lt; .001\n## \n## Alternative hypothesis: two.sided\n\n\nAnd finally we have a test statistic: a \\(\\chi^2\\) value of 759.265, which is huge and definitely statistically significant. In a hypothetical world where there’s no difference in the proportions, the probability of seeing a difference of at least 18 percentage points is super tiny (p &lt; 0.001). We have enough evidence to confidently reject the null hypothesis and declare that the proportions of the groups are not the same. With the confidence interval, we can say that we are 95% confident that the interval 0.17–0.189 captures the true population parameter. We can’t say that there’s a 95% chance that the true value falls in this range—we can only talk about the range.\nWe can also do this without using a matrix by feeding prop.test() two vectors: one with counts of people who read comics often and one with counts of total respondents:\n\nCodeoften_values &lt;- reading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\")\n\n(n_often &lt;- as.numeric(c(often_values[1, 4], often_values[2, 4])))\n## [1] 10605   524\n(n_respondents &lt;- as.numeric(c(often_values[1, 5], often_values[2, 5])))\n## [1] 37641  5142\n\nprop.test(n_often, n_respondents)\n## \n##  2-sample test for equality of proportions with continuity correction\n## \n## data:  n_often out of n_respondents\n## X-squared = 759, df = 1, p-value &lt;2e-16\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  0.170 0.189\n## sample estimates:\n## prop 1 prop 2 \n##  0.282  0.102\n\n\nThe results are the same."
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-stan",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-stan",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Bayesianly with Stan",
    "text": "Bayesianly with Stan\nWhy Bayesian modeling?\nI don’t like null hypotheses and I don’t like flowcharts.\n\n\n\n“Golem” by smokeghost\nRegular classical statistics classes teach about null hypotheses and flowcharts, but there’s a better way. In his magisterial Statistical Rethinking, Richard McElreath describes how in legends people created mythological clay robots called golems that could protect cities from attacks, but that could also spin out of control and wreak all sorts of havoc. McElreath uses the idea of golems as a metaphor for classical statistical models focused on null hypothesis significance testing, which also consist of powerful quasi-alchemical procedures that have to be followed precisely with specific flowcharts:\n\n\n\n\nStatistical models as golems (slide 9 in Statistical Rethinking 2022 Lecture 01)\n\n\n\nMcElreath argues that this golem-based approach to statistics is incredibly limiting, since (1) you have to choose the right test, and if it doesn’t exist, you have to wait for some fancy statistician to invent it, and (2) you have to focus on rejecting null hypotheses instead of exploring research hypotheses.\nFor instance, in the null hypothesis framework section above, this was the actual question undergirding the analysis:\n\nIn a hypothetical world where \\(\\theta = 0\\) (or where there’s no difference between the proportions) what’s the probability that this one-time collection of data fits in that world—and if the probability is low, is there enough evidence to confidently reject that hypothetical world of no difference?\n\noof. We set our prop.test() golem to work and got a p-value for the probability of seeing the 18 percentage point difference in a world where there’s actually no difference. That p-value was low, so we confidently declared \\(\\theta\\) to be statistically significant and not zero. But that was it. We rejected the null world. Yay. But that doesn’t say much about our main research hypothesis. Boo.\nOur actual main question is far simpler:\n\nGiven the data here, what’s the probability that there’s no difference between the proportions, or that \\(\\theta \\neq 0\\)?\n\nBayesian-flavored statistics lets us answer this question and avoid null hypotheses and convoluted inference. Instead of calculating the probability of seeing some data given a null hypothesis (\\(P(\\text{Data} \\mid H_0)\\)), we can use Bayesian inference to calculate the probability of a hypothesis given some data (\\(P(\\theta \\neq 0 \\mid \\text{Data})\\)).\nFormal model\nSo instead of thinking about a specific statistical golem, we can think about modeling the data-generating process that could create the counts and proportions that we see in the PISA data.\nRemember that our data looks like this, with n showing a count of the people who read comic books often in each country and total showing a count of the people who responded to the question.\n\nCodereading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\")\n## # A tibble: 2 × 6\n##   country       book_type   frequency     n total  prop\n##   &lt;fct&gt;         &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 Mexico        Comic books Often     10605 37641 0.282\n## 2 United States Comic books Often       524  5142 0.102\n\n\nThe actual process for generating that n involved asking thousands of individual, independent people if they read comic books often. If someone says yes, it’s counted as a “success” (or marked as “often”); if they say no, it’s not marked as “often”. It’s a binary choice repeated across thousands of independent questions, or “trials.” There’s also an underlying overall probability for reporting “often,” which corresponds to the proportion of people selecting it.\nThe official statistical term for this kind of data-generating processes (a bunch of independent trials with some probability of success) is a binomial distribution, and it’s defined like this, with three parameters:\n\\[\ny \\sim \\operatorname{Binomial}(n, \\pi)\n\\]\n\nNumber of successes (\\(y\\): the number of people responding yes to “often,” or n in our data\nProbability of success (\\(\\pi\\)): the probability someone says yes to “often,” or the thing we want to model for each country\nNumber of trials (\\(n\\)): the total number of people asked the question, or total in our data\n\nThe number of successes and trials are just integers—they’re counts—and we already know those, since they’re in the data. The probability of success \\(\\pi\\) is a percentage and ranges somewhere between 0 and 1. We don’t know this value, but we can estimate it with Bayesian methods by defining a prior and a likelihood, churning through a bunch of Monte Carlo Markov Chain (MCMC) simulations, and finding a posterior distribution of \\(\\pi\\).\nWe can use a Beta distribution to model \\(\\pi\\), since Beta distributions are naturally bound between 0 and 1 and they work well for probability-scale things. Beta distributions are defined by two parameters: (1) \\(\\alpha\\) or \\(a\\) or shape1 in R and (2) \\(\\beta\\) or \\(b\\) or shape2 in R. See this section of my blog post on zero-inflated Beta regression for way more details about how these parameters work and what they mean.\nSuper quickly, we’re interested in the probability of a “success” (or where “often” is yes), which is the number of “often”s divided by the total number of responses:\n\\[\n\\frac{\\text{Number of successes}}{\\text{Number of trials}}\n\\]\nor\n\\[\n\\frac{\\text{Number of often = yes}}{\\text{Number of responses}}\n\\]\nWe can separate that denominator into two parts:\n\\[\n\\frac{\\text{Number of often = yes}}{(\\text{Number of often = yes}) + (\\text{Number of often } \\neq \\text{yes})}\n\\]\nThe \\(\\alpha\\) and \\(\\beta\\) parameters correspond to the counts of successes and failures::\n\\[\n\\frac{\\alpha}{\\alpha + \\beta}\n\\]\nWith these two shape parameters, we can create any percentage or fraction we want. We can also control the uncertainty of the distribution by tinkering with the scale of the parameters. For instance, if we think there’s a 40% chance of something happening, this could be represented with \\(\\alpha = 4\\) and \\(\\beta = 6\\), since \\(\\frac{4}{4 + 6} = 0.4\\). We could also write it as \\(\\alpha = 40\\) and \\(\\beta = 60\\), since \\(\\frac{40}{40 + 60} = 0.4\\) too. Both are centered at 40%, but Beta(40, 60) is a lot narrower and less uncertain.\n\nCodeggplot() +\n  stat_function(fun = ~dbeta(., 4, 6), geom = \"area\",\n                aes(fill = \"Beta(4, 6)\"), alpha = 0.5) +\n  stat_function(fun = ~dbeta(., 40, 60), geom = \"area\",\n                aes(fill = \"Beta(40, 60)\"), alpha = 0.5) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = c(\"Beta(4, 6)\" = \"#FFDC00\",\n                               \"Beta(40, 60)\" = \"#F012BE\")) +\n  labs(x = \"π\", y = NULL, fill = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        legend.position = \"top\")\n\n\n\nBeta(4, 6) and Beta(40, 60) distributions\n\n\n\nWe’ve already seen the data and know that the proportion of students who read comic books often is 10ish% in the United States and 30ish% in Mexico, but we’ll cheat and say that we think that around 25% of students read comic books often, ± some big amount. This implies something like a Beta(2, 6) distribution (since \\(\\frac{2}{2+6} = 0.25\\)), with lots of low possible values, but not a lot of \\(\\pi\\)s are above 50%. We could narrow this down by scaling up the parameters (like Beta(20, 60) or Beta(10, 40), etc.), but leaving the prior distribution of \\(\\pi\\) wide like this allows for more possible responses (maybe 75% of students in one country read comic books often!).\n\nCodeggplot() +\n  stat_function(fun = ~dbeta(., 2, 6), geom = \"area\", fill = \"#AC3414\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Possible values for π\", y = NULL, fill = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank())\n\n\n\nPrior distribution for π using Beta(2, 6)\n\n\n\nOkay, so with our Beta(2, 6) prior, we now have all the information we need to specify the official formal model of the data generating process for our estimand \\(\\theta\\) without any flowchart golems or null hypotheses:\n\\[\n\\begin{aligned}\n&\\ \\textbf{Estimand} \\\\\n\\theta =&\\ \\pi_{\\text{comic books, often}_\\text{Mexico}} - \\pi_{\\text{comic books, often}_\\text{US}} \\\\[10pt]\n&\\ \\textbf{Beta-binomial model for Mexico} \\\\\ny_{n \\text{ comic books, often}_\\text{Mexico}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}_\\text{Mexico}}, \\pi_{\\text{comic books, often}_\\text{Mexico}}) \\\\\n\\pi_{\\text{comic books, often}_\\text{Mexico}} \\sim&\\ \\operatorname{Beta}(\\alpha_\\text{Mexico}, \\beta_\\text{Mexico}) \\\\[10pt]\n&\\ \\textbf{Beta-binomial model for the United States} \\\\\ny_{n \\text{ comic books, often}_\\text{US}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}_\\text{US}}, \\pi_{\\text{comic books, often}_\\text{US}}) \\\\\n\\pi_{\\text{comic books, often}_\\text{US}} \\sim&\\ \\operatorname{Beta}(\\alpha_\\text{US}, \\beta_\\text{US}) \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\alpha_\\text{Mexico}, \\alpha_\\text{US} =&\\ 2 \\\\\n\\beta_\\text{Mexico}, \\beta_\\text{US} =&\\ 6\n\\end{aligned}\n\\]\nBasic Stan model\nThe neat thing about Stan is that it translates fairly directly from this mathematical model notation into code. Here we’ll define three different blocks in a Stan program:\n\nData that gets fed into the model, or the counts of respondents (or \\(y\\) and \\(n\\))\nParameters to estimate, or \\(\\pi_\\text{US}\\) and \\(\\pi_\\text{Mexico}\\)\n\nThe prior and model for estimating those parameters, or \\(\\operatorname{Beta}(2, 6)\\) and \\(y \\sim \\operatorname{Binomial}(n, \\pi)\\)\n\n\n\n\nprops-basic.stan\n\n// Stuff from R\ndata {\n  int&lt;lower=0&gt; often_us;\n  int&lt;lower=0&gt; total_us;\n  int&lt;lower=0&gt; often_mexico;\n  int&lt;lower=0&gt; total_mexico;\n}\n\n// Things to estimate\nparameters {\n  real&lt;lower=0, upper=1&gt; pi_us;\n  real&lt;lower=0, upper=1&gt; pi_mexico;\n}\n\n// Prior and likelihood\nmodel {\n  pi_us ~ beta(2, 6);\n  pi_mexico ~ beta(2, 6);\n  \n  often_us ~ binomial(total_us, pi_us);\n  often_mexico ~ binomial(total_mexico, pi_mexico);\n}\n\nUsing {cmdstanr} as our interface with Stan, we first have to compile the script into an executable file:\n\nCodemodel_props_basic &lt;- cmdstan_model(\"props-basic.stan\")\n\n\nWe can then feed it a list of data and run a bunch of MCMC chains:\n\nCodeoften_values &lt;- reading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\")\n\n(n_often &lt;- as.numeric(c(often_values[1, 4], often_values[2, 4])))\n## [1] 10605   524\n(n_respondents &lt;- as.numeric(c(often_values[1, 5], often_values[2, 5])))\n## [1] 37641  5142\n\nprops_basic_samples &lt;- model_props_basic$sample(\n  data = list(often_us = n_often[2],\n              total_us = n_respondents[2],\n              often_mexico = n_often[1],\n              total_mexico = n_respondents[1]),\n  chains = CHAINS, iter_warmup = WARMUP, iter_sampling = (ITER - WARMUP), seed = BAYES_SEED,\n  refresh = 0\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.3 seconds.\n\nprops_basic_samples$print(\n  variables = c(\"pi_us\", \"pi_mexico\"), \n  \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.025, 0.975))\n)\n##   variable mean median   sd 2.5% 97.5%\n##  pi_us     0.10   0.10 0.00 0.09  0.11\n##  pi_mexico 0.28   0.28 0.00 0.28  0.29\n\n\nWe have the two proportions—10% and 28%—and they match what we found in the original table and in the frequentist prop.test() results (yay!). Let’s visualize these things:\n\nCodeprops_basic_samples %&gt;% \n  gather_draws(pi_us, pi_mexico) %&gt;% \n  ggplot(aes(x = .value, y = .variable, fill = .variable)) +\n  stat_halfeye() +\n  # Multiply axis limits by 1.5% so that the right \"%\" isn't cut off\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\n\n\nPosterior distributions of the proportion of students who read comic books often in the United States and Mexico\n\n\n\nThis plot is neat for a couple reasons. First it shows the difference in variance across these two distributions. The sample size for Mexican respondents is huge, so the average is a lot more precise and ■the distribution is narrower than ■the American one. Second, by just eyeballing the plot we can see that there’s definitely no overlap between the two distributions, which implies that ■the difference (θ) between the two is definitely not zero—Mexican respondents are way more likely than Americans to read comic books often. We can find ■that difference by taking the pairwise difference between the two with compare_levels() from {tidybayes}, which subtracts one group’s posterior from the other:\n\nCodebasic_diffs &lt;- props_basic_samples %&gt;% \n  gather_draws(pi_us, pi_mexico) %&gt;% \n  ungroup() %&gt;% \n  # compare_levels() subtracts things using alphabetical order, so by default it\n  # would calculate pi_us - pi_mexico, but we want the opposite, so we have to\n  # make pi_us the first level\n  mutate(.variable = fct_relevel(.variable, \"pi_us\")) %&gt;% \n  compare_levels(.value, by = .variable, comparison = \"pairwise\")\n\nbasic_diffs %&gt;% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\nPosterior distribution of the difference in proportions of students who read comic books often in the United States and Mexico\n\n\n\nWe can also do some Bayesian inference and find the probability that ■that difference between the two groups is greater than 0 (or a kind of Bayesian p-value, but way more logical than null hypothesis p-values). We can calculate how many posterior draws are bigger than 0 and divide that by the number of draws to get the official proportion.\n\nCodebasic_diffs %&gt;% \n  summarize(median = median_qi(.value),\n            p_gt_0 = sum(.value &gt; 0) / n()) %&gt;% \n  unnest(median)\n## # A tibble: 1 × 8\n##   .variable             y  ymin  ymax .width .point .interval p_gt_0\n##   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;\n## 1 pi_mexico - pi_us 0.180 0.170 0.189   0.95 median qi             1\n\n\nThere’s a 100% chance that ■that difference is not zero, or a 100% chance that Mexican respondents are way more likely than their American counterparts to read comic books often.\nStan model improvements\nThis basic Stan model is neat, but we can do a couple things to make it better:\n\nRight now we have to feed it 4 separate numbers (counts and totals for the US and Mexico). It would be nice to just feed it a vector of counts and a vector of totals (or even a whole matrix like we did with prop.test()).\nRight now we have to manually calculate the difference between the two groups (0.28 − 0.10). It would be nice to have Stan do that work for us.\n\nWe’ll tackle each of these issues in turn.\nFirst we’ll change how the script handles the data so that it’s more dynamic. Now instead of defining explicit variables and parameters as total_us or pi_mexico or whatever, we’ll use arrays and vectors so that we can use any arbitrary number of groups if we want:\n\n\nprops-better.stan\n\n// Stuff from R\ndata {\n  int&lt;lower=0&gt; n;\n  array[n] int&lt;lower=0&gt; often;\n  array[n] int&lt;lower=0&gt; total;\n}\n\n// Things to estimate\nparameters {\n  vector&lt;lower=0, upper=1&gt;[n] pi;\n}\n\n// Prior and likelihood\nmodel {\n  pi ~ beta(2, 6);\n  \n  // We could specify separate priors like this\n  // pi[1] ~ beta(2, 6);\n  // pi[2] ~ beta(2, 6);\n  \n  often ~ binomial(total, pi);\n}\n\nLet’s make sure it works. Note how we now have to feed it an n for the number of countries and vectors of counts for often and total:\n\nCodemodel_props_better &lt;- cmdstan_model(\"props-better.stan\")\n\n\n\nCodeprops_better_samples &lt;- model_props_better$sample(\n  data = list(n = 2,\n              often = c(n_often[2], n_often[1]),\n              total = c(n_respondents[2], n_respondents[1])),\n  chains = CHAINS, iter_warmup = WARMUP, iter_sampling = (ITER - WARMUP), seed = BAYES_SEED,\n  refresh = 0\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\n\nprops_better_samples$print(\n  variables = c(\"pi[1]\", \"pi[2]\"), \n  \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.025, 0.975))\n)\n##  variable mean median   sd 2.5% 97.5%\n##     pi[1] 0.10   0.10 0.00 0.09  0.11\n##     pi[2] 0.28   0.28 0.00 0.28  0.29\n\n\nIt worked and the results are the same! The parameter names are now ■“pi[1]” and ■“pi[2]” and we’re responsible for keeping track of which subscripts correspond to which countries, which is annoying, but that’s Stan :shrug:.\nFinally, we can modify the script a little more to automatically calculate ■θ. We’ll add a generated quantities block for that:\n\n\nprops-best.stan\n\n// Stuff from R\ndata {\n  int&lt;lower=0&gt; n;\n  array[n] int&lt;lower=0&gt; often;\n  array[n] int&lt;lower=0&gt; total;\n}\n\n// Things to estimate\nparameters {\n  vector&lt;lower=0, upper=1&gt;[n] pi;\n}\n\n// Prior and likelihood\nmodel {\n  pi ~ beta(2, 6);\n  \n  often ~ binomial(total, pi);\n}\n\n// Stuff Stan will calculate before sending back to R\ngenerated quantities {\n  real theta;\n  theta = pi[2] - pi[1];\n}\n\n\nCodemodel_props_best &lt;- cmdstan_model(\"props-best.stan\")\n\n\n\nCodeprops_best_samples &lt;- model_props_best$sample(\n  data = list(n = 2,\n              often = c(n_often[2], n_often[1]),\n              total = c(n_respondents[2], n_respondents[1])),\n  chains = CHAINS, iter_warmup = WARMUP, iter_sampling = (ITER - WARMUP), seed = BAYES_SEED,\n  refresh = 0\n)\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.3 seconds.\n\nprops_best_samples$print(\n  variables = c(\"pi[1]\", \"pi[2]\", \"theta\"), \n  \"mean\", \"median\", \"sd\", ~quantile(.x, probs = c(0.025, 0.975))\n)\n##  variable mean median   sd 2.5% 97.5%\n##     pi[1] 0.10   0.10 0.00 0.09  0.11\n##     pi[2] 0.28   0.28 0.00 0.28  0.29\n##     theta 0.18   0.18 0.00 0.17  0.19\n\n\nCheck that out! Stan returned our ■18 percentage point difference and we didn’t need to use compare_levels()! We can plot it directly:\n\nCode# Raw Stan doesn't preserve the original country names or order, so we have to\n# do a bunch of reversing and relabeling on our own here\np1 &lt;- props_best_samples %&gt;% \n  spread_draws(pi[i]) %&gt;% \n  mutate(i = factor(i)) %&gt;% \n  ggplot(aes(x = pi, y = fct_rev(i), fill = fct_rev(i))) +\n  stat_halfeye() +\n  # Multiply axis limits by 1.5% so that the right \"%\" isn't cut off\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_y_discrete(labels = rev(c(\"United States\", \"Mexico\"))) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\np2 &lt;- props_best_samples %&gt;% \n  spread_draws(theta) %&gt;% \n  ggplot(aes(x = theta)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n(p1 / plot_spacer() / p2) + \n  plot_layout(heights = c(0.785, 0.03, 0.185))\n\n\n\nPosterior distribution of the proportions and difference in proportions of students who read comic books often in the United States and Mexico; results from raw Stan code"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-brms",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-brms",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Bayesianly with {brms}",
    "text": "Bayesianly with {brms}\nWorking with raw Stan code like that is fun and exciting—understanding the inner workings of these models is really neat and important! But in practice, I rarely use raw Stan. It takes too much pre- and post-processing for my taste (the data has to be fed in a list instead of a nice rectangular data frame; the variable names get lost unless you do some extra programming work; etc.).\nInstead, I use {brms} for pretty much all my Bayesian models. It uses R’s familiar formula syntax, it works with regular data frames, it maintains variable names, and it’s just an all-around super-nice-and-polished frontend for working with Stan.\nWith a little formula finagling, we can create the same beta binomial model we built with raw Stan using {brms}\nCounts and trials as formula outcomes\nIn R’s standard formula syntax, you put the outcome on the left-hand side of a ~ and the explanatory variables on the right-hand side:\nlm(y ~ x, data = whatever)\nYou typically feed the model function a data frame with columns for each of the variables included. One neat and underappreciated feature of the glm() function is that you can feed function aggregated count data (instead of long data with lots of rows) by specifying the number of successes and the total number of failures as the outcome part of the formula. This runs something called aggregated logistic regression or aggregated binomial regression.\nglm(cbind(n_successes, n_failures) ~ x, data = whatever, family = binomial)\n{brms} uses slightly different syntax for aggregated logistic regression. Instead of the number of failures, it needs the total number of trials, and it doesn’t use cbind(...)—it uses n | trials(total), like this:\nbrm(\n  bf(n | trials(total) ~ x)\n  data = whatever,\n  family = binomial\n)\nOur comic book data is already in this count form, and we have columns for the number of “successes” (number of respondents reading comic books often) and the total number of “trials” (number of respondents reading comic books):\n\nCodeoften_comics_only &lt;- reading_counts %&gt;% \n  filter(book_type == \"Comic books\", frequency == \"Often\")\noften_comics_only\n## # A tibble: 2 × 6\n##   country       book_type   frequency     n total  prop\n##   &lt;fct&gt;         &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 Mexico        Comic books Often     10605 37641 0.282\n## 2 United States Comic books Often       524  5142 0.102\n\n\nBinomial model with logistic link\nSince we have columns for n, total, and country, we can run an aggregated binomial logistic regression model like this:\n\nCodeoften_comics_model_logit &lt;- brm(\n  bf(n | trials(total) ~ country),\n  data = often_comics_only,\n  family = binomial(link = \"logit\"),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\n\n\n\nCodeoften_comics_model_logit\n##  Family: binomial \n##   Links: mu = logit \n## Formula: n | trials(total) ~ country \n##    Data: often_comics_only (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept              -0.94      0.01    -0.96    -0.91 1.00     4316     2499\n## countryUnitedStates    -1.24      0.05    -1.33    -1.15 1.01      966      721\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBecause we used a logit link for the binomial family, these results are on the log odds scale, which, ew. We can’t really interpret them directly unless we do some extra math with plogis() (see here for more about how to do that). The logistic-ness of the results is also apparent in the formal mathy model for this approach, which no longer uses a Beta distribution for estimating \\(\\pi\\):\n\\[\n\\begin{aligned}\ny_{n \\text{ comic books, often}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}}, \\pi_{\\text{comic books, often}}) \\\\\n\\operatorname{logit}(\\pi_{\\text{comic books, often}}) =&\\ \\beta_0 + \\beta_1 \\text{United States} \\\\[5pt]\n\\beta_0, \\beta_1 =& \\text{Whatever brms uses as default priors}\\\\\n\\end{aligned}\n\\]\nWe can still work with percentage point values if we use epred_draws() and a bit of data wrangling, since that automatically back-transforms \\(\\operatorname{logit}(\\pi)\\) from log odds to counts (see here for an explanation of how and why). We can convert these posterior counts to a proportion again by dividing each predicted count by the total for each row.\n\nCode# brms keeps all the original factor/category names, so there's no need for\n# extra manual work here!\ndraws_logit &lt;- often_comics_model_logit %&gt;% \n  # This gives us counts...\n  epred_draws(newdata = often_comics_only) %&gt;% \n  # ...so divide by the original total to get proportions again\n  mutate(.epred_prop = .epred / total)\n\np1 &lt;- draws_logit %&gt;% \n  ggplot(aes(x = .epred_prop, y = country, fill = country)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\np2 &lt;- draws_logit %&gt;% \n  ungroup() %&gt;% \n  # compare_levels() subtracts things using alphabetical order, so so we have to\n  # make the United States the first level\n  mutate(country = fct_relevel(country, \"United States\")) %&gt;% \n  compare_levels(.epred_prop, by = \"country\") %&gt;% \n  ggplot(aes(x = .epred_prop)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  # Make it so the pointrange doesn't get cropped\n  coord_cartesian(clip = \"off\") + \n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n(p1 / plot_spacer() / p2) + \n  plot_layout(heights = c(0.785, 0.03, 0.185))\n\n\n\nPosterior distribution of the proportions and difference in proportions of students who read comic books often in the United States and Mexico; results from logistic regression model in {brms}\n\n\n\n\nCodedraws_logit %&gt;% \n  group_by(country) %&gt;% \n  median_qi(.epred_prop)\n## # A tibble: 2 × 7\n##   country       .epred_prop .lower .upper .width .point .interval\n##   &lt;fct&gt;               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Mexico              0.282 0.277   0.286   0.95 median qi       \n## 2 United States       0.102 0.0942  0.111   0.95 median qi\n\n\nCool cool, all the results are the same as using raw Stan.\nBinomial model with identity link\nWe didn’t set any priors here, and if we want to be good Bayesians, we should. However, given the logit link, we’d need to specify priors on the log odds scale, and I can’t naturally think in logits. I can think about percentages though, which is why I like the Beta distribution for priors for proportions—it just makes sense.\nAlso, the raw Stan models spat out percentage-point scale parameters—it’d be neat if {brms} could too.\nAnd it can! We just have to change the link function for the binomial family from \"logit\" to \"identity\". This isn’t really documented anywhere (I don’t think?), and it feels weird and wrong, but it works. Note how we take the “logit” out of the second line of the model—we’re no longer using a link function:\n\\[\n\\begin{aligned}\ny_{n \\text{ comic books, often}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}}, \\pi_{\\text{comic books, often}}) \\\\\n\\pi_{\\text{comic books, often}} =&\\ \\beta_0 + \\beta_1 \\text{United States} \\\\[5pt]\n\\beta_0, \\beta_1 =&\\ \\text{Whatever brms uses as default priors}\\\\\n\\end{aligned}\n\\]\nDoing this works, but there are some issues. The identity link in a binomial model means that the model parameters won’t be transformed to the logit scale and will instead stay on the proportion scale. We’ll get some errors related to MCMC values because the outcome needs to be constrained between 0 and 1, and the MCMC chains will occasionally wander down into negative numbers and make Stan mad. The model will mostly fit if we specify initial MCMC values at 0.1 or something, but it’ll still complain.\n\nCodeoften_comics_model_identity &lt;- brm(\n  bf(n | trials(total) ~ country),\n  data = often_comics_only,\n  family = binomial(link = \"identity\"),\n  init = 0.1,\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## Chain 1 Rejecting initial value:\n## Chain 1   Error evaluating the log probability at the initial value.\n## Chain 1 Exception: binomial_lpmf: Probability parameter[2] is -0.00152456, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpC5PR8i/model-6fd51f72abc7.stan', line 35, column 4 to column 44)\n## Chain 1 Rejecting initial value:\n## Chain 1   Error evaluating the log probability at the initial value.\n## Chain 1 Exception: binomial_lpmf: Probability parameter[2] is -0.0379836, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpC5PR8i/model-6fd51f72abc7.stan', line 35, column 4 to column 44)\n## Chain 2 Rejecting initial value:\n## Chain 2   Error evaluating the log probability at the initial value.\n## Chain 2 Exception: binomial_lpmf: Probability parameter[2] is -0.00730659, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpC5PR8i/model-6fd51f72abc7.stan', line 35, column 4 to column 44)\n## Chain 3 Rejecting initial value:\n## Chain 3   Error evaluating the log probability at the initial value.\n## Chain 3 Exception: binomial_lpmf: Probability parameter[1] is -0.0734376, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpC5PR8i/model-6fd51f72abc7.stan', line 35, column 4 to column 44)\n## Chain 3 Rejecting initial value:\n## Chain 3   Error evaluating the log probability at the initial value.\n## Chain 3 Exception: binomial_lpmf: Probability parameter[1] is -0.084938, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpC5PR8i/model-6fd51f72abc7.stan', line 35, column 4 to column 44)\n## Chain 4 Rejecting initial value:\n## Chain 4   Error evaluating the log probability at the initial value.\n## Chain 4 Exception: binomial_lpmf: Probability parameter[1] is -0.0585092, but must be in the interval [0, 1] (in '/var/folders/17/g3pw3lvj2h30gwm67tbtx98c0000gn/T/RtmpC5PR8i/model-6fd51f72abc7.stan', line 35, column 4 to column 44)\n## Chain 1 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## Chain 2 finished in 0.2 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.1 seconds.\n## Total execution time: 0.4 seconds.\n\n\n\nCodeoften_comics_model_identity\n##  Family: binomial \n##   Links: mu = identity \n## Formula: n | trials(total) ~ country \n##    Data: often_comics_only (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept               0.28      0.00     0.28     0.29 1.00     3372     2728\n## countryUnitedStates    -0.18      0.00    -0.19    -0.17 1.00     1292     1659\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOne really nice thing about this identity-link model is that the coefficient for countryUnitedStates shows us the percentage-point-scale difference in proportions: −0.18! This is just a regular regression model, so the intercept shows us the average proportion when United States is false (i.e. for Mexico), and the United States coefficient shows the offset from the intercept.\nWorking with the countryUnitedStates coefficient directly is convenient—there’s no need to divide predicted values by totals or use compare_levels() to find the difference between the United States and Mexico. We have ■our estimand immediately.\n\nCodedraws_diffs_identity &lt;- often_comics_model_identity %&gt;% \n  gather_draws(b_countryUnitedStates) %&gt;% \n  # Reverse the value since our theta is Mexico - US, not US - Mexico\n  mutate(.value = -.value)\n\ndraws_diffs_identity %&gt;% \n  ggplot(aes(x = .value)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\nPosterior distribution of the difference in proportions of students who read comic books often in the United States and Mexico; results from binomial model with identity link in {brms}\n\n\n\n\nCodedraws_diffs_identity %&gt;% \n  median_qi(.value)\n## # A tibble: 1 × 7\n##   .variable             .value .lower .upper .width .point .interval\n##   &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 b_countryUnitedStates  0.179  0.170  0.189   0.95 median qi\n\n\nIntercept-free binomial model with identity link\nHowever, I’m still not entirely happy with it. For one thing, I don’t like all the initial MCMC errors. The model still eventually fit, but I’d prefer it to have a less rocky start. I could probably tinker with more options to get it working, but that’s a hassle.\nMore importantly, though, is the issue of priors. We still haven’t set any. Also, we’re no longer using a beta-binomial model—this is just regular old logistic regression, which means we’re working with intercepts and slopes. If we use the n | trials(total) ~ country model with an identity link, we’d need to set priors for the intercept and for the difference, which means we need to think about two types of values: (1) the prior average percentage for Mexico and (2) the prior average difference between Mexico and the United States. In the earlier raw Stan model, we set priors for the average percentages for each country and didn’t worry about thinking about the difference. Conceptually, I think this is easier. In my own work, I can think about the prior distributions for specific survey response categories (30% might agree, 50% might disagree, 20% might be neutral), but thinking about differences is less natural and straightforward (there might be a 20 percentage point difference between agree and disagree? that feels weird).\nTo get percentages for each country and avoid the odd initial value errors and set more natural priors, and ultimately use a beta-binomial model, we can fit an intercept-free model by including a 0 in the right-hand side of the formula. This disables the Mexico reference category and returns estimates for both Mexico and the United States. Now we can finally set a prior too. Here, as I did with the Stan model earlier, I use Beta(2, 6) for both countries, but it could easily be different for each country too. This is one way to force {brms} to essentially use a beta-binomial model, and results in something like this:\n\\[\n\\begin{aligned}\ny_{n \\text{ comic books, often}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}}, \\pi_{\\text{comic books, often}}) \\\\\n\\pi_{\\text{comic books, often}} =&\\ \\beta_\\text{Mexico} + \\beta_\\text{United States} \\\\[10pt]\n\\beta_\\text{Mexico}, \\beta_\\text{United States} =&\\ \\operatorname{Beta}(2, 6)\\\\\n\\end{aligned}\n\\]\n\nCodeoften_comics_model &lt;- brm(\n  bf(n | trials(total) ~ 0 + country),\n  data = often_comics_only,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(2, 6), class = b, lb = 0, ub = 1)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\n\n\n\nCodeoften_comics_model\n##  Family: binomial \n##   Links: mu = identity \n## Formula: n | trials(total) ~ 0 + country \n##    Data: often_comics_only (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## countryMexico           0.28      0.00     0.28     0.29 1.00     3627     2727\n## countryUnitedStates     0.10      0.00     0.09     0.11 1.00     3632     2560\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe coefficients here represent the average proportions for each country. The ■main estimand we care about is still the difference between the two, so we need to do a little bit of data manipulation to calculate that, just like we did with the first logit version of the model:\n\nCodep1 &lt;- often_comics_model %&gt;% \n  epred_draws(newdata = often_comics_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ggplot(aes(x = .epred_prop, y = country, fill = country)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\np2 &lt;- often_comics_model %&gt;% \n  epred_draws(newdata = often_comics_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ungroup() %&gt;% \n  # compare_levels() subtracts things using alphabetical order, so so we have to\n  # make the United States the first level\n  mutate(country = fct_relevel(country, \"United States\")) %&gt;% \n  compare_levels(.epred_prop, by = country,\n                 comparison = \"pairwise\") %&gt;% \n  ggplot(aes(x = .epred_prop)) +\n  stat_halfeye(fill = clr_diff) +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  # Make it so the pointrange doesn't get cropped\n  coord_cartesian(clip = \"off\") + \n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n(p1 / plot_spacer() / p2) + \n  plot_layout(heights = c(0.785, 0.03, 0.185))\n\n\n\nPosterior distribution of the proportions and difference in proportions of students who read comic books often in the United States and Mexico; results from an intercept-free binomial model with identity link in {brms}\n\n\n\nActual beta-binomial model\nUntil {brms} 2.17, there wasn’t an official beta-binomial distribution for {brms}, but it was used as the example for creating your own custom family. It is now implemented in {brms} and allows you to define both a mean (\\(\\mu\\)) and precision (\\(\\phi\\)) for a Beta distribution, just like {brms}’s other Beta-related models (like zero-inflated, etc.—see here for a lot more about those). This means that we can model both parts of the distribution simultaneously, which is neat, since it allows us to deal with potential overdispersion in outcomes. Paul Bürkner’s original rationale for not including it was that a regular binomial model with a random effect for the observation id also allows you to account for overdispersion, so there’s not really a need for an official beta-binomial family. But in March 2022 the beta_binomial family was added as an official distributional family, which is neat.\nWe can use it here instead of family = binomial(link = \"identity\") with a few adjustments. The family uses a different mean/precision parameterization of the Beta distribution instead of the two shapes \\(\\alpha\\) and \\(\\beta\\), but we can switch between them with some algebra (see this for more details):\n\\[\n\\begin{equation}\n\\begin{aligned}[t]\n\\text{Shape 1:} && \\alpha &= \\mu \\phi \\\\\n\\text{Shape 2:} && \\beta &= (1 - \\mu) \\phi\n\\end{aligned}\n\\qquad\\qquad\\qquad\n\\begin{aligned}[t]\n\\text{Mean:} && \\mu &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\text{Precision:} && \\phi &= \\alpha + \\beta\n\\end{aligned}\n\\end{equation}\n\\]\nBy default, \\(\\mu\\) is modeled on the log odds scale and \\(\\phi\\) is modeled on the log scale, but I find both of those really hard to think about, so we can use an identity link for both parameters like we did before with binomial() to think about counts and proportions instead. This makes it so the \\(\\phi\\) parameter measures the standard deviation of the count on the count scale, so a prior like Exponential(1 / 1000) would imply that the precision (or variance-ish) of the count could vary by mostly low numbers, but maybe up to ±5000ish, which seems reasonable, especially since the Mexico part of the survey has so many respondents:\n\nCodeggplot() +\n  stat_function(fun = ~dexp(., 0.001), geom = \"area\", fill = \"#AC3414\") +\n  scale_x_continuous(labels = label_number(big.mark = \",\"), limits = c(0, 5000)) +\n  labs(x = \"Possible values for φ\", y = NULL, fill = NULL) +\n  theme_nice() +\n  theme(axis.text.y = element_blank())\n\n\n\nExponential(1/1000) distribution\n\n\n\n\nCodeoften_comics_model_beta_binomial &lt;- brm(\n  bf(n | trials(total) ~ 0 + country),\n  data = often_comics_only,\n  family = beta_binomial(link = \"identity\", link_phi = \"identity\"),\n  prior = c(prior(beta(2, 6), class = \"b\", dpar = \"mu\", lb = 0, ub = 1),\n            prior(exponential(0.001), class = \"phi\", lb = 0)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\n\n\n\n\n\n\n\n\nSeparate model for \\(\\phi\\)\n\n\n\nIf we wanted to be super fancy, we could define a completely separate model for the \\(\\phi\\) part of the distribution like this, but we don’t need to here:\noften_comics_model_beta_binomial &lt;- brm(\n  bf(n | trials(total) ~ 0 + country,\n     phi ~ 0 + country),\n  data = often_comics_only,\n  family = beta_binomial(link = \"identity\", link_phi = \"identity\"),\n  prior = c(prior(beta(2, 6), class = \"b\", dpar = \"mu\", lb = 0, ub = 1),\n            prior(exponential(0.001), dpar = \"phi\", lb = 0)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n\n\nCheck out the results:\n\nCodeoften_comics_model_beta_binomial\n##  Family: beta_binomial \n##   Links: mu = identity; phi = identity \n## Formula: n | trials(total) ~ 0 + country \n##    Data: often_comics_only (Number of observations: 2) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## countryMexico           0.28      0.03     0.22     0.33 1.00     1563      812\n## countryUnitedStates     0.11      0.02     0.08     0.15 1.00     1686     1441\n## \n## Further Distributional Parameters:\n##     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## phi  1011.93    993.85    35.55  3647.19 1.00      761      790\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThose coefficients are the group proportions, as expected, and we have a \\(\\phi\\) parameter representing the overall variation in counts. The proportions here are a little more uncertain than before, though, which is apparent if we plot the distributions. The distributions have a much wider range now (note that the x-axis now goes all the way up to 60%), and the densities are a lot bumpier and jankier. I don’t know why though! This is weird! I’m probably doing something wrong!\n\nCodeoften_comics_model_beta_binomial %&gt;% \n  epred_draws(newdata = often_comics_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ggplot(aes(x = .epred_prop, y = country, fill = country)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent(), expand = c(0, 0.015)) +\n  scale_fill_manual(values = c(clr_usa, clr_mex)) +\n  guides(fill = \"none\") +\n  labs(x = \"Proportion of students who read comic books often\",\n       y = NULL) +\n  theme_nice()\n\n\n\nPosterior distributions of the proportion of students who read comic books often in the United States and Mexico"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#final-answer-to-the-question",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#final-answer-to-the-question",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Final answer to the question",
    "text": "Final answer to the question\nPhew, that was a lot of slow pedagogical exposure. What’s our ■official estimand? What’s our final answer to the question “Do respondents in Mexico read comic books more often than respondents in the United States?”?\nYes. They most definitely do.\nIn an official sort of report or article, I’d write something like this:\n\nStudents in Mexico are far more likely to read comic books often than students in the United States. On average, 28.2% (between 27.7% and 28.6%) of PISA respondents in Mexico read comic books often, compared to 10.2% (between 9.4% and 11.1%) in the United States. There is a 95% posterior probability that the difference between these proportions is between 17.0 and 18.9 percentage points, with a median of 18.0 percentage points. This difference is substantial, and there’s a 100% chance that the difference is not zero."
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#estimand-1",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#estimand-1",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Estimand",
    "text": "Estimand\nFor this question, we want to know the differences in the the proportions of American newspaper-reading frequencies, or whether the differences between (1) ■rarely and ■sometimes, (2) ■sometimes and ■often, and (3) ■rarely and ■often are greater than zero:\n\nCodefancy_table %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = colorspace::lighten(\"#E51B24\", 0.8)),\n      cell_text(color = \"#E51B24\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = 3, rows = 4)\n    )\n  ) %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = colorspace::lighten(\"#FFDE00\", 0.8)),\n      cell_text(color = colorspace::darken(\"#FFDE00\", 0.1), weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = 4, rows = 4)\n    )\n  ) %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(color = colorspace::lighten(\"#009DDC\", 0.8)),\n      cell_text(color = \"#009DDC\", weight = \"bold\")\n    ),\n    locations = list(\n      cells_body(columns = 5, rows = 4)\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nRarely\nSometimes\nOften\n\n\n\nMexico\n\n\nComic books\n26.29%(9,897)\n45.53%(17,139)\n28.17%(10,605)\n\n\nNewspapers\n18.02%(6,812)\n32.73%(12,369)\n49.25%(18,613)\n\n\nUnited States\n\n\nComic books\n62.95%(3,237)\n26.86%(1,381)\n10.19%(524)\n\n\nNewspapers\n25.27%(1,307)\n37.22%(1,925)\n37.51%(1,940)\n\n\n\n\n\n\nWe’ll again call this estimand \\(\\theta\\), but have three different versions of it:\n\\[\n\\begin{aligned}\n\\theta_1 &= \\pi_\\text{US, newspapers, often} - \\pi_\\text{US, newspapers, sometimes} \\\\\n\\theta_2 &= \\pi_\\text{US, newspapers, sometimes} - \\pi_\\text{US, newspapers, rarely} \\\\\n\\theta_3 &= \\pi_\\text{US, newspapers, often} - \\pi_\\text{US, newspapers, rarely}\n\\end{aligned}\n\\] We just spent a bunch of time talking about comic books, and now we’re looking at data about newspapers and America. Who represents all three of these things simultaneously? Clark Kent / Superman, obviously, the Daily Planet journalist and superpowered alien dedicated to truth, justice, and the American way a better tomorrow. I found this palette at Adobe Color.\n\nCode# US newspaper question colors\nclr_often &lt;- \"#009DDC\"\nclr_sometimes &lt;- \"#FFDE00\"\nclr_rarely &lt;- \"#E51B24\"\n\n\n\n\n\n“Superman” by Hannaford"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-brms-1",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#bayesianly-with-brms-1",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Bayesianly with {brms}",
    "text": "Bayesianly with {brms}\nLet’s first extract the aggregated data we’ll work with—newspaper frequency in the United States only:\n\nCodenewspapers_only &lt;- reading_counts %&gt;% \n  filter(book_type == \"Newspapers\", country == \"United States\")\nnewspapers_only\n## # A tibble: 3 × 6\n##   country       book_type  frequency     n total  prop\n##   &lt;fct&gt;         &lt;chr&gt;      &lt;fct&gt;     &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 United States Newspapers Rarely     1307  5172 0.253\n## 2 United States Newspapers Sometimes  1925  5172 0.372\n## 3 United States Newspapers Often      1940  5172 0.375\n\n\nWe’ll define this formal beta-binomial model for each of the group proportions and we’ll use a Beta(2, 6) prior again (so 25% ± a bunch):\n\\[\n\\begin{aligned}\n&\\ \\textbf{Estimands} \\\\\n\\theta_1 =&\\ \\pi_{\\text{newspapers, often}_\\text{US}} - \\pi_{\\text{newspapers, sometimes}_\\text{US}} \\\\\n\\theta_2 =&\\ \\pi_{\\text{newspapers, sometimes}_\\text{US}} - \\pi_{\\text{newspapers, rarely}_\\text{US}} \\\\\n\\theta_3 =&\\ \\pi_{\\text{newspapers, often}_\\text{US}} - \\pi_{\\text{newspapers, rarely}_\\text{US}} \\\\[10pt]\n&\\ \\textbf{Beta-binomial model} \\\\\ny_{n \\text{ newspapers, [frequency]}_\\text{US}} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total students}_\\text{US}}, \\pi_{\\text{newspapers, [frequency]}_\\text{US}}) \\\\\n\\pi_{\\text{newspapers, [frequency]}_\\text{US}} \\sim&\\ \\operatorname{Beta}(\\alpha, \\beta) \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\alpha =&\\ 2 \\\\\n\\beta =&\\ 6\n\\end{aligned}\n\\]\nWe can estimate this model with {brms} using an intercept-free binomial model with an identity link:\n\nCodefreq_newspapers_model &lt;- brm(\n  bf(n | trials(total) ~ 0 + frequency),\n  data = newspapers_only,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(2, 6), class = b, lb = 0, ub = 1)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\n\n\n\nCodefreq_newspapers_model\n##  Family: binomial \n##   Links: mu = identity \n## Formula: n | trials(total) ~ 0 + frequency \n##    Data: newspapers_only (Number of observations: 3) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## frequencyRarely        0.25      0.01     0.24     0.26 1.00     4059     2461\n## frequencySometimes     0.37      0.01     0.36     0.39 1.00     4577     2890\n## frequencyOften         0.37      0.01     0.36     0.39 1.00     4055     2824\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIt works! These group proportions are the same as what we found in the contingency table:\n\nCodeplot_props_newspaper &lt;- freq_newspapers_model %&gt;% \n  epred_draws(newdata = newspapers_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ggplot(aes(x = .epred_prop, y = frequency, fill = frequency)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = c(clr_rarely, clr_sometimes, clr_often)) +\n  labs(x = \"Proportion of frequencies of newspaper reading\", y = NULL) +\n  guides(fill = \"none\") +\n  theme_nice()\nplot_props_newspaper\n\n\n\nPosterior distributions of the proportions of the frequency of reading newspapers among American students\n\n\n\nWe’re interested in our three \\(\\theta\\)s, or the posterior differences between each of these proportions. We can again use compare_levels() to find these all at once. If we specify comparison = \"pairwise\", {tidybayes} will calculate the differences between each pair of proportions.\n\nCodefreq_newspapers_diffs &lt;- freq_newspapers_model %&gt;% \n  epred_draws(newdata = newspapers_only) %&gt;% \n  mutate(.epred_prop = .epred / total) %&gt;% \n  ungroup() %&gt;% \n  compare_levels(.epred_prop, by = frequency) %&gt;% \n  # Put these in the right order\n  mutate(frequency = factor(frequency, levels = c(\"Often - Sometimes\", \n                                                  \"Sometimes - Rarely\",\n                                                  \"Often - Rarely\")))\n\nfreq_newspapers_diffs %&gt;% \n  ggplot(aes(x = .epred_prop, y = fct_rev(frequency), fill = frequency)) +\n  stat_halfeye(fill = clr_diff) +\n  geom_vline(xintercept = 0, color = \"#F012BE\") +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  guides(fill = \"none\") +\n  theme_nice()\n\n\n\nPosterior distributions of the differences between various frequencies of reading newspapers among American students; all density plots are filled with the same color for now\n\n\n\nWe can also calculate the official median differences and the probabilities that the posteriors are greater than 0:\n\nCodefreq_newspapers_diffs %&gt;% \n  summarize(median = median_qi(.epred_prop, .width = 0.95),\n            p_gt_0 = sum(.epred_prop &gt; 0) / n()) %&gt;% \n  unnest(median)\n## # A tibble: 3 × 8\n##   frequency                y    ymin   ymax .width .point .interval p_gt_0\n##   &lt;fct&gt;                &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;\n## 1 Often - Sometimes  0.00296 -0.0158 0.0215   0.95 median qi         0.622\n## 2 Sometimes - Rarely 0.119    0.101  0.137    0.95 median qi         1    \n## 3 Often - Rarely     0.122    0.105  0.140    0.95 median qi         1\n\n\nThere’s only a 60ish% chance that the difference between ■often and ■sometimes is bigger than zero, so there’s probably not an actual difference between those two categories, but there’s a 100% chance that the differences between ■sometimes and ■rarely and ■often and ■rarely are bigger than zero."
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#better-fill-colors",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#better-fill-colors",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Better fill colors",
    "text": "Better fill colors\nBefore writing up the final official answer, we need to tweak the plot of differences. With the comic book question, we used some overly-simplified-and-wrong color theory and created a ■yellow color for the difference (since ■blue − ■green = ■yellow). We could maybe do that here too, but we’ve actually used all primary colors in our Superman palette. I don’t know what ■blue − ■yellow or ■yellow − ■red would be, and even if I calculated it somehow, it wouldn’t be as cutely intuitive as blue minus green.\nSo instead, we’ll do some fancy fill work with the neat {ggpattern} package, which lets us fill ggplot geoms with multiply-colored patterns. We’ll fill each distribution of \\(\\theta\\)s with the combination of the two colors: we’ll fill the difference between ■often and ■sometimes with stripes of those two colors, and so on.\nWe can’t use geom/stat_halfeye() because {tidybayes} does fancier geom work when plotting its density slabs, but we can use geom_density_pattern() to create normal density plots:\n\nCodefreq_newspapers_diffs %&gt;% \n  ggplot(aes(x = .epred_prop, fill = frequency, pattern_fill = frequency)) +\n  geom_density_pattern(\n    pattern = \"stripe\",  # Stripes\n    pattern_density = 0.5,  # Take up 50% of the pattern (i.e. stripes equally sized)\n    pattern_spacing = 0.2,  # Thicker stripes\n    pattern_size = 0,  # No border on the stripes\n    trim = TRUE,  # Trim the ends of the distributions\n    linewidth = 0  # No border on the distributions\n  ) +\n  geom_vline(xintercept = 0, color = \"#F012BE\") +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  # Set colors for fills and pattern fills\n  scale_fill_manual(values = c(clr_often, clr_sometimes, clr_often)) +\n  scale_pattern_fill_manual(values = c(clr_sometimes, clr_rarely, clr_rarely)) +\n  guides(fill = \"none\", pattern_fill = \"none\") +  # Turn off legends\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  facet_wrap(vars(frequency), ncol = 1) +\n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\nPosterior distributions of the differences between various frequencies of reading newspapers among American students; all density plots are filled with two stripes corresponding to the difference of their two categories\n\n\n\nAhhh that’s so cool!\nThe only thing that’s missing is the pointrange that we get from stat_halfeye() that shows the median and the 50%, 80%, and 95% credible intervals. We can calculate those ourselves and add them with geom_pointinterval():\n\nCode# Find medians and credible intervals\nfreq_newspapers_intervals &lt;- freq_newspapers_diffs %&gt;% \n  group_by(frequency) %&gt;% \n  median_qi(.epred_prop, .width = c(0.5, 0.8, 0.95))\n\nplot_diffs_nice &lt;- freq_newspapers_diffs %&gt;% \n  ggplot(aes(x = .epred_prop, fill = frequency, pattern_fill = frequency)) +\n  geom_density_pattern(\n    pattern = \"stripe\",  # Stripes\n    pattern_density = 0.5,  # Take up 50% of the pattern (i.e. stripes equally sized)\n    pattern_spacing = 0.2,  # Thicker stripes\n    pattern_size = 0,  # No border on the stripes\n    trim = TRUE,  # Trim the ends of the distributions\n    linewidth = 0  # No border on the distributions\n  ) +\n  # Add 50%, 80%, and 95% intervals + median\n  geom_pointinterval(data = freq_newspapers_intervals, \n                     aes(x = .epred_prop, xmin = .lower, xmax = .upper)) +\n  geom_vline(xintercept = 0, color = \"#F012BE\") +\n  # Multiply axis limits by 0.5% so that the right \"pp.\" isn't cut off\n  scale_x_continuous(labels = label_pp, expand = c(0, 0.005)) +\n  # Set colors for fills and pattern fills\n  scale_fill_manual(values = c(clr_often, clr_sometimes, clr_often)) +\n  scale_pattern_fill_manual(values = c(clr_sometimes, clr_rarely, clr_rarely)) +\n  guides(fill = \"none\", pattern_fill = \"none\") +  # Turn off legends\n  labs(x = \"Percentage point difference in proportions\",\n       y = NULL) +\n  facet_wrap(vars(frequency), ncol = 1) +\n  # Make it so the pointrange doesn't get cropped\n  coord_cartesian(clip = \"off\") + \n  theme_nice() +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank())\nplot_diffs_nice\n\n\n\nPosterior distributions of the differences between various frequencies of reading newspapers among American students; density plots are filled with stripes and a pointrange shows the median and 50%, 80%, and 95% credible intervals\n\n\n\nPerfect!"
  },
  {
    "objectID": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#final-answer-to-the-question-1",
    "href": "blog/2023/05/15/fancy-bayes-diffs-props/index.html#final-answer-to-the-question-1",
    "title": "A guide to Bayesian proportion tests with R and {brms}",
    "section": "Final answer to the question",
    "text": "Final answer to the question\nSo, given these results, what’s the answer to the question “Do students in the United States vary in their frequency of reading newspapers?”? What are our three \\(\\theta\\)s? The frequencies vary, but only between ■rarely and the other two categories. There’s no difference between ■sometimes and ■often\n\nCode(\n  (plot_props_newspaper + labs(x = \"Proportions\") + \n     facet_wrap(vars(\"Response proportions\"))) | \n    plot_spacer() |\n    (plot_diffs_nice + labs(x = \"Percentage point differences\"))\n) +\n  plot_layout(widths = c(0.475, 0.05, 0.475))\n\n\n\nPosterior distribution of the proportions and difference in proportions of the frequency of reading newspapers among American students\n\n\n\nHere’s how I’d write about the results:\n\nStudents in the United States tend to read the newspaper at least sometimes, and are least likely to read it rarely. On average, 25.3% of American PISA respondents report reading the newspaper rarely (with a 95% credible interval of between 24.1% and 26.4%), compared to 37.2% reading sometimes (35.9%–38.5%) and 37.5% reading sometimes (36.1%–38.8%).\nThere is no substantial difference in proportions between those reporting reading newspapers often and sometimes. The posterior median difference is between −1.6 and 2.2 percentage points, with a median of 0.3 percentage points, and there’s only a 62.2% probability that this difference is greater than 0, which implies that the two categories are indistinguishable from each other.\nThere is a clear substantial difference between the proportion reading newspapers rarely and the other two responses, though. The posterior median difference between sometimes and rarely is between 10.1 and 13.7 percentage points (median = 11.9), while the difference between often and rarely is between 10.5 and 14.0 percentage points (median = 12.2). The probability that each of these differences is greater than 0 is 100%.\n\n\nEt voila! Principled, easily interpretable, non-golem-based tests of differences in proportions using Bayesian statistics!"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html",
    "title": "How old was Aragorn in regular human years?",
    "section": "",
    "text": "In The Two Towers, while talking with Eowyn, Aragorn casually mentions that he’s actually 87 years old.\nWhen Aragorn is off running for miles and miles and fighting orcs and trolls and Uruk-hai and doing all his other Lord of the Rings adventures, he hardly behaves like a regular human 87-year-old. How old is he really?\nIt turns out that Tolkien left us a clue in some of his unfinished writings about Númenor, and we can use that information to make some educated guesses about Aragon’s actual human-scale age. In this post I’ll (1) look at Tolkien’s Númenórean years → human years conversion system and (2) extrapolate that system through two types of statistical simulation—(a) just drawing random numbers and (b) Bayesian modeling—to make some predictions about a range of possible ages.\nBut first, some context about why Aragon is so old!"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#super-quick-crash-course-in-the-ages-of-arda-númenor",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#super-quick-crash-course-in-the-ages-of-arda-númenor",
    "title": "How old was Aragorn in regular human years?",
    "section": "Super quick crash course in the Ages of Arda + Númenor",
    "text": "Super quick crash course in the Ages of Arda + Númenor\nThe Lord of the Rings occurs at the end of the Third Age of the world. In Arda (the whole world Tolkien created) there are four recorded ages, plus some pre-game stuff:\n\nCreation: The main god Eru Ilúvatar and a couple dozen sub-gods (Ainur) and bunch of angel-like-sub-sub-gods (Maiar, including Gandalf, Saruman, and Sauron) created the world through music. This is all covered in the first part of The Silmarillion in a sub-book called The Ainulindalë.\nThe First Age: After the creation, the Elves all lived in a place outside of the main world called Valinor until the main bad guy, a fallen Ainu named Morgoth (aka Melkor) destroyed key parts of it and divided the Elves so that a bunch fled to the far west of Middle-earth to a land called Beleriand. This starts the First Age, which is mostly about the Elves of Beleriand and their battles with Morgoth/Melkor. Sauron (the main bad guy of The Lord of the Rings) serves as Morgoth’s lieutenant and they destroy a ton of cities (with armies of balrogs and orcs) and kill a ton of elves. The elves eventually win, but all of Beleriand sinks into the ocean and the elves either go back to Valinor (technically just outside of Valinor, which serves as elvish heaven), or to eastern Middle-earth. This is all covered in the bulk of The Silmarillion.\n\nThe Second Age: While the First Age is mostly about elves, mortal men eventually show up in Beleriand and they play a key role in the battle against Morgoth. They also intermingle with the immortal elves, sometimes falling in love—including the famous Lúthien and Beren (who are stand-ins for Tolkien and his wife Edith). Beren and Lúthien had kids, and their kids had kids, and so on until two half-elf brothers were born at the end of the First Age: Elrond (the same Elrond from The Hobbit and The Lord of the Rings and The Rings of Power) and Elros.\nAs Beleriand sinks, Elrond and Elros escape to eastern Middle-earth and are then given a choice of how to proceed with their futures. Elrond decides to become immortal like an elf; Elros decides to become mortal like a man, eventually building Rivendell. The gods reward Elros and the men who helped the elves against Morgoth by creating a utopic island in the middle of the ocean between Valinor and Middle-earth named Númenor. The gods also granted them super long life (400+ years, as we’ll explore below), but imposed a strict ban on them—the Númenóreans were forbidden from ever sailing west toward Valinor. Thus begins the two parallel stories of the Second Age—(1) Elrond and other refugees from Valinor doing stuff in Middle-earth, and (2) Elros and his descendants doing stuff on the island of Númenor. This is all covered in a final short part of The Silmarillion (the Akallabêth), and in random appendices in The Lord of the Rings, and in the newer The Fall of Númenor, and in Amazon’s TV series The Rings of Power.\nNúmenor and Middle-earth hum along happily for three thousand years until Sauron (who fled to Middle-earth after Morgoth was destroyed) shows up. He disguises himself as a super affable friendly dude who everyone loves and then sows chaos. He visits the elves and convinces them to make a bunch of rings, and then he heads to Númenor to convince them to violate the ban and sail to Valinor. The Númenóreans do, they get in trouble, the gods sink their island, and Númenórean refugees flee to Middle-earth, led by Elendil and his sons Isildur and Anárion, who set up a Númenórean kingdom in Gondor. Sauron starts using his fancy new One Ring and tries to conquer Middle-earth; there’s a big war against him (see the first few minutes of The Fellowship of the Ring); Sauron kills Elendil; Isildur cuts off Sauron’s finger and makes him lose the ring, which destroys him; Isildur keeps the ring; he gets killed and the kingdom of Gondor falls, thus ending the Second Age.\n\n\nThe Third Age: The ring disappears for a few thousand years until Gollum picks it up, then Bilbo gets it in The Hobbit, and then Frodo gets it in The Fellowship of the Ring and destroys it in The Return of the King.\nMeanwhile, Gondor is ruled by a series of stewards who are supposed to take care of the kingdom until a Númenórean king returns to take his place. The magic long life of the Númenóreans starts declining, except for some special refugees from Gondor named the Dúnedain (singular Dúnadan), of which Aragorn is one. These Dúnedain maintain some of the magic Númenórean longevity—they don’t live 400+ years like their Númenórean ancestors, but they live well beyond 150. After the ring is destroyed, Aragorn is installed as king of Gondor, all the remaining elves go back to Valinor (except Arwen, Aragorn’s now-wife), and the Third Age ends.\n\nThe Fourth Age: Aragorn reigns until he’s 210, then he dies, someone else takes over, and Middle-earth lives happily ever after."
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#packages",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#packages",
    "title": "How old was Aragorn in regular human years?",
    "section": "Packages",
    "text": "Packages\nBefore diving into the data and simulations, we need to load some R libraries and make some helper functions. For the sake of narrative here, the code is automatically collapsed—click on the little triangle arrow to show it.\n\nCodelibrary(tidyverse)\nlibrary(broom)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(glue)\nlibrary(ggtext)\nlibrary(ggimage)\n\nset.seed(1234)\n\nupdate_geom_defaults(\"label\", list(family = \"Diavlo Medium\"))\nupdate_geom_defaults(\"richtext\", list(family = \"Diavlo Medium\"))\n\n# Diavlo: https://www.exljbris.com/diavlo.html\ntheme_numenor &lt;- function() {\n  theme_minimal(base_family = \"Diavlo Medium\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\", hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 0),\n          legend.title = element_text(face = \"bold\"))\n}\n\nclrs &lt;- c(\"#0E2B50\", \"#415B6B\", \"#0B4C7D\", \n          \"#EE9359\", \"#8C6D46\", \"#BF3B0B\", \"#400101\")"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#tolkiens-númenórean-years-normal-human-years-system",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#tolkiens-númenórean-years-normal-human-years-system",
    "title": "How old was Aragorn in regular human years?",
    "section": "Tolkien’s Númenórean years → normal human years system",
    "text": "Tolkien’s Númenórean years → normal human years system\nIn the appendix of the newly published The Fall of Númenor and Other Tales From the Second Age of Middle-earth is a fascinating footnote that explains exactly how to convert Second Age Númenórean years into normal human years. Tolkien writes:\n\n\nDeduct 20: Since at 20 years a Númenórean would be at about the same stage of development as an ordinary person.\n\nAdd to this 20 the remainder divided by 5. Thus a Númenórean man or woman of years [X] would be approximately of the “age” [Y] (Tolkien 2022, “The Life of the Númenóreans,” note 8, p. 262)\n\n\n\nAnd he provides this helpful table, which we’ll stick in an R data frame so we can play with it:\n\nages &lt;- tibble(\n  numenor_age = c(25, 50, 75, 100, 125, 150, 175, 200, 225, \n                  250, 275, 300, 325, 350, 375, 400, 425),\n  normal_human_age = c(21, 26, 31, 36, 41, 46, 51, 56, 61, \n                       66, 71, 76, 81, 86, 91, 96, 101)\n)\n\n\nCodeages |&gt; \n  rename(\"Númenórean age\" = numenor_age,\n         \"Normal human age\" = normal_human_age) |&gt; \n  t() |&gt; knitr::kable()\n\n\nTable 1: Tolkien’s original table for converting between Númenórean and human ages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNúmenórean age\n25\n50\n75\n100\n125\n150\n175\n200\n225\n250\n275\n300\n325\n350\n375\n400\n425\n\n\nNormal human age\n21\n26\n31\n36\n41\n46\n51\n56\n61\n66\n71\n76\n81\n86\n91\n96\n101\n\n\n\n\n\n\n\n\nTolkien’s logic is a little convoluted, but it works. For example, if a Númenórean is 125, subtract 20, then divide the remainder by 5 and add that to 20 to get 41 normal human years:\n\\[\n\\left(\\frac{125-20}{5} + 20\\right) = (21 + 20) = 41\n\\]\nIf we plot all of Tolkien’s example ages, we get a nice linear relationship between Númenórean ages and human ages:\n\nCodeggplot(ages, aes(x = numenor_age, y = normal_human_age)) +\n  geom_point() +\n  labs(x = \"Age in Númenórean years\", y = \"Age in normal human years\") +\n  theme_numenor()\n\n\n\n\n\n\nFigure 1: Scatterplot of Tolkien’s original table for converting between Númenórean and human ages\n\n\n\n\nSince it’s linear, we can skip the convoluted subtract-20-add-divided-remainder logic and instead figure out a slope and intercept for the line.\n\nCodeage_model &lt;- lm(normal_human_age ~ numenor_age, data = ages)\nage_model\n## \n## Call:\n## lm(formula = normal_human_age ~ numenor_age, data = ages)\n## \n## Coefficients:\n## (Intercept)  numenor_age  \n##        16.0          0.2\n\n\nThe line starts at the y-axis at 16 normal human years and then increases by 0.2 for every Númenórean year (the 0.2 is that divide-by-5 rule, since 1/5 = 0.2).\n\\[\n\\text{Normal human years} = 16 + (0.2 \\times \\text{Númenórean years})\n\\]\n\nCodeggplot(ages, aes(x = numenor_age, y = normal_human_age)) +\n  geom_smooth(method = \"lm\", color = clrs[3]) +\n  geom_point() +\n  labs(x = \"Age in Númenórean years\", y = \"Age in normal human years\") +\n  annotate(geom = \"richtext\", x = 300, y = 40, \n           label = \"Normal human years =&lt;br&gt;16 + (0.2 × Númenórean years)\") +\n  theme_numenor()\n\n\n\n\n\n\nFigure 2: Linear model for converting between Númenórean and human ages"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#estimating-the-dúnedain-normal-human-age-system",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#estimating-the-dúnedain-normal-human-age-system",
    "title": "How old was Aragorn in regular human years?",
    "section": "Estimating the Dúnedain → normal human age system",
    "text": "Estimating the Dúnedain → normal human age system\nAragorn was a Dúnedan, or a descendant of the Númenórean refugees Elendil and Isildur, so he inherited their unnaturally long lifespan. Aragorn was 87 at the end of the Third Age, and he lived until he was 210. If we naively assume he lived as long as a standard Númenórean, he would have gone through the events of The Lord of the Rings at 33 and died surprisingly young at 58:\n\nCodeggplot(ages, aes(x = numenor_age, y = normal_human_age)) +\n  geom_smooth(method = \"lm\", color = clrs[3]) +\n  geom_point() +\n  geom_vline(xintercept = c(87, 210)) +\n  annotate(geom = \"segment\", x = -Inf, xend = 87, y = 33, yend = 33, \n           linewidth = 0.5, linetype = \"21\", color = \"grey50\") +\n  annotate(geom = \"segment\", x = -Inf, xend = 210, y = 58, yend = 58, \n           linewidth = 0.5, linetype = \"21\", color = \"grey50\") +\n  geom_image(data = tibble(numenor_age = 87, normal_human_age = 80, \n                           image = \"img/aragorn-alive.jpg\"),\n             aes(image = image), size = 0.15, asp = 1.618) +\n  annotate(geom = \"richtext\", x = 87, y = 62, \n           label = glue(\"&lt;span style='color:{clrs[1]};'&gt;87 Númenórean years&lt;/span&gt;\",\n                        \"&lt;br&gt;\",\n                        \"&lt;span style='color:{clrs[6]};'&gt;33 human years&lt;/span&gt;\")) +\n  geom_image(data = tibble(numenor_age = 210, normal_human_age = 80, \n                           image = \"img/aragorn-dead.jpg\"),\n             aes(image = image), size = 0.15, asp = 1.618) +\n  annotate(geom = \"richtext\", x = 210, y = 98, \n           label = glue(\"&lt;span style='color:{clrs[1]};'&gt;210 Númenórean years&lt;/span&gt;\",\n                        \"&lt;br&gt;\",\n                        \"&lt;span style='color:{clrs[6]};'&gt;58 human years&lt;/span&gt;\")) +\n  labs(x = \"Age in Númenórean years\", y = \"Age in normal human years\") +\n  theme_numenor() +\n  theme(axis.text.x = element_text(color = clrs[1]),\n        axis.title.x = element_text(color = clrs[1]),\n        axis.text.y = element_text(color = clrs[6]),\n        axis.title.y = element_text(color = clrs[6]))\n\n\n\n\n\n\nFigure 3: A naive estimate of Aragorn’s human ages, assuming he’s a full Númenórean\n\n\n\n\nBut the refugee Númenóreans (the Dúnedain) gradually lost their long-living powers after Númenor was destroyed, so this Númenórean formula doesn’t really apply to Aragorn.\nAccording to supplemental Tolkien writings, the 7th steward of Gondor was the last person to live to 150 years, and by the time of the events of The Lord of the Rings, nobody in Gondor had lived past 100 years since Belecthor II, the 21st steward of Gondor. Denethor II—the tomato-massacring father of Boromir and Faramir—was the 26th steward and took the position 112 years after Belecthor II died. So it had been a long time since anyone had lived that long. With the exception of Aragorn and the other Dúnedan Rangers of the North, all the magic Númenórean power had waned.\nAfter the Ring was destroyed, something seems to have changed, though. Faramir—likely distantly related to the Dúnedain—lived until 120, so something new was in the air at the beginning of the post-Sauron Fourth Age. Aragorn—an actual Dúnedan and descendant of Númenor—made it to 210, but he maybe had some special elf help from Arwen.\nSo given that some of the old Númenórean power seems to have returned (and given that Aragorn was unnaturally long-lived), we can try to figure out the Dúnedan → regular human age conversion three different ways.\nArbitrary maximum age\nSince the Númenórean → regular human age line is perfectly linear, we’ll assume that the Dúnedan → regular human age line is also perfectly linear, just scaled down. We’ll start the line at 16 again, but now we’ll pretend that 210 years in Fourth Age Dúnedan years is 100 in human years (Aragorn lived a stunningly long time).\nTo figure out the equation for the new Dúnedan line, we need to figure out the slope. We have two points (\\((0, 16)\\) and \\((210, 100)\\)) that we can use to calculate the slope:\nBy hand:\n\\[\n\\begin{aligned}\n\\text{Slope} &= \\frac{y_2 - y_1}{x_2 - x_1} \\\\\n&= \\frac{100 - 16}{210 - 0} \\\\\n&= \\frac{84}{210} \\\\\n&= 0.4\n\\end{aligned}\n\\]\nOr with R:\n\nfind_slope &lt;- function(point1, point2) {\n  (point2[2] - point1[2]) / (point2[1] - point1[1])\n}\n\nslope &lt;- find_slope(c(0, 16), c(210, 100))\nslope\n## [1] 0.4\n\nThat gives us this equation:\n\\[\n\\text{Normal human years} = 16 + (0.4 \\times \\text{Dúnedan years})\n\\]\nTo make it easier to plot things and plug numbers into the formula, we’ll generate a dataset with a range of possible Dúnedan and regular human ages:\n\nages_dunedain &lt;- tibble(dunedain_age = seq(20, 210, by = 1)) |&gt;\n  mutate(normal_human_age = 16 + slope * dunedain_age)\n\nUsing this equation, we can figure out Aragorn’s normal human age during The Lord of the Rings and when he died:\n\nage_model_dunedain &lt;- lm(normal_human_age ~ dunedain_age, data = ages_dunedain)\n\naugment(age_model_dunedain, newdata = tibble(dunedain_age = c(87, 210))) |&gt;\n  rename(normal_human_age = .fitted)\n## # A tibble: 2 × 2\n##   dunedain_age normal_human_age\n##          &lt;dbl&gt;            &lt;dbl&gt;\n## 1           87             50.8\n## 2          210            100\n\nWhen Aragorn tells Eowyn that he’s 87, that’s actually the equivalent of 51ish. This fits with Tolkien’s writings, since in The Fellowship of the Ring, Aragorn was nearing the prime of life (Tolkien 2012, bk. 1, ch. 10, “Strider”).\nHere’s what that looks like across the whole hypothetical Dúnedan lifespan:\n\nCodeggplot(ages_dunedain, aes(x = dunedain_age, y = normal_human_age)) +\n  geom_smooth(method = \"lm\", color = clrs[7]) +\n  geom_vline(xintercept = c(87, 210)) +\n  annotate(geom = \"segment\", x = -Inf, xend = 87, y = 50.8, yend = 50.8, \n           linewidth = 0.5, linetype = \"21\", color = \"grey50\") +\n  annotate(geom = \"segment\", x = -Inf, xend = 210, y = 100, yend = 100, \n           linewidth = 0.5, linetype = \"21\", color = \"grey50\") +\n  geom_image(data = tibble(dunedain_age = 87, normal_human_age = 15, \n                           image = \"img/aragorn-alive.jpg\"),\n             aes(image = image), size = 0.15, asp = 1.618) +\n  annotate(geom = \"richtext\", x = 87, y = 41, \n           label = glue(\"&lt;span style='color:{clrs[3]};'&gt;87 Dúnedan years&lt;/span&gt;\",\n                        \"&lt;br&gt;\",\n                        \"&lt;span style='color:{clrs[6]};'&gt;50.8 human years&lt;/span&gt;\")) +\n  geom_image(data = tibble(dunedain_age = 210, normal_human_age = 60, \n                           image = \"img/aragorn-dead.jpg\"),\n             aes(image = image), size = 0.15, asp = 1.618) +\n  annotate(geom = \"richtext\", x = 210, y = 86, \n           label = glue(\"&lt;span style='color:{clrs[3]};'&gt;210 Dúnedan years&lt;/span&gt;\",\n                        \"&lt;br&gt;\",\n                        \"&lt;span style='color:{clrs[6]};'&gt;100 human years&lt;/span&gt;\")) +\n  labs(x = \"Age in Dúnedan years\", y = \"Age in normal human years\") +\n  coord_cartesian(xlim = c(0, 240), ylim = c(0, 110)) +\n  theme_numenor() +\n  theme(axis.text.x = element_text(color = clrs[3]),\n        axis.title.x = element_text(color = clrs[3]),\n        axis.text.y = element_text(color = clrs[6]),\n        axis.title.y = element_text(color = clrs[6]))\n\n\n\n\n\n\nFigure 4: A better estimate of Aragorn’s human ages, scaling down the full Númenórean range of ages to a more plausible range of Dúnedan ages\n\n\n\n\nSimulating a bunch of slopes\nDeciding that 210 Dúnedan years was 100 human years was a pretty arbitrary choice. Maybe Aragorn lived to be the equivalent of 90? Or 80? Or 120 like Faramir?\nInstead of choosing one single endpoint, we can simulate the uncertainty around the final age at death. We’ll say that 210 years in Fourth Age-era Dúnedan years is the equivalent of somewhere between 80 and 120 regular human years.\n\nCode# Generate a bunch of maximum human ages, centered around 100, ± 20ish\nlots_of_slopes &lt;- tibble(max_human_age = rnorm(1000, 100, 10)) %&gt;% \n  # Find the slope of each of these new lines\n  mutate(slope = map_dbl(max_human_age, ~find_slope(c(0, 16), c(210, .x)))) %&gt;% \n  # Generate data for each of the new lines\n  mutate(ages = map(slope, ~{\n    tibble(dunedain_age = seq(20, 210, by = 1)) |&gt;\n      mutate(normal_human_age = 16 + .x * dunedain_age)\n  })) %&gt;% \n  mutate(id = 1:n()) %&gt;% \n  unnest(ages)\n\n\nEach of these simulated lines is a plausible age conversion formula.\n\nCodelots_of_slopes %&gt;% \n  ggplot(aes(x = dunedain_age, y = normal_human_age)) +\n  geom_line(aes(group = id), method = \"lm\", stat = \"smooth\", alpha = 0.05, color = clrs[5]) +\n  geom_smooth(method = \"lm\", color = clrs[7]) +\n  geom_vline(xintercept = c(87, 210)) +\n  labs(x = \"Age in Dúnedan years\", y = \"Age in normal human years\") +\n  theme_numenor() +\n  theme(axis.text.x = element_text(color = clrs[3]),\n        axis.title.x = element_text(color = clrs[3]),\n        axis.text.y = element_text(color = clrs[6]),\n        axis.title.y = element_text(color = clrs[6]))\n\n\n\n\n\n\nFigure 5: Plausible Dúnedan-to-human age conversion equations\n\n\n\n\nAt lower values of Dúnedan ages, there’s a lot less of a range of uncertainty, but as Dúnedan age increases, so too does the range of possible human ages. We can look at the distribution of the predicted normal human ages at both 87 and 210 to get a sense for these ranges:\n\nCodelots_of_slopes |&gt; \n  filter(dunedain_age %in% c(87, 210)) |&gt; \n  mutate(dunedain_age = glue(\"{dunedain_age} Dúnedan years\"),\n         dunedain_age = fct_inorder(dunedain_age)) |&gt; \n  ggplot(aes(x = normal_human_age, fill = dunedain_age)) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[3], clrs[4]), guide = \"none\") +\n  labs(x = \"Normal human age\", y = \"Density\", fill = NULL) +\n  facet_wrap(vars(dunedain_age), scales = \"free_x\") +\n  theme_numenor() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\nFigure 6: Distribution of predicted human ages at 87 and 210 Dúnedan years\n\n\n\n\nBayesian simulation\nAs a final approach for guessing at Aragorn’s age, we’ll use a Bayesian model to generate a posterior distribution of plausible conversion lines and predicted ages. Technically this isn’t a true posterior—there’s no actual data or anything, so we’ll sample just from the prior distributions that we feed the model. But it’s still a helpful exercise in simulation.\nWe’ll define this statistical model:\n\\[\n\\begin{aligned}\n\\text{Normal human age} &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu &= \\beta_0 + \\beta_1\\ \\text{Dúnedan age} \\\\[10pt]\n\\beta_0 &= 16 \\\\\n\\beta_1 &\\sim \\mathcal{N}(0.4, 0.05) \\\\\n\\sigma &\\sim \\operatorname{Exponential}(1)\n\\end{aligned}\n\\]\nWe fix the intercept at 16 as before, and we say that the slope is around 0.4 ± 0.1ish. We’ll use Stan (through brms) to fit a model based on just these priors:\n\nCode# Stan likes to work with mean-centered variables, so we'll center dunedain_age\n# here, so that 0 represents 115\nages_dunedain_centered &lt;- ages_dunedain |&gt; \n  mutate(dunedain_age = scale(dunedain_age, center = TRUE, scale = FALSE))\n\n# Set some priors\npriors &lt;- c(\n  prior(constant(16), class = Intercept),  # Constant 16 for the intercept\n  prior(normal(0.4, 0.05), class = b, coef = \"dunedain_age\"),  # Slope of 0.4 ± 0.1\n  prior(exponential(1), class = sigma)\n)\n\n# Run some MCMC chains just with the priors, since we don't have any actual data\nage_model_bayes &lt;- brm(\n  bf(normal_human_age ~ dunedain_age),\n  data = ages_dunedain_centered,\n  prior = priors,\n  sample_prior = \"only\",\n  chains = 4, cores = 4, backend = \"cmdstanr\",\n  seed = 1234, refresh = 0\n)\n## Start sampling\n## Running MCMC with 4 parallel chains...\n## \n## Chain 1 finished in 0.0 seconds.\n## Chain 2 finished in 0.0 seconds.\n## Chain 3 finished in 0.0 seconds.\n## Chain 4 finished in 0.0 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.0 seconds.\n## Total execution time: 0.2 seconds.\nage_model_bayes\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: normal_human_age ~ dunedain_age \n##    Data: ages_dunedain_centered (Number of observations: 191) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept       16.00      0.00    16.00    16.00   NA       NA       NA\n## dunedain_age     0.40      0.05     0.30     0.50 1.00     2258     2191\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.99      0.98     0.02     3.51 1.00     1985     1394\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe results from the model aren’t too surprising, given (1) we’ve seen similar results with the other methods, and (2) the 16 intercept and 0.4 slope match the priors, wince we’re only dealing with priors.\nNow that we have a “posterior” (again, it’s not a true posterior since there’s no actual data), we can play with it in a few different ways. First we can look at the whole range of Dúnedan ages and see lots of plausible slopes. As expected, most are around 0.4, resulting in a final age of 100, but some lines are steeper and some are shallower. Since these are posterior distributions, we can find credible intervals too (which we can interpret much more naturally than convoluted confidence intervals):\n\nCodedraws_prior &lt;- tibble(dunedain_age = seq(25, 210, 1)) |&gt; \n  add_epred_draws(age_model_bayes, ndraws = 500)\n\ndraws_prior |&gt; \n  ggplot(aes(x = dunedain_age, y = .epred)) +\n  geom_line(aes(group = .draw), alpha = 0.05, color = clrs[5]) +\n  geom_vline(xintercept = c(87, 210)) +\n  labs(x = \"Age in Dúnedan years\", y = \"Age in normal human years\") +\n  theme_numenor() +\n  theme(axis.text.x = element_text(color = clrs[3]),\n        axis.title.x = element_text(color = clrs[3]),\n        axis.text.y = element_text(color = clrs[6]),\n        axis.title.y = element_text(color = clrs[6]))\n\n\n\n\n\n\nFigure 7: Spaghetti plot of plausible posterior Dúnedan → human conversions\n\n\n\n\nWe can also look at the posterior distribution of predicted human ages at just 87 and 210, along with credible intervals. There’s a 95% chance that at 87, he’s actually between 42 and 59 (with an average of 51ish), and at 210 he’s actually between 79ish and 121.\n\nCodedraws_aragorn_ages &lt;- tibble(dunedain_age = c(87, 210)) |&gt; \n  add_epred_draws(age_model_bayes, ndraws = 500)\n\ndraws_aragorn_ages |&gt; \n  group_by(dunedain_age) |&gt; \n  median_hdci(.width = 0.95)\n## # A tibble: 2 × 7\n##   dunedain_age .epred .lower .upper .width .point .interval\n##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1           87   50.7   41.9   59.2   0.95 median hdci     \n## 2          210   99.8   78.4  120.    0.95 median hdci\n\n\n\nCodedraws_aragorn_ages |&gt; \n  mutate(dunedain_age = glue(\"{dunedain_age} Dúnedan years\"),\n         dunedain_age = fct_inorder(dunedain_age)) |&gt; \n  ggplot(aes(x = .epred, fill = factor(dunedain_age))) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[3], clrs[4]), guide = \"none\") +\n  labs(x = \"Normal human age\", y = \"Density\", fill = NULL) +\n  facet_wrap(vars(dunedain_age), scales = \"free_x\") +\n  theme_numenor() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\nFigure 8: Posterior distribution of predicted human ages at 87 and 210 Dúnedan years"
  },
  {
    "objectID": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#conclusion",
    "href": "blog/2023/03/21/aragorn-dunedan-numenorean-simulation/index.html#conclusion",
    "title": "How old was Aragorn in regular human years?",
    "section": "Conclusion",
    "text": "Conclusion\nGiven all the evidence we have about Númenórean ages, and after making some reasonable assumptions about Dúnedan and human lifespans, when Aragorn tells Eowyn that that he’s 87, that’s really the equivalent of 50ish, with a 95% chance that he’s somewhere between 42 and 59.\n\n\nAragorn announcing his actual age"
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "",
    "text": "When I started my first master’s degree program in 2008, I decided to stop using Word for all my academic writing and instead use plain text Markdown for everything. Markdown itself had been a thing for 4 years, and MultiMarkdown—a pandoc-like extension of Markdown that could handle BibTeX bibliographies—was brand new. I did all my writing for my courses and my thesis in Markdown and converted it all to PDF through LaTeX using MultiMarkdown. I didn’t know about pandoc yet, so I only ever converted to PDF, not HTML or Word.\nI stored all my bibliographic references in a tiny little references.bib BibTeX file that I managed with BibDesk. BibDesk is a wonderful and powerful program with an active developer community and it does all sorts of neat stuff like auto-filing PDFs, importing references from DOIs, searching for references on the internet from inside the program, and just providing a nice overall front end for dealing with BibTeX files.\nI kept using my MultiMarkdown + LaTeX output system throughout my second master’s degree, and my references.bib file and PDF database slowly grew. R Markdown hadn’t been invented yet and I still hadn’t discovered pandoc, so living in a mostly LaTeX-based world was fine.\nWhen I started my PhD in 2012, something revolutionary happened: the {knitr} package was invented. The new R Markdown format let you to mix R code with Markdown text and create multiple outputs (HTML, LaTeX, and docx) through pandoc. I abandoned MultiMarkdown and fully converted to pandoc (thanks also in part to Kieran Healy’s Plain Person’s Gide to Plain Text Social Science). Since 2012, I’ve written exclusively in pandoc-flavored Markdown and always make sure that I can convert everything to PDF, HTML, and Word (see the “Manuscript” entry in the navigation bar here, for instance, where you can download the preprint version of that paper in a ton of different formats). I recently converted a bunch of my output templates to Quarto pandoc too.\nDuring all this time, I didn’t really keep up with other reference managers. I used super early Zotero as an undergrad back in 2006–2008, but it didn’t fit well with my Markdown-based workflow, so I kind of ignored it. I picked it up again briefly at the beginning of my PhD, but I couldn’t get it to play nicely with R Markdown and pandoc, so I kept using trusty old BibDesk. My references.bib file got bigger and bigger as I took more and more doctoral classes and did more research, but BibDesk handled the growing library just fine. As of today, I’ve got 1,400 items in there with nearly 1,000 PDFs, and everything still works great—mostly."
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#my-longstanding-workflow-for-writing-citing-and-pdf-management",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#my-longstanding-workflow-for-writing-citing-and-pdf-management",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "",
    "text": "When I started my first master’s degree program in 2008, I decided to stop using Word for all my academic writing and instead use plain text Markdown for everything. Markdown itself had been a thing for 4 years, and MultiMarkdown—a pandoc-like extension of Markdown that could handle BibTeX bibliographies—was brand new. I did all my writing for my courses and my thesis in Markdown and converted it all to PDF through LaTeX using MultiMarkdown. I didn’t know about pandoc yet, so I only ever converted to PDF, not HTML or Word.\nI stored all my bibliographic references in a tiny little references.bib BibTeX file that I managed with BibDesk. BibDesk is a wonderful and powerful program with an active developer community and it does all sorts of neat stuff like auto-filing PDFs, importing references from DOIs, searching for references on the internet from inside the program, and just providing a nice overall front end for dealing with BibTeX files.\nI kept using my MultiMarkdown + LaTeX output system throughout my second master’s degree, and my references.bib file and PDF database slowly grew. R Markdown hadn’t been invented yet and I still hadn’t discovered pandoc, so living in a mostly LaTeX-based world was fine.\nWhen I started my PhD in 2012, something revolutionary happened: the {knitr} package was invented. The new R Markdown format let you to mix R code with Markdown text and create multiple outputs (HTML, LaTeX, and docx) through pandoc. I abandoned MultiMarkdown and fully converted to pandoc (thanks also in part to Kieran Healy’s Plain Person’s Gide to Plain Text Social Science). Since 2012, I’ve written exclusively in pandoc-flavored Markdown and always make sure that I can convert everything to PDF, HTML, and Word (see the “Manuscript” entry in the navigation bar here, for instance, where you can download the preprint version of that paper in a ton of different formats). I recently converted a bunch of my output templates to Quarto pandoc too.\nDuring all this time, I didn’t really keep up with other reference managers. I used super early Zotero as an undergrad back in 2006–2008, but it didn’t fit well with my Markdown-based workflow, so I kind of ignored it. I picked it up again briefly at the beginning of my PhD, but I couldn’t get it to play nicely with R Markdown and pandoc, so I kept using trusty old BibDesk. My references.bib file got bigger and bigger as I took more and more doctoral classes and did more research, but BibDesk handled the growing library just fine. As of today, I’ve got 1,400 items in there with nearly 1,000 PDFs, and everything still works great—mostly."
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#why-switch-away-from-bibtex-and-bibdesk",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#why-switch-away-from-bibtex-and-bibdesk",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "Why switch away from BibTeX and BibDesk?",
    "text": "Why switch away from BibTeX and BibDesk?\nBibDesk got me through my dissertation and all my research projects up until now, so why consider switching away to some other system? Over the past few years, as I’ve done more reading on my iPad and worked on more coauthored projects, I’ve run into a few pain points in my citation workflow.\nProblem 1: Cross-device reading\nI enjoy reading PDFs on my iPad (particularly in the iAnnotate app), but getting PDFs from BibDesk onto the iPad has always required a bizarre dance:\n\nStore references.bib and the BibDesk-managed folder of PDFs in Dropbox\nUse the References iPad app to open the BibTeX file from Dropbox on the iPad\nUse iAnnotate to navigate Dropbox and find the PDF I want to read\nRead and annotate the PDF in iAnnotate\nSend the finished PDF from iAnnotate back to Dropbox and go back to References to ensure that the annotated PDF updates\n\nI’d often get sick of this convoluted process and just find the PDF on my computer and AirDrop it to my iPad directly, completely circumventing Dropbox. I’d then AirDrop it back to my computer and attach the marked up PDF to the reference in BibDesk. It’s inconvenient, but less inconvenient than bouncing around a bunch of different apps and hoping everything works.\nProblem 2: Collaboration across many projects with many coauthors\nCollaboration with a single huge references.bib file is impossible. I could share my Dropbox folder with coauthors, but then they’d see all my entries and have access to all my annotated PDFs, which seems like overkill. As I started working with coauthors, I decided to make smaller project-specific .bib files that would be shareable and editable.\nThis is great for project modularity—see how this bibliography.bib file only contains things we cited? But it caused major synchronization problems. If me or a coauthor makes any edits to the project-specific files (adding a DOI to an existing entry, adding a new entry, etc.), those changes don’t show up in my big master references.bib file. I have to remember to copy those changes to the main file, and I never remember. With some recent projects, I’ve actually been copying some entries from previous projects’ .bib files rather than from the big references.bib file. Everything’s diverging and it’s a pain.\nProblem 3: BibTeX was designed for LaTeX—but just LaTeX\nBibTeX works great with LaTeX. That’s why it was invented in the first place! The fact that things like pandoc work with it is partially a historical accident—.bib files were a convenient and widely used plain text bibliography format, so pandoc and MultiMarkdown used BibTeX for citations.\nBut citations are often more complicated than BibTeX can handle. Consider the LaTeX package biblatex-chicago—in order to be fully compliant with all the intricacies of the Chicago Manual of Style, it has to expand the BibTeX (technically BibLaTeX) format to include fields like entrysubtype for distinguishing between magazine/newspaper articles and journal articles, among dozens of other customizations and tweaks. BibTeX has a limited set of entry types, and anything that’s not one of those types gets shoehorned into the misc type.\nInternally, programs like pandoc that can read BibTeX files convert them into a standard Citation Style Language (CSL) format, which it then uses to format references as Chicago, APA, MLA, or whatever. It would be great to store all my citations in a CSL-compliant format in the first place rather than as a LaTeX-only format that has to be constantly converted on-the-fly when converting to any non-LaTeX output.\nThe solution: Zotero\nZotero conveniently fixes all these issues:\n\nIt has a synchronization service that works across platforms (including iOS). It can work with Dropbox too if you don’t want to be bound by their file size limit or pay for extra storage, though I ended up paying for storage to (1) support open source software and (2) not have to deal with multiple programs. I’ve been doing the BibDesk → iAnnotate → Dropbox → MacBook → AirDrop dance for too many years—I just want Zotero to handle all the syncing for me.\n\nIt’s super easy to collaborate with Zotero. You can create shared group libraries with different sets of coauthors and not worry about Dropbox synchronization issues or accidental deletion of } characters in the .bib file. For one of my reading-intensive class, I’ve even created a shared Zotero group library that all the students can join and cite from, which is neat.\nIt’s also far easier to maintain a master list of references. You can create a Zotero collection for specific projects, and items can live in multiple collections. Editing an item in one collection updates that item in all other collections. Zotero treats collections like iTunes/Apple Music playlists—just like songs can belong to multiple playlists, bibliographic entries can belong to multiple collections.\n\nZotero follows the CSL standard that pandoc uses. It was the first program to adopt CSL (way back in 2006!). It supports all kinds of entry types and fields, beyond what BibTeX supports."
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#preparing-for-the-migration",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#preparing-for-the-migration",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "Preparing for the migration",
    "text": "Preparing for the migration\nMigrating my big .references.bib file to Zotero was a relatively straightforward process, but it required a few minor shenanigans to get everything working right.\nMake a backup\nPreparing everything for migration meant I had to make a ton of edits to the original references.bib file, so I made a copy of it first and worked with the copy.\nInstall extensions\nTo make Zotero work nicely with a pandoc-centric writing workflow, and to make file management and tag management easier, I installed these three extensions:\n\nBetter BibTeX\nZotFile\nZotero Tag\nRatings and read status\nBibDesk allows you to add a couple extra metadata fields to entries for ratings and to mark them as read. I’ve used these fields for years and find them super useful for keeping track of how much I like articles and for remembering which ones I’ve actually finished.\nInternally, BibDesk stores this data as entries in the raw BibTex:\n@article{the_citekey_for_this_entry,\n    author = {Whoever},\n    title = {Whatever},\n    ...\n    rating = {4},\n    read = {1}}\nThese fields are preserved and transferred to Zotero when you import the file, but they show up in the “Extra” field and aren’t easily filterable or sortable there:\n\n\nExtra fields from a BibTeX file\n\nI decided to treat these as Zotero tags, which BibDesk calls keywords. I considered making some sort of programmatic solution and writing a script to convert all the rating and read fields to keywords, but that seemed like too much work—many entries have existing keywords and parsing the file and concatenating ratings and read status to the list of keywords would be hard.\nSo instead I sorted all my entries in BibDesk by rating, selected all the 5 star ones and added a zzzzz tag, selected all the 4 star ones and added a zzzz tag, and so on (so that 1 star entries got a z) tag. I then sorted the entries by read status and assigned xxx to all the ones I’ve read. These tag names were just temporary—in Zotero I changed these to emojis (⭐️⭐️⭐️ and ✅), but because I was worried about transferring complex Unicode characters like emojis across programs, I decided to simplify things by temporarily just using ASCII characters.\nFiles\nA note on BibDesk’s stored filename\nBibDesk can autofile attached PDFs and manage their location. To keep track of where the files are, it stores their path as a base64-encoded path in a bdsk-file-N field in the .bib file, like this:\n@article{HeissKelley:2017,\n    author = {Andrew Heiss and Judith G. Kelley},\n    doi = {10.1086/691218},\n    journal = {Journal of Politics},\n    month = {4},\n    number = {2},\n    pages = {732--41},\n    title = {Between a Rock and a Hard Place: International {NGOs} and the Dual Pressures of Donors and Host Governments},\n    volume = {79},\n    year = {2017},\n    bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBcUGFwZXJzL0hlaXNzS2VsbGV5MjAxNyAtIEJldHdlZW4gYSBSb2NrIGFuZCBhIEhhcmQgUGxhY2UgSW50ZXJuYXRpb25hbCBOR09zIGFuZCB0aGUgRHVhbC5wZGZPEQJ8AAAAAAJ8AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfgQ51QkQAAf////8fSGVpc3NLZWxsZXkyMDE3IC0gI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9T5sk0AAAAAAAAAAAABAAMAAAogY3UAAAAAAAAAAAAAAAAABlBhcGVycwACAHwvOlVzZXJzOmFuZHJldzpEcm9wYm94OlJlYWRpbmdzOlBhcGVyczpIZWlzc0tlbGxleTIwMTcgLSBCZXR3ZWVuIGEgUm9jayBhbmQgYSBIYXJkIFBsYWNlIEludGVybmF0aW9uYWwgTkdPcyBhbmQgdGhlIER1YWwucGRmAA4ArABVAEgAZQBpAHMAcwBLAGUAbABsAGUAeQAyADAAMQA3ACAALQAgAEIAZQB0AHcAZQBlAG4AIABhACAAUgBvAGMAawAgAGEAbgBkACAAYQAgAEgAYQByAGQAIABQAGwAYQBjAGUAIABJAG4AdABlAHIAbgBhAHQAaQBvAG4AYQBsACAATgBHAE8AcwAgAGEAbgBkACAAdABoAGUAIABEAHUAYQBsAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgB6VXNlcnMvYW5kcmV3L0Ryb3Bib3gvUmVhZGluZ3MvUGFwZXJzL0hlaXNzS2VsbGV5MjAxNyAtIEJldHdlZW4gYSBSb2NrIGFuZCBhIEhhcmQgUGxhY2UgSW50ZXJuYXRpb25hbCBOR09zIGFuZCB0aGUgRHVhbC5wZGYAEwABLwAAFQACAA3//wAAAAgADQAaACQAgwAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAMD}}\nZotero doesn’t parse that gnarly field—it needs a field named file—and it doesn’t decode that messy string into a plain text file path, so the attached PDF won’t get imported correctly.\nHowever, thanks to Emiliano Heyns, the Better BibTeX add-on will automatically convert these base64-encoded paths to plain text fields that Zotero can work with just fine. All PDFs will import automatically!\nCustomizing Zotero’s renaming rules\nI wanted all the PDFs that Zotero would manage to have nice predictable filenames. In BibDesk, I used this pattern:\ncitekey - First few words of title.pdf\nThat’s been fine, but it uses spaces in the file name and doesn’t remove any punctuation or special characters, so it was a little trickier to work with in the terminal or with scripts or for easy consistent searching (especially when searching in the iPad Dropbox app when looking for a PDF to read). But because I set up that pattern in 2008, path dependency kind of locked me in and I’ve been unwilling to change it since.\nSince I’m starting with a whole new reference manager, I figured it was time to adopt a better PDF naming system. In the ZotFile preferences, I set this pattern:\n{%a-}{%y-}{%t}\n…which translates to\nup_to_three_last_names-year-first_few_characters_of_title.pdf\n(see this for a list of all the possible wildcards)\n…with - separating the three logical units (authors, year, title), and _ separating all the words within each unit (which follows Jenny Bryan’s principles of file naming). In practice, the pattern looks like this:\nheiss_kelley-2017-between_a_rock_and_a_hard_place.pdf\nI had to tweak a few other renaming settings too. Here’s the final set of preferences:\n\n\nZotFile preferences\n\nI wanted to switch the roles of - and _ and do\nheiss-kelley_2017_between-a-rock-and-a-hard-place.pdf\n…but Zotero and/or ZotFile seems to hardwire _ as the space replacement in its titles. Oh well.\nCitekeys\nIn BibDesk, I’ve had a citation key pattern that I’ve used for years: Lastname:Year, with up to three last names for coauthored things, and an incremental lowercase letter in the case of duplicates:\nHeissKelley:2017\nHeissKelley:2017a\nImbens:2021\nLundbergJohnsonStewart:2021\nZotero and Better BibTeX preserve citekeys when you import a .bib file, but I wanted to make sure I keep using this system for new items I add going forward, so I changed the Better BibTeX preferences to use the same pattern:\nauth(0,1) + auth(0,2) + auth(0,3) + \":\" + year\n\n\nBetter BibTeX settings"
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#post-import-tweaks",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#post-import-tweaks",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "Post-import tweaks",
    "text": "Post-import tweaks\nWith all that initial prep work done, I imported the .bib file into my Zotero library (File &gt; Import…). I made sure “Place imported collections and items to new collection” was checked and that files were copied to the Zotero storage folder:\n\n\nZotero’s import dialog\n\nRatings and read status\nThe Tags panel in Zotero then showed all the project/class-specific keywords from BibDesk, in addition to the ratings and read status tags I added previously:\n\n\nTags before renaming\n\nI renamed each of the zzz* rating tags to use emoji stars and renamed the xxx read tag to use ✅.\n\n\nTags after renaming\n\nZotero has the ability to assign tags specific colors and pin them in a specific order, which also makes the tags display in the main Zotero library list. Following advice from the Zotero Tag extension, I pinned the read status ✅ tag as the first tag, the 5-star rating as the second tag, the 4-star rating as the third tag, and so on.\nNow the read status and ratings tags are easily accessible and appear directly in the main Zotero library list!\n\n\nZotero library with read status and ratings tags\n\nTags to collections\nZotero has two different methods for categorizing entries—tags and collections—while BibDesk / BibTeX only uses keywords, which Zotero treats as tags.\nI decided that in Zotero I’d use both tags and collections. Tags are reserved for things like general topics, ratings, to-read designations, etc., while collections represent specific projects or classes.\nI already assigned project- and class-specific keywords in BibDesk, so I just needed to move those keyworded entries into Zotero collections. There’s no way (that I could find) to include collection information in the .bib file and have it import into Zotero, so I ended up manually creating collections for each of the imported keywords. I filtered the library to only show items from one of the future collections, selected all the items, right-clicked, and chose “Add to collection” &gt; “New collection…” and created a new collection. I then deleted the tag.\nFor instance, here’s what Zotero looked like after I assigned these 6 items, tagged as “Polsci 733”, to the new “Polsci 733” collection (shown in the folder in the sidebar). I just had to delete the tag after:\n\n\nExample of the Polsci 733 tag after being converted to a collection\n\n\nincollection / inbook and crossref\n\n\n\n\n\n\n\nTip\n\n\n\nThis used to cause problems with child references not importing fields from their parents, but thanks to Emiliano Heynes, this all works flawlessly if you have verison 6.7.47+ of Better BibTeX installed.\n\n\nBibDesk natively supports the crossref field, which biber and biblatex use when working with LaTeX. This field lets you set up child/parent relationships with items, where children inherit fields from their parents. For instance, consider these two items—an edited book with lots of chapters from different authors and a chapter from that book:\n@inbook{El-HusseiniToeplerSalamon:2004,\n    author = {Hashem El-Husseini and Stefan Toepler and Lester M. Salamon},\n    chapter = {12},\n    crossref = {SalamonSokolowski:2004},\n    pages = {227--32},\n    title = {Lebanon}}\n\n@book{SalamonSokolowski:2004,\n    address = {Bloomfield, CT},\n    editor = {Lester M. Salamon and S. Wojciech Sokolowski},\n    publisher = {Kumarian Press},\n    title = {Global Civil Society: Dimensions of the Nonprofit Sector},\n    volume = {2},\n    year = {2004}}\nIn BibDesk, the chapter displays like this:\n\n\nBibDesk editor window for a book chapter that inherits its parent book’s attributes\n\nFields like book title, publisher, year, etc., are all greyed out because they’re inherited from the parent book, with the citekey SalamonSokolowski:2004\nIf you install version 6.7.47+ of the Better BibTeX add-on, the chapter will inherit all the information from its parent book—the book title, date, publisher, etc., will all be imported correctly:\n\n\nCross referenced parent attributes in Zotero are imported correctly\n\nAll done!\nAnd with that, I have a complete version of my 15-year-old references.bib file inside Zotero!\n\n\nComplete Zotero library"
  },
  {
    "objectID": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#example-workflow-with-quarto-r-markdown-pandoc",
    "href": "blog/2023/01/08/bibdesk-to-zotero-pandoc/index.html#example-workflow-with-quarto-r-markdown-pandoc",
    "title": "How to migrate from BibDesk to Zotero for pandoc-based writing",
    "section": "Example workflow with Quarto / R Markdown / pandoc",
    "text": "Example workflow with Quarto / R Markdown / pandoc\nPart of the reason I’ve been hesitant to switch away from BibDesk for so long is because I couldn’t figure out a way to connect a Markdown document to my Zotero database. With documents that get parsed through pandoc (like R Markdown or Quarto), you add a line in the YAML front matter to specify what file contains your references:\n---\ntitle: Whatever\nauthor: Whoever\nbibliography: references.bib\n---\nSince Zotero keeps everything in one big database, I didn’t see a way to add something like bibliography: My Zotero Database to the YAML front matter—pandoc requires that you point to a plain text file like .bib or .json or .yml, not a Zotero database.\nHowever, the magical Better BibTeX add-on clarified everything for me and makes it super easy to point pandoc at a single file that contains a collection of reference items.\nExport collection to .bib file\nFirst, create a collection of items that you want to cite in your writing project. Since collections are like playlists and items can belong to multiple collections, there’s no need to manage duplicate entries or anything (like I was running into with Problem 2 above).\nRight click on the collection name and choose “Export collection…”.\n\n\nExporting a Zotero collection\n\nChange the format to “Better BibLaTeX”, check “Keep updated”, and choose a place to save the resulting .bib file.\n\n\nChanging the export format\n\n\n\n\n\n\n\nTip\n\n\n\nYou could also export it as “Better CSL JSON” or “Better CSL YAML”, which would create a .json or .yml file that you could then point to in your YAML front matter, which would keep everything in CSL format instead of converting things to .bib and back again (see Problem 3 above). However, in my academic writing projects I still like to let LaTeX, BibLaTeX, and biber handle the citation generation instead of pandoc for PDFs, so I still rely on .bib files. But if you’re not converting to PDF, or if you’re letting the CSL style template handle the citations instead of BibLaTeX, you should probably keep everything as JSON or YAML instead of .bib.\n\n\nThe “Keep updated” option is the magical part of this whole thing. If you add an item or edit an existing item in the collection in Zotero, Better BibTeX will automatically re-export the collection to the .bib file. You can have one central repository of citations and lots of dynamically updated plain text .bib files that you don’t have to edit or keep track of. Truly magical.\nPoint the .qmd / .Rmd / .md to the exported file\nYou’ll now have a .bib file that contains all the references that you can cite. Put that filename in your front matter (use .json or .yml if you export the file as JSON or YAML instead):\n---\ntitle: Whatever\nauthor: Whoever\nbibliography: name_of_file_you_exported_from_zotero.bib\n---\nCite things\nCite things like normal.\nBecause the front matter is pointed at a plain text .bib file that contains all the bibliographic references, it’ll generate the citations correctly. And because Better BibTeX is configured to automatically update the exported plain text file, any changes you make in Zotero will automatically be reflected. Again, this is magic.\n\n\nVisual Studio Code with a Quarto Markdown file configured to look at an auto-updating .bib file exported from Zotero\n\nRStudio-based alternative\nAlternatively, if you write in RStudio, you can connect RStudio to your Zotero database and have it do a similar auto-export thing. You can also tell it to use Better BibTeX to keep things automatically synced:\n\n\nRStudio preferences pane for enabling Zotero\n\n(See here for more details about Zotero citations in RStudio)\nOne extra nice thing about using RStudio is its fancy Insert Citation dialog, which makes adding citations in Markdown just like adding citations in Word or Google Docs. It only works in the Visual Markdown Editor, though, which I don’t normally use, so I just use Better BibTeX alone rather than RStudio’s Zotero connection when I write in RStudio."
  },
  {
    "objectID": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html",
    "href": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html",
    "title": "Marginal and conditional effects for GLMMs with {marginaleffects}",
    "section": "",
    "text": "As a field, statistics is really bad at naming things.\nTake, for instance, the term “fixed effects.” In econometrics and other social science-flavored statistics, this typically refers to categorical terms in a regression model. Like, if we run a model like this with gapminder data…\nlibrary(gapminder)\n\nsome_model &lt;- lm(lifeExp ~ gdpPercap + country,\n                 data = gapminder)\n…we can say that we’ve added “country fixed effects.”\nThat’s all fine and good until we come to the world of hierarchical or multilevel models, which has its own issues with nomenclature and can’t decide what to even call itself:\nImage by Chelsea Parlett-Pelleriti\nIf we fit a model like this with country-based offsets to the intercept…\nlibrary(lme4)\n\nsome_multilevel_model &lt;- lmer(lifeExp ~ gdpPercap + (1 | country), \n                              data = gapminder)\n…then we get to say that there are “country random effects” or “country group effects”, while gdpPercap is actually a “fixed effect” or “population-level effect”\n“Fixed effects” in multilevel models aren’t at all the same as “fixed effects” in econometrics-land.\nWild.\nlol statisticians call opposite things the same thing\nAnother confusing term is the idea of “marginal effects.” One common definition of marginal effects is that they are slopes, or as the {marginaleffects} vignette says…\nThere’s a whole R package ({marginaleffects}) dedicated to calculating these, and I have a whole big long guide about this. Basically marginal effects are the change in the outcome in a regression model when you move one of the explanatory variables up a little while holding all other covariates constant.\nBut there’s also another definition (seemingly?) unrelated to the idea of partial derivatives or slopes! And once again, it’s a key part of the multilevel model world. I’ve run into it many times when reading about multilevel models (and I’ve even kind of alluded to it in past blog posts like this), but I’ve never fully understood what multilevel marginal effects are and how they’re different from slope-based marginal effects.\nIn multilevel models, you can calculate both marginal effects and conditional effects. Neither are necessarily related to slopes (though they both can be). They’re often mixed up. Even {brms} used to have a function named marginal_effects() that they’ve renamed to conditional_effects().\nI’m not alone in my inability to remember the difference between marginal and conditional effects in multilevel models, it seems. Everyone mixes these up. TJ Mahr recently tweeted about the confusion:\nTJ studies language development in children and often works with data with repeated child subjects. His typical models might look something like this, with observations grouped by child:\ntj_model &lt;- lmer(y ~ x1 + x2 + (1 | child),\n                 data = whatever)\nHis data has child-based clusters, since individual children have repeated observations over time. We can find two different kinds of effects given this type of multilevel model: we can look at the effect of x1 or x2 in one typical child, or we can look at the effect of x1 or x2 across all children on average. The confusingly-named terms “conditional effect” and “marginal effect” refer to each of these “flavors” of effect:\nIf we have country random effects like (1 | country) like I do in my own work, we can calculate the same two kinds of effects. Imagine a multilevel model like this:\nlibrary(lme4)\n\nsome_multilevel_model &lt;- lmer(lifeExp ~ gdpPercap + (1 | country), \n                              data = gapminder)\nOr more formally,\n\\[\n\\begin{aligned}\n\\text{lifeExp} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Life expectancy within countries } j \\\\\n\\mu_{i_j} &= (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{gdpPercap}_{i_j} & \\text{Model of within-country variation} \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random country offsets from global average}\n\\end{aligned}\n\\]\nWith this model, we can look at two different types of effects:\nThis conditional vs. marginal distinction applies to any sort of hierarchical structure in multilevel models:\nCalculating these different effects can be tricky, even with OLS-like normal or Gaussian regression, and interpreting them can get extra complicated with generalized linear mixed models (GLMMs) where we use links like Poisson, negative binomial, logistic, or lognormal families. The math with GLMMs gets complicated—particularly with lognormal models. Kristoffer Magnusson has several incredible blog posts that explore the exact math behind each of these effects in a lognormal GLMM.\nVincent Arel-Bundock’s magisterial {marginaleffects} R package can calculate both conditional and marginal effects automatically. I accidentally stumbled across the idea of multilevel marginal and conditional effects in an earlier blog post, but there I did everything with {emmeans} rather than {marginaleffects}, and as I explore here, {marginaleffects} is great for calculating average marginal effects (AMEs) rather than marginal effects at the mean (MEMs). Also in that earlier guide, I don’t really use this “conditional” vs. “marginal” distinction and just end up calling everything marginal. So everything here is more in line with the seemingly standard multilevel model ideas of “conditional” and “marginal” effects.\nLet’s load some libraries, use some neat colors and a nice ggplot theme, and get started.\nCodelibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(marginaleffects)\nlibrary(broom.mixed)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(ggtext)\nlibrary(patchwork)\n\n# Southern Utah colors\nclrs &lt;- NatParksPalettes::natparks.pals(\"BryceCanyon\")\n\n# Custom ggplot themes to make pretty plots\n# Get Noto Sans at https://fonts.google.com/specimen/Noto+Sans\ntheme_nice &lt;- function() {\n  theme_bw(base_family = \"Noto Sans\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\"),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}"
  },
  {
    "objectID": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#magnussons-data-and-model-the-effect-of-a-treatment-on-gambling-losses",
    "href": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#magnussons-data-and-model-the-effect-of-a-treatment-on-gambling-losses",
    "title": "Marginal and conditional effects for GLMMs with {marginaleffects}",
    "section": "Magnusson’s data and model: the effect of a treatment on gambling losses",
    "text": "Magnusson’s data and model: the effect of a treatment on gambling losses\nTo make sure I’ve translated Magnusson’s math into the corresponding (and correct) {marginaleffects} syntax, I recreate his analysis here. He imagines some sort of intervention or treatment \\(\\text{TX}\\) that is designed to reduce the amount of dollars lost in gambling each week (\\(Y\\)). The individuals in this situation are grouped into some sort of clusters—perhaps neighborhoods, states, or countries, or even the same individuals over time if we have repeated longitudinal observations. The exact kind of cluster doesn’t matter here—all that matters is that observations are nested in groups, and those groups have their own specific characteristics that influence individual-level outcomes. In this simulated data, there are 20 clusters, with 30 individuals in each cluster, with 600 total observations.\nTo be more formal about the structure, we can say that every outcome \\(Y\\) gets two subscripts for the cluster (\\(j\\)) and person inside each cluster (\\(i_j\\)). We thus have \\(Y_{i_j}\\) where \\(i_j \\in \\{1, 2, \\dots, 30\\}\\) and \\(j \\in \\{1, 2, \\dots, 20\\}\\). The nested, hierarchical, multilevel nature of the data makes the structure look something like this:\n\n\n\n\n\nIndividuals grouped into clusters\n\n\n\n\n \nI’ve included Magnusson’s original code for generating this data here, but you can also download an .rds version of it here, or use the URL directly with readr::read_rds():\n\nd &lt;- readr::read_rds(\"https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/df_example_lognormal.rds\")\n\n\nKristoffer Magnusson’s original data generation code#' Generate lognormal data with a random intercept\n#'\n#' @param n1 patients per cluster\n#' @param n2 clusters per treatment\n#' @param B0 log intercept\n#' @param B1 log treatment effect\n#' @param sd_log log sd\n#' @param u0 SD of log intercepts (random intercept)\n#'\n#' @return a data.frame\ngen_data &lt;- function(n1, n2, B0, B1, sd_log, u0) {\n  \n  cluster &lt;- rep(1:(2 * n2), each = n1)\n  TX &lt;- rep(c(0, 1), each = n1 * n2)\n  u0 &lt;- rnorm(2 * n2, sd = u0)[cluster]\n  \n  mulog &lt;- (B0 + B1 * TX + u0)\n  y &lt;- rlnorm(2 * n1 * n2, meanlog = mulog, sdlog = sd_log)\n  \n  d &lt;- data.frame(cluster,\n                  TX,\n                  y)\n  d\n}\n\nset.seed(4445)\npars &lt;- list(\"n1\" = 30, # observations per cluster\n             \"n2\" = 10, # clusters per treatment\n             \"B0\" = log(500),\n             \"B1\" = log(0.5),\n             \"sd_log\" = 0.5,\n             \"u0\" = 0.5)\nd &lt;- do.call(gen_data,\n             pars)\n\n\nThe model of the effect of \\(\\text{TX}\\) on gambling losses for individuals nested in clusters can be written formally like this, with cluster \\(j\\)-specific offsets to the \\(\\beta_0\\) intercept term (i.e. \\(b_{0_j}\\), or cluster random effects):\n\\[\n\\begin{aligned}\n\\log (Y_{i_j}) &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Gambling losses for individual $i$ within cluster } j \\\\\n\\mu_{i_j} &= (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{TX}_{i_j} & \\text{Model of within-cluster variation} \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random cluster offsets from global average}\n\\end{aligned}\n\\]\nWe can fit this model with {brms} (or lme4::lmer() if you don’t want to be Bayesian):\n\nfit &lt;- brm(\n  bf(y ~ 1 + TX + (1 | cluster)), \n  family = lognormal(), \n  data = d,\n  chains = 4, iter = 5000, warmup = 1000, seed = 4445\n)\n\n\nfit\n##  Family: lognormal \n##   Links: mu = identity; sigma = identity \n## Formula: y ~ 1 + TX + (1 | cluster) \n##    Data: dat (Number of observations: 600) \n##   Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 16000\n## \n## Group-Level Effects: \n## ~cluster (Number of levels: 20) \n##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sd(Intercept)     0.63      0.12     0.45     0.92 1.00     2024     3522\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     6.21      0.20     5.81     6.62 1.00     2052     3057\n## TX           -0.70      0.29    -1.28    -0.13 1.00     2014     2843\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.51      0.01     0.48     0.54 1.00     7316     8256\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThere are four parameters that we care about in that huge wall of text. We’ll pull them out as standalone objects (using TJ Mahr’s neat model-to-list trick) and show them in a table so we can keep track of everything easier.\n\nCoder_fit &lt;- fit %&gt;% \n  tidy() %&gt;% \n  mutate(term = janitor::make_clean_names(term)) %&gt;% \n  split(~term)\n\nB0 &lt;- r_fit$intercept$estimate\nB1 &lt;- r_fit$tx$estimate\nsigma_y &lt;- r_fit$sd_observation$estimate\nsigma_0 &lt;- r_fit$sd_intercept$estimate\n\n\n\nCodefit %&gt;% \n  tidy() %&gt;% \n  mutate(Parameter = c(\"\\\\(\\\\beta_0\\\\)\", \"\\\\(\\\\beta_1\\\\)\", \n                       \"\\\\(\\\\sigma_0\\\\)\", \"\\\\(\\\\sigma_y\\\\)\")) %&gt;% \n  mutate(Description = c(\"Global average gambling losses across all individuals\",\n                         \"Effect of treatment on gambling losses for all individuals\",\n                         \"Between-cluster variability of average gambling losses\",\n                         \"Within-cluster variability of gambling losses\")) %&gt;% \n  mutate(term = glue::glue(\"&lt;code&gt;{term}&lt;/code&gt;\"),\n         estimate = round(estimate, 3)) %&gt;% \n  select(Parameter, Term = term, Description, Estimate = estimate) %&gt;% \n  kbl(escape = FALSE) %&gt;% \n  kable_styling(full_width = FALSE)\n\n\n\nParameter\nTerm\nDescription\nEstimate\n\n\n\n\\(\\beta_0\\)\n(Intercept)\nGlobal average gambling losses across all individuals\n6.210\n\n\n\\(\\beta_1\\)\nTX\nEffect of treatment on gambling losses for all individuals\n-0.702\n\n\n\\(\\sigma_0\\)\nsd__(Intercept)\nBetween-cluster variability of average gambling losses\n0.635\n\n\n\\(\\sigma_y\\)\nsd__Observation\nWithin-cluster variability of gambling losses\n0.507\n\n\n\n\n\nThere are a few problems with these estimates though: (1) they’re on the log odds scale, which isn’t very interpretable, and (2) neither the intercept term nor the \\(\\text{TX}\\) term incorporate any details about the cluster-level effects beyond the extra information we get through partial pooling. So our goal here is to transform these estimates into something interpretable that also incorporates group-level information."
  },
  {
    "objectID": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#conditional-effects-or-effect-of-a-variable-in-an-average-cluster",
    "href": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#conditional-effects-or-effect-of-a-variable-in-an-average-cluster",
    "title": "Marginal and conditional effects for GLMMs with {marginaleffects}",
    "section": "Conditional effects, or effect of a variable in an average cluster",
    "text": "Conditional effects, or effect of a variable in an average cluster\n\n\n\n\n\n\nConditional effects\n\n\n\nConditional effects = average or typical cluster; random offsets \\(b_{0_j}\\) set to 0\n\n\nConditional effects refer to the effect of a variable in a typical group—country, cluster, school, subject, or whatever else is in the (1 | group) term in the model. “Typical” here means that the random offset \\(b_{0_j}\\) is set to zero, or that there are no random effects involved.\nAverage outcomes for a typical cluster\nThe average outcome \\(Y_{i_j}\\) across the possible values of \\(\\text{TX}\\) for a typical cluster is formally defined as\n\\[\n\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = \\{0, 1\\})\n\\]\nExactly how you calculate this mathematically depends on the distribution family. For a lognormal distribution, it is this:\n\\[\n\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = \\{0, 1\\}) =\n\\exp \\left((\\beta_0 + b_{0_j}) + \\beta_1 \\text{TX}_i + \\frac{\\sigma_y^2}{2}\\right)\n\\]\n\nTXs &lt;- c(\"0\" = 0, \"1\" = 1)\nb0j &lt;- 0\n\nexp((B0 + b0j) + (B1 * TXs) + (sigma_y^2 / 2))\n##   0   1 \n## 566 281\n\nWe can calculate this automatically with marginaleffects::predictions() by setting re_formula = NA to ignore all random effects, or to set all the random \\(b_{0_j}\\) offsets to zero:\n\npredictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1)), \n  by = \"TX\", \n  re_formula = NA\n)\n\n\n\n## # A tibble: 2 × 6\n##   rowid type        TX predicted conf.low conf.high\n##   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1     1 response     0      566.     379.      857.\n## 2     2 response     1      281.     188.      425.\n\n\nBecause we’re working with Bayesian posteriors, we might as well do neat stuff with them instead of just collapsing them down to single-number point estimates. The posteriordraws() function in {marginaleffects} lets us extract the modified/calculated MCMC draws, and then we can plot them with {tidybayes} / {ggdist}:\n\nconditional_preds &lt;- predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1)), \n  by = \"TX\", \n  re_formula = NA\n) %&gt;% \n  posteriordraws()\n\n\np_conditional_preds &lt;- conditional_preds %&gt;% \n  ggplot(aes(x = draw, fill = factor(TX))) +\n  stat_halfeye() +\n  scale_fill_manual(values = c(clrs[5], clrs[1])) +\n  scale_x_continuous(labels = label_dollar()) +\n  labs(x = \"Gambling losses\", y = \"Density\", fill = \"TX\",\n       title = \"Conditional cluster-specific means\",\n       subtitle = \"Typical cluster where *b*&lt;sub&gt;0&lt;sub&gt;j&lt;/sub&gt;&lt;/sub&gt; = 0\") +\n  coord_cartesian(xlim = c(100, 1000)) +\n  theme_nice() +\n  theme(plot.subtitle = element_markdown())\np_conditional_preds\n\n\n\n\n\n\n\nNeat.\nATE for a typical cluster\nThe average treatment effect (ATE) for a binary treatment is the difference between the two averages when \\(\\text{TX} = 1\\) and \\(\\text{TX} = 0\\):\n\\[\n\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 1) - \\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 0)\n\\]\nFor a lognormal family, it’s this:\n\\[\n\\begin{aligned}\n&\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 1) - \\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 0) = \\\\\n&\\qquad \\exp \\left((\\beta_0 + b_{0_j}) + \\beta_1 + \\frac{\\sigma_y^2}{2}\\right) - \\exp \\left((\\beta_0 + b_{0_j}) + \\frac{\\sigma_y^2}{2}\\right)\n\\end{aligned}\n\\]\n\nTXs &lt;- c(\"0\" = 0, \"1\" = 1)\nb0j &lt;- 0\n\n(exp((B0 + b0j) + (B1 * TXs[2]) + (sigma_y^2 / 2)) - \n    exp((B0 + b0j) + (B1 * TXs[1]) + (sigma_y^2 / 2))) %&gt;% \n  unname()\n## [1] -286\n\nWe can again calculate this by setting re_formula = NA in marginaleffects::comparisons():\n\n# Cluster-specific average treatment effect (when offset is 0)\ncomparisons(\n  fit, \n  variables = \"TX\",\n  re_formula = NA\n) %&gt;% \n  tidy()\n\n\n\n## # A tibble: 1 × 6\n##   type     term  contrast estimate conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 response TX    1 - 0       -282.    -590.     -51.3\n\n\nAnd here’s what the posterior of that conditional ATE looks like:\n\nconditional_ate &lt;- comparisons(\n  fit, \n  variables = \"TX\",\n  re_formula = NA\n) %&gt;% \n  posteriordraws()\n\n\np_conditional_ate &lt;- conditional_ate %&gt;% \n  ggplot(aes(x = draw)) +\n  stat_halfeye(fill = clrs[3]) +\n  scale_x_continuous(labels = label_dollar(style_negative = \"minus\")) +\n  labs(x = \"(TX = 1) − (TX = 0)\", y = \"Density\", \n       title = \"Conditional cluster-specific ATE\",\n       subtitle = \"Typical cluster where *b*&lt;sub&gt;0&lt;sub&gt;j&lt;/sub&gt;&lt;/sub&gt; = 0\") +\n  coord_cartesian(xlim = c(-900, 300)) +\n  theme_nice() +\n  theme(plot.subtitle = element_markdown())\np_conditional_ate"
  },
  {
    "objectID": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#marginal-effects-or-effect-of-a-variable-across-clusters-on-average",
    "href": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#marginal-effects-or-effect-of-a-variable-across-clusters-on-average",
    "title": "Marginal and conditional effects for GLMMs with {marginaleffects}",
    "section": "Marginal effects, or effect of a variable across clusters on average",
    "text": "Marginal effects, or effect of a variable across clusters on average\n\n\n\n\n\n\nMarginal effects\n\n\n\nMarginal effects = global/population-level effect; clusters on average; random offsets \\(b_{0_j}\\) are incorporated into the estimate\n\n\nMarginal effects refer to the global- or population-level effect of a variable. In multilevel models, coefficients can have random group-specific offsets to a global mean. That’s what the \\(b_{0_j}\\) in \\((\\beta_0 + b_{0_j})\\) is in the formal model we defined earlier:\n\\[\n\\begin{aligned}\n\\mu_{i_j} &= (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{TX}_i & \\text{Model of within-cluster variation} \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random cluster offsets}\n\\end{aligned}\n\\]\nBy definition, these offsets are distributed normally with a mean of 0 and a standard deviation of \\(\\sigma_0\\), or sd__(Intercept) in {brms} output. We can visualize these cluster-specific offsets to get a better feel for how they work:\n\nCodefit %&gt;% \n  linpred_draws(tibble(cluster = unique(d$cluster),\n                       TX = 0)) %&gt;% \n  mutate(offset = B0 - .linpred) %&gt;% \n  ungroup() %&gt;% \n  mutate(cluster = fct_reorder(factor(cluster), offset, .fun = mean)) %&gt;% \n  ggplot(aes(x = offset, y = cluster)) +\n  geom_vline(xintercept = 0, color = clrs[2]) +\n  stat_pointinterval(color = clrs[4]) +\n  labs(x = \"*b*&lt;sub&gt;0&lt;/sub&gt; offset from β&lt;sub&gt;0&lt;/sub&gt;\") +\n  theme_nice() +\n  theme(axis.title.x = element_markdown())\n\n\n\n\n\n\n\nThe intercept for Cluster 1 here is basically the same as the global \\(\\beta_0\\) coefficient; Cluster 19 has a big positive offset, while Cluster 11 has a big negative offset.\nThe model parameters show the whole range of possible cluster-specific intercepts, or \\(\\beta_0 \\pm \\sigma_0\\):\n\nggplot() +\n  stat_function(fun = ~dnorm(., mean = B0, sd = sigma_0^2),\n                geom = \"area\", fill = clrs[4]) +\n  xlim(4, 8) +\n  labs(x = \"Possible cluster-specific intercepts\", y = \"Density\",\n       title = glue::glue(\"Normal(µ = {round(B0, 3)}, σ = {round(sigma_0, 3)}&lt;sup&gt;2&lt;/sup&gt;)\")) +\n  theme_nice() +\n  theme(plot.title = element_markdown())\n\n\n\n\n\n\n\nWhen generating population-level estimates, then, we need to somehow incorporate this range of possible cluster-specific intercepts into the population-level predictions. We can do this a couple different ways: we can (1) average, marginalize or integrate across them, or (2) integrate them out.\nAverage population-level outcomes\nThe average outcome \\(Y_{i_j}\\) across the possible values of \\(\\text{TX}\\) for all clusters together is formally defined as\n\\[\n\\textbf{E}(Y_{i_j} \\mid \\text{TX} = \\{0, 1\\})\n\\]\nAs with the conditional effects, the equation for calculating this depends on the family you’re using. For lognormal families, it’s this incredibly scary formula:\n\\[\n\\textbf{E}(Y_{i_j} \\mid \\text{TX} = \\{0, 1\\}) = \\int \\exp \\left(x + \\sigma_y^2 / 2 \\right) \\, f_{\\texttt{dnorm}} \\left(x, \\left(\\beta_0 + \\beta_1 \\text{TX} \\right), \\sigma_0^2 \\right) \\,dx\n\\]\nWild. This is a mess because it integrates over the normally-distributed cluster-specific offsets, thus incorporating them all into the overall effect.\nWe can calculate this integral in a few different ways. Kristoffer Magnusson shows three different ways to calculate this hairy integral in his original post:\n\n\nNumeric integration with integrate():\n\nB_TXs &lt;- c(B0, B0 + B1) %&gt;% set_names(c(\"0\", \"1\"))\n\nB_TXs %&gt;% \n  map(~{\n    integrate(\n      f = function(x) {\n        exp(x + sigma_y ^ 2 / 2) * dnorm(x, ., sd = sigma_0)\n      },\n      lower = B0 - 10 * sigma_0,\n      upper = B0 + 10 * sigma_0\n    )$value\n  })\n## $`0`\n## [1] 692\n## \n## $`1`\n## [1] 343\n\n\n\nA magical moment-generating function for the lognormal distribution:\n\nexp(B_TXs + (sigma_0^2 + sigma_y^2)/2)\n##   0   1 \n## 692 343\n\n\n\nBrute force Monte Carlo integration, where we create a bunch of hypothetical cluster offsets \\(b_{0_j}\\) with a mean of 0 and a standard deviation of \\(\\sigma_0\\), calculate the average outcome, then take the average of all those hypothetical clusters:\n\n# A bunch of hypothetical cluster offsets\nsigma_0_i &lt;- rnorm(1e5, 0, sigma_0)\nB_TXs %&gt;% \n  map(~{\n    mean(exp(. + sigma_0_i + sigma_y^2/2))\n  })\n## $`0`\n## [1] 692\n## \n## $`1`\n## [1] 343\n\n\n\nThose approaches are all great, but the math can get really complicated if there are interaction terms or splines or if you have more complex random effects structures (random slope offsets! nested groups!)\nSo instead we can use {marginaleffects} to handle all that complexity for us.\n\n\nAverage / marginalize / integrate across existing random effects: Here we calculate predictions for \\(\\text{TX} = \\{0, 1\\}\\) within each of the existing clusters. We then collapse them into averages for each level of \\(\\text{TX}\\). The values here are not identical to what we found with the earlier approaches, though they’re in the same general area. I’m not 100% why—I’m guessing it’s because there aren’t a lot of clusters to work with, so the averages aren’t really stable.\n\npredictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = unique), \n  by = \"TX\", \n  re_formula = NULL\n)\n\n\n\n## # A tibble: 2 × 5\n##   type        TX predicted conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 response     0      647.     502.      905.\n## 2 response     1      321.     250.      443.\n\n\nWe can visualize the posteriors too:\n\nmarginal_preds &lt;- predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = unique), \n  by = \"TX\", \n  re_formula = NULL\n) %&gt;% \n  posteriordraws()\n\n\np_marginal_preds &lt;- marginal_preds %&gt;% \n  ggplot(aes(x = draw, fill = factor(TX))) +\n  stat_halfeye() +\n  scale_fill_manual(values = colorspace::lighten(c(clrs[5], clrs[1]), 0.4)) +\n  scale_x_continuous(labels = label_dollar()) +\n  labs(x = \"Gambling losses\", y = \"Density\", fill = \"TX\",\n       title = \"Marginal population-level means\",\n       subtitle = \"Random effects averaged / marginalized / integrated\") +\n  coord_cartesian(xlim = c(100, 1500)) +\n  theme_nice()\np_marginal_preds\n\n\n\n\n\n\n\n\n\nIntegrate out random effects: Instead of using the existing cluster intercepts, we can integrate out the random effects by generating predictions for a bunch of clusters (like 100), and then collapse those predictions into averages. This is similar to the intuition of brute force Monte Carlo integration in approach #3 earlier. This takes a long time! It results in the same estimates we found with the mathematical approaches in #1, #2, and #3 earlier.\n\npredictions(fit, newdata = datagrid(TX = c(0, 1), cluster = c(-1:-100)),\n            allow_new_levels = TRUE,\n            sample_new_levels = \"gaussian\",\n            by = \"TX\")\n\n\n\n## # A tibble: 2 × 5\n##   type        TX predicted conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 response     0      682.     461.     1168.\n## 2 response     1      340.     227.      577.\n\n\n\nmarginal_preds_int &lt;- predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\",\n  by = \"TX\"\n) %&gt;% \n  posteriordraws()\n\n\np_marginal_preds_int &lt;- marginal_preds_int %&gt;% \n  ggplot(aes(x = draw, fill = factor(TX))) +\n  stat_halfeye() +\n  scale_fill_manual(values = colorspace::lighten(c(clrs[5], clrs[1]), 0.4)) +\n  scale_x_continuous(labels = label_dollar()) +\n  labs(x = \"Gambling losses\", y = \"Density\", fill = \"TX\",\n       title = \"Marginal population-level means\",\n       subtitle = \"Random effects integrated out\") +\n  coord_cartesian(xlim = c(100, 1500)) +\n  theme_nice()\np_marginal_preds_int\n\n\n\n\n\n\n\n\nPopulation-level ATE\nThe average treatment effect (ATE) for a binary treatment is the difference between the two averages when \\(\\text{TX} = 1\\) and \\(\\text{TX} = 0\\), after somehow incorporating all the random cluster-specific offsets:\n\\[\n\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1) - \\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0)\n\\]\nFor a lognormal family, it’s this terrifying thing:\n\\[\n\\begin{aligned}\n&\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1) - \\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0) = \\\\\n&\\qquad \\int \\exp \\left(x + \\sigma_y^2 / 2 \\right) \\, f_{\\texttt{dnorm}} \\left(x, \\left(\\beta_0 + \\beta_1 \\right), \\sigma_0^2 \\right) \\,dx \\ - \\\\\n&\\qquad \\int \\exp \\left(x + \\sigma_y^2 / 2 \\right) \\, f_{\\texttt{dnorm}} \\left(x, \\beta_0, \\sigma_0^2 \\right) \\,dx\n\\end{aligned}\n\\]\nThat looks scary, but really it’s just the difference in the two estimates we found before: \\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1)\\) and \\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0)\\). We can use the same approaches from above and just subtract the two estimates, like this with the magical moment-generating function thing:\n\n\nPopulation-level ATE with moment-generating function:\n\nexp(B_TXs[2] + (sigma_0^2 + sigma_y^2)/2) - \n  exp(B_TXs[1] + (sigma_0^2 + sigma_y^2)/2)\n##    1 \n## -349\n\n\n\nWe can do this with {marginaleffects} too, either by averaging / marginalizing / integrating across existing clusters (though again, this weirdly gives slightly different results) or by integrating out the random effects from a bunch of hypothetical clusters (which gives the same result as the more analytical / mathematical estimates):\n\n\nAverage / marginalize / integrate across existing random effects:\n\n# Marginal treatment effect (or global population level effect)\ncomparisons(\n  fit, \n  variables = \"TX\", \n  re_formula = NULL\n) %&gt;% \n  tidy()\n\n\n\n## # A tibble: 1 × 6\n##   type     term  contrast estimate conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 response TX    1 - 0       -326.    -652.     -60.9\n\n\n\nmarginal_ate &lt;- comparisons(\n  fit, \n  variables = \"TX\", \n  re_formula = NULL\n) %&gt;%\n  posteriordraws()\n\n\np_marginal_ate &lt;- marginal_ate %&gt;% \n  group_by(drawid) %&gt;% \n  summarize(draw = mean(draw)) %&gt;% \n  ggplot(aes(x = draw)) +\n  stat_halfeye(fill = colorspace::lighten(clrs[3], 0.4)) +\n  scale_x_continuous(labels = label_dollar(style_negative = \"minus\")) +\n  labs(x = \"(TX = 1) − (TX = 0)\", y = \"Density\", \n       title = \"Marginal population-level ATE\",\n       subtitle = \"Random effects averaged / marginalized / integrated\") +\n  coord_cartesian(xlim = c(-900, 300)) +\n  theme_nice()\np_marginal_ate\n\n\n\n\n\n\n\n\n\nIntegrate out random effects\n\n# This takes a *really* long time\ncomparisons(\n  fit, \n  variables = \"TX\", \n  newdata = datagrid(cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\"\n) %&gt;% \n  tidy()\n\n\n\n## # A tibble: 1 × 6\n##   type     term  contrast estimate conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 response TX    1 - 0       -338.    -779.     -64.0\n\n\n\nmarginal_ate_int &lt;- comparisons(\n  fit, \n  variables = \"TX\", \n  newdata = datagrid(cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\"\n) %&gt;% \n  posteriordraws()\n\n\np_marginal_ate_int &lt;- marginal_ate_int %&gt;% \n  group_by(drawid) %&gt;% \n  summarize(draw = mean(draw)) %&gt;% \n  ggplot(aes(x = draw)) +\n  stat_halfeye(fill = colorspace::lighten(clrs[3], 0.4)) +\n  scale_x_continuous(labels = label_dollar(style_negative = \"minus\")) +\n  labs(x = \"(TX = 1) − (TX = 0)\", y = \"Density\", \n       title = \"Marginal population-level ATE\",\n       subtitle = \"Random effects integrated out\") +\n  coord_cartesian(xlim = c(-900, 300)) +\n  theme_nice()\np_marginal_ate_int"
  },
  {
    "objectID": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#ratios-and-multiplicative-effects",
    "href": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#ratios-and-multiplicative-effects",
    "title": "Marginal and conditional effects for GLMMs with {marginaleffects}",
    "section": "Ratios and multiplicative effects",
    "text": "Ratios and multiplicative effects\nFinally, we can work directly with the coefficients to get more slope-like effects, which is especially helpful when the coefficient of interest isn’t for a binary variable. Typically with GLMs with log or logit links (like logit, Poisson, negative binomial, lognormal, etc.) we can exponentiate the coefficient to get it as an odds ratio or a multiplicative effect. That works here too:\n\nexp(B1)\n##  b_TX \n## 0.495\n\nA one-unit increase in \\(\\text{TX}\\) causes a 51% decrease (exp(B1) - 1) in the outcome. Great.\nThat’s all fine here because the lognormal model doesn’t have any weird nonlinearities or interactions, but in the case of logistic regression or anything with interaction terms, life gets more complicated, so it’s better to work with marginaleffects() instead of exponentiating things by hand. If we use type = \"link\" we’ll keep the results as logged odds, and then we can exponentiate them. All the other random effects options that we used before (re_formula = NA, re_formula = NULL, integrating effects out, and so on) work here too.\n\nmarginaleffects(\n  fit, \n  variable = \"TX\", \n  type = \"link\",\n  newdata = datagrid(TX = 0)\n) %&gt;% \n  mutate(across(c(estimate, conf.low, conf.high), ~exp(.))) %&gt;% \n  select(rowid, term, estimate, conf.low, conf.high)\n## \n##  Term Estimate CI low CI high\n##    TX    0.496  0.279    0.88\n## \n## Columns: rowid, term, estimate, conf.low, conf.high\n\nWe can visualize the odds-ratio-scale posterior for fun:\n\nmarginaleffects(\n  fit, \n  variable = \"TX\", \n  type = \"link\",\n  newdata = datagrid(TX = 0)\n) %&gt;% \n  posteriordraws() %&gt;% \n  mutate(draw = exp(draw) - 1) %&gt;% \n  ggplot(aes(x = draw)) +\n  stat_halfeye(fill = colorspace::darken(clrs[3], 0.4)) +\n  geom_vline(xintercept = 0) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Percent change in outcome\", y = \"Density\") +\n  theme_nice()\n\n\n\n\n\n\n\nIf we use type = \"response\", we can get slopes at specific values of the coefficient (which is less helpful here, since \\(\\text{TX}\\) can only be 0 or 1; but it’s useful for continuous coefficients of interest)."
  },
  {
    "objectID": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#summary",
    "href": "blog/2022/11/29/conditional-marginal-marginaleffects/index.html#summary",
    "title": "Marginal and conditional effects for GLMMs with {marginaleffects}",
    "section": "Summary",
    "text": "Summary\nPhew, that was a lot. Here’s a summary table to reference to help keep things straight.\n\n\nCodewrap_r &lt;- function(x) glue::glue('&lt;div class=\"sourceCode cell-code\"&gt;&lt;pre class=\"sourceCode r\"&gt;&lt;code class=\"sourceCode r\"&gt;{x}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;')\n\nconditional_out &lt;- r\"{predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1)), \n  by = \"TX\", \n  re_formula = NA\n)}\"\n\nconditional_ate &lt;- r\"{comparisons(\n  fit, \n  variables = \"TX\",\n  re_formula = NA\n)}\"\n\nmarginal_out &lt;- r\"{predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = unique), \n  by = \"TX\", \n  re_formula = NULL\n)}\"\n\nmarginal_out_int &lt;- r\"{predictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\",\n  by = \"TX\"\n)}\"\n\nmarginal_ate &lt;- r\"{comparisons(\n  fit, \n  variables = \"TX\", \n  re_formula = NULL\n) %&gt;% \n  tidy()\n}\"\n\nmarginal_ate_int &lt;- r\"{comparisons(\n  fit, \n  variables = \"TX\", \n  newdata = datagrid(cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\"\n) %&gt;% \n  tidy()\n}\"\n\ntribble(\n  ~Effect, ~Formula, ~`{marginaleffects} code`,\n  \"Average outcomes in typical group\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid b_{0_j} = 0, \\\\text{TX} = \\\\{0, 1\\\\})\\\\)\", wrap_r(conditional_out),\n  \"ATE in typical group\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid b_{0_j} = 0, \\\\text{TX} = 1) -\\\\)&lt;br&gt; \\\\(\\\\quad\\\\textbf{E}(Y_{i_j} \\\\mid b_{0_j} = 0, \\\\text{TX} = 0)\\\\)\", wrap_r(conditional_ate),\n  \"Average population-level outcomes (marginalized)\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = \\\\{0, 1\\\\})\\\\)\", wrap_r(marginal_out),\n  \"Average population-level outcomes (integrated out)\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = \\\\{0, 1\\\\})\\\\)\", wrap_r(marginal_out_int),\n  \"Population-level ATE (marginalized)\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = 1) -\\\\)&lt;br&gt; \\\\(\\\\quad\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = 0)\\\\)\", wrap_r(marginal_ate),\n  \"Population-level ATE (integrated out)\", \"\\\\(\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = 1) -\\\\)&lt;br&gt; \\\\(\\\\quad\\\\textbf{E}(Y_{i_j} \\\\mid \\\\text{TX} = 0)\\\\)\", wrap_r(marginal_ate_int)\n) %&gt;% \n  kbl(escape = FALSE, align = c(\"l\", \"l\", \"l\")) %&gt;% \n  kable_styling(htmltable_class = \"table table-sm\") %&gt;% \n  pack_rows(index = c(\"Conditional effects\" = 2, \"Marginal effects\" = 4)) %&gt;% \n  column_spec(1, width = \"25%\") |&gt; \n  column_spec(2, width = \"35%\") |&gt; \n  column_spec(3, width = \"40%\")\n\n\n\n\n\n\n\n\nEffect\nFormula\n{marginaleffects} code\n\n\n\nConditional effects\n\n\nAverage outcomes in typical group\n\\(\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = \\{0, 1\\})\\)\n\npredictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1)), \n  by = \"TX\", \n  re_formula = NA\n)\n\n\n\nATE in typical group\n\\(\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 1) -\\)\n\\(\\quad\\textbf{E}(Y_{i_j} \\mid b_{0_j} = 0, \\text{TX} = 0)\\)\n\ncomparisons(\n  fit, \n  variables = \"TX\",\n  re_formula = NA\n)\n\n\n\nMarginal effects\n\n\nAverage population-level outcomes (marginalized)\n\\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = \\{0, 1\\})\\)\n\npredictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = unique), \n  by = \"TX\", \n  re_formula = NULL\n)\n\n\n\nAverage population-level outcomes (integrated out)\n\\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = \\{0, 1\\})\\)\n\npredictions(\n  fit, \n  newdata = datagrid(TX = c(0, 1), \n                     cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\",\n  by = \"TX\"\n)\n\n\n\nPopulation-level ATE (marginalized)\n\\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1) -\\)\n\\(\\quad\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0)\\)\n\ncomparisons(\n  fit, \n  variables = \"TX\", \n  re_formula = NULL\n) %&gt;% \n  tidy()\n\n\n\nPopulation-level ATE (integrated out)\n\\(\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 1) -\\)\n\\(\\quad\\textbf{E}(Y_{i_j} \\mid \\text{TX} = 0)\\)\n\ncomparisons(\n  fit, \n  variables = \"TX\", \n  newdata = datagrid(cluster = c(-1:-100)),\n  re_formula = NULL,\n  allow_new_levels = TRUE,\n  sample_new_levels = \"gaussian\"\n) %&gt;% \n  tidy()\n\n\n\n\n\n\n\nAnd here are all the posteriors all together, for easier comparison:\n\nCode((p_conditional_preds + coord_cartesian(xlim = c(0, 1200))) | p_conditional_ate) /\n  ((p_marginal_preds + coord_cartesian(xlim = c(0, 1200))) | p_marginal_ate) /\n  ((p_marginal_preds_int + coord_cartesian(xlim = c(0, 1200))) | p_marginal_ate_int)"
  },
  {
    "objectID": "blog/2022/06/23/long-labels-ggplot/index.html",
    "href": "blog/2022/06/23/long-labels-ggplot/index.html",
    "title": "Quick and easy ways to deal with long labels in ggplot2",
    "section": "",
    "text": "In one of the assignments for my data visualization class, I have students visualize the number of essential construction projects that were allowed to continue during New York City’s initial COVID shelter-in-place order in March and April 2020. It’s a good dataset to practice visualizing amounts and proportions and to practice with dplyr’s group_by() and summarize() and shows some interesting trends.\nThe data includes a column for CATEGORY, showing the type of construction project that was allowed. It poses an interesting (and common!) visualization challenge: some of the category names are really long, and if you plot CATEGORY on the x-axis, the labels overlap and become unreadable, like this:\nlibrary(tidyverse)  # dplyr, ggplot2, and friends\nlibrary(scales)     # Functions to format things nicely\n\n# Load pandemic construction data\nessential_raw &lt;- read_csv(\"https://datavizs22.classes.andrewheiss.com/projects/04-exercise/data/EssentialConstruction.csv\")\n\nessential_by_category &lt;- essential_raw %&gt;%\n  # Calculate the total number of projects within each category\n  group_by(CATEGORY) %&gt;%\n  summarize(total = n()) %&gt;%\n  # Sort by total\n  arrange(desc(total)) %&gt;%\n  # Make the category column ordered\n  mutate(CATEGORY = fct_inorder(CATEGORY))\nggplot(essential_by_category,\n       aes(x = CATEGORY, y = total)) +\n  geom_col() +\n  scale_y_continuous(labels = comma) +\n  labs(x = NULL, y = \"Total projects\")\nEw. The middle categories here get all blended together into an unreadable mess.\nFortunately there are a bunch of different ways to fix this, each with their own advantages and disadvantages!"
  },
  {
    "objectID": "blog/2022/06/23/long-labels-ggplot/index.html#option-a-make-the-plot-wider",
    "href": "blog/2022/06/23/long-labels-ggplot/index.html#option-a-make-the-plot-wider",
    "title": "Quick and easy ways to deal with long labels in ggplot2",
    "section": "Option A: Make the plot wider",
    "text": "Option A: Make the plot wider\nOne quick and easy way to fix this is to change the dimensions of the plot so that there’s more space along the x-axis. If you’re using R Markdown or Quarto, you can modify the chunk options and specify fig.width:\n\n```{r plot-wider, fig.width=10, fig.height=4}\nggplot(essential_by_category,\n       aes(x = CATEGORY, y = total)) +\n  geom_col() +\n  scale_y_continuous(labels = comma) +\n  labs(x = NULL, y = \"Total projects\")\n```\n\n\n\n\n\n\n\nIf you’re using ggsave(), you can specify the height and width there too:\n\nggsave(name_of_plot, width = 10, height = 4, units = \"in\")\n\nThat works, but now the font is tiny, so we need to adjust it up with theme_gray(base_size = 18):\n\n```{r plot-wider-bigger, fig.width=10, fig.height=4}\nggplot(essential_by_category,\n       aes(x = CATEGORY, y = total)) +\n  geom_col() +\n  scale_y_continuous(labels = comma) +\n  labs(x = NULL, y = \"Total projects\") +\n  theme_gray(base_size = 18)\n```\n\n\n\n\n\n\n\nNow the font is bigger, but the labels overlap again! We could make the figure wider again, but then we’d need to increase the font size again, and now we’re in an endless loop.\nVerdict: 2/10, easy to do, but more of a quick band-aid-style solution; not super recommended."
  },
  {
    "objectID": "blog/2022/06/23/long-labels-ggplot/index.html#option-b-swap-the-x--and-y-axes",
    "href": "blog/2022/06/23/long-labels-ggplot/index.html#option-b-swap-the-x--and-y-axes",
    "title": "Quick and easy ways to deal with long labels in ggplot2",
    "section": "Option B: Swap the x- and y-axes",
    "text": "Option B: Swap the x- and y-axes\nAnother quick and easy solution is to switch the x- and y-axes. If we put the categories on the y-axis, each label will be on its own line so the labels can’t overlap with each other anymore:\n\nggplot(essential_by_category,\n       aes(y = fct_rev(CATEGORY), x = total)) +\n  geom_col() +\n  scale_x_continuous(labels = comma) +\n  labs(y = NULL, x = \"Total projects\")\n\n\n\n\n\n\n\nThat works really well! However, it forces you to work with horizontal bars. If that doesn’t fit with your overall design (e.g., if you really want vertical bars), this won’t work. Additionally, if you have any really long labels, it can substantially shrink the plot area, like this:\n\n# Make one of the labels super long for fun\nessential_by_category %&gt;%\n  mutate(CATEGORY = recode(CATEGORY, \"Schools\" = \"Preschools, elementary schools, middle schools, high schools, and other schools\")) %&gt;%\n  ggplot(aes(y = fct_rev(CATEGORY), x = total)) +\n  geom_col() +\n  scale_x_continuous(labels = comma) +\n  labs(y = NULL, x = \"Total projects\")\n\n\n\n\n\n\n\nVerdict: 6/10, easy to do and works well if you’re happy with horizontal bars; can break if labels are too long (though long y-axis labels are fixable with the other techniques in this post too)."
  },
  {
    "objectID": "blog/2022/06/23/long-labels-ggplot/index.html#option-c-recode-some-longer-labels",
    "href": "blog/2022/06/23/long-labels-ggplot/index.html#option-c-recode-some-longer-labels",
    "title": "Quick and easy ways to deal with long labels in ggplot2",
    "section": "Option C: Recode some longer labels",
    "text": "Option C: Recode some longer labels\nInstead of messing with the width of the plot, we can mess with the category names themselves. We can use recode() from dplyr to recode some of the longer category names or add line breaks (\\n) to them:\n\nessential_by_category_shorter &lt;- essential_by_category %&gt;%\n  mutate(CATEGORY = recode(CATEGORY, \n                           \"Affordable Housing\" = \"Aff. Hous.\",\n                           \"Hospital / Health Care\" = \"Hosp./Health\",\n                           \"Public Housing\" = \"Pub. Hous.\",\n                           \"Homeless Shelter\" = \"Homeless\\nShelter\"))\n\nggplot(essential_by_category_shorter,\n       aes(x = CATEGORY, y = total)) +\n  geom_col() +\n  scale_y_continuous(labels = comma) +\n  labs(x = NULL, y = \"Total projects\")\n\n\n\n\n\n\n\nThat works great! However, it reduces readibility (does “Aff. Hous.” mean affordable housing? affluent housing? affable housing?). It also requires more manual work and a lot of extra typing. If a new longer category gets added in a later iteration of the data, this code won’t automatically shorten it.\nVerdict: 6/10, we have more control over the labels, but too much abbreviation reduces readibility, and it’s not automatic."
  },
  {
    "objectID": "blog/2022/06/23/long-labels-ggplot/index.html#option-d-rotate-the-labels",
    "href": "blog/2022/06/23/long-labels-ggplot/index.html#option-d-rotate-the-labels",
    "title": "Quick and easy ways to deal with long labels in ggplot2",
    "section": "Option D: Rotate the labels",
    "text": "Option D: Rotate the labels\nSince we want to avoid manually recoding categories, we can do some visual tricks to make the labels readable without changing any of the lable text. First we can rotate the labels a little. Here we rotate the labels 30°, but we could also do 45°, 90°, or whatever we want. If we add hjust = 0.5 (horizontal justification), the rotated labels will be centered in the columns, and vjust (vertical justification) will center the labels vertically.\n\nggplot(essential_by_category,\n       aes(x = CATEGORY, y = total)) +\n  geom_col() +\n  scale_y_continuous(labels = comma) +\n  labs(x = NULL, y = \"Total projects\") +\n  theme(axis.text.x = element_text(angle = 30, hjust = 0.5, vjust = 0.5))\n\n\n\n\n\n\n\nEverything fits great now, but I’m not a big fan of angled text. I’m also not happy with the all the empty vertical space between the axis and the shorter labels like “Schools” and “Utility”. It would look a lot nicer to have all these labels right-aligned to the axis, but there’s no way easy to do that.\nVerdict: 5.5/10, no manual work needed, but angled text is harder to read and there’s lots of extra uneven whitespace."
  },
  {
    "objectID": "blog/2022/06/23/long-labels-ggplot/index.html#option-e-dodge-the-labels",
    "href": "blog/2022/06/23/long-labels-ggplot/index.html#option-e-dodge-the-labels",
    "title": "Quick and easy ways to deal with long labels in ggplot2",
    "section": "Option E: Dodge the labels",
    "text": "Option E: Dodge the labels\nSecond, instead of rotating, as of ggplot2 v3.3.0 we can automatically dodge the labels and make them offset across multiple rows with the guide_axis(n.dodge = N) function in scale_x_*():\n\nggplot(essential_by_category,\n       aes(x = CATEGORY, y = total)) +\n  geom_col() +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  scale_y_continuous(labels = comma) +\n  labs(x = NULL, y = \"Total projects\")\n\n\n\n\n\n\n\nThat’s pretty neat. Again, this is all automatic and we don’t have to manually adjust any labels. The text is all horizontal so it’s more readable. But I’m not a huge fan of the gaps above the second-row labels. Maybe it would look better if the corresponding axis ticks were a little longer, idk.\nVerdict: 7/10, no manual work needed, labels easy to read, but there’s extra whitespace that can sometimes feel unbalanced."
  },
  {
    "objectID": "blog/2022/06/23/long-labels-ggplot/index.html#option-f-automatically-add-line-breaks",
    "href": "blog/2022/06/23/long-labels-ggplot/index.html#option-f-automatically-add-line-breaks",
    "title": "Quick and easy ways to deal with long labels in ggplot2",
    "section": "Option F: Automatically add line breaks",
    "text": "Option F: Automatically add line breaks\nThe easiest and quickest and nicest way to fix these long labels, though, is to use the label_wrap() function from the scales package. This will automatically add line breaks after X characters in labels with lots of text—you just have to tell it how many characters to use. The function is smart enough to try to break after word boundaries—that is, if you tell it to break after 5 characters, it won’t split something like “Approved” into “Appro” and “ved”; it’ll break after the end of the word.\n\nggplot(essential_by_category,\n       aes(x = CATEGORY, y = total)) +\n  geom_col() +\n  scale_x_discrete(labels = label_wrap(10)) +\n  scale_y_continuous(labels = comma) +\n  labs(x = NULL, y = \"Total projects\")\n\n\n\n\n\n\n\nLook at how the x-axis labels automatically break across lines! That’s so neat!\nVerdict: 11/10, no manual work needed, labels easy to read, everything’s perfect. This is the way.\nBonus: For things that aren’t axis labels, like titles and subtitles, you can use str_wrap() from stringr to break long text at X characters (specified with width):\n\nggplot(essential_by_category,\n       aes(x = CATEGORY, y = total)) +\n  geom_col() +\n  scale_x_discrete(labels = label_wrap(10)) +\n  scale_y_continuous(labels = comma) +\n  labs(x = NULL, y = \"Total projects\",\n       title = str_wrap(\n         \"Here's a really long title that will go off the edge of the figure unless it gets broken somewhere\", \n         width = 50),\n       subtitle = str_wrap(\n         \"Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\",\n         width = 70))"
  },
  {
    "objectID": "blog/2022/06/23/long-labels-ggplot/index.html#summary",
    "href": "blog/2022/06/23/long-labels-ggplot/index.html#summary",
    "title": "Quick and easy ways to deal with long labels in ggplot2",
    "section": "Summary",
    "text": "Summary\nHere’s a quick comparison of all these different approaches:\n\n\n\n\n\n\n\n\n\n\nSession Info\n\n\n## ─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n##  setting  value\n##  version  R version 4.3.0 (2023-04-21)\n##  os       macOS Ventura 13.4.1\n##  system   aarch64, darwin20\n##  ui       X11\n##  language (EN)\n##  collate  en_US.UTF-8\n##  ctype    en_US.UTF-8\n##  tz       America/New_York\n##  date     2023-08-25\n##  pandoc   2.19.2 @ /opt/homebrew/bin/ (via rmarkdown)\n##  quarto   1.3.433 @ /usr/local/bin/quarto\n## \n## ─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n##  ! package     * version date (UTC) lib source\n##  P dplyr       * 1.1.2   2023-04-20 [?] CRAN (R 4.3.0)\n##  P forcats     * 1.0.0   2023-01-29 [?] CRAN (R 4.3.0)\n##  P ggplot2     * 3.4.2   2023-04-03 [?] CRAN (R 4.3.0)\n##  P lubridate   * 1.9.2   2023-02-10 [?] CRAN (R 4.3.0)\n##  P patchwork   * 1.1.2   2022-08-19 [?] CRAN (R 4.3.0)\n##  P purrr       * 1.0.1   2023-01-10 [?] CRAN (R 4.3.0)\n##  P readr       * 2.1.4   2023-02-10 [?] CRAN (R 4.3.0)\n##  P scales      * 1.2.1   2022-08-20 [?] CRAN (R 4.3.0)\n##  P sessioninfo * 1.2.2   2021-12-06 [?] CRAN (R 4.3.0)\n##  P stringr     * 1.5.0   2022-12-02 [?] CRAN (R 4.3.0)\n##  P tibble      * 3.2.1   2023-03-20 [?] CRAN (R 4.3.0)\n##  P tidyr       * 1.3.0   2023-01-24 [?] CRAN (R 4.3.0)\n##  P tidyverse   * 2.0.0   2023-02-22 [?] CRAN (R 4.3.0)\n## \n##  [1] /Users/andrew/Sites/ath-quarto/renv/library/R-4.3/aarch64-apple-darwin20\n##  [2] /Users/andrew/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/84ba8b13\n## \n##  P ── Loaded and on-disk path mismatch.\n## \n## ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "blog/2022/05/09/hurdle-lognormal-gaussian-brms/index.html",
    "href": "blog/2022/05/09/hurdle-lognormal-gaussian-brms/index.html",
    "title": "A guide to modeling outcomes that have lots of zeros with Bayesian hurdle lognormal and hurdle Gaussian regression models",
    "section": "",
    "text": "In a research project I’ve been working on for several years now, we’re interested in the effect of anti-NGO legal crackdowns on various foreign aid-related outcomes: the amount of foreign aid a country receives and the proportion of that aid dedicated to contentious vs. non-contentious causes or issues. These outcome variables are easily measurable thanks to the AidData project, but they post a tricky methodological issue. The amount of foreign aid countries receive can both be huge (in the hundreds of millions or even billions of dollars), or completely zero. Moreover, the proportion of aid allocated to specific purposes is inherently bound between 0% and 100%, but can sometimes be exactly 0% or 100%. Using a statistical model that fits the distribution of these kinds of variables is important for modeling accuracy, but it’s a more complicated process than running a basic linear OLS regression with lm().\nIn a previous post, I wrote a guide to doing beta, zero-inflated beta, and zero-one-inflated beta regression for outcome variables that are bound between 0 and 1 and that can include 0 and/or 1. (That post was a side effect of working on this project on foreign aid and anti-NGO restrictions.)\nBeta regression (and its zero- and zero-one-inflated varieties) works really well with these kinds of outcome variables, and you can end up with richly defined models and well-fitting models. Zero-inflated beta regression doesn’t work on outcomes that aren’t limited to 0–1, though. For outcome variables that extend beyond 1, we can use hurdle models instead, which follow the same general approach as zero-inflated models. We define a mixture of models for two separate processes:\nHurdle models do a great job at fitting the data well and providing accurate predictions. They’re a little unwieldy to work with though, since they involve so many different moving parts.\nThis post is a guide (mostly for future me) for how to create, manipulate, understand, analyze, and plot hurlde models in a Bayesian way using R and Stan (through brms).\nThroughout this post, we’ll use data from two datasets to explore a couple different questions:\nNeither GDP per capita nor flipper length have any naturally ocurring zeros, so we’ll manipulate the data beforehand and build in some zeros based on some arbitrary rules. For the health/weatlh gapminder data, we’ll make it so countries with lower life expectancy will have a higher chance of seeing a 0 in GDP per capita; for the penguin data, we’ll make it so that shorter flipper length will increase the chance of seeing a 0 in body mass."
  },
  {
    "objectID": "blog/2022/05/09/hurdle-lognormal-gaussian-brms/index.html#who-this-post-is-for",
    "href": "blog/2022/05/09/hurdle-lognormal-gaussian-brms/index.html#who-this-post-is-for",
    "title": "A guide to modeling outcomes that have lots of zeros with Bayesian hurdle lognormal and hurdle Gaussian regression models",
    "section": "Who this post is for",
    "text": "Who this post is for\nHere’s what I assume you know:\n\nYou’re familiar with R and the tidyverse (particularly dplyr and ggplot2).\nYou’re familiar with brms for running Bayesian regression models. See the vignettes here, examples like this, or resources like these for an introduction. You can also see this guide on beta regression or this guide on average marginal effects for more examples.\nYou’re somewhat familiar with multilevel models. See this guide on using multilevel models with panel data for an extended example and a ton of extra resources.\n\nI use brms and Bayesian models throughout, since brms has built-in support for hurdled lognormal models (and can be extended to support other hurdled models). If you prefer frequentism, you can use pscl::hurdle() for frequentist hurdle models (see this post or this post for fully worked out examples), and lognormal regression can be done frequentistly with base R’s lm() function after logging the outcome:\n\n# Pre-log the outcome:\nwhatever &lt;- whatever |&gt; \n  mutate(y_logged = log(y))\n\nlm(y_logged ~ x, data = whatever)\n\n# Log the outcome in the regression itself:\nlm(log(y) ~ x, data = whatever)\n\nLet’s get started by loading all the libraries we’ll need (and creating some couple helper functions):\n\nlibrary(tidyverse)       # ggplot, dplyr, and friends\nlibrary(brms)            # Bayesian modeling through Stan\nlibrary(emmeans)         # Calculate marginal effects in fancy ways\nlibrary(tidybayes)       # Manipulate Stan objects in a tidy way\nlibrary(broom)           # Convert model objects to data frames\nlibrary(broom.mixed)     # Convert brms model objects to data frames\nlibrary(scales)          # For formatting numbers with commas, percents, and dollars\nlibrary(patchwork)       # For combining plots\nlibrary(ggh4x)           # For nested facets in ggplot\nlibrary(ggtext)          # Use markdown and HTML in ggplot text\nlibrary(MetBrewer)       # Use pretty artistic colors\nlibrary(gapminder)       # Country-year panel data from the Gapminder project\nlibrary(palmerpenguins)  # Penguin data!\n\n# Use the cmdstanr backend for Stan because it's faster and more modern than the\n# default rstan You need to install the cmdstanr package first\n# (https://mc-stan.org/cmdstanr/) and then run cmdstanr::install_cmdstan() to\n# install cmdstan on your computer.\noptions(mc.cores = 4,\n        brms.backend = \"cmdstanr\")\n\n# Set some global Stan options\nCHAINS &lt;- 4\nITER &lt;- 2000\nWARMUP &lt;- 1000\nBAYES_SEED &lt;- 1234\n\n# Use the Johnson color palette\nclrs &lt;- MetBrewer::met.brewer(\"Johnson\")\n\n# Tell bayesplot to use the Johnson palette (for things like pp_check())\nbayesplot::color_scheme_set(c(\"grey30\", clrs[2], clrs[1], clrs[3], clrs[5], clrs[4]))\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Jost\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Jost\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(family = \"Jost\", face = \"bold\"),\n          axis.title = element_text(family = \"Jost Medium\"),\n          strip.text = element_text(family = \"Jost\", face = \"bold\",\n                                    size = rel(1), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA))\n}"
  },
  {
    "objectID": "blog/2022/05/09/hurdle-lognormal-gaussian-brms/index.html#exponentially-distributed-outcomes-with-zeros",
    "href": "blog/2022/05/09/hurdle-lognormal-gaussian-brms/index.html#exponentially-distributed-outcomes-with-zeros",
    "title": "A guide to modeling outcomes that have lots of zeros with Bayesian hurdle lognormal and hurdle Gaussian regression models",
    "section": "Exponentially distributed outcomes with zeros",
    "text": "Exponentially distributed outcomes with zeros\nLike Hans Rosling, we’re interested in the relationship between health and wealth. How do a country’s GDP per capita and life expectancy move together? We’ll look at this a few different ways, dealing with both the exponential shape of the data and the presence of all those zeros we added.\nAll these examples are the reverse of the standard way of looking at the health/wealth relationship—in Rosling’s TED talk, GDP is on the x-axis and life expectancy is on the y-axis, since his theory is that more wealth leads to better health and longer life. But since I want to illustrate how to model logged outcome, we’ll put GDP on the y-axis here, which means we’ll be seeing what happens to wealth as life expectancy changes by one year (i.e. instead of the more standard lifeExp ~ gdpPercap, we’ll look at gdpPercap ~ lifeExp).\nThe data from the Gapminder Project is nice and clean and complete—there are no missing values, and there are no zero values. That means we’ll need to mess with the data a little to force it to have some zeros that we can work with. We’ll change some of the existing GDP per capita values to 0 based on two conditions:\n\nIf life expectancy is less than 50 years, there will be a 30% chance that the value of GDP per capita is 0\nIf life expectancy is greater than 50 years, there will be a 2% chance that the value of GDP per capita is 0\n\nThis means that countries with lower life expectancy will be more likely to have 0 GDP per capita in a given year.\n\nset.seed(1234)\ngapminder &lt;- gapminder::gapminder |&gt; \n  filter(continent != \"Oceania\") |&gt; \n  # Make a bunch of GDP values 0\n  mutate(prob_zero = ifelse(lifeExp &lt; 50, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         gdpPercap = ifelse(will_be_zero, 0, gdpPercap)) |&gt; \n  select(-prob_zero, -will_be_zero) |&gt; \n  # Make a logged version of GDP per capita\n  mutate(log_gdpPercap = log1p(gdpPercap)) |&gt; \n  mutate(is_zero = gdpPercap == 0)\n\nExplore data\nLet’s check if it worked:\n\ngapminder |&gt; \n  count(is_zero) |&gt; \n  mutate(prop = n / sum(n))\n## # A tibble: 2 × 3\n##   is_zero     n  prop\n##   &lt;lgl&gt;   &lt;int&gt; &lt;dbl&gt;\n## 1 FALSE    1502 0.894\n## 2 TRUE      178 0.106\n\nPerfect! We broke the pristine data and now 10.6% of the values of GDP per capita are zero.\nWe can visualize this too (and we’ll cheat a little for the sake of plotting by temporarily changing all zeros to a negative number so we can get a separate count in the histogram).\n\nplot_dist_unlogged &lt;- gapminder |&gt; \n  mutate(gdpPercap = ifelse(is_zero, -0.1, gdpPercap)) |&gt; \n  ggplot(aes(x = gdpPercap)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 5000, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) + \n  scale_x_continuous(labels = label_dollar(scale_cut = cut_short_scale())) +\n  scale_fill_manual(values = c(clrs[4], clrs[1]), \n                    guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"GDP per capita\", y = \"Count\", fill = \"Is zero?\",\n       subtitle = \"Nice and exponentially shaped, with a bunch of zeros\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\nplot_dist_logged &lt;- gapminder |&gt; \n  mutate(log_gdpPercap = ifelse(is_zero, -0.1, log_gdpPercap)) |&gt; \n  ggplot(aes(x = log_gdpPercap)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 0.5, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) +\n  scale_x_continuous(labels = label_math(e^.x)) +\n  scale_fill_manual(values = c(clrs[4], clrs[1]), \n                    guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"GDP per capita\", y = \"Count\", fill = \"Is zero?\",\n       subtitle = \"Nice and normally shaped, with a bunch of zeros;\\nit's hard to interpret intuitively though\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n(plot_dist_unlogged | plot_dist_logged) +\n  plot_layout(guides = \"collect\") +\n  plot_annotation(title = \"GDP per capita, original vs. logged\",\n                  theme = theme(plot.title = element_text(family = \"Jost\", face = \"bold\"),\n                                legend.position = \"bottom\"))\n\n\n\n\n\n\n\nThe left panel here shows the exponential distribution of GDP per capita. There are a lot of countries with low values (less than $500!), and increasingly fewer countries with more per capita wealth. The right panel shows the natural log (i.e. log base e) of GDP per capita. It’s now a lot more normally distributed (if we disregard the column showing the count of zeros), but note the x-axis scale—instead of using easily interpretable dollars, it shows the exponent that e is raised to, which is really unintuitive. Logged values are helpful when working with regression models, and this CrossValidated post reviews how to interpret model coefficients when different variables are logged or not. In this case, if we use logged GDP per capita as the outcome variable, any explanatory coefficients we have will represent the percent change in GDP per capita—a one-year increase in life expectancy will be associated with a X% change in GDP per capita, on average.\nIdeally, it’d be great if we could work with dollar-scale coefficients and marginal effects rather than percent change-based coefficients (since I find dollars more intuitive than percent changes), but because of mathy and statsy reasons, it’s best to model this outcome using a log scale. With some neat tricks, though, we can back-transform the log-scale results to dollars.\n1. Regular OLS model on an unlogged outcome\nNow that we’ve built in some zeros, let’s model the relationship between health and wealth.\nFirst, we’ll use a basic linear regression model with all the data at its original scale—no logging, no accounting for zeros, just drawing a straight line through data using ordinal least squares (OLS). Let’s look at a scatterplot first:\n\nggplot(gapminder, aes(x = lifeExp, y = gdpPercap)) +\n  geom_point(aes(color = continent), size = 1, alpha = 0.5) + \n  geom_smooth(method = \"lm\", color = \"#0074D9\") +\n  scale_y_continuous(labels = label_dollar(scale_cut = cut_short_scale())) +\n  scale_color_manual(values = clrs) +\n  labs(x = \"Life expectancy\", y = \"GDP per capita\", color = NULL,\n       title = \"OLS model fit on unlogged GDP\") +\n  guides(color = guide_legend(override.aes = list(size = 1.5, alpha = 1))) +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nHahaha that line is, um, great. Because of the exponential distribution of GDP per capita, the scatterplot follows a sort of hockey stick shape: the points are almost horizontal at low levels of life expectancy and they start trending at more of a 45º angle at at around 65 years The corresponding OLS trend line cuts across empty space in a ridiculous way and predicts negative income at low life expectancy and quite muted income at high life expectancy\nEven so, many social science disciplines love OLS and use it for literally everything, so we’ll use it here too. Let’s make a basic OLS model:\n\nmodel_gdp_basic &lt;- brm(\n  bf(gdpPercap ~ lifeExp),\n  data = gapminder,\n  family = gaussian(),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2\n)\n\n\ntidy(model_gdp_basic)\n## # A tibble: 3 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)      -19434.     943.   -21216.   -17608.\n## 2 fixed    cond      &lt;NA&gt;     lifeExp             442.      15.6     412.      472.\n## 3 ran_pars cond      Residual sd__Observation    8054.     140.     7789.     8337.\n\nBased on this model, when life expectancy is 0, the average GDP per capita is -$19,433.69, though since no countries have life expectancy that low, we shouldn’t really interpret that intercept—it mostly just exists so that the line can start somewhere. We care the most about the lifeExp coefficient: a one-year increase in life expectancy is associated with a $442.49 increase in GDP per capita, on average.\nWe can already tell that the fitted OLS trend isn’t that great. Let’s do a posterior predictive check to see how well the actual observed GDP per capita (in ■ light blue) aligns with simulated GDP per capita from the posterior distribution of the model (in ■ orange):\n\npp_check(model_gdp_basic)\n\n\n\n\n\n\n\n\n\nAgain, not fantastic. The actual distribution of GDP per capita is (1) exponential, so it has lots of low values, and (2) zero-inflated, so it has a lot of zeros. The posterior from the model shows a nice normal distribution of GDP per capita centered around $15,000ish. It’s not a great model at all.\n2. Regular OLS model on a logged outcome\nA standard approach when working with exponentially distributed outcomes like this is to log them. If we look at this scatterplot with base-10-logged GDP per capita, the shape of the data is much more linear and modelable. I put the 0s in a separate facet so that they’re easier to see too. We can easily see the two different processes—there’s a positive relationship between life expectancy and logged GDP per capita (shown with the ■ blue line), and there are a bunch of observations with 0 GDP per capita, generally clustered among rows with life expectancy less than 50 (which is by design, since we made it so all those rows had a 20% chance of getting a 0).\n\nplot_health_wealth_facet &lt;- gapminder |&gt; \n  mutate(nice_zero = factor(is_zero, levels = c(FALSE, TRUE),\n                            labels = c(\"GDP &gt; $0\", \"0\"),\n                            ordered = TRUE)) |&gt; \n  ggplot(aes(x = lifeExp, y = gdpPercap + 1)) +\n  geom_point(aes(color = continent), size = 1, alpha = 0.5) +\n  geom_smooth(aes(linetype = is_zero), method = \"lm\", se = FALSE, color = \"#0074D9\") +\n  scale_y_log10(labels = label_dollar(scale_cut = cut_short_scale()),\n                breaks = 125 * (2^(1:10))) +\n  scale_color_manual(values = clrs) +\n  scale_linetype_manual(values = c(\"solid\", \"blank\"), guide = NULL) +\n  labs(x = \"Life expectancy\", y = \"GDP per capita (base 10 logged)\", color = NULL,\n       title = \"OLS model fit only for rows where GDP &gt; 0\",\n       subtitle = \"GDP per capita logged with base 10\") +\n  guides(color = guide_legend(override.aes = list(size = 1.5, alpha = 1))) +\n  coord_cartesian(xlim = c(30, 80)) +\n  facet_grid(rows = vars(nice_zero), scales = \"free_y\", space = \"free\") + \n  theme_nice() + \n  theme(legend.position = \"bottom\",\n        strip.text = element_text(size = rel(0.7)))\nplot_health_wealth_facet\n\n\n\n\n\n\n\nThe same relationship holds when we use the natural log of GDP per capita, where values range from 6–12ish, as we saw in the histogram earlier. For statistical reasons, using the natural log works better and makes the interpretation of the coefficients easier since we can talk about percent changes in the outcome. Again, this CrossValidated post is indispensable for remembering how to interpret coefficients that are or aren’t logged when outcomes are or aren’t logged.\n\nplot_health_wealth_facet +\n  # Switch the column on the y-axis to logged GDP per capita\n  aes(y = log_gdpPercap) +\n  scale_y_continuous(labels = label_math(e^.x), breaks = 5:12) +\n  labs(y = \"GDP per capital (logged)\",\n       subtitle = \"GDP per capita logged with base e\")\n\n\n\n\n\n\n\nHowever, those zeros are going to cause problems. The nice ■ blue line is only fit using the non-zero data. If we include the zeros, like the ■ fuchsia line in this plot, the relationship is substantially different:\n\nggplot(gapminder, aes(x = lifeExp, y = log_gdpPercap)) +\n  geom_point(aes(color = continent), size = 1, alpha = 0.5) + \n  geom_smooth(method = \"lm\", color = \"#F012BE\") +\n  geom_smooth(data = filter(gapminder, gdpPercap != 0), method = \"lm\", color = \"#0074D9\") +\n  scale_y_continuous(labels = label_math(e^.x)) +\n  scale_color_manual(values = clrs) +\n  labs(x = \"Life expectancy\", y = \"GDP per capita (log)\", color = NULL) +\n  guides(color = guide_legend(override.aes = list(size = 1.5, alpha = 1))) +\n  coord_cartesian(xlim = c(30, 80)) +\n  labs(title = 'OLS models &lt;span style=\"color:#F012BE;\"&gt;with&lt;/span&gt; and &lt;span style=\"color:#0074D9;\"&gt;without&lt;/span&gt; zeros') +\n  theme_nice() +\n  theme(legend.position = \"bottom\",\n        plot.title = element_markdown())\n\n\n\n\n\n\n\nLet’s make a couple models that use logged GDP with and without zeros and compare the life expectancy coefficients:\n\nmodel_log_gdp_basic &lt;- brm(\n  bf(log_gdpPercap ~ lifeExp),\n  data = gapminder,\n  family = gaussian(),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2\n)\n\nmodel_log_gdp_no_zeros &lt;- brm(\n  bf(log_gdpPercap ~ lifeExp),\n  data = filter(gapminder, !is_zero),\n  family = gaussian(),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2\n)\n\n\ntidy(model_log_gdp_basic)\n## # A tibble: 3 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       -0.468   0.256     -0.973    0.0252\n## 2 fixed    cond      &lt;NA&gt;     lifeExp            0.132   0.00420    0.124    0.141 \n## 3 ran_pars cond      Residual sd__Observation    2.21    0.0379     2.14     2.29\ntidy(model_log_gdp_no_zeros)\n## # A tibble: 3 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       3.48     0.0943    3.29      3.66  \n## 2 fixed    cond      &lt;NA&gt;     lifeExp           0.0784   0.00152   0.0754    0.0814\n## 3 ran_pars cond      Residual sd__Observation   0.735    0.0133    0.708     0.761\n\nIn the model that includes the zeros (the ■ fuchsia line), a one-year increase in life expectancy is associated with a 13.2% increase in GDP per capita, on average, but if we omit the zeros (the ■ blue line), the slope changes substantially—here, a one-year increase in life expectancy is associated with a 7.8% increase in GDP per capita. The zero-free model fits the data better, but omits the zeros; the model with the zeros included is arguably more accurate, but is simultaneously less accurate given how poorly it predicts values with low life expectancy.\nThis is also apparent when we look at posterior predictive checks. The model that includes zeros has two very clear peaks in the distributions of the actual values of GDP per capita (in ■ light blue), while the simulated values of GDP per capita (in ■ orange) are unimodal and both under- and over-estimates\n\npp_check(model_log_gdp_basic)\npp_check(model_log_gdp_no_zeros)\n\n\n\n\n\n\n\n\n\n3. Hurdle lognormal model\nWe thus have a dilemma. If we omit the zeros, we’ll get a good, accurate model fit for non-zero data, but we’ll be throwing away all the data with zeros (in this case that’s like 10% of the data!). If we include the zeros, we won’t be throwing any data away, but we’ll get a strange-fitting model that both under- and over-predicts values. So what do we do?!\nWe give up.\nWe live like economists and embrace OLS.\nWe use a model that incorporates information about both the zeros and the non-zeros!\nIn an earlier post, I show how zero-inflated beta regression works. Beta regression works well for outcomes that are bounded between 0 and 1, but they cannot model values that are exactly 0 or 1. To get around this, it’s possible to model multiple processes simultaneously:\n\nA logistic regression model that predicts if an outcome is 0 or not\nA beta regression model that predicts if an outcome is between 0 and 1 if it’s not zero\n\nWe can use a similar mixture process for outcomes that don’t use beta regression. For whatever reason, instead of calling these models zero-inflated, we call them hurdle models, and brms includes four different built-in families:\n\nUse a logistic regression model that predicts if an outcome is 0 or not (this is the hurdle part)\nUse a lognormal (hurdle_lognormal()), gamma (hurdle_gamma()), Poisson (hurdle_poisson()), or negative binomial (hurdle_negbinomial()) model for outcomes that are not zero\n\nAs we do with zero-inflated beta regression, we have to specify two different processes when dealing with hurdle models: (1) the main outcome and (2) the binary hurdle process, or hu.\nIntercept-only hurdle model\nTo help with the intuition, we’ll first run a model where we don’t actually define a real model for hu—it’ll just return the intercept, which will show the proportion of GDP per capita that is zero. We’ll use the hurdle_lognormal() family, since, as we’ve already seen, our GDP per capita measure is exponentially distributed. Through the magic of the lognormal family, we don’t actually need to feed the model logged GDP per capita—it handles the logging for us behind the scenes automatically.\n\nmodel_gdp_hurdle &lt;- brm(\n  bf(gdpPercap ~ lifeExp,\n     hu ~ 1),\n  data = gapminder,\n  family = hurdle_lognormal(),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2\n)\n\n\ntidy(model_gdp_hurdle)\n## # A tibble: 4 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       3.47     0.0956    3.28      3.66  \n## 2 fixed    cond      &lt;NA&gt;     hu_(Intercept)   -2.13     0.0781   -2.28     -1.98  \n## 3 fixed    cond      &lt;NA&gt;     lifeExp           0.0786   0.00154   0.0755    0.0816\n## 4 ran_pars cond      Residual sd__Observation   0.736    0.0133    0.710     0.762\n\nThe results from this model are a little different now. The coefficients for the regular part of the model ((Intercept) and lifeExp) are nearly identical to the zero-free model we made earlier (model_log_gdp_no_zeros), and we can interpret them just like we did before: a one-year increase in life expectancy is associated with a 7.9% increase in GDP per capita, on average.\nWe also have a coefficient with a hu_ prefix: hu_(Intercept). This is the intercept for the logistic regression model used for the hurdle (0/not 0) process, and it’s measured on the logit scale. That means we can back-transform it to proportions or probabilities with plogis():\n\nhu_intercept &lt;- tidy(model_gdp_hurdle) |&gt; \n  filter(term == \"hu_(Intercept)\") |&gt; \n  pull(estimate)\n\n# Logit scale intercept\nhu_intercept\n## b_hu_Intercept \n##          -2.13\n\n# Transformed to a probability/proportion\nplogis(hu_intercept)\n## b_hu_Intercept \n##          0.106\n\nSince the hurdle part of the model includes only the intercept, this value represents the proportion of zeros in the data, or 10.6%. We can confirm with some dplyr magic:\n\ngapminder |&gt; \n  count(is_zero) |&gt; \n  mutate(prop = n / sum(n))\n## # A tibble: 2 × 3\n##   is_zero     n  prop\n##   &lt;lgl&gt;   &lt;int&gt; &lt;dbl&gt;\n## 1 FALSE    1502 0.894\n## 2 TRUE      178 0.106\n\nThey’re the same!\nThe coefficients for the non-zero part of the model are basically the same as what we found with model_log_gdp_no_zeros, so why go through the hassle of creating a mixture model with the zero-process? Why not just filter out the zeros?\nBecause we combined the two processes in the same model, both processes are incorporated in the posterior distribution of GDP per capita. We can confirm this with a posterior predictive check. In the plot on the left, we can see that the model fits the exponential distribution of the data pretty well. It fits the zeros too, but we can’t actually see that part because of how small the plot is. To highlight the zero process, in the plot on the right we log the predicted values. Look how well those ■ orange posterior draws fit the actual ■ blue data!\n\n# Exponential\npp_check(model_gdp_hurdle)\n\n# Logged\npred &lt;- posterior_predict(model_gdp_hurdle)\nbayesplot::ppc_dens_overlay(y = log1p(gapminder$gdpPercap), \n                            yrep = log1p(pred[1:10,]))\n\n\n\n\n\n\n\n\n\nAdditionally, when we make predictions, most of the predicted values will be some big number, but about 10% of them will be 0, corresponding to the modeled proportion of zeros. Any predictions we make using the posterior from the model should inherently reflect the zero process. If we make a histogram of predicted draws from the model, we’ll see around 10% of the predictions are 0, as expected:\n\npred_gdp_hurdle &lt;- model_gdp_hurdle |&gt; \n  predicted_draws(newdata = tibble(lifeExp = 60)) |&gt;\n  mutate(is_zero = .prediction == 0,\n         .prediction = ifelse(is_zero, .prediction - 0.1, .prediction))\n\nggplot(pred_gdp_hurdle, aes(x = .prediction)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 2500, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) + \n  scale_x_continuous(labels = label_dollar(scale_cut = cut_short_scale())) +\n  scale_fill_manual(values = c(clrs[4], clrs[1]), \n                    guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"GDP per capita\", y = \"Count\", fill = \"Is zero?\",\n       title = \"Predicted GDP per capita from hurdle model\") +\n  coord_cartesian(xlim = c(-2500, 75000)) +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nHurdle model with additional terms\nRight now, all we’ve modeled is the overall proportion of 0s. We haven’t modeled what determines those zeros. Fortunately in this case we know what that process is—we made it so that rows with a life expectancy of less than 50 had a 30% chance of being zero, while rows with a life expectancy of greater than 50 had a 2% chance of being zero. Life expectancy should thus strongly predict the 0/not 0 process. Let’s include it in the hu part of the model:\n\nmodel_gdp_hurdle_life &lt;- brm(\n  bf(gdpPercap ~ lifeExp,\n     hu ~ lifeExp),\n  data = gapminder,\n  family = hurdle_lognormal(),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2\n)\n\n\ntidy(model_gdp_hurdle_life)\n## # A tibble: 5 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       3.47     0.0924    3.29      3.65  \n## 2 fixed    cond      &lt;NA&gt;     hu_(Intercept)    3.15     0.415     2.34      3.99  \n## 3 fixed    cond      &lt;NA&gt;     lifeExp           0.0785   0.00149   0.0756    0.0815\n## 4 fixed    cond      &lt;NA&gt;     hu_lifeExp       -0.0992   0.00844  -0.116    -0.0830\n## 5 ran_pars cond      Residual sd__Observation   0.736    0.0131    0.710     0.761\n\nExtracting and interpreting coefficients from a hurdle model\n\n\n\n\n\n\nMore on average marginal effects\n\n\n\nAfter writing this guide, I wrote another detailed guide about the exact differences between average marginal effects, marginal effects at the mean, and a bunch of other marginal-related things, comparing emmeans with marginaleffects, two packages that take different approaches to calculating marginal effects from regression models. In the rest of this guide, I’m pretty inexact about the nuances between marginal effects and the output of emtrends(), which technically returns marginal effects at the mean and not average marginal effects. So keep that caveat in mind throughout.\nYou can extract non-hurdled effects, hurdled effects, and expected values from these models using either emmeans or marginaleffects. I use emmeans throughout the rest of this guide; see this vignette for an example with marginaleffects.\n\n\nIn this model, the non-hurdled coefficients are the same as before—a one-year increase in life expectancy is still associated with a 7.8% increase in GDP per capita, on average. But now we have a new term, hu_lifeExp, which shows the effect of life expectancy in the hurdling process. Since this is on the logit scale, we can combine it with the hurdle intercept and transform the coefficient into percentage points (see Steven Miller’s lab script here for more on this process, or this section on fractional logistic regression):\n\nhurdle_intercept &lt;- tidy(model_gdp_hurdle_life) |&gt; \n  filter(term == \"hu_(Intercept)\") |&gt; \n  pull(estimate)\n\nhurdle_lifeexp &lt;- tidy(model_gdp_hurdle_life) |&gt; \n  filter(term == \"hu_lifeExp\") |&gt; \n  pull(estimate)\n\nplogis(hurdle_intercept + hurdle_lifeexp) - plogis(hurdle_intercept)\n## b_hu_Intercept \n##       -0.00408\n\nA one-year increase in life expectancy thus decreases the probability of seeing a 0 in GDP per capita by 0.41 percentage points, on average. That’s neat!\nDoing the complicated plogis(intercept + coefficient) - plogis(intercept) to convert logit-scale marginal effects and logit-scale predicted values as percentage points is tricky, though, especially once more coefficients are involved. It’s easy to mess up that math.\nInstead, we can calculate marginal effects automatically using a couple different methods:\n\n\nbrms’s conditonal_effects() will plot predicted values of specific coefficients while holding all other variables constant, and it converts the results to their original scales. It automatically creates a plot, which is nice, but extracting the data out of the object is a little tricky and convoluted.\nPackages like emmeans or marginaleffects can calculate marginal effects and predicted values on their original scales too. For more details, see my blog post on beta regression, or the documentation for emmeans or marginaleffects. Also see this mega detailed guide here for more about marginal effects.\n\nbrms::conditional_effects() with hurdle models\nSince we’re working with a mixture model, the syntax for dealing with conditional/marginal effects is a little different, as the lifeExp variable exists in both the non-zero and the zero processes of the model. We can specify which version of lifeExp we want to work with using the dpar argument (distributional parameter) in conditional_effects(). By default, conditional_effects() will return the marginal/conditional means of the combined process using both the non-zero part mu and the zero part hu. If we want to see the 0/not 0 hurdle part, we can specify dpar = \"hu“. The conditional_effects() function helpfully converts the predicted values into their original scales: when showing predicted life expectancy, it unlogs the values; when showing predicted proportion of zeros, it unlogits the values.\nWe can see both processes of the model simultaneously—at low levels of life expectancy, the probability of reporting no GDP per capita is fairly high, and the predicted level of GDP per capita is really low. As life expectancy increases, the probability of seeing a 0 drops and predicted wealth increases.\n(Note that these plots look fancier than what you normally get when just running conditional_effects(model_name) in R. That’s because I extracted the plot data and built my own graph for this post. You can see the code for that at the source Rmd for this post.)\n\nconditional_effects(model_gdp_hurdle_life)\nconditional_effects(model_gdp_hurdle_life, dpar = \"hu\")\n\n\n\n\n\n\n\n\n\n\nemmeans with hurdle models\nWhile conditional_effects() is great for a quick marginal effects plot like this, we can’t see just the mu part of the model in isolation. More importantly, plots alone don’t help with determining the exact slope of the line at each point on the original scale. We know from the model results that a one-year increase in life expectancy is associated with a 7.8% increase in GDP per capita, and that’s apparent in the curviness of the conditional effects plot. But if we want to know the original-scale slope at 60 years, for instance (e.g. an increase in life expectancy from 60 to 61 years is associated with a $X increase in GDP per capita), that’s trickier. It’s even trickier when working with the hurdle part of the model. Moving from 60 to 61 years decreases the probability of seeing a 0 by… some percentage point amount… but without doing a bunch of plogis() math, it’s not obvious how to get that value. Fortunately emmeans::emtrends() makes this easy. Like brms::conditional_effects(), by default, functions from emmeans like emmeans() and emtrends() will return the marginal means of the combined non-zero and zero/not-zero process, or both mu and hu. We can specify dpar = \"mu\" to get just the mu part (both logged and unlogged) and dpar = \"hu\" to get the logit part (both as logits and probabilities).\nNon-zero mu part only\nWe can feed the model a bunch of possible different life expectancy values to see how steep the slope of the non-zero part is at each value. Because there are no interaction terms or random effects or anything fancy, this log-scale slope will be the same at every possible value of life expectancy—GDP per capita increases by 7.8% for each one-year increase in life expectancy.\n\nmodel_gdp_hurdle_life |&gt; \n  emtrends(~ lifeExp, var = \"lifeExp\", dpar = \"mu\",\n           at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp lifeExp.trend lower.HPD upper.HPD\n##       30        0.0784    0.0756    0.0815\n##       40        0.0784    0.0756    0.0815\n##       50        0.0784    0.0756    0.0815\n##       60        0.0784    0.0756    0.0815\n##       70        0.0784    0.0756    0.0815\n##       80        0.0784    0.0756    0.0815\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nIf we want to interpret more concrete numbers (i.e. dollars instead of percents), we should back-transform these slopes to the original dollar scale. This is a little tricky, though, because GDP per capita was logged before going into the model. If we try including regrid = \"response\" to have emtrends() back-transform the result, it won’t change anything:\n\nmodel_gdp_hurdle_life |&gt; \n  emtrends(~ lifeExp, var = \"lifeExp\", dpar = \"mu\", regrid = \"response\",\n           at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp lifeExp.trend lower.HPD upper.HPD\n##       30        0.0784    0.0756    0.0815\n##       40        0.0784    0.0756    0.0815\n##       50        0.0784    0.0756    0.0815\n##       60        0.0784    0.0756    0.0815\n##       70        0.0784    0.0756    0.0815\n##       80        0.0784    0.0756    0.0815\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nBehind the scenes, emtrends() calculates the numeric derivative (or instantaneous slope) by adding a tiny amount to each of the given levels of lifeExp (i.e. lifeExp = 30 and lifeExp = 30.001) and then calculates the slope of the line that goes through each of those predicted points. In order to back-transform the slopes, we need to transform the predicted values before calculating the instantaneous slopes. To get this ordering right, we need to include a couple extra arguments: tran = \"log\" to trick emtrends() into thinking that it’s working with logged values and type = \"response\" to tell emtrends() to unlog the estimates after calculating the slopes. (This only works with the development version of brms installed on or after May 31, 2022; huge thanks to Mattan Ben-Shachar for originally pointing this out and for opening an issue and getting it fixed at both brms and emmeans!)\n\nmodel_gdp_hurdle_life |&gt; \n  emtrends(~ lifeExp, var = \"lifeExp\", dpar = \"mu\", \n           regrid = \"response\", tran = \"log\", type = \"response\",\n           at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp lifeExp.trend lower.HPD upper.HPD\n##       30            27        25        29\n##       40            59        56        61\n##       50           128       124       133\n##       60           281       268       296\n##       70           617       573       664\n##       80          1352      1219      1489\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nNotice how they’re no longer the same at each level—that’s because in the non-logged world, this trend is curved. These represent the instantaneous slopes at each of these values of life expectancy in just the mu part of the model. At 60 years, a one-year increase in life expectancy is associated with a \\$281 increase in GDP per capita; at 70, it’s associated with a \\$617 increase.\nAlternatively, we can use the marginaleffects package to get the instantaneous mu-part slopes as well. Here we just need to tell it to exponentiate the predicted values for each level of life expectancy before calculating the numeric derivative using the transform_pre = \"expdydx\" argument. (This also only works with the development version of marginaleffects installed on or after May 31, 2022.)\n\nlibrary(marginaleffects)\n\nmodel_gdp_hurdle_life |&gt; comparisons(\n  newdata = datagrid(lifeExp = seq(30, 80, 10)),\n  dpar = \"mu\", \n  transform_pre = \"expdydx\"\n)\n## \n##     Term   Contrast Estimate  2.5 % 97.5 %\n##  lifeExp exp(dY/dX)     26.5   24.9   28.2\n##  lifeExp exp(dY/dX)     58.1   55.7   60.7\n##  lifeExp exp(dY/dX)    127.4  122.8  132.2\n##  lifeExp exp(dY/dX)    279.1  265.6  293.9\n##  lifeExp exp(dY/dX)    611.5  569.4  660.3\n##  lifeExp exp(dY/dX)   1340.9 1215.3 1490.3\n## \n## Columns: rowid, term, contrast, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, gdpPercap, lifeExp\n\nFor fun, we can plot the values of these slopes across a range of marginal effects. Importantly, the y-axis here is not the predicted value of GDP per capita—it’s the slope of life expectancy, or the first derivative of the ■ red line we made earlier with conditional_effects().\n\nmodel_gdp_hurdle_life |&gt; \n  emtrends(~ lifeExp, var = \"lifeExp\", dpar = \"mu\", \n           regrid = \"response\", tran = \"log\", type = \"response\",\n           at = list(lifeExp = seq(30, 80, 1))) |&gt; \n  gather_emmeans_draws() |&gt; \n  ggplot(aes(x = lifeExp, y = .value)) +\n  stat_lineribbon(size = 1, color = clrs[1]) +\n  scale_fill_manual(values = colorspace::lighten(clrs[1], c(0.95, 0.7, 0.4))) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Life expectancy\", y = \"Value of lifeExp coefficient\\n(marginal effect)\",\n       fill = \"Credible interval\",\n       title = \"Marginal effect of life expectancy on GDP per capita\",\n       subtitle = \"(mu part of model only)\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nZero hu part only\nWe can also deal with the hurdled parts of the model if we specify dpar = \"hu\", like here, where we can look at the hu_lifeExp coefficient at different levels of life expectancy. The slope is consistent across the whole range, but that’s because it’s measured in logit units:\n\n# Logit-scale slopes\nmodel_gdp_hurdle_life |&gt; \n  emtrends(~ lifeExp, var = \"lifeExp\", dpar = \"hu\",\n           at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp lifeExp.trend lower.HPD upper.HPD\n##       30       -0.0991    -0.117   -0.0839\n##       40       -0.0991    -0.117   -0.0839\n##       50       -0.0991    -0.117   -0.0839\n##       60       -0.0991    -0.117   -0.0839\n##       70       -0.0991    -0.117   -0.0839\n##       80       -0.0991    -0.117   -0.0839\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nWe can convert these slopes into proportions or percentage points by including the regrid = \"response\" argument. Note that we don’t need to also include tran = \"log\" and type = \"response\" like we did for the mu part—that’s because we’re not working with an outcome that’s on a strange pre-logged scale like gdpPercap is in the mu part. Here emtrends() knows that there’s a difference between the link (logit) and the response (percentage point) scales and it can switch between them easily.\n\n# Percentage-point-scale slopes\nmodel_gdp_hurdle_life |&gt; \n  emtrends(~ lifeExp, var = \"lifeExp\", dpar = \"hu\", regrid = \"response\",\n           at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp lifeExp.trend lower.HPD upper.HPD\n##       30      -0.02452  -0.02790  -0.02116\n##       40      -0.02102  -0.02626  -0.01669\n##       50      -0.01194  -0.01414  -0.00964\n##       60      -0.00532  -0.00612  -0.00450\n##       70      -0.00213  -0.00264  -0.00163\n##       80      -0.00082  -0.00114  -0.00053\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nThe slope of the hurdle part is fairly steep and negative at low levels of life expectancy (-0.025 at 30 years), but it starts leveling out as life expectancy increases (-0.002 at 70 years).\nBeyond emtrends(), we can use emmeans() to calculate predicted values of GDP per capita while holding all other variables constant. This is exactly what we did with conditional_effects() earlier—this just makes it easier to deal with the actual data itself instead of extracting the data from the ggplot object that conditional_effects() creates.\nAs we did with emtrends(), when dealing with the non-zero mu part we have to include all three extra regrid, tran, and type arguments to trick emmeans() into working with the pre-logged GDP values:\n\n# Predicted GDP per capita, logged\nmodel_gdp_hurdle_life |&gt; \n  emmeans(~ lifeExp, var = \"lifeExp\", dpar = \"mu\",\n          at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp emmean lower.HPD upper.HPD\n##       30   5.83      5.73      5.93\n##       40   6.61      6.54      6.68\n##       50   7.40      7.35      7.45\n##       60   8.18      8.15      8.22\n##       70   8.97      8.92      9.01\n##       80   9.75      9.69      9.82\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\n# Predicted GDP per capita, unlogged and back-transformed\nmodel_gdp_hurdle_life |&gt; \n  emmeans(~ lifeExp, var = \"lifeExp\", dpar = \"mu\",\n          regrid = \"response\", tran = \"log\", type = \"response\",\n          at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp response lower.HPD upper.HPD\n##       30      340       309       374\n##       40      745       695       797\n##       50     1632      1558      1714\n##       60     3579      3446      3710\n##       70     7843      7480      8193\n##       80    17184     16088     18392\n## \n## Point estimate displayed: median \n## Results are back-transformed from the log scale \n## HPD interval probability: 0.95\n\nWhen dealing with the hurdled part, we can convert the resulting logit-scale predictions to percentage points with just regrid = \"response\", again like we did with emtrends():\n\n# Predicted proportion of zeros, logits\nmodel_gdp_hurdle_life |&gt; \n  emmeans(~ lifeExp, var = \"lifeExp\", dpar = \"hu\",\n          at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp emmean lower.HPD upper.HPD\n##       30   0.18     -0.15      0.54\n##       40  -0.82     -1.02     -0.59\n##       50  -1.81     -1.97     -1.63\n##       60  -2.80     -3.06     -2.55\n##       70  -3.79     -4.21     -3.42\n##       80  -4.78     -5.33     -4.24\n## \n## Point estimate displayed: median \n## Results are given on the logit (not the response) scale. \n## HPD interval probability: 0.95\n\n# Predicted proportion of zeros, percentage points\nmodel_gdp_hurdle_life |&gt; \n  emmeans(~ lifeExp, var = \"lifeExp\", dpar = \"hu\", regrid = \"response\",\n          at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp response lower.HPD upper.HPD\n##       30    0.544     0.464     0.633\n##       40    0.306     0.260     0.352\n##       50    0.140     0.121     0.163\n##       60    0.057     0.044     0.072\n##       70    0.022     0.014     0.031\n##       80    0.008     0.005     0.014\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nWe can plot the results of emmeans() and create plots like the ones we made with conditional_effects(). I do this all the time in my regular research—I like being able to customize the plot fully rather than having to manipulate and readjust the pre-made conditional_effects() plot.\n\nplot_emmeans1 &lt;- model_gdp_hurdle_life |&gt; \n  emmeans(~ lifeExp, var = \"lifeExp\", dpar = \"mu\",\n          regrid = \"response\", tran = \"log\", type = \"response\",\n          at = list(lifeExp = seq(30, 80, 1))) |&gt; \n  gather_emmeans_draws() |&gt; \n  mutate(.value = exp(.value)) |&gt; \n  ggplot(aes(x = lifeExp, y = .value)) +\n  stat_lineribbon(size = 1, color = clrs[1]) +\n  scale_fill_manual(values = colorspace::lighten(clrs[1], c(0.95, 0.7, 0.4))) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Life expectancy\", y = \"Predicted GDP per capita\",\n       subtitle = \"Regular part of the model (dpar = \\\"mu\\\")\",\n       fill = \"Credible interval\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\nplot_emmeans2 &lt;- model_gdp_hurdle_life |&gt; \n  emmeans(~ lifeExp, var = \"lifeExp\", dpar = \"hu\", regrid = \"response\",\n          at = list(lifeExp = seq(30, 80, 1))) |&gt; \n  gather_emmeans_draws() |&gt; \n  ggplot(aes(x = lifeExp, y = .value)) +\n  stat_lineribbon(size = 1, color = clrs[5]) +\n  scale_fill_manual(values = colorspace::lighten(clrs[5], c(0.95, 0.7, 0.4))) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(x = \"Life expectancy\", y = \"Predicted probability\\nof seeing $0 GDP per capita\",\n       subtitle = \"Hurdle part of the model (dpar = \\\"hu\\\")\",\n       fill = \"Credible interval\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n(plot_emmeans1 / plot_emmeans2) +\n  plot_annotation(title = \"Conditional effect of life expectancy on GDP per capita\",\n                  subtitle = \"Made with emmeans()\",\n                  theme = theme(plot.title = element_text(family = \"Jost\", face = \"bold\"),\n                                plot.subtitle = element_text(family = \"Jost\")))\n\n\n\n\n\n\n\nBoth parts of the model simultaneously\nSo far we’ve looked at the mu and hu parts individually, but part of the magic of these models is that we can work with both processes simultaneously. When we ran conditional_effects(model_gdp_hurdle_life) without any dpar arguments, we saw predictions for both parts at the same time, but so far with emmeans() and emtrends(), we haven’t seen any of these combined values. To do this, we can calculate the expected value from the model and incorporate both parts simultaneously. In mathy terms, for this lognormal hurdle model, this is:\n\\[\n\\operatorname{E}[Y] = (1-\\texttt{hu}) \\times e^{\\texttt{mu} + (\\texttt{sigma}^2)/2}\n\\]\nWe can calculate this with emtrends() by including the epred = TRUE argument (to get expected values). We don’t need to transform anything to the response scale, and we don’t need to define a specific part of the model (dpar = \"mu\" or dpar = \"hu\"), since the output will automatically be on the dollar scale:\n\n# Dollar-scale slopes, incorporating both the mu and hu parts\nmodel_gdp_hurdle_life |&gt; \n  emtrends(~ lifeExp, var = \"lifeExp\", epred = TRUE,\n           at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp lifeExp.trend lower.HPD upper.HPD\n##       30            27        24        30\n##       40            74        69        79\n##       50           170       162       179\n##       60           373       354       394\n##       70           812       757       879\n##       80          1775      1612      1970\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nWe can also get predicted values with emmeans():\n\n# Dollar-scale predictions, incorporating both the mu and hu parts\nmodel_gdp_hurdle_life |&gt; \n  emmeans(~ lifeExp, epred = TRUE,\n          at = list(lifeExp = seq(30, 80, 10)))\n##  lifeExp emmean lower.HPD upper.HPD\n##       30    203       162       248\n##       40    677       612       745\n##       50   1839      1737      1947\n##       60   4425      4213      4597\n##       70  10056      9572     10564\n##       80  22336     20964     24078\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nAnd we can plot these combined predictions too. This is identical to conditional_effects().\n\nmodel_gdp_hurdle_life |&gt; \n  emmeans(~ lifeExp, epred = TRUE,\n          at = list(lifeExp = seq(30, 80, 1))) |&gt; \n  gather_emmeans_draws() |&gt; \n  ggplot(aes(x = lifeExp, y = .value)) +\n  stat_lineribbon(size = 1, color = clrs[4]) +\n  scale_fill_manual(values = colorspace::lighten(clrs[4], c(0.95, 0.7, 0.4))) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Life expectancy\", y = \"Predicted GDP per capita\",\n       title = \"Predicted values incorporating both hu and mu\",\n       subtitle = \"epred = TRUE\",\n       fill = \"Credible interval\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAll three types of predictions at the same time\nFor fun, we can plot the combined predictions, the mu-only predictions, and the hu-only predictions at the same time to see how they differ:\n\nplot_emmeans0 &lt;- model_gdp_hurdle_life |&gt; \n  emmeans(~ lifeExp, var = \"lifeExp\", epred = TRUE,\n          at = list(lifeExp = seq(30, 80, 1))) |&gt; \n  gather_emmeans_draws() |&gt; \n  ggplot(aes(x = lifeExp, y = .value)) +\n  stat_lineribbon(size = 1, color = clrs[4]) +\n  scale_fill_manual(values = colorspace::lighten(clrs[4], c(0.95, 0.7, 0.4))) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Life expectancy\", y = \"Predicted GDP per capita\",\n       subtitle = \"Expected values from mu and hu parts (epred = TRUE)\",\n       fill = \"Credible interval\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n(plot_emmeans0 / plot_emmeans1 / plot_emmeans2) +\n  plot_annotation(title = \"Conditional effect of life expectancy on GDP per capita\",\n                  theme = theme(plot.title = element_text(family = \"Jost\", face = \"bold\"),\n                                plot.subtitle = element_text(family = \"Jost\")))\n\n\n\n\n\n\n\nThe combined expected values produce larger predictions (with $10,000 at 70 years, for instance) than the mu part alone (with $7,500ish at 70 years). In real life, it’s probably best to work with predictions that incorporate both parts of the model, since that’s the whole point of doing this combined mixture model in the first place (otherwise we could just fit two separate models ourselves), but it’s super neat that we can still work with the predictions and slopes for the mu and hu parts separately.\nPredictions with hurdle models\nThe underlying prediction functions from brms—as well as other functions that wrap around them like gather_draws() from tidybayes—also incorporate the zero process. We can confirm this by predicting GDP per capita for two different values of life expectancy. There’s a huge proportion of zeros for countries with low life expectancy, and overall predicted GDP per capita is really low. There’s a much smaller proportion of zeros for countries with high life expectancy, and the distribution of wealth is more spread out.\n\npred_gdp_hurdle_life &lt;- model_gdp_hurdle_life |&gt; \n  predicted_draws(newdata = tibble(lifeExp = c(40, 70))) |&gt;\n  mutate(is_zero = .prediction == 0,\n         .prediction = ifelse(is_zero, .prediction - 0.1, .prediction)) |&gt; \n  mutate(nice_life = paste0(lifeExp, \" year life expectancy\"))\n\nggplot(pred_gdp_hurdle_life, aes(x = .prediction)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 2500, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) + \n  scale_x_continuous(labels = label_dollar(scale_cut = cut_short_scale())) +\n  scale_fill_manual(values = c(clrs[4], clrs[1]), \n                    guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"GDP per capita\", y = NULL, fill = \"Is zero?\",\n       title = \"Predicted GDP per capita at different life expectancies\") +\n  coord_cartesian(xlim = c(-2500, 50000)) +\n  facet_wrap(vars(nice_life)) +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n4. Hurdle lognormal model with fancy multilevel things\nSo far we’ve actually been modeling this data all wrong. This is panel data, which means each country appears multiple times in the data across different years. We’ve been treating each row as completely independent, but that’s not the case—GDP per capita in all these Afghanistan rows, for instance, is dependent on the year, and there are Afghanistan-specific trends that distinguish its GDP per capita from that of Albania. We need to take the structure of the panel data into account.\n\nhead(gapminder)\n## # A tibble: 6 × 8\n##   country     continent  year lifeExp      pop gdpPercap log_gdpPercap is_zero\n##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;lgl&gt;  \n## 1 Afghanistan Asia       1952    28.8  8425333      779.          6.66 FALSE  \n## 2 Afghanistan Asia       1957    30.3  9240934      821.          6.71 FALSE  \n## 3 Afghanistan Asia       1962    32.0 10267083      853.          6.75 FALSE  \n## 4 Afghanistan Asia       1967    34.0 11537966      836.          6.73 FALSE  \n## 5 Afghanistan Asia       1972    36.1 13079460        0           0    TRUE   \n## 6 Afghanistan Asia       1977    38.4 14880372      786.          6.67 FALSE\n\nI have a whole guide about how to do this with multilevel models—you should check it out for tons of details.\nHere we’ll make it so each continent gets its own intercept, as well as continent-specific offsets to the slopes for life expectancy and year. Since we know the zero process (low values of life expectancy make it more likely to be 0), we’ll define a simple model there (though we could get as complex as we want). Ordinarily we’d want to do country-specific effects, but for the sake of computational time, we’ll just look at continents instead. (And even then, this takes a while! On my fancy M1 MacBook, this takes about 4 minutes; and there are all sorts of issues with divergent transitions that I’m going to ignore. In real life I’d set proper priors and scale the data down, but in this example I won’t worry about all that.)\n\n# Shrink down year to make the model go faster\ngapminder_scaled &lt;- gapminder |&gt; \n  mutate(year_orig = year,\n         year = year - 1952)\n\nmodel_gdp_hurdle_panel &lt;- brm(\n  bf(gdpPercap ~ lifeExp + year + (1 + lifeExp + year | continent),\n     hu ~ lifeExp,\n     decomp = \"QR\"),\n  data = gapminder_scaled,\n  family = hurdle_lognormal(),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2,\n  threads = threading(2)  # Two CPUs per chain to speed things up\n)\n## Warning: 248 of 4000 (6.0%) transitions ended with a divergence.\n## See https://mc-stan.org/misc/warnings for details.\n## Warning: 541 of 4000 (14.0%) transitions hit the maximum treedepth limit of 10.\n## See https://mc-stan.org/misc/warnings for details.\n\n\ntidy(model_gdp_hurdle_panel)\n## # A tibble: 12 × 8\n##    effect   component group     term                     estimate std.error conf.low conf.high\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 fixed    cond      &lt;NA&gt;      (Intercept)               3.29      1.03     1.19      5.47   \n##  2 fixed    cond      &lt;NA&gt;      hu_(Intercept)            3.15      0.417    2.36      3.99   \n##  3 fixed    cond      &lt;NA&gt;      hu_lifeExp               -0.0993    0.00845 -0.117    -0.0834 \n##  4 fixed    cond      &lt;NA&gt;      lifeExp                   0.0846    0.0169   0.0491    0.120  \n##  5 fixed    cond      &lt;NA&gt;      year                     -0.00990   0.00949 -0.0309    0.00985\n##  6 ran_pars cond      continent sd__(Intercept)           2.03      0.889    0.880     4.30   \n##  7 ran_pars cond      continent sd__lifeExp               0.0335    0.0164   0.0140    0.0749 \n##  8 ran_pars cond      continent sd__year                  0.0169    0.0148   0.00374   0.0568 \n##  9 ran_pars cond      continent cor__(Intercept).lifeExp -0.655     0.322   -0.986     0.215  \n## 10 ran_pars cond      continent cor__(Intercept).year    -0.161     0.412   -0.823     0.682  \n## 11 ran_pars cond      continent cor__lifeExp.year         0.0337    0.423   -0.759     0.779  \n## 12 ran_pars cond      Residual  sd__Observation           0.678     0.0127   0.653     0.703\n\nThis complex model has a whole bunch of moving parts now! We have coefficients for the 0/not 0 process, coefficients for the non-zero process, and random effects (and their corresponding correlations) for the non-zero process. The hurdle coefficients are the same that we saw earlier, since that part of the model is unchanged. The non-zero coefficients now take the panel structure of the data into account, showing that on average, a one-year change in life expectancy is associated with a 8.5% increase in GDP per capita, on average.\nThe actual dollar amount of GDP per capita depends on the existing level of life expectancy, but as before, we can visualize this with either conditional_effects() or emmeans(). We’ll do it both ways for fun, and we’ll look at predicted GDP per capita across different continents and years too, since we have that information.\nIn order to calculate conditional effects with conditional_effects(), we have to use some special syntax. We need to (1) specify re_formula = NULL so that the predictions take the random effects structure into account (see this post for way more about that), and (2) create a data frame of year and continent values, along with a special cond__ column that will be used for the facet titles.\n\nconditions &lt;- expand_grid(year = c(0, 55),\n                          continent = unique(gapminder$continent)) |&gt; \n  mutate(cond__ = paste0(year + 1952, \": \", continent))\n\nconditional_effects(model_gdp_hurdle_panel, effects = \"lifeExp\",\n                    conditions = conditions,\n                    re_formula = NULL) |&gt; \n  plot(ncol = 4)\n\n\n\n\n\n\n\n\n\nThis is so neat! Across all continents, GDP per capita increases as life expectancy increases, but that relationship looks different across continents (compare Asia with Africa, for instance), and across time (1952 Asia looks a lot different from 2007 Asia).\nThe conditional_effects() function is great for a quick look at the predicted values, but if we want complete control over the predictions and the plot, we can use emmeans() instead:\n\nmodel_gdp_hurdle_panel |&gt; \n  emmeans(~ lifeExp + year + continent, var = \"lifeExp\",\n          at = list(year = c(0, 55), \n                    continent = unique(gapminder$continent),\n                    lifeExp = seq(30, 80, 1)),\n          epred = TRUE, re_formula = NULL, allow_new_levels = TRUE) |&gt; \n  gather_emmeans_draws() |&gt; \n  mutate(year = year + 1952) |&gt; \n  ggplot(aes(x = lifeExp, y = .value)) +\n  stat_lineribbon(aes(color = continent, fill = continent), size = 1, alpha = 0.25) +\n  scale_fill_manual(values = clrs[1:4], guide = \"none\") +\n  scale_color_manual(values = clrs[1:4], guide = \"none\") +\n  scale_y_continuous(labels = label_dollar()) +\n  facet_nested_wrap(vars(year, continent), nrow = 2, \n                    strip = strip_nested(\n                      text_x = list(element_text(family = \"Jost\", \n                                                 face = \"bold\"), NULL),\n                      background_x = list(element_rect(fill = \"grey92\"), NULL),\n                      by_layer_x = TRUE)) +\n  labs(x = \"Life expectancy\", y = \"Predicted GDP per capita\",\n       title = \"Predicted GDP per capita across life expectancy, continent, and time\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe can also look at the slopes or marginal effects of these predictions, which lets us answer questions like “how much does GDP per capita increase in Asia countries with high life expectancy in 2007.” To do this, we’ll turn to emtrends() to see predicted slopes for lifeExp while holding other model variables constant.\nIf we leave the coefficients on the log scale, there won’t be any change across life expectancy, since the trend increases by the same percentage each year. There are continent-specific differences now, though: the relationship is steeper in Europe (11.4%) than in Africa (6.1%)\n\name_model_gdp_hurdle_panel_log &lt;- model_gdp_hurdle_panel |&gt; \n  emtrends(~ lifeExp + year + continent,\n           var = \"lifeExp\",\n           at = list(year = 0, continent = unique(gapminder$continent),\n                     lifeExp = c(40, 70)),\n           re_formula = NULL, allow_new_levels = TRUE)\name_model_gdp_hurdle_panel_log\n##  lifeExp year continent lifeExp.trend lower.HPD upper.HPD\n##       40    0 Africa           0.0607    0.0529    0.0676\n##       70    0 Africa           0.0607    0.0529    0.0676\n##       40    0 Americas         0.0676    0.0561    0.0793\n##       70    0 Americas         0.0676    0.0561    0.0793\n##       40    0 Asia             0.0945    0.0863    0.1029\n##       70    0 Asia             0.0945    0.0863    0.1029\n##       40    0 Europe           0.1140    0.0959    0.1329\n##       70    0 Europe           0.1140    0.0959    0.1329\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\name_model_gdp_hurdle_panel_log |&gt; \n  gather_emmeans_draws() |&gt; \n  mutate(nice_life = paste0(lifeExp, \" year life expectancy\")) |&gt; \n  ggplot(aes(x = .value, y = continent, fill = continent)) +\n  stat_slabinterval() +\n  scale_fill_manual(values = clrs[1:4], guide = \"none\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Marginal effect of life expectancy on GDP per capita\", y = NULL,\n       title = \"Marginal effect of life expectancy, log-scale coefficients\") +\n  facet_wrap(vars(nice_life), ncol = 1) +\n  theme_nice()\n\n\n\n\n\n\n\nHowever, if we convert the coefficients to the original unlogged dollar scale, there are substantial changes across life expectancy, as we saw earlier. Only now we can also incorporate continent and year effects too. Asian countries with a 40-year life expectancy have a life expectancy slope of \\$82, while Asian countries with a 70-year life expectancy have a slope of \\$1,532, on average.\nNeat!\n\name_model_gdp_hurdle_panel_nolog &lt;- model_gdp_hurdle_panel |&gt; \n  emtrends(~ lifeExp + year + continent,\n           var = \"lifeExp\",\n           at = list(year = 0, continent = unique(gapminder$continent),\n                     lifeExp = c(40, 70)),\n           re_formula = NULL, epred = TRUE, allow_new_levels = TRUE)\name_model_gdp_hurdle_panel_nolog\n##  lifeExp year continent lifeExp.trend lower.HPD upper.HPD\n##       40    0 Africa               85        72        97\n##       70    0 Africa              508       343       696\n##       40    0 Americas            115        96       136\n##       70    0 Americas            881       591      1223\n##       40    0 Asia                 82        70        94\n##       70    0 Asia               1532      1130      2021\n##       40    0 Europe               39        25        54\n##       70    0 Europe             1367      1017      1773\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\name_model_gdp_hurdle_panel_nolog |&gt; \n  gather_emmeans_draws() |&gt; \n  mutate(nice_life = paste0(lifeExp, \" year life expectancy\")) |&gt; \n  ggplot(aes(x = .value, y = continent, color = continent)) +\n  stat_pointinterval() +\n  scale_color_manual(values = clrs[1:4], guide = \"none\") +\n  scale_x_continuous(labels = label_dollar()) +\n  labs(x = \"Marginal effect of life expectancy on GDP per capita\", y = NULL,\n       title = \"Marginal effect of life expectancy, dollar-scale coefficients\") +\n  facet_wrap(vars(nice_life), ncol = 1, scales = \"free_x\") +\n  theme_nice()\n\n\n\n\n\n\n\nFor extra bonus fun, we can look at how the dollar-scale marginal effect changes across more granular changes in life expectancy and across time and across continent.\n\name_model_gdp_hurdle_panel_nolog_fancy &lt;- model_gdp_hurdle_panel |&gt; \n  emtrends(~ lifeExp + year + continent,\n           var = \"lifeExp\",\n           at = list(year = c(0, 55), continent = unique(gapminder$continent),\n                     lifeExp = seq(40, 70, 2)),\n           epred = TRUE, re_formula = NULL, allow_new_levels = TRUE)\n\name_model_gdp_hurdle_panel_nolog_fancy |&gt; \n  gather_emmeans_draws() |&gt; \n  mutate(year = year + 1952) |&gt; \n  ggplot(aes(x = .value, y = continent, color = lifeExp, group = lifeExp)) +\n  stat_pointinterval(position = \"dodge\", size = 0.75) +\n  scale_color_gradient(low = clrs[5], high = clrs[2],\n                       guide = guide_colorbar(barwidth = 12, barheight = 0.5, direction = \"horizontal\")) +\n  scale_x_continuous(labels = label_dollar()) +\n  labs(x = \"Value of lifeExp coefficient (marginal effect)\", y = NULL,\n       color = \"Life expectancy\",\n       title = \"Marginal effect of life expectancy on GDP per capita\\nacross continent, time, and different values of life expectancy\",\n       subtitle = \"…all while incorporating information about the hurdle process!\") +\n  facet_wrap(vars(year), ncol = 1) +\n  theme_nice() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_text(vjust = 1))\n\n\n\n\n\n\n\nAHHH this is so cool. As life expectancy increases, the estimated lifeExp increases across all continents and across all years, but the trend behaves differently within each continent and within each year."
  },
  {
    "objectID": "blog/2022/05/09/hurdle-lognormal-gaussian-brms/index.html#normally-distributed-outcomes-with-zeros",
    "href": "blog/2022/05/09/hurdle-lognormal-gaussian-brms/index.html#normally-distributed-outcomes-with-zeros",
    "title": "A guide to modeling outcomes that have lots of zeros with Bayesian hurdle lognormal and hurdle Gaussian regression models",
    "section": "Normally distributed outcomes with zeros",
    "text": "Normally distributed outcomes with zeros\nbrms comes with 4 different hurdle families and 6 different zero-inflated families. Working with hurdle_lognormal() in the gapminder example above was great because GDP per capita is exponentially distributed and well-suited for logging. But what if your outcome isn’t exponentially distributed, and doesn’t fit one of the other built-in hurdle or zero-inflated families (gamma, Poisson, negative binomial, beta, etc.)? What if you have something nice and linear already, but that also has a built-in zero process? Let’s see what we can do with that kind of outcome variable.\nExplore data\nWe’ll play with the delightful palmerpenguins data. It’s nice and pristine and has no zeros, but like we did with the gapminder data, we’ll build in some of our own zeros. Here we’ll say that penguins with a flipper length of less than 190 mm will have a 30% chance of recording a 0 in body mass, while those with flippers longer than 190 mm will have a 2% chance.\n\npenguins &lt;- palmerpenguins::penguins |&gt; \n  drop_na(sex) |&gt; \n  # Make a bunch of weight values 0\n  mutate(prob_zero = ifelse(flipper_length_mm &lt; 190, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         body_mass_g = ifelse(will_be_zero, 0, body_mass_g)) |&gt; \n  select(-prob_zero, -will_be_zero) |&gt; \n  mutate(is_zero = body_mass_g == 0)\n\nLet’s see how many zeros we ended up with:\n\npenguins |&gt; \n  count(is_zero) |&gt; \n  mutate(prop = n / sum(n))\n## # A tibble: 2 × 3\n##   is_zero     n  prop\n##   &lt;lgl&gt;   &lt;int&gt; &lt;dbl&gt;\n## 1 FALSE     296 0.889\n## 2 TRUE       37 0.111\n\nCool— 11.1% of the values of body mass are zero, which coincidentally is pretty close to the proportion we got in the gapminder example. Here’s what the distribution looks like:\n\npenguins |&gt; \n  mutate(body_mass_g = ifelse(is_zero, -0.1, body_mass_g)) |&gt; \n  ggplot(aes(x = body_mass_g)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 100,\n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(\"darkorange\", \"purple\")) +\n  scale_x_continuous(labels = comma_format()) +\n  labs(x = \"Body mass (g)\", y = \"Count\", fill = \"Is zero?\",\n       title = \"Distribution of penguin body mass\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n1. Regular OLS model on a non-exponential outcome\nNow that we have some artificial zeros, let’s model the relationship between body mass and bill length. Since body mass is relatively normally distributed, we’ll use regular old OLS and not worry about any logging.\nThe zeros we added are going to cause some problems. In this scatterplot, the ■ blue line is fit using only the non-zero data. If we include the zeros, like the ■ fuchsia line in this plot, the relationship changes a little—the model underpredicts the body mass of shorter-billed penguins.\n\nggplot(penguins, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species), size = 1.5) + \n  geom_smooth(method = \"lm\", color = \"#F012BE\") +\n  geom_smooth(data = filter(penguins, body_mass_g != 0), method = \"lm\", color = \"#0074D9\") +\n  scale_y_continuous(labels = label_comma()) +\n  scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n  labs(x = \"Bill length (mm)\", y = \"Body mass (g)\", color = \"Species\",\n       title = 'OLS models &lt;span style=\"color:#F012BE;\"&gt;with&lt;/span&gt; and &lt;span style=\"color:#0074D9;\"&gt;without&lt;/span&gt; zeros') +\n  theme_nice() +\n  theme(legend.position = \"bottom\",\n        plot.title = element_markdown())\n\n\n\n\n\n\n\nLet’s make a couple models that predict body mass based on bill length and species, both with and without the zeros:\n\nmodel_mass_basic &lt;- brm(\n  bf(body_mass_g ~ bill_length_mm + species),\n  data = penguins,\n  family = gaussian(),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2\n)\n\nmodel_mass_basic_no_zero &lt;- brm(\n  bf(body_mass_g ~ bill_length_mm + species),\n  data = filter(penguins, !is_zero),\n  family = gaussian(),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2\n)\n\n\ntidy(model_mass_basic)\n## # A tibble: 5 × 8\n##   effect   component group    term             estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)        -1474.     737.   -2916.      -39.5\n## 2 fixed    cond      &lt;NA&gt;     bill_length_mm       122.      18.9     84.9     159. \n## 3 fixed    cond      &lt;NA&gt;     speciesChinstrap   -1046.     240.   -1516.     -571. \n## 4 fixed    cond      &lt;NA&gt;     speciesGentoo        769.     209.     381.     1182. \n## 5 ran_pars cond      Residual sd__Observation     1002.      39.3    929.     1082.\ntidy(model_mass_basic_no_zero)\n## # A tibble: 5 × 8\n##   effect   component group    term             estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)         219.     287.     -316.       794.\n## 2 fixed    cond      &lt;NA&gt;     bill_length_mm       90.4      7.31     75.6      104.\n## 3 fixed    cond      &lt;NA&gt;     speciesChinstrap   -887.      92.6   -1064.      -704.\n## 4 fixed    cond      &lt;NA&gt;     speciesGentoo       572.      77.4     419.       725.\n## 5 ran_pars cond      Residual sd__Observation     370.      15.0     342.       401.\n\nThese models give different estimates for the effect of bill length on body mass. In the model that omits the zeros (■ blue), a 1 mm increase in bill length is associated with a 121.91 gram increase in body mass, on average. If we include the zeros, though (■ fuchsia), the effect drops to 90.42 grams.\nIf we look at a posterior predictive check for the ■ blue zero-free model we can see that the model doesn’t do a great job of fitting the distribution of the outcome. Compare the distribution of actual observed body mass (in ■ light blue) with simulated body mass from the posterior distribution (in ■ orange). Oh no:\n\npp_check(model_mass_basic)\n\n\n\n\n\n\n\n\n\nWe need to somehow account for these zeros, but how?\n2. Hurdle lognormal model on a non-exponential outcome\nUnfortunately, there’s no built-in hurdle_gaussian() family we can use with brms. But maybe we can fake it and use hurdle_lognormal(). If we log body mass it shrinks the values down to ≈8.5, and we still have a normal-looking distribution (albeit with a really small range).\n\npenguins |&gt; \n  mutate(body_mass_g = log1p(body_mass_g)) |&gt; \n  mutate(body_mass_g = ifelse(is_zero, -0.01, body_mass_g)) |&gt; \n  ggplot(aes(x = body_mass_g)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 0.1,\n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) +\n  scale_fill_manual(values = c(\"darkorange\", \"purple\")) +\n  labs(x = \"Body mass (g)\", y = \"Count\", fill = \"Is zero?\",\n       title = \"Distribution of logged penguin body mass\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe can probably plausibly use a hurdled lognormal model here like we did with the gapminder example. This makes the coefficients a little harder to interpret—we can’t say that a 1 mm increase in bill length is associated with a X gram change in body mass, but we can say that a 1 mm increase in bill length is associated with a X% change in body mass. And we can back-transform these logged coefficients to the gram scale using emmeans(). Let’s try it.\nSince we know the hurdle process (flipper length determined the 0/not 0 choice), we’ll use flipper length in the hu formula.\n\nmodel_mass_hurdle_log &lt;- brm(\n  bf(body_mass_g ~ bill_length_mm + species,\n     hu ~ flipper_length_mm),\n  data = penguins,\n  family = hurdle_lognormal(),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2\n)\n\n\ntidy(model_mass_hurdle_log)\n## # A tibble: 7 × 8\n##   effect   component group    term                 estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)            7.41     0.0692    7.28      7.55  \n## 2 fixed    cond      &lt;NA&gt;     hu_(Intercept)        27.4      5.95     16.5      39.7   \n## 3 fixed    cond      &lt;NA&gt;     bill_length_mm         0.0208   0.00176   0.0173    0.0242\n## 4 fixed    cond      &lt;NA&gt;     speciesChinstrap      -0.202    0.0224   -0.246    -0.157 \n## 5 fixed    cond      &lt;NA&gt;     speciesGentoo          0.131    0.0192    0.0936    0.169 \n## 6 fixed    cond      &lt;NA&gt;     hu_flipper_length_mm  -0.155    0.0317   -0.220    -0.0980\n## 7 ran_pars cond      Residual sd__Observation        0.0888   0.00366   0.0822    0.0961\n\nWe have results, but they’re on the log scale so we have to think about the coefficients differently. According to the bill_length_mm coefficient, a one-millimeter increase in bill length is associated with a 2.1% increase in body mass.\nWe can also look at the logit-scale hurdle process, though it’s a little tricky since we can’t just use plogis() on the coefficient. We can use emtrends() to extract the slope, though. Since we’re on a logit scale, the actual slope depends on the value of flipper length depends on flipper length itself. We can feed emtrends() any flipper lengths we want—a range of possible flipper lengths; the average for the data; whatever—and get the corresponding slope or marginal effect. If we include regrid = \"response\" we’ll see the results on the percentage point-scale\n\nmodel_mass_hurdle_log |&gt; \n  emtrends(~ flipper_length_mm, var = \"flipper_length_mm\", \n           dpar = \"hu\", regrid = \"response\",\n           # Show the effect for the mean and for a range\n           at = list(flipper_length_mm = c(mean(penguins$flipper_length_mm), \n                                           seq(170, 230, 10))))\n##  flipper_length_mm flipper_length_mm.trend lower.HPD upper.HPD\n##                201                 -0.0032   -0.0049  -0.00144\n##                170                 -0.0292   -0.0364  -0.02038\n##                180                 -0.0352   -0.0532  -0.01646\n##                190                 -0.0144   -0.0211  -0.00798\n##                200                 -0.0036   -0.0056  -0.00184\n##                210                 -0.0008   -0.0018  -0.00017\n##                220                 -0.0002   -0.0006  -0.00001\n##                230                  0.0000   -0.0002   0.00000\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nThe probability of seeing a zero decreases at different rates depending on existing flipper lengths. For short-flippered penguins, a one-mm change from 170 mm to 171 mm is associated with a -2.92 percentage point decrease in the probability of seeing a zero. For long-flippered penguins, a one-mm change from 220 mm to 221 mm is associated with a -0.02 percentage point decrease. For penguins with an average flipper length (200.97 mm), the change is -0.32 percentage points. These slopes are all visible in the conditional_effects() plot of the hurdle component of the model, which I’ll show below.\nWe can also check how well the model fits the data:\n\npp_check(model_mass_hurdle_log)\n\n\n\n\n\n\n\n\n\nIt accounts for the zeros, as expected.\nNext we’ll look at the marginal/conditional effects of bill length on body mass and flipper length on the proportion of zeros. We can do it with both conditional_effects() (for quick and easy plots) and with emmeans() (for more control over everything). Unfortunately there’s no easy way to plot only the mu part using conditional_effects()—it defaults to showing expected predictions. We can show only the mu part with emmeans() though.\n\nconditional_effects(model_mass_hurdle_log, effects = \"bill_length_mm\")\nconditional_effects(model_mass_hurdle_log, effects = \"flipper_length_mm\", dpar = \"hu\")\n\n\n\n\n\n\n\n\n\n\nplot_penguins_emmeans0 &lt;- model_mass_hurdle_log |&gt; \n  emmeans(~ bill_length_mm, var = \"bill_length_mm\",\n          at = list(bill_length_mm = seq(30, 60, 1)),\n          epred = TRUE) |&gt; \n  gather_emmeans_draws() |&gt; \n  ggplot(aes(x = bill_length_mm, y = .value)) +\n  stat_lineribbon(size = 1, color = \"cyan4\") +\n  scale_fill_manual(values = colorspace::lighten(\"cyan4\", c(0.95, 0.7, 0.4))) +\n  scale_y_continuous(labels = label_comma()) +\n  labs(x = \"Bill length (mm)\", y = \"Predicted body mass (g)\",\n       subtitle = \"Expected values from mu and hu parts (epred = TRUE)\",\n       fill = \"Credible interval\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\nplot_penguins_emmeans1 &lt;- model_mass_hurdle_log |&gt; \n  emmeans(~ bill_length_mm, var = \"bill_length_mm\", dpar = \"mu\",\n          regrid = \"response\", tran = \"log\", type = \"response\",\n          at = list(bill_length_mm = seq(30, 60, 1))) |&gt; \n  gather_emmeans_draws() |&gt; \n  mutate(.value = exp(.value)) |&gt; \n  ggplot(aes(x = bill_length_mm, y = .value)) +\n  stat_lineribbon(size = 1, color = \"darkorange\") +\n  scale_fill_manual(values = colorspace::lighten(\"darkorange\", c(0.95, 0.7, 0.4))) +\n  scale_y_continuous(labels = label_comma()) +\n  labs(x = \"Bill length (mm)\", y = \"Predicted body mass (g)\",\n       subtitle = \"Regular part of the model (dpar = \\\"mu\\\")\",\n       fill = \"Credible interval\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\nplot_penguins_emmeans2 &lt;- model_mass_hurdle_log |&gt; \n  emmeans(~ flipper_length_mm, var = \"flipper_length_mm\", dpar = \"hu\",\n          at = list(flipper_length_mm = seq(170, 240, 1))) |&gt; \n  gather_emmeans_draws() |&gt; \n  mutate(.value = plogis(.value)) |&gt; \n  ggplot(aes(x = flipper_length_mm, y = .value)) +\n  stat_lineribbon(size = 1, color = \"purple\") +\n  scale_fill_manual(values = colorspace::lighten(\"purple\", c(0.95, 0.7, 0.4))) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(x = \"Flipper length (mm)\", y = \"Predicted probability\\nof seeing 0 body mass\",\n       subtitle = \"Hurdle part of the model (dpar = \\\"hu\\\")\",\n       fill = \"Credible interval\") +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n(plot_penguins_emmeans0 / plot_penguins_emmeans1 / plot_penguins_emmeans2) +\n  plot_annotation(title = \"Conditional effects of bill length and flipper length\",\n                  subtitle = \"Made with emmeans()\",\n                  theme = theme(plot.title = element_text(family = \"Jost\", face = \"bold\"),\n                                plot.subtitle = element_text(family = \"Jost\")))\n\n\n\n\n\n\n\nThat trend in predicted body mass looks surprisingly linear even though we modeled it with a log-based family! In general it shows that body mass increases by 2.1% for each millimeter increase in bill length. Since we used a log-based family, the actual slope on the original scale will be different across the whole range of bill length. emtrends() lets us see this:\n\n# Slopes for the combined mu and hu parts of the model\nmodel_mass_hurdle_log |&gt; \n  emtrends(~ bill_length_mm, var = \"bill_length_mm\",\n           at = list(bill_length_mm = seq(30, 60, 10)),\n           epred = TRUE)\n##  bill_length_mm bill_length_mm.trend lower.HPD upper.HPD\n##              30                 62.0      54.4      68.8\n##              40                 76.3      64.5      87.5\n##              50                 93.9      76.7     111.2\n##              60                115.6      91.1     141.5\n## \n## Results are averaged over the levels of: species \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nFor penguins with short bills like 30 mm, the slope of bill length is 62; for penguins with long bills like 60 mm, the slope is almost twice that at 115.6. That seems like a huge difference, but body mass ranges in the thousands of grams, so in the plot of conditional effects a 60 mm difference in slope barely registers visually.\n3. Hurdle gaussian model with a custom brms family\nModeling body mass with a lognormal model works pretty well despite its fairly-normal-and-not-at-all-exponential distribution. We have to do some additional data acrobatics to convert coefficients from logs to not-logs, which can be annoying. More importantly, though, it feels weird to knowingly use the wrong family to model this outcome. The whole point of doing this hurdle model stuff is to use models that most accurately reflect the underlying data (rather than throwing OLS at everything). It would be great if we could use something like hurdle_gaussian() to avoid all this roundabout log work.\nFortunately brms has the ability to create custom families, and Paul Bürkner has a whole vignette about how to do it. With some R and Stan magic, we can create our own hurdle_gaussian() family. The vignette goes into much more detail about each step, and I worked through most of it a year-ish ago after hours of googling and debugging (and big help from different Stan forum users). Here’s the bare minimum of what’s needed:\n\n\nCreate a custom brms family with custom_family() to define the name, distributional parameters, links, and other model details.\nHere we’ll make a family that accepts arguments for mu, sigma, and hu, which will be modeled using the identity (i.e. original) scale, log scale, and logit scale, respectively.\n\nhurdle_gaussian &lt;- \n# Create a custom family that is logit if y = 0, normal/gaussian if not\n  custom_family(\"hurdle_gaussian\", \n                dpars = c(\"mu\", \"sigma\", \"hu\"),\n                links = c(\"identity\", \"log\", \"logit\"),\n                lb = c(NA, 0, NA),\n                type = \"real\")\n\n\n\nProvide some raw Stan code to handle the actual sampling.\nHere’s where we tell Stan to use a binomial family when the outcome is 0 and a normal Gaussian distribution if not.\n\n# Stan code\nstan_funs &lt;- \"\n  real hurdle_gaussian_lpdf(real y, real mu, real sigma, real hu) { \n    if (y == 0) { \n      return bernoulli_lpmf(1 | hu); \n    } else { \n      return bernoulli_lpmf(0 | hu) +  \n             normal_lpdf(y | mu, sigma); \n    } \n  }\n\"\n\n# Prepare Stan code for use in brm()\nstanvars &lt;- stanvar(scode = stan_funs, block = \"functions\")\n\n\n\nCreate some post-processing functions so that things like predict() work:\n\nposterior_predict_hurdle_gaussian &lt;- function(i, prep, ...) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  sigma &lt;- brms::get_dpar(prep, \"sigma\", i = i)\n  theta &lt;- brms::get_dpar(prep, \"hu\", i = i)\n\n  hu &lt;- runif(prep$ndraws, 0, 1)\n  ifelse(hu &lt; theta, 0, rnorm(prep$ndraws, mu,sigma))\n}\n\nposterior_epred_hurdle_gaussian &lt;- function(prep) {\n  with(prep$dpars, mu * (1 - hu))\n}\n\n\n\nAnd that’s it!\nTo use the custom family, we need to specify the name in family and pass the Stan code through the stanvars argument. Let’s use it!\n\nmodel_mass_hurdle_gaussian &lt;- brm(\n  bf(body_mass_g ~ bill_length_mm + species,\n     hu ~ flipper_length_mm),\n  data = penguins,\n  family = hurdle_gaussian,  # &lt;--- This is new\n  stanvars = stanvars,  # &lt;--- This is new\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2\n)\n\n\ntidy(model_mass_hurdle_gaussian)\n## # A tibble: 7 × 8\n##   effect   component group    term                 estimate std.error  conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)           208.     290.      -374.     762.    \n## 2 fixed    cond      &lt;NA&gt;     hu_(Intercept)         27.4      6.05      16.8     40.1   \n## 3 fixed    cond      &lt;NA&gt;     bill_length_mm         90.7      7.39      76.4    105.    \n## 4 fixed    cond      &lt;NA&gt;     speciesChinstrap     -889.      95.1    -1075.    -703.    \n## 5 fixed    cond      &lt;NA&gt;     speciesGentoo         570.      78.6      415.     724.    \n## 6 fixed    cond      &lt;NA&gt;     hu_flipper_length_mm   -0.156    0.0323    -0.222   -0.0991\n## 7 ran_pars cond      Residual sd__Observation       370.      14.9      342.     399.\n\nThe coefficients for the hurdled part like hu_(Intercept) and hu_flipper_length_mm are still on the logit scale, as expected, but now the coefficients for the non-zero part are on the original regular scale! A one-mm change in bill length is associated with a 90.69 gram increase in body mass; Chinstrap penguins are 888.87 grams lighter than Adélie penguins, while Gentoos are 570.32 grams heavier than Adélies. Nothing is logged and there are no percent changes to worry about. It’s all easy!\nIf we use pp_check() we can see that the model successfully takes the zeros into account:\n\npp_check(model_mass_hurdle_gaussian)\n\n\n\n\n\n\n\n\n\nThe conditional/marginal effects are more straightforward now too. If we use conditional_effects() we’ll see a perfectly straight line:\n\n# No need for epred = TRUE!\nconditional_effects(model_mass_hurdle_gaussian, effects = \"bill_length_mm\")\n\n\n\n\n\n\n\n\n\nAnd if we use emtrends() to check the slope of that line across different values of bill length, we’ll see the same slope instead of a range from 61.976 to 115.556 like we saw with the hurdle lognormal family:\n\nmodel_mass_hurdle_gaussian |&gt; \n  emtrends(~ bill_length_mm, var = \"bill_length_mm\",\n           at = list(bill_length_mm = seq(30, 60, 10)),\n           epred = TRUE)\n##  bill_length_mm bill_length_mm.trend lower.HPD upper.HPD\n##              30                 88.7      74.6       103\n##              40                 88.7      74.6       103\n##              50                 88.7      74.6       103\n##              60                 88.7      74.6       103\n## \n## Results are averaged over the levels of: species \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nImportantly that slope (88.663) is different from what we found when looking at the model results with tidy() (90.69). That’s because emmeans() and emtrends() take the zero process into account by calculating the expected value (epred = TRUE).\nWe can confirm that there are predicted zeros. For penguins with a shorter bill like 30 mm, the overall distribution of predicted body mass is centered around 3,000ish grams, while those with longer bills (60 mm) are predicted to be clustered around 6,000 grams. Flipper length determines the distribution of zeros (since that’s the effect we built in; remember that we made it so that penguins with a flipper length of less than 190 mm have a 30% chance of being zero). The model picks this up. If we set flipper length to 189 mm (just under the cutoff), the model predicts a big proportion of zeros; if we set it to 210, it predicts a small proportion of zeros.\n\npred_mass_hurdle_gaussian &lt;- model_mass_hurdle_gaussian |&gt; \n  predicted_draws(newdata = expand_grid(bill_length_mm = c(30, 60),\n                                        species = \"Adelie\",\n                                        flipper_length_mm = c(189, 210))) |&gt;\n  mutate(is_zero = .prediction == 0,\n         .prediction = ifelse(is_zero, .prediction - 0.1, .prediction)) |&gt; \n  mutate(nice_bill = paste0(bill_length_mm, \" mm bill\"),\n         nice_flipper = paste0(flipper_length_mm, \" mm flippers\"))\n\nggplot(pred_mass_hurdle_gaussian, aes(x = .prediction)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 100, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) + \n  scale_x_continuous(labels = label_comma()) +\n  scale_fill_manual(values = c(\"darkorange\", \"purple\"), \n                    guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"Predicted body mass (g)\", y = NULL, fill = \"Is zero?\",\n       title = \"Predicted body mass across different bill and flipper lengths\",\n       subitlte = \"Results from hurdled Gaussian model\") +\n  facet_nested_wrap(vars(nice_bill, nice_flipper), nrow = 2, \n                    strip = strip_nested(\n                      text_x = list(element_text(family = \"Jost\", \n                                                 face = \"bold\"), NULL),\n                      background_x = list(element_rect(fill = \"grey92\"), NULL),\n                      by_layer_x = TRUE)) +\n  theme_nice() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnd that’s it! A fully working hurdle Gaussian model in brms! This single model contains an incredible amount of detail about all these different moving parts. Magical!\nI lied—this custom family isn’t all the way complete!\nIt would be really neat to have hurdle_gaussian() as an official native family in brms, and the vignette makes it sound like it’s possible to submit a pull request to formally add custom families like this, BUT what I have here doesn’t quite work all the way.\nWe’re still missing one post-processing function for calculating the log likelihood for the family. If we try to use a function that relies on the log likelihood like loo(), we’ll get an error:\n\nloo(model_mass_hurdle_gaussian)\n## Error in get(out, family$env): object 'log_lik_hurdle_gaussian' not found\n\nIn addition to the posterior_predict_hurdle_gaussian() and posterior_epred_hurdle_gaussian() functions we defined earlier, we also need a log_lik_hurdle_gaussian() function. The only problem is that I have no idea how to make this.\n\n# idk\nlog_lik_hurdle_gaussian &lt;- function(???) {\n  # ????????\n}\n\nAlas.\nDespite this, for now the custom hurdle Gaussian model works well for most other things."
  },
  {
    "objectID": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html",
    "href": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html",
    "title": "How to use Bayesian propensity scores and inverse probability weights",
    "section": "",
    "text": "This post combines two of my long-standing interests: causal inference and Bayesian statistics. I’ve been teaching a course on program evaluation and causal inference for a couple years now and it has become one of my favorite classes ever. It has reshaped how I do my research, and I’ve been trying to carefully incorporate causal approaches in my different project—as evidenced by an ever-growing series of blog posts here about different issues I run into and figure out (like this and this and this and this.)\nAdditionally, ever since stumbling on this blog post as a PhD student back in 2016 following the invention of rstanarm and brms, both of which make it easy to use Stan with R, I’ve been as Bayesian as possible in all my research. I find Bayesian approaches to inference way more intuitive than frequentist null hypothesis significance testing. My work with Bayesian approaches has also led to a bunch of blog posts here (like this and this and this).\nHowever, the combination of these two interests is a little fraught. In one of my projects, I’m using marginal structural models and inverse probability weights to account for confounding and make causal claims. I also want to do this Bayesianly so that I can work with posterior distributions and use Bayesian inference rather than null hypotheses and significance. But using inverse probability weights and Bayesian methods simultaneously seems to be impossible! Robins, Hernán, and Wasserman (2015) even have an article where they explicitly say that “Bayesian inference must ignore the propensity score,” effectively making it impossible to use things like inverse probability weights Bayesianly. Oh no!\nI recently came across a new article that gives me hope though! There’s a group of epidemiologists and biostatisticians who have been working on finding ways to use a Bayesian approach to propensity scores and weights (Saarela et al. (2015); Zigler (2016); Liao and Zigler (2020), among others), and Liao and Zigler (2020) provide a useful (and understandable!) approach for doing it. This post is my attempt at translating Liao and Zigler’s paper from conceptual math and MCMCPack-based R code into tidyverse, brms, and Stan-based code. Here we go!"
  },
  {
    "objectID": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html#who-this-post-is-for",
    "href": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html#who-this-post-is-for",
    "title": "How to use Bayesian propensity scores and inverse probability weights",
    "section": "Who this post is for",
    "text": "Who this post is for\nHere’s what I assume you know:\n\nYou’re familiar with R and the tidyverse (particularly dplyr and ggplot2).\nYou’re familiar with brms for running Bayesian regression models. See the vignettes here, examples like this, or resources like these for an introduction.\nYou know a little about DAGs and causal model-based approaches to causal inference, and you’ve heard about statistical adjustment to isolate causal effects (i.e. “closing backdoors in a DAG”)"
  },
  {
    "objectID": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html#general-approach-to-inverse-probability-weighting",
    "href": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html#general-approach-to-inverse-probability-weighting",
    "title": "How to use Bayesian propensity scores and inverse probability weights",
    "section": "General approach to inverse probability weighting",
    "text": "General approach to inverse probability weighting\nI won’t go into the details of how inverse probability weighting works here. For more details, check out this fully worked out example or this chapter, which has references to lots of other more detailed resources. Instead, I’ll provide a super short abbreviated overview of how inverse probability weights are used for causal inference and why and how we can use them.\nWhen trying to make causal inferences with observational data, there is inevitably confounding—people self-select into (or out of) treatment conditions because of a host of external factors. We can adjust for these confounding factors with lots of different methods. Quasi-experimental approaches like difference-in-differences, regression discontinuity, and instrumental variables let us use specific (and often weird) situations to adjust for confounding and make comparable treatment and control groups. Alternatively, we can use model-based inference using DAGs and do-calculus, identifying which variables open up backdoor pathways between treatment and outcome, and statistically adjusting for those variables to isolate the treatment → outcome pathway.\nOne way to adjust for confounders is to use inverse probability weighting. In short, here’s how it works for a binary (0/1) treatment:\n\nCreate a model that predicts treatment (often called a treatment model or design stage). Use confounders (identified with a DAG) as the covariates. Use whatever modeling approach you want here—logistic regression, random forests, fancy machine learning things, etc.\nUse the results of the treatment model to calculate propensity scores.\n\nConvert those propensity scores into inverse probability of treatment weights (IPTW) using this formula:\n\\[\n\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}\n\\]\n\nCreate a model that estimates the effect of treatment on outcome, weighted by the IPTWs (often called an outcome model or analysis stage). The coefficient for the treatment variable is the average treatment effect (ATE).\n\nBasic frequentist example\nThroughout this post, we’ll use some simulated data for a fake hypothetical social program that distributes mosquito nets in order to reduce malaria risk. I created this data for my course on program evaluation and causal inference and use it for teaching adjustment with inverse probability weighting.\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(ggdag)\nlibrary(MetBrewer)\n\nset.seed(3273)  # From random.org\n\n# Use the delightful Isfahan1 palette from the MetBrewer package\nisfahan &lt;- MetBrewer::met.brewer(\"Isfahan1\")\n\n# Custom ggplot theme to make pretty plots\n# Get Archivo Narrow at https://fonts.google.com/specimen/Archivo+Narrow\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Archivo Narrow\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\", size = rel(0.8), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}\n\n# Use this theme on all plots\ntheme_set(\n  theme_nice()\n)\n\n# Make all labels use Archivo by default\nupdate_geom_defaults(\"label\", \n                     list(family = \"Archivo Narrow\",\n                          fontface = \"bold\"))\n\n\nnets &lt;- read_csv(\"https://evalf21.classes.andrewheiss.com/data/mosquito_nets.csv\")\n\nI normally use a more complicated DAG with other nodes that don’t need to be adjusted for, but for the sake of simplicity here, this DAG only includes the confounders. The relationship between net usage (measured as a 0/1 binary variable where 1 = person used a net) and malaria risk (measured on a scale of 0-100, with higher values representing higher risk) is confounded by monthly income (in USD), health (measured on a scale of 0-100, with higher values representing better health), and nighttime temperatures at the person’s home (measured in Celsius).\n\nmosquito_dag &lt;- dagify(\n  malaria_risk ~ net + income + health + temperature,\n  net ~ income + health + temperature,\n  health ~ income,\n  exposure = \"net\",\n  outcome = \"malaria_risk\",\n  coords = list(x = c(malaria_risk = 7, net = 3, income = 4, health = 5, temperature = 6),\n                y = c(malaria_risk = 2, net = 2, income = 3, health = 1, temperature = 3)),\n  labels = c(malaria_risk = \"Risk of malaria\", net = \"Mosquito net\", income = \"Income\",\n             health = \"Health\", temperature = \"Nighttime temperatures\",\n             resistance = \"Insecticide resistance\")\n)\n\n# Turn DAG into a tidy data frame for plotting\nmosquito_dag_tidy &lt;- mosquito_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  node_status()   # Add column for exposure/outcome/latent\n\nstatus_colors &lt;- c(exposure = isfahan[2], outcome = isfahan[7], latent = \"grey50\")\n\n# Fancier graph\nggplot(mosquito_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = -0.3) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\nFollowing the logic of do-calculus, we need to adjust for three of these nodes in order to isolate the pathway between net usage and malaria risk:\n\nlibrary(dagitty)\nadjustmentSets(mosquito_dag)\n## { health, income, temperature }\n\nWe can adjust for these variables using inverse probability weighting. We’ll first make a treatment model (or “design stage” in the world of biostats) that uses these confounders to predict net use, then we’ll create propensity scores and inverse probability treatment weights, and then we’ll use those weights in an outcome model (or “analysis stage” in the world of biostats) to calculate the average treatment effect (ATE) of net usage.\nI built in a 10 point decrease in malaria risk due to nets (hooray for fake data!), so let’s see if we can recover that treatment effect:\n\n# Step 1: Create model that predicts treatment status using confounders\nmodel_treatment_freq &lt;- glm(net ~ income + temperature + health,\n                            data = nets,\n                            family = binomial(link = \"logit\"))\n\n# Step 2: Use the treatment model to calculate propensity scores, and\n# Step 3: Use the propensity scores to calculate inverse probability of treatment weights\nnets_with_weights &lt;- augment(model_treatment_freq, nets,\n                             type.predict = \"response\") %&gt;%\n  rename(propensity = .fitted) %&gt;% \n  mutate(iptw = (net_num / propensity) + ((1 - net_num) / (1 - propensity)))\n\n# Step 4: Use the IPTWs in a model that estimates the effect of treatment on outcome\nmodel_outcome_freq &lt;- lm(malaria_risk ~ net,\n                         data = nets_with_weights,\n                         weights = iptw)\n\n# Coefficient for `net` should be -10ish\ntidy(model_outcome_freq)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)     39.7     0.468      84.7 0       \n## 2 netTRUE        -10.1     0.658     -15.4 3.21e-50\n\nIt worked! After going through this two-step process of (1) creating propensity scores and weights, and (2) using those weights to estimate the actual effect, we successfully closed the backdoor pathways that confounded the relationship between net use and malaria risk, ending up with an unbiased ATE. Neato.\nPseudo-populations\nBefore looking at how to do this analysis Bayesianly, it’s helpful to understand what these weights are actually doing behind the scenes. The point of these IPTWs is to create pseudo-populations of treated and untreated observations that are comparable across all the different levels of confounders. They’re essentially a way to let us fake treatment and control groups so that we can interpret the results of outcome models causally.\nVisualizing the propensity scores for treated and untreated people can help show what’s going on. Here are the distributions of propensity scores for these two groups: the treated group is in the top half in brown; the untreated group is in the bottom half in turquoise. (Thanks to Lucy D’Agostino McGowan for this really neat way of looking at weight distributions!)\n\nggplot() + \n  geom_histogram(data = filter(nets_with_weights, net_num == 1), \n                 bins = 50, aes(x = propensity), \n                 fill = isfahan[2]) + \n  geom_histogram(data = filter(nets_with_weights, net_num == 0), \n                 bins = 50, aes(x = propensity, y = -after_stat(count)),\n                 fill = isfahan[6]) +\n  geom_hline(yintercept = 0) +\n  annotate(geom = \"label\", x = 0.1, y = 20, label = \"Treated\", \n           fill = isfahan[2], color = \"white\", hjust = 0) +\n  annotate(geom = \"label\", x = 0.1, y = -20, label = \"Untreated\", \n           fill = isfahan[6], color = \"white\", hjust = 0) +\n  scale_y_continuous(label = abs) +\n  coord_cartesian(xlim = c(0.1, 0.8), ylim = c(-80, 100)) +\n  labs(x = \"Propensity\", y = \"Count\")\n\n\n\n\n\n\n\nWe can learn a few different things from this plot. Fewer people received the treatment than didn’t—there are more people in the untreated part of the graph. We can confirm this really quick:\n\nnets %&gt;% count(net)\n## # A tibble: 2 × 2\n##   net       n\n##   &lt;lgl&gt; &lt;int&gt;\n## 1 FALSE  1071\n## 2 TRUE    681\n\nYep. There are ≈400 more net-non-users than net-users.\nWe can also see that those who did not receive treatment tend to have a lower probability of doing so—the bulk of the untreated distribution is clustered in the low end of propensity scores. This makes sense! If people have a low chance of using a mosquito net, there should be fewer people ultimately using a bed net.\nBut these two groups—treated and untreated—aren’t exactly comparable at this point. There are confounding factors that make people who didn’t use nets less likely to use them. To make causal inferences about the effect of nets, we’d need to look at a “treatment” and a “control” group with similar characteristics and with similar probabilities of using nets.\nTo get around this, we can create two pseudo-populations of treated and untreated people. We can give less statistical importance (or weight) to those who had a low probability of being treated and who subsequently weren’t treated (since there are a ton of those people) and more statistical weight to those who had a high probability of being treated but weren’t. Similarly, we can give more weight to treated people who had a low probability of being treated (that’s surprising!) and less weight to treated people who had a high probability of being treated (that’s not surprising!). If we scale each person by their weight, given the confounders of income, temperature, and health, we can create comparable treated and untreated populations:\n\nggplot() + \n  geom_histogram(data = filter(nets_with_weights, net_num == 1), \n                 bins = 50, aes(x = propensity, weight = iptw), \n                 fill = colorspace::lighten(isfahan[2], 0.35)) + \n  geom_histogram(data = filter(nets_with_weights, net_num == 0), \n                 bins = 50, aes(x = propensity, weight = iptw, y = -after_stat(count)),\n                 fill = colorspace::lighten(isfahan[6], 0.35)) +\n  geom_histogram(data = filter(nets_with_weights, net_num == 1), \n                 bins = 50, aes(x = propensity), \n                 fill = isfahan[2]) + \n  geom_histogram(data = filter(nets_with_weights, net_num == 0), \n                 bins = 50, aes(x = propensity, y = -after_stat(count)),\n                 fill = isfahan[6]) +\n  annotate(geom = \"label\", x = 0.8, y = 70, label = \"Treated (actual)\", \n           fill = isfahan[2], color = \"white\", hjust = 1) +\n  annotate(geom = \"label\", x = 0.8, y = 90, label = \"Treated (IPTW pseudo-population)\", \n           fill = colorspace::lighten(isfahan[2], 0.35), color = \"white\", hjust = 1) +\n  annotate(geom = \"label\", x = 0.8, y = -60, label = \"Untreated (actual)\", \n           fill = isfahan[6], color = \"white\", hjust = 1) +\n  annotate(geom = \"label\", x = 0.8, y = -80, label = \"Untreated (IPTW pseudo-population)\", \n           fill = colorspace::lighten(isfahan[6], 0.35), color = \"white\", hjust = 1) +\n  geom_hline(yintercept = 0, color = \"white\", linewidth = 0.25) +\n  scale_y_continuous(label = abs) +\n  coord_cartesian(xlim = c(0.1, 0.8), ylim = c(-80, 100)) +\n  labs(x = \"Propensity\", y = \"Count\")\n\n\n\n\n\n\n\nThis plot shows the original distributions for treated and untreated propensities in darker colors and overlays the weighted/adjusted propensities in lighter colors. These pseudo-populations mirror each other pretty well now! These two rescaled and reweighted groups are now much more equally sized and comparable: the low-propensity net users have a much higher weight, while high-propensity non-net users also get more weight."
  },
  {
    "objectID": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html#bayesian-inverse-probability-weighting",
    "href": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html#bayesian-inverse-probability-weighting",
    "title": "How to use Bayesian propensity scores and inverse probability weights",
    "section": "Bayesian inverse probability weighting",
    "text": "Bayesian inverse probability weighting\nWhy even do this Bayesianly?\nWe successfully found the causal effect of -10 malaria risk points using regular frequentist regression, so why am I trying to make life more complex and do this with Bayesian methods instead? Mostly because I’m not a fan of null hypothesis signficance testing (NHST), or the whole process of proposing a null hypothesis, generating a statistical test, finding a p-value, and seeing if the test/p-value/confidence interval provides enough evidence to reject the null hypothesis. With frequentist statistics we test for the probability of the data given a null hypothesis, or \\(P(\\text{data} \\mid H_0)\\), while with Bayesian statistics, we get to test for the probability of a hypothesis given the data, or \\(P(H \\mid \\text{data})\\).\nTesting a hypothesis directly with Bayesian inference is a lot more intuitive, with Bayesian credible intervals and inferential approaches like measuring the probability that a parameter is greater/less than 0 (i.e. probability of direction), or measuring the proportion of a posterior that falls within a null region of practical equivalence, or ROPE. See this page for an overview of all these methods and a comparison with frequentism, and check out this amazing (and free!) textbook on Bayesianism in general.\nPlus, the amazing brms package lets us make all sorts of powerful and complex models with multilevel nested intercepts and slopes and fancy families like beta, zero-inflated beta, hurdled lognormal, and so on.\nFundamental problem with Bayesian propensity scores\nHowever, there are serious mathematical and philosophical issues with using propensity scores (and IPTWs) in Bayesian models. Put simply, IPTWs aren’t actually part of the model! They’re a neat way to scale and shift the population into comparable pseudo-populations, but they’re not part of the data-generating process for any element of a Bayesian model.\nHere’s how Robins, Hernán, and Wasserman (2015), Zigler (2016), and Liao and Zigler (2020) explain it (but substantially simplified to the point of being a little bit wrong, but acceptably wrong). We can more formally define the average treatment effect (ATE) of mosquito nets on malaria risk using this estimand:\n\\[\n\\Delta_{\\text{ATE}} = E[ \\overbrace{E \\left( Y_i \\mid T_i = 1, X_i \\right)}^{\\substack{\\text{Average outcome } Y \\text{ when} \\\\ \\text{treated, given confounders }X}} - \\overbrace{E \\left( Y_i \\mid T_i = 0, X_i \\right)}^{\\substack{\\text{Average outcome } Y \\text{ when} \\\\ \\text{not treated, given confounders }X}} ]\n\\]\nThere are three moving parts in the equation for the ATE here: the treatment effect is a function of the outcome \\(Y\\) (malaria risk), the treatment \\(T\\) (nets), and all other covariates \\(X\\) (health, income, temperatures). In order to calculate this ATE, we need to create some sort of function that incorporates all three, or something like this that calculates \\(\\Delta\\) given \\(T\\), \\(X\\), and \\(Y\\):\n\\[\nf(\\Delta \\mid \\boldsymbol{T}, \\boldsymbol{X}, \\boldsymbol{Y})\n\\]\nThis works well with Bayesian methods. We don’t know what \\(\\Delta\\) is, so we can use Bayes’ theorem to estimate it given our existing data for \\(T\\), \\(X\\), and \\(Y\\). To quote from Liao and Zigler (2020),\n\n[T]raditional Bayeisan inference for \\(\\Delta\\) would follow from specification of a likelihood for \\((\\boldsymbol{T}, \\boldsymbol{X}, \\boldsymbol{Y})\\) conditional on unknown parameters, \\(\\theta\\), a prior distribution for \\(\\theta\\), and some function relating the data and \\(\\theta\\) to the quantity \\(\\Delta\\).\n\nNote that the equation below is slightly wrong—ordinarily in Bayesian modeling we’re interested in estimating an unknown \\(\\theta\\) parameter, so if we wanted to be super official we’d need to create “some function relating the data and \\(\\theta\\) to the quantity \\(\\Delta\\)”, but for the sake of simplicity and intuition we’ll skip that part and pretend that \\(\\Delta\\) is the output of the model:\n\\[\n\\overbrace{P[\\Delta \\mid (\\boldsymbol{T}, \\boldsymbol{X}, \\boldsymbol{Y})]}^{\\substack{\\text{Posterior estimate} \\\\ \\text{of } \\Delta \\text{, given data}}} \\propto \\overbrace{P[(\\boldsymbol{T}, \\boldsymbol{X}, \\boldsymbol{Y}) \\mid \\Delta]}^{\\substack{\\text{Likelihood for existing} \\\\ \\text{data, given unknown }\\Delta}} \\times \\overbrace{P[\\Delta]}^{\\substack{\\text{Prior} \\\\ \\text{for }\\Delta}}\n\\]\nBut guess what’s missing entirely from this equation?! Our weights! Propensity scores and inverse probability weights have no place in this kind of Bayesian estimation. They would theoretically show up in the likelihood part of the Bayesian equation, but in practice, weights aren’t part of the data-generating process and thus aren’t actually part of the likelihood. This is the key problem with Bayesian weights: these weights are not part of the model for calculating the ATE (\\(f(\\Delta \\mid \\boldsymbol{T}, \\boldsymbol{X}, \\boldsymbol{Y})\\)). We can’t set a prior on the weight parameter—there isn’t even a weight parameter to work with in the likelihood. In order to be a true Bayesian, we can’t use propensity scores and weighting. This is the argument that Robins, Hernán, and Wasserman (2015) make: because propensity scores (and thus weights) aren’t part of the likelihood, we can’t do anything Bayesian with them.\nAnd that’s disappointing because Bayesian inference is great! I find it far more intuitive (and fun) to make inferences with posterior distributions rather than work with null hypothesis significance testing.\nA legal way to use weights Bayesianly\nFortunately, Robins, Hernán, and Wasserman (2015) conclude by saying that there are possible compromises that we can use to work with weights in a Bayesian framework, and Liao and Zigler (2020) explore one of these compromises and propose a method of incorporating propensity scores into Bayesian estimation of causal effects. Their general approach is to think of the propensity score calculation as a new parameter \\(\\nu\\) (nu). Instead of calculating a single value of \\(\\nu\\) (i.e. a single set of propensity scores or weights) like we did with frequentist estimation, we incorporate a range of reasonable values of \\(\\nu\\) from the posterior distribution of the treatment/design model into the outcome model, which essentially lets us get rid of the \\(\\nu\\) term. Mathematically the approach looks like this:\n\\[\n\\overbrace{f(\\Delta \\mid \\boldsymbol{T}, \\boldsymbol{X}, \\boldsymbol{Y})}^{\\substack{\\text{Estimand for} \\\\ \\text{the ATE, without } \\nu}} = \\int_\\nu \\overbrace{f(\\Delta \\mid \\boldsymbol{T}, \\boldsymbol{X}, \\boldsymbol{Y}, \\nu)}^{\\substack{\\text{Outcome model} \\\\ \\text{with } \\nu}}\\ \\overbrace{f(\\nu \\mid \\boldsymbol{T}, \\boldsymbol{X})}^{\\substack{\\text{Treatment model} \\\\ \\text{creating propensity} \\\\ \\text{scores with } T \\text{ and } X}}\\ \\mathrm{d} \\nu\n\\]\nThe \\(f(\\nu \\mid \\boldsymbol{T}, \\boldsymbol{X})\\) part of the equation is the treatment/design model and it doesn’t have any information about the outcome \\(Y\\) in it. In our running example, this is net ~ confounders. The \\(f(\\Delta \\mid \\boldsymbol{T}, \\boldsymbol{X}, \\boldsymbol{Y}, \\nu)\\) part of the equation is the outcome model and it incorporates \\(\\nu\\). This isn’t the final estimand, which doesn’t have the propensity scores (\\(\\nu\\)) in it. We ultimately get rid of that \\(\\nu\\) term by marginalizing over the distribution for \\(\\Delta\\).\nTo simplify this process more, here’s the basic process for doing this:\n\nUse a Bayesian model to estimate the likelihood of treatment and generate propensity scores (\\(\\nu\\)). This is the treatment model (or design model) and is analogous to the logistic regression model model_treatment_freq that we ran earlier.\nGenerate \\(K\\) samples of propensity scores based on the posterior distribution of propensity scores. This can be whatever number you want—often it’s the number of posterior chains from the Bayesian model (like 2,000 or however many iterations you use).\nFor each of the \\(K\\) samples, generate inverse probability weights and run an outcome model using those weights. This essentially means that we’ll be running the outcome model malaria_risk ~ net a bunch of times based on different weights each time.\nCombine the results from the outcome model to create the final \\(\\nu\\)-free ATE.\n\nThis process is similar to multiple imputation or bootstrapping: run the same model a bunch of times on slightly different data and combine the results.\nHere’s what this looks like in practice. First we’ll predict net usage based on the confounders of income, temperature, and health using brms (with default priors and settings):\n\nmodel_treatment &lt;- brm(\n  bf(net ~ income + temperature + health,\n     decomp = \"QR\"),  # QR decomposition handles scaling and unscaling for us\n  family = bernoulli(),  # Logistic regression\n  data = nets,\n  chains = 4, cores = 4, iter = 1000,\n  seed = 1234, backend = \"cmdstanr\"\n)\n## Start sampling\n\nWe’ll then use posterior_epred() to plug the original data into each of the posterior draws to calculate propensity scores for each draw. This will give us a lot of propensity scores: 2,000 probabilities for each of the 1,752 people in the data.\n\n# Extract posterior predicted propensity scores\npred_probs_chains &lt;- posterior_epred(model_treatment)\ndim(pred_probs_chains)\n## [1] 2000 1752\n\n# Rows are chains; columns are individuals\nhead(pred_probs_chains, c(5, 10))\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n## [1,] 0.382 0.383 0.157 0.282 0.342 0.436 0.470 0.406 0.399 0.378\n## [2,] 0.357 0.395 0.140 0.242 0.286 0.437 0.497 0.452 0.421 0.344\n## [3,] 0.342 0.369 0.117 0.203 0.236 0.394 0.470 0.443 0.396 0.303\n## [4,] 0.339 0.352 0.124 0.206 0.237 0.372 0.439 0.415 0.373 0.298\n## [5,] 0.338 0.359 0.120 0.210 0.246 0.388 0.454 0.420 0.382 0.306\n\nEach column here is a person in the dataset; each row is a draw from the posterior distribution. Note how there’s a lot of uncertainty in these propensity scores—we want to incorporate this uncertainty into our outcome model somehow. But as we’ve seen, there’s no weight parameter in our model, so there’s no way to directly add this uncertainty to the outcome model. Instead, we’ll run the outcome model a bunch of times, or \\(K\\) times. For now we’ll set \\(K\\) to 2,000—we’ll create an outcome model for each of the posterior draws that we have.\nDoing this will require a little bit of purrr magic. We’ll make a dataset with 2,000 (\\(K\\)) rows and put the propensity scores for all 1,752 people into their own cell to make it easier to keep track of these scores.\n\n# Put each set of individual propensity scores into its own cell\npred_probs_nested &lt;- pred_probs_chains %&gt;% \n  # Convert this matrix to a data frame\n  as_tibble(.name_repair = \"unique\") %&gt;% \n  # Add a column for the draw number\n  mutate(draw = 1:n()) %&gt;% \n  # Make this long so that each draw gets its own row\n  pivot_longer(-draw, names_to = \"row\", values_to = \"prob\") %&gt;% \n  # Clean up the draw number \n  mutate(row = as.numeric(str_remove(row, \"...\"))) %&gt;% \n  # Group by draw and nest all the scores in a cell\n  group_by(draw) %&gt;% \n  nest() %&gt;% \n  ungroup()\npred_probs_nested\n## # A tibble: 2,000 × 2\n##     draw data                \n##    &lt;int&gt; &lt;list&gt;              \n##  1     1 &lt;tibble [1,752 × 2]&gt;\n##  2     2 &lt;tibble [1,752 × 2]&gt;\n##  3     3 &lt;tibble [1,752 × 2]&gt;\n##  4     4 &lt;tibble [1,752 × 2]&gt;\n##  5     5 &lt;tibble [1,752 × 2]&gt;\n##  6     6 &lt;tibble [1,752 × 2]&gt;\n##  7     7 &lt;tibble [1,752 × 2]&gt;\n##  8     8 &lt;tibble [1,752 × 2]&gt;\n##  9     9 &lt;tibble [1,752 × 2]&gt;\n## 10    10 &lt;tibble [1,752 × 2]&gt;\n## # ℹ 1,990 more rows\n\nWe’ll then run an outcome model using each of these nested propensity scores. We’ll take the scores, calculate weights, and run a basic frequentist lm() model with those weights. We’ll then use tidy() from broom to extract different parts of the results.\n\noutcome_models &lt;- pred_probs_nested %&gt;% \n  mutate(outcome_model = map(data, ~{\n    # Add this version of propensity scores to the original data and calculate\n    # weights. We could also do this prior to nesting everything.\n    df &lt;- bind_cols(nets, .) %&gt;% \n      mutate(iptw = (net_num / prob) + ((1 - net_num) / (1 - prob)))\n    \n    # Create outcome model with this iteration of weights\n    model &lt;- lm(malaria_risk ~ net, data = df, weights = iptw)\n  })) %&gt;% \n  # Extract results\n  mutate(tidied = map(outcome_model, ~tidy(.)),\n         ate = map_dbl(tidied, ~filter(., term == \"netTRUE\") %&gt;% pull(estimate)),\n         ate_se = map_dbl(tidied, ~filter(., term == \"netTRUE\") %&gt;% pull(std.error)))\noutcome_models\n## # A tibble: 2,000 × 6\n##     draw data                 outcome_model tidied              ate ate_se\n##    &lt;int&gt; &lt;list&gt;               &lt;list&gt;        &lt;list&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n##  1     1 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.71  0.662\n##  2     2 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.37  0.665\n##  3     3 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.15  0.670\n##  4     4 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.89  0.665\n##  5     5 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.40  0.669\n##  6     6 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt; -10.8   0.654\n##  7     7 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.03  0.669\n##  8     8 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -8.58  0.673\n##  9     9 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt;  -9.67  0.664\n## 10    10 &lt;tibble [1,752 × 2]&gt; &lt;lm&gt;          &lt;tibble [2 × 5]&gt; -11.0   0.651\n## # … with 1,990 more rows\n\nThe ate column here is the coefficient for net, and we have 2,000 of them to work with now, each based on a different set of weights. If we average all these estimates, we can get one final estimate of the ATE that successfully incorporates the uncertainty from the treatment/design model. We can’t take the direct average of the standard errors, but we can combine them using Rubin’s rules. Rubin (1987) outlines an set of rules for combining the results from multiply imputed datasets that reflects the averages and accounts for differences in standard errors (they’re essentially a fancier, more robust way of averaging standard errors across multiple models). We didn’t use multiply imputed datasets here, but the same principle applies—we used the same model on lots of slightly different datasets.\n\n# Combined average treatment effect\nmean(outcome_models$ate)\n## [1] -10.1\n\n# Combined standard errors (this is wrong)\nmean(outcome_models$ate_se)\n## [1] 0.659\n\n# Combined standard errors with Rubin's rules (this is correct)\nrubin_se &lt;- function(ates, sigmas) {\n  sqrt(mean(sigmas^2) + var(ates))\n}\n\nrubin_se(outcome_models$ate, outcome_models$ate_se)\n## [1] 1.02\n\nThe overall average treatment effect is still −10ish, like we found earlier, but now we have a bunch of extra uncertainty from the treatment model, which is neat. We can also visualize the distribution of the ATE, almost like a Bayesian posterior:\n\nggplot(outcome_models, aes(x = ate)) +\n  geom_density(fill = isfahan[4], color = NA) +\n  labs(x = \"Average treatment effect of using a mosquito net\", y = NULL)\n\n\n\n\n\n\n\nWe did it!"
  },
  {
    "objectID": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html#things-i-dont-know-yet-but-want-to-know",
    "href": "blog/2021/12/18/bayesian-propensity-scores-weights/index.html#things-i-dont-know-yet-but-want-to-know",
    "title": "How to use Bayesian propensity scores and inverse probability weights",
    "section": "Things I don’t know yet but want to know!",
    "text": "Things I don’t know yet but want to know!\nSo, thanks to Liao and Zigler (2020), we have a fully Bayesian treatment/design model and we can incorporate the uncertainty from that model’s propensity scores and weights into the outcome/analysis model. That’s so neat!\nHowever, in their paper Liao and Zigler use a frequentist outcome model, like we just did here. While combining the ATEs from 2,000 different frequentist OLS-based models feels quasi-Bayesian, I don’t know if we can legally talk about these results Bayesianly. Can we pretend that this distribution of ATEs is similar to a posterior distribution and use Bayesian inference rather than null hypothesis significance testing, or do we still need to talk about null hypotheses? These 2,000 models are essentially a mathematical transformation of the posterior, so maybe it’s legal? But the Bayesian model is for predicting treatment, not the outcome, so maybe it’s not legal? idk. (Turns out the answer is no—there’s no uncertainty in the outcome model here, since all the uncertainty comes from the treatment model. Ah! Well. Nevertheless.)\nWe could technically run a Bayesian outcome model with brm(), but we’d have to run it 2,000 times—one model per set of weights—and that would take literally forever and might melt my computer. There could be a way to only run a single outcome model once and use one set of weights for each of the iterations (i.e. use the first column of propensity scores for the first iteration of the outcome model, the second for the second, and so on), but that goes beyond my skills with brms. (UPDATE: Thanks to Jordan Nafa, this is actually possible! See here!)"
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "",
    "text": "At the end of my previous post on beta and zero-inflated-beta regression, I included an example of a multilevel model that predicted the proportion of women members of parliament based on whether a country implements gender-based quotas for their legislatures, along with a few different control variables. I also included random effects for year and region in order to capture time- and geography-specific trends. When interpreting the results, I naively said that calculating posterior predictions from the model (i.e. plugging arbitrary values into the model and finding the predicted value of the outcome) magically and automatically includes information about year and region (or the random effects included in the model). But I was wrong-ish! Incorporating information about the random effects of a multilevel model is actually a lot more involved and detailed than I thought, and the nuances are really important and make a big difference!\nAs TJ Mahr recently tweeted, this is hard stuff!\nFortunately, over the past couple days I’ve been part of several really long Twitter threads (with people who know waaaay more about this than me!) about different ways of incorporating group-level effects into posterior predictions. HUGE THANKS to Isabella Ghement, Brenton Wiernik, TJ Mahr, Donald Williams, Solomon Kurz, and Mattan Ben-Shachar for all their help and tweeting! (This set of tweets by Brenton was pivotal!)"
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html#who-this-guide-is-for",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html#who-this-guide-is-for",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "Who this guide is for",
    "text": "Who this guide is for\nTo borrow from Solomon Kurz, here’s what this guide assumes you know:\n\nYou’re familiar with R and the tidyverse (particularly dplyr and ggplot2).\nYou’re familiar with brms for running Bayesian regression models. See the vignettes here, examples like this, or resources like these for an introduction.\n\nYou’re somewhat familiar with multilevel models. Confusingly, these are also called mixed effect models, random effect models, and hierarchical models, among others! They’re all the same thing! (image below by Chelsea Parlett-Pelleriti)\n\n\n\n\n\n\n\n\nSee examples like this or this or this or this. Basically Google “lme4 example” (lme4 is what you use for frequentist, non-Bayesian multilevel models with R) or “brms multilevel example” and you’ll find a bunch. For a more formal treatment, see chapter 12 in Richard McElreath’s Statistcal Rethinking book (or this R translation of it by Solomon Kurz)."
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html#example-data",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html#example-data",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "Example data",
    "text": "Example data\nPut simply, multilevel models let you account for nested structures in data:\n\nStudents inside schools inside school districts inside states\nStudy participants inside cities inside states across months\nCountries within regions and across years\n\nSince most of my own work deals with international politics, we’ll use an example with countries and years. We’ll use data from the Varieties of Democracy project (V-Dem) to explore the relationship between press freedom and the degree to which political opposition is allowed. Is there more press freedom in countries that allow opposition parties?\nThere’s a great R package for accessing V-Dem data without needing to download it manually from their website, so we’ll use that to make a smaller dataset of countries for just 2015. Let’s load all the libraries we need, clean up the data, and get started!\n\nlibrary(tidyverse)    # ggplot, dplyr, %&gt;%, and friends\nlibrary(brms)         # Bayesian modeling through Stan\nlibrary(tidybayes)    # Manipulate Stan objects in a tidy way\nlibrary(broom)        # Convert model objects to data frames\nlibrary(broom.mixed)  # Convert brms model objects to data frames\nlibrary(emmeans)      # Calculate marginal effects in even fancier ways\nlibrary(vdemdata)     # Use data from the Varieties of Democracy (V-Dem) project\nlibrary(patchwork)    # Combine ggplot objects\nlibrary(ggokabeito)   # Neat accessible color palette\nlibrary(gghalves)     # Special half geoms\nlibrary(ggbeeswarm)   # Special distribution-shaped point jittering\n\n# Custom ggplot theme to make pretty plots\n# Get the News Cycle font at https://fonts.google.com/specimen/News+Cycle\ntheme_clean &lt;- function() {\n  theme_minimal(base_family = \"News Cycle\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\", size = rel(1), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}\n\nV-Dem includes hundreds of different variables, but we only need a few, and we’ll make a few adjustments to the ones we do need. Here’s what we’ll do:\n\n\nMain outcome: Alternative sources of information index (v2xme_altinf in V-Dem). This is a 0–1 scale that measures…\n\nTo what extent the [is] media (a) un-biased in their coverage or lack of coverage of the opposition, (b) allowed to be critical of the regime, and (c) representative of a wide array of political perspectives\n\nHigher values represent more media freedom.\n\n\nMain binary explanatory variable: Opposition parties autonomy (v2psoppaut in V-Dem). This is an ordinal variable with these possible values:\n\n\n0: Opposition parties are not allowed.\n1: There are no autonomous, independent opposition parties. Opposition parties are either selected or co-opted by the ruling regime.\n2: At least some opposition parties are autonomous and independent of the ruling regime.\n3: Most significant opposition parties are autonomous and independent of the ruling regime.\n4: All opposition parties are autonomous and independent of the ruling regime.\n\n\nFor the sake of simplicity, we’ll collapse this into a binary variable. Parties are autonomous if they score a 3 or a 4; they’re not autonomous if they score a 0, 1, or 2. Also, there are a handful of countries (Saudi Arabia, Qatar, etc.) with missing data for this column, since they don’t have any opposition parties. We’ll treat them as not-autonomous here.\n\nMain continuous explanatory variable: Civil liberties index (v2x_civlib). This is a continuous variable measured from 0–1 with higher values representing better respect for human rights and civil liberties.\n\nGrouping: Finally, we’ll use region as groups in these models (i.e. countries nested in regions). V-Dem provides multiple regional variables with varying specificity (19 different regions, 10 different regions, and 6 different regions). We’ll use the 6-region version (e_regionpol_6C) for simplicity here:\n\n\n1: Eastern Europe and Central Asia (including Mongolia)\n2: Latin America and the Caribbean\n3: The Middle East and North Africa (including Israel and Turkey, excluding Cyprus)\n4: Sub-Saharan Africa\n5: Western Europe and North America (including Cyprus, Australia and New Zealand)\n6: Asia and Pacific (excluding Australia and New Zealand)\n\n\n\n\n\n# Make a subset of the full V-Dem data\nvdem_2015 &lt;- vdem %&gt;% \n  select(country_name, country_text_id, year, region = e_regionpol_6C,\n         media_index = v2xme_altinf, party_autonomy_ord = v2psoppaut_ord,\n         polyarchy = v2x_polyarchy, civil_liberties = v2x_civlib) %&gt;% \n  filter(year == 2015) %&gt;% \n  mutate(party_autonomy = party_autonomy_ord &gt;= 3,\n         party_autonomy = ifelse(is.na(party_autonomy), FALSE, party_autonomy)) %&gt;% \n  mutate(region = factor(region, \n                         labels = c(\"Eastern Europe and Central Asia\",\n                                    \"Latin America and the Caribbean\",\n                                    \"Middle East and North Africa\",\n                                    \"Sub-Saharan Africa\",\n                                    \"Western Europe and North America\",\n                                    \"Asia and Pacific\")))"
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html#explore-the-data",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html#explore-the-data",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "Explore the data",
    "text": "Explore the data\nWe’re interested in a couple relationships here. First, we want to know if there is more press freedom in countries that allow opposition parties in elections. Let’s look at the data really quick:\n\nautonomy_halves &lt;- ggplot(vdem_2015, aes(x = party_autonomy, y = media_index)) +\n  geom_half_point(aes(color = party_autonomy), \n                  transformation = position_quasirandom(width = 0.1),\n                  side = \"l\", size = 0.5, alpha = 0.5) +\n  geom_half_boxplot(aes(fill = party_autonomy), side = \"r\") + \n  scale_fill_okabe_ito() +\n  scale_color_okabe_ito() +\n  guides(color = \"none\", fill = \"none\") +\n  labs(x = \"Opposition party autonomy\", y = \"Media freedom index\") +\n  theme_clean()\n\nautonomy_densities &lt;- ggplot(vdem_2015, aes(x = media_index, fill = party_autonomy)) +\n  geom_density(alpha = 0.6) +\n  scale_fill_okabe_ito() +\n  labs(x = \"Media freedom index\", y = \"Density\", fill = \"Opposition party autonomy\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\nautonomy_halves | autonomy_densities\n\n\n\n\n\n\n\nThat confirms our initial suspicions: countries that do not allow opposition parties to participate in elections tend to have substantially lower media freedom index scores than countries that have opposition parties.\nLet’s see how respect for civil liberties is related to media freedom:\n\nggplot(vdem_2015, aes(x = civil_liberties, y = media_index)) +\n  geom_point(aes(color = region)) +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Civil liberties index\", y = \"Media freedom index\",\n       color = \"Region\") +\n  scale_color_okabe_ito() +\n  theme_clean()\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCountries with greater respect for civil liberties and human rights tend to have higher values of media freedom. There are perhaps some regional trends here too—most of the countries in Western Europe and North America are clustered in the top right corner of the plot, but all other regions are spread throughout."
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html#model",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html#model",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "Model",
    "text": "Model\nLet’s make a model! We’ll look just at 2015 data to estimate the effect of party autonomy (a binary variable) and civil liberties (a continuous variable) on a country’s media freedom index. We’ll include random effects for region, assuming that there are regional differences in this relationship (i.e. the relationship between party autonomy and media freedom looks different in Asia from Western Europe).\nBecause media_index ranges from 0 to 1 but does not include 0 or 1, we’ll use a beta distribution. It’s a really neat and fun distribution to work with, and it has extra moving parts we can play with—we can model the \\(\\phi\\) parameter (or the precision/variance of the distribution) with random effects too.\nFormally, we can define the full model like this:\n\\[\n\\begin{aligned}\n&\\text{[likelihood]} \\\\\n\\text{Media freedom index}_i &\\sim \\operatorname{Beta}(\\mu_i, \\phi_i) \\\\\n\\ \\\\\n&\\text{[} \\mu \\text{ part of beta distribution]} \\\\\n\\operatorname{logit}(\\mu_i) &= \\alpha_{j[i]} + \\beta_1 \\text{Party autonomy}_i + \\beta_2 \\text{Civil liberties}_i\\\\\n\\ \\\\\n&\\text{[} \\phi \\text{ part of beta distribution]} \\\\\n\\log(\\phi_i) &= \\alpha_{j[i]}\\\\\n\\ \\\\\n& \\text{[region-specific intercepts]} \\\\\n\\alpha_{j} &\\sim \\mathcal{N}(\\mu_{\\alpha_j}, \\sigma^2_{\\alpha_j}), \\text{ for region } j \\text{ in } 1 .. J\n\\end{aligned}\n\\]\nWe’ll model the \\(\\mu\\) (or mean) parameter of the beta distribution with party_autonomy, civil_liberties, and random intercepts for region, and we’ll model the \\(\\phi\\) part (or precision) with just random intercepts for region (though we could use whatever, too—see this for an explanation of what this parameter does).\nThe syntax for defining random effects can get complex, depending on if the effects are nested or crossed, and if slopes get involved. This table by Ben Bolker is indispensable for remembering how to define which kinds of effects. Here we’re just adding a random intercept for region, so we’ll use a (1 | region) term in the model code.\n\nmodel_basic &lt;- brm(\n  bf(media_index ~ party_autonomy + civil_liberties + (1 | region),\n     phi ~ (1 | region)),\n  data = vdem_2015,\n  family = Beta(),\n  control = list(adapt_delta = 0.9),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 12345,\n  # Use the cmdstanr backend for Stan because it's faster and more modern than\n  # the default rstan. You need to install the cmdstanr package first\n  # (https://mc-stan.org/cmdstanr/) and then run cmdstanr::install_cmdstan() to\n  # install cmdstan on your computer.\n  backend = \"cmdstanr\"\n)\n## Start sampling\n## Warning: 1 of 4000 (0.0%) transitions ended with a divergence.\n## See https://mc-stan.org/misc/warnings for details."
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html#posterior-predictions",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html#posterior-predictions",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "Posterior predictions",
    "text": "Posterior predictions\nBecause this model uses beta regression, the coefficients for the \\(\\mu\\) part are on the logit (log odds) scale, while the coefficients for the \\(\\phi\\) part are on the log scale. This makes them really hard to interpret when just looking at a table of coefficient estimates, like this one here:\n\ntidy(model_basic)\n## # A tibble: 6 × 8\n##   effect   component group  term                estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;   (Intercept)           -2.46      0.302   -3.04      -1.87\n## 2 fixed    cond      &lt;NA&gt;   phi_(Intercept)        2.35      0.474    1.35       3.22\n## 3 fixed    cond      &lt;NA&gt;   party_autonomyTRUE     1.03      0.172    0.698      1.38\n## 4 fixed    cond      &lt;NA&gt;   civil_liberties        3.51      0.316    2.90       4.12\n## 5 ran_pars cond      region sd__(Intercept)        0.499     0.256    0.211      1.19\n## 6 ran_pars cond      region sd__phi_(Intercept)    0.925     0.451    0.365      2.06\n\nEven if we back-transform the estimates to the response scale (with plogis() for the logits and exp() for the logs), we still cannot interpret the coefficients directly. Unlike regular OLS-like regression models, we can’t say things like “a one-unit increase in party_autonomy is associated with a plogis(2.27) unit increase in media_freedom”—we have to incorporate the intercept and all the other model parameters in order to make marginal interpretations like that.\nIn this case, we are interested in these marginal effects, though. What is the effect of flipping party_autonomy from FALSE to TRUE? What is the effect of a one-unit increase in the respect for human rights and civil liberties measured by civil_liberties? You can’t easily figure it out with the raw results of the model because there are so many moving parts: the different pieces (intercept, other coefficients) for the \\(\\mu\\) part need to be combined, the precision parameter \\(\\phi\\) needs to be incorporated somehow, and random regional effects for both the \\(\\mu\\) and the \\(\\phi\\) parts need to be accounted for. That’s a mess.\nInstead of trying to algebraically piece all this together, we can plug hypothetical data into the model’s posterior distribution and generate posterior predictions of our media freedom outcome.\nDraws from the posterior predictive distribution\nThere are a few general approaches to generating posterior predictions with brms. First, we can use posterior_predict() to plug a new dataset into the model. As an example, we’ll create a little dataset where party_autonomy is both TRUE and FALSE, and we’ll arbitrarily set civil_liberties to 0.5 (right in the middle of its range) and region to the Middle East and North Africa. We’ll then plug that in to the model and generate predictions\n\nnewdata &lt;- expand_grid(party_autonomy = c(TRUE, FALSE),\n                       civil_liberties = c(0.5),\n                       region = \"Middle East and North Africa\")\nnewdata\n## # A tibble: 2 × 3\n##   party_autonomy civil_liberties region                      \n##   &lt;lgl&gt;                    &lt;dbl&gt; &lt;chr&gt;                       \n## 1 TRUE                       0.5 Middle East and North Africa\n## 2 FALSE                      0.5 Middle East and North Africa\n\nposterior_predict(model_basic, newdata) %&gt;% head()\n##       [,1]  [,2]\n## [1,] 0.894 0.401\n## [2,] 0.400 0.751\n## [3,] 0.864 0.476\n## [4,] 0.374 0.332\n## [5,] 0.544 0.495\n## [6,] 0.564 0.596\n\nThis returns a matrix with 4,000 rows and 2 columns—a column for when party_autonomy is TRUE and one for when it is FALSE. This format isn’t in the nicest shape for plotting and doing tidy type of stuff with, so we can alternatively use predicted_draws() from the tidybayes package. This is really just a fancy wrapper around brms::posterior_predict() that returns a much nicer tidier data frame:\n\ntidy_pred &lt;- model_basic %&gt;% \n  predicted_draws(newdata = newdata)\ntidy_pred\n## # A tibble: 8,000 × 8\n## # Groups:   party_autonomy, civil_liberties, region, .row [2]\n##    party_autonomy civil_liberties region          .row .chain .iteration .draw .prediction\n##    &lt;lgl&gt;                    &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n##  1 TRUE                       0.5 Middle East a…     1     NA         NA     1       0.669\n##  2 TRUE                       0.5 Middle East a…     1     NA         NA     2       0.811\n##  3 TRUE                       0.5 Middle East a…     1     NA         NA     3       0.378\n##  4 TRUE                       0.5 Middle East a…     1     NA         NA     4       0.329\n##  5 TRUE                       0.5 Middle East a…     1     NA         NA     5       0.559\n##  6 TRUE                       0.5 Middle East a…     1     NA         NA     6       0.518\n##  7 TRUE                       0.5 Middle East a…     1     NA         NA     7       0.927\n##  8 TRUE                       0.5 Middle East a…     1     NA         NA     8       0.823\n##  9 TRUE                       0.5 Middle East a…     1     NA         NA     9       0.475\n## 10 TRUE                       0.5 Middle East a…     1     NA         NA    10       0.802\n## # ℹ 7,990 more rows\n\nThat’s much nicer to work with!\nExpected values of the posterior predictive distribution\nWe don’t necessarily want to use these predicted draws when thinking about marginal effects of coefficients, though. When we use posterior_predict()/predicted_draws() here, brms accounts for the uncertainty of all these things:\n\nThe uncertainty of the fixed coefficients (e.g., party_autonomy and civil_liberties)\nThe uncertainty of the variance parameters of the groups (e.g., sd__phi_(Intercept) for each region)\nThe uncertainty for each individual observation (e.g., observational-level residual variance)\n\nWhen thinking about marginal effects, though, we’re more interested in the expected value of the outcome, which means we’re more focused on the uncertainty in the model parameters and not necessarily the individual-level residuals (See this post or this discussion for an explanation of the difference between the two types of predictions). To get these expected values, we can use brms::posterior_epred() (or the nicer tidybayes::epred_draws()), which accounts for the uncertainty of just these two things:\n\nThe uncertainty of the fixed coefficients (e.g., party_autonomy and civil_liberties)\nThe uncertainty of the variance parameters of the groups (e.g., sd__phi_(Intercept) for each region)\n\nThe averages of these draws that are generated by regular posterior_predict() and posterior_epred() should be generally the same, but the variance for expected values will be smaller (since we’re not dealing with individual observation-level variance).\n\n# Expected values with brms\nposterior_epred(model_basic, newdata) %&gt;% head()\n##       [,1]  [,2]\n## [1,] 0.677 0.413\n## [2,] 0.605 0.387\n## [3,] 0.619 0.395\n## [4,] 0.593 0.304\n## [5,] 0.697 0.389\n## [6,] 0.527 0.353\n\n# Expected values with tidybayes\ntidy_epred &lt;- model_basic %&gt;% \n  epred_draws(newdata = newdata)\ntidy_epred\n## # A tibble: 8,000 × 8\n## # Groups:   party_autonomy, civil_liberties, region, .row [2]\n##    party_autonomy civil_liberties region               .row .chain .iteration .draw .epred\n##    &lt;lgl&gt;                    &lt;dbl&gt; &lt;chr&gt;               &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n##  1 TRUE                       0.5 Middle East and No…     1     NA         NA     1  0.677\n##  2 TRUE                       0.5 Middle East and No…     1     NA         NA     2  0.605\n##  3 TRUE                       0.5 Middle East and No…     1     NA         NA     3  0.619\n##  4 TRUE                       0.5 Middle East and No…     1     NA         NA     4  0.593\n##  5 TRUE                       0.5 Middle East and No…     1     NA         NA     5  0.697\n##  6 TRUE                       0.5 Middle East and No…     1     NA         NA     6  0.527\n##  7 TRUE                       0.5 Middle East and No…     1     NA         NA     7  0.756\n##  8 TRUE                       0.5 Middle East and No…     1     NA         NA     8  0.659\n##  9 TRUE                       0.5 Middle East and No…     1     NA         NA     9  0.524\n## 10 TRUE                       0.5 Middle East and No…     1     NA         NA    10  0.701\n## # ℹ 7,990 more rows\n\nHere’s what the difference in variance actually looks like:\n\nplot_preds &lt;- bind_rows(\n  \"Predicted draws\" = tidy_pred,\n  \"Expectation of predicted draws\" = rename(tidy_epred, .prediction = .epred),\n  .id = \"draw_type\") %&gt;% \n  mutate(draw_type = fct_inorder(draw_type))\n\nplot_preds %&gt;% \n  group_by(draw_type, party_autonomy) %&gt;% \n  median_hdi(.prediction)\n## # A tibble: 4 × 8\n##   draw_type               party_autonomy .prediction .lower .upper .width .point .interval\n##   &lt;fct&gt;                   &lt;lgl&gt;                &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Predicted draws         FALSE                0.355 0.0183  0.725   0.95 median hdi      \n## 2 Predicted draws         TRUE                 0.638 0.242   0.976   0.95 median hdi      \n## 3 Expectation of predict… FALSE                0.369 0.287   0.461   0.95 median hdi      \n## 4 Expectation of predict… TRUE                 0.622 0.532   0.707   0.95 median hdi\n\nggplot(plot_preds, aes(x = .prediction, fill = party_autonomy)) +\n  stat_halfeye() +\n  labs(x = \"Predicted media index\", y = \"Density\", fill = \"Party autonomy\") +\n  facet_wrap(vars(draw_type)) +\n  scale_fill_okabe_ito() +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nThe median values from both types of draws are the same. When there’s no party autonomy, the median predicted media index is around 0.11; when there is party autonomy, the median predicted media index is around 0.6. But the variance is wildly different and is much narrower when looking at the expectation of predicted draws.\nFor all our marginal effects calculations here, we’re going to use expected values, or epred_draws()."
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html#average-marginal-effects",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html#average-marginal-effects",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "Average marginal effects",
    "text": "Average marginal effects\nIn the plot above, we got predicted values of our outcome across different levels of party autonomy when holding civil liberties and region constant, and that’s neat, but what we’re really interested in is the difference between those two distributions, or the marginal effect of flipping party autonomy from false to true. How much of an increase in the predicted media index when there is party autonomy?\nWe already have all the information we need to calculate this with our tidy_epred. If we rearrange the dataset so that it’s wide, with a column for party autonomy being true and a column for when it’s false, we can subtract the two columns and get the difference:\n\ntidy_epred %&gt;% \n  ungroup() %&gt;% \n  select(-.row) %&gt;% \n  pivot_wider(names_from = \"party_autonomy\", values_from = \".epred\") %&gt;% \n  mutate(autonomy_effect = `TRUE` - `FALSE`)\n## # A tibble: 4,000 × 8\n##    civil_liberties region           .chain .iteration .draw `TRUE` `FALSE` autonomy_effect\n##              &lt;dbl&gt; &lt;chr&gt;             &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n##  1             0.5 Middle East and…     NA         NA     1  0.677   0.413           0.265\n##  2             0.5 Middle East and…     NA         NA     2  0.605   0.387           0.218\n##  3             0.5 Middle East and…     NA         NA     3  0.619   0.395           0.224\n##  4             0.5 Middle East and…     NA         NA     4  0.593   0.304           0.290\n##  5             0.5 Middle East and…     NA         NA     5  0.697   0.389           0.308\n##  6             0.5 Middle East and…     NA         NA     6  0.527   0.353           0.174\n##  7             0.5 Middle East and…     NA         NA     7  0.756   0.500           0.256\n##  8             0.5 Middle East and…     NA         NA     8  0.659   0.454           0.205\n##  9             0.5 Middle East and…     NA         NA     9  0.524   0.294           0.230\n## 10             0.5 Middle East and…     NA         NA    10  0.701   0.424           0.276\n## # ℹ 3,990 more rows\n\nWe could then plot that autonomy_effect column and see the average marginal effect of party autonomy. But that’s a lot of data wrangling and pivoting! There’s an easier way! Instead of manually working with the posterior predictions like this, we can use the emmeans package, which was designed specifically for this kind of work.\nWith emmeans(), we don’t have to create our own hypothetical newdata data frame—the function does that for us. We need to specify epred = TRUE to get the expectation of predicted draws, since it’ll give us predicted draws by default.\nIf we run emmeans() by itself, it’ll print a summary table of the predicted values:\n\nmodel_basic %&gt;% \n  emmeans(~ party_autonomy,\n          at = list(civil_liberties = 0.5,\n                    region = \"Middle East and North Africa\"),\n          epred = TRUE)\n##  party_autonomy emmean lower.HPD upper.HPD\n##  FALSE           0.329     0.223     0.445\n##   TRUE           0.578     0.461     0.691\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nWe also don’t need to calculate the autonomy effect by hand—the constrast() function will do that for us too (the revpairwise option here makes it so it’ll calculate TRUE - FALSE instead of FALSE - TRUE):\n\nmodel_basic %&gt;% \n  emmeans(~ party_autonomy,\n          at = list(civil_liberties = 0.5,\n                    region = \"Middle East and North Africa\"),\n          epred = TRUE) %&gt;% \n  contrast(method = \"revpairwise\")\n##  contrast     estimate lower.HPD upper.HPD\n##  TRUE - FALSE    0.246     0.171     0.327\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nIf we store the results of emmeans() as an object, we actually get access to all the posterior draws that it created for the predictions, which we can then extract and rearrange with tidybayes’s gather_emmeans_draws(). (Without the contrast() step, we’d get predicted values for both levels of party_autonomy.)\n\nautonomy_effect_draws &lt;- model_basic %&gt;% \n  emmeans(~ party_autonomy,\n          at = list(civil_liberties = 0.5,\n                    region = \"Middle East and North Africa\"),\n          epred = TRUE) %&gt;% \n  contrast(method = \"revpairwise\") %&gt;% \n  gather_emmeans_draws()\nautonomy_effect_draws\n## # A tibble: 4,000 × 5\n## # Groups:   contrast [1]\n##    contrast     .chain .iteration .draw .value\n##    &lt;chr&gt;         &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n##  1 TRUE - FALSE     NA         NA     1  0.254\n##  2 TRUE - FALSE     NA         NA     2  0.217\n##  3 TRUE - FALSE     NA         NA     3  0.223\n##  4 TRUE - FALSE     NA         NA     4  0.292\n##  5 TRUE - FALSE     NA         NA     5  0.311\n##  6 TRUE - FALSE     NA         NA     6  0.169\n##  7 TRUE - FALSE     NA         NA     7  0.275\n##  8 TRUE - FALSE     NA         NA     8  0.205\n##  9 TRUE - FALSE     NA         NA     9  0.237\n## 10 TRUE - FALSE     NA         NA    10  0.270\n## # ℹ 3,990 more rows\n\nautonomy_effect_draws %&gt;% median_hdi()\n## # A tibble: 1 × 7\n##   contrast     .value .lower .upper .width .point .interval\n##   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 TRUE - FALSE  0.246  0.171  0.327   0.95 median hdi\n\nggplot(autonomy_effect_draws, aes(x = .value)) +\n  stat_halfeye(fill = palette_okabe_ito(order = 5)) +\n  labs(x = \"Average marginal effect of party autonomy\", y = \"Density\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nThat’s so cool! There’s our marginal effect. On average, having party autonomy is associated with a 0.247 point higher media freedom index score. Since it’s measured on a 0–1 scale, that’s a pretty sizable effect!\nWhere emmeans really shines is figuring out the average marginal effect for continuous predictors. Working with party autonomy was easy enough: get the predicted value when it’s false, get the predicted value when it’s true, and find the difference. Getting a continuous effect, though, is a lot trickier since it involves slopes and first derivatives and calculus. Plus we’re working with a nonlinear model, so the slope is different across the whole range of predictions.\nFor instance, here’s the predicted media freedom index across a range of respect for civil liberties. It’s curvy! The slope is really flat down at low levels of civil liberties, and it’s steep up at high levels\n\npred_civlib &lt;- model_basic %&gt;% \n  epred_draws(newdata = expand_grid(party_autonomy = FALSE,\n                                    region = \"Middle East and North Africa\",\n                                    civil_liberties = seq(0, 1, by = 0.05)))\n\nggplot(pred_civlib, aes(x = civil_liberties, y = .epred)) +\n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Reds\") +\n  labs(x = \"Civil liberties index\", y = \"Predicted media freedom index\",\n       fill = \"Credible interval\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nThe emtrends() function from emmeans lets us find the slope at different hypothetical values. (Technically behind the scenes, if we tell it to show us the slope when civil_liberties is 0.5, it’ll calculate the predicted value at 0.5 and the predicted value at 0.50001, and then average those predictions, rather than try to figure out the true calculus-based derivative).\nIf we don’t specify possible values of civil_liberties, it will give us the slope at the average value of civil_liberties (0.698):\n\nmodel_basic %&gt;% \n  emtrends(~ civil_liberties,\n           var = \"civil_liberties\",\n           at = list(party_autonomy = FALSE,\n                     region = \"Middle East and North Africa\"),\n           epred = TRUE)\n##  civil_liberties civil_liberties.trend lower.HPD upper.HPD\n##            0.696                 0.865     0.702      1.03\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nIf we use ~ 1 in the formula instead of ~ civil_liberties, it will give us the average overall slope:\n\nmodel_basic %&gt;% \n  emtrends(~ 1,\n           var = \"civil_liberties\",\n           at = list(party_autonomy = FALSE,\n                     region = \"Middle East and North Africa\"),\n           epred = TRUE)\n##  1       civil_liberties.trend lower.HPD upper.HPD\n##  overall                 0.865     0.702      1.03\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nOr we can specify specific values of civil_liberties to get the slope at those points:\n\nmodel_basic %&gt;% \n  emtrends(~ civil_liberties,\n           var = \"civil_liberties\",\n           at = list(party_autonomy = FALSE,\n                     region = \"Middle East and North Africa\",\n                     civil_liberties = c(0.2, 0.5, 0.8)),\n           epred = TRUE)\n##  civil_liberties civil_liberties.trend lower.HPD upper.HPD\n##              0.2                 0.436     0.295     0.607\n##              0.5                 0.767     0.577     0.951\n##              0.8                 0.838     0.695     0.982\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nThe civil liberties effect varies a lot depending on existing levels of civil liberties! Let’s visualize these three effects. We’ll scale the effect down, though, since civil liberties is on a 0–1 scale and a 1-unit change means going from 0 to 1, which is huge. We’ll think about this as a 0.1-point increase in the civil liberties scale.\n\name_civlib &lt;- model_basic %&gt;% \n  emtrends(~ civil_liberties,\n           var = \"civil_liberties\",\n           at = list(party_autonomy = FALSE,\n                     region = \"Middle East and North Africa\",\n                     civil_liberties = c(0.2, 0.5, 0.8)),\n           epred = TRUE) %&gt;% \n  gather_emmeans_draws() %&gt;% \n  # Scale this down\n  mutate(.value = .value / 10)\n\nggplot(ame_civlib, aes(x = .value, fill = factor(civil_liberties))) +\n  stat_halfeye(slab_alpha = 0.75) +\n  scale_fill_okabe_ito(order = c(3, 4, 6)) +\n  labs(x = \"Average marginal effect of a\\n0.1-point increase in the civil liberties index\",\n       y = \"Density\", fill = \"Civil liberties index\",\n       caption = \"80% and 95% credible intervals shown in black\") +\n  theme_clean() + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFor countries with low levels of civil liberties, a 0.1-point increase in respect for civil liberties (e.g., moving from 0.2 to 0.3) is associated with a 0.0436-point increase in the media freedom index. For a middle ground country, though, moving from 0.5 to 0.6 in civil liberties is associated with a 0.0763-point increase in the media freedom index, while countries with strong civil liberties see a strong increase in the media freedom index (0.0833 points). Cool cool cool."
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html#different-kinds-of-average-predictions-with-multilevel-models",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html#different-kinds-of-average-predictions-with-multilevel-models",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "Different kinds of average predictions with multilevel models",
    "text": "Different kinds of average predictions with multilevel models\nBUT THERE’S A HUGE CAVEAT HERE!\nGuess what?! All these predicted average effects aren’t quite what we think they are. (That’s what TJ was referring to in the tweet at the beginning of this post!)\nBecause we’re working with multilevel models, we actually have other moving parts to think about: the regional effects. When we talk about average marginal effects with multilevel models, we have to be explicit about what kinds of averages we’re working with. There are a few different kinds of averages we can calculate:\n\nGlobal grand mean\nConditional effects for existing groups\nConditional effects for a single hypothetical group, either typical or brand new\n\nWe can specify all these different kinds of calculations with certain combinations of a few arguments to posterior_epred() / epred_draws() / emmeans() / emmtrends():\n\n\nnewdata: Specify which regions to include in the predictions\n\nre_formula: Specify how (and whether) to handle the model’s random effects in the predictions\n\nsample_new_levels: Specify how to handle the uncertainty in new random effects in the predictions\n\n(These are all documented in the help pages for ?brms::prepare_predictions())\nGlobal grand mean\nWith a global grand mean, we calculate the expected value of the media freedom index while ignoring any region-specific deviations of the intercept or slope. We do not incorporate any of the region-specific information from the model into the predictions. This provides us with a global grand mean—an average that transcends regional differences.\nTo calculate this, we need to feed brms a new dataset that doesn’t include any region, and we need to tell it to not use any random effects by including re_formula = NA.\nBinary effect\nLet’s look at the global average marginal effect for party autonomy, which is binary:\n\n# Posterior predictions across autonomy\ngrand_mean_autonomy_dist &lt;- model_basic %&gt;% \n  epred_draws(newdata = expand_grid(party_autonomy = c(TRUE, FALSE),\n                                    civil_liberties = c(0.5)), \n              re_formula = NA)\n\nplot_grand_mean_autonomy &lt;- ggplot(grand_mean_autonomy_dist, \n                                   aes(x = .epred, y = \"Grand mean\", \n                                       fill = party_autonomy)) +\n  stat_halfeye() +\n  scale_fill_okabe_ito() +\n  labs(x = \"Predicted media index\", y = NULL,\n       fill = \"Opposition parties allowed\",\n       subtitle = \"Posterior predictions\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n# emmeans()-based difference across autonomy\ngrand_mean_autonomy_ame &lt;- model_basic %&gt;% \n  emmeans(~ party_autonomy,\n          at = list(civil_liberties = 0.5),\n          epred = TRUE, re_formula = NA) %&gt;% \n  contrast(method = \"revpairwise\") %&gt;% \n  gather_emmeans_draws()\n\nplot_grand_mean_autonomy_ame &lt;- ggplot(grand_mean_autonomy_ame, \n                                   aes(x = .value, y = \"Grand AME\")) +\n  stat_halfeye(fill = palette_okabe_ito(order = 7)) +\n  labs(x = \"Average marginal effect of party autonomy\", y = NULL,\n       subtitle = \"Marginal effect (TRUE − FALSE)\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n# Combined plot\n(plot_grand_mean_autonomy | plot_grand_mean_autonomy_ame) +\n  plot_annotation(title = \"Global grand mean\",\n                  subtitle = \"re_formula = NA; no region in newdata\",\n                  theme = theme_clean())\n\n\n\n\n\n\n\nAnd here are the actual medians and credible intervals:\n\ngrand_mean_autonomy_ame %&gt;% median_hdi()\n## # A tibble: 1 × 7\n##   contrast     .value .lower .upper .width .point .interval\n##   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 TRUE - FALSE  0.246  0.171  0.327   0.95 median hdi\n\nThe region-free grand average marginal effect for party autonomy is thus 0.247.\nContinuous effect\nNext we can look at the average marginal effect for civil liberties, which is continuous:\n\n# Posterior predictions across civil_liberties\ngrand_mean_civlib_dist &lt;- model_basic %&gt;% \n  epred_draws(newdata = expand_grid(party_autonomy = FALSE,\n                                    civil_liberties = seq(0, 1, by = 0.05)), \n              re_formula = NA)\n\nplot_grand_mean_civlib &lt;- ggplot(grand_mean_civlib_dist, \n                                 aes(x = civil_liberties, y = .epred)) +\n  stat_lineribbon() +\n  scale_fill_brewer(palette = \"Reds\") +\n  labs(x = \"Civil liberties index\", y = \"Predicted media freedom index\",\n       fill = \"Credible interval\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n# emtrends()-based AMEs\ngrand_mean_civlib_ame &lt;- model_basic %&gt;% \n  emtrends(~ civil_liberties,\n           var = \"civil_liberties\",\n           at = list(party_autonomy = FALSE,\n                     civil_liberties = c(0.2, 0.8)),\n           epred = TRUE, re_formula = NA) %&gt;% \n  gather_emmeans_draws()\n\nplot_grand_mean_civlib_ame &lt;- ggplot(grand_mean_civlib_ame,\n                                     aes(x = .value / 10, fill = factor(civil_liberties))) +\n  stat_halfeye(slab_alpha = 0.75) +\n  scale_fill_okabe_ito(order = c(3, 4)) +\n  labs(x = \"Average marginal effect of a\\n0.1-point increase in the civil liberties index\",\n       y = \"Density\", fill = \"Civil liberties index\") +\n  theme_clean() + \n  theme(legend.position = \"bottom\")\n\n# Combined plot\n(plot_grand_mean_civlib | plot_grand_mean_civlib_ame) +\n  plot_annotation(title = \"Grand mean\",\n                  subtitle = \"re_formula = NA; no region in newdata\",\n                  theme = theme_clean())\n\n\n\n\n\n\n\nAnd here are the actual medians for these effects:\n\ngrand_mean_civlib_ame %&gt;% median_hdi()\n## # A tibble: 2 × 7\n##   civil_liberties .value .lower .upper .width .point .interval\n##             &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1             0.2  0.436  0.295  0.607   0.95 median hdi      \n## 2             0.8  0.838  0.695  0.982   0.95 median hdi\n\nThe region-free grand average marginal effect for civil liberties is 0.436 for low values of civil liberties (0.2), and 0.833 for high values (0.8).\nConditional effects for existing groups\nInstead of calculating a grand mean without any group information, we can incorporate regional effects into our predictions. We can create conditional effects for specific regions that already exist in the data, incorporating their region-specific deviations in slope and intercept.\nTo calculate this, we need to feed brms a new dataset that does include one or more regions that are already in the data, and we need to tell it to incorporate the random effects into its predictions by including re_formula = NULL. That feels weird, since re_formula = NA made it so there were no group effects, but that’s the syntax.\nWe can also be more explicit about which group effects to include. When using re_formula = NULL, all group effects are included. If we made a model with both region and year effects, for instance, and only wanted to predict using the region effects, we could use re_formula = ~ (1 | region). Or if we don’t want to remember that NULL means “everything”, we could also just use re_formula = ~ (1 | region) on our basic model with just regional effects. It’s all the same.\nBinary effect\nLet’s look at the binary region-specific average marginal effect for party autonomy across all six regions:\n\n# Posterior predictions across autonomy and region\nall_regions_autonomy_dist &lt;- model_basic %&gt;% \n  epred_draws(newdata = expand_grid(party_autonomy = c(TRUE, FALSE),\n                                    region = levels(vdem_2015$region),\n                                    civil_liberties = c(0.5)), \n              re_formula = NULL)  # or re_formula = ~ (1 | region)\n\nplot_all_regions_autonomy &lt;- ggplot(all_regions_autonomy_dist, \n                                    aes(x = .epred, y = region, \n                                        fill = party_autonomy)) +\n  stat_halfeye() +\n  scale_fill_okabe_ito() +\n  labs(x = \"Predicted media index\", y = NULL,\n       fill = \"Opposition parties allowed\",\n       subtitle = \"Posterior predictions\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n# emmeans()-based difference across autonomy, by region\nall_regions_autonomy_ame &lt;- model_basic %&gt;% \n  emmeans(~ party_autonomy + region,\n          at = list(civil_liberties = 0.5,\n                    region = levels(vdem_2015$region)),\n          epred = TRUE, re_formula = NULL) %&gt;% \n  contrast(method = \"revpairwise\", by = \"region\") %&gt;% \n  gather_emmeans_draws()\n\nplot_all_regions_autonomy_ame &lt;- ggplot(all_regions_autonomy_ame, \n                                        aes(x = .value, y = region)) +\n  stat_halfeye(fill = palette_okabe_ito(order = 7)) +\n  labs(x = \"Average marginal effect of party autonomy\", y = NULL,\n       subtitle = \"Marginal effect (TRUE − FALSE)\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n# Combined plot\n(plot_all_regions_autonomy | plot_all_regions_autonomy_ame + theme(axis.text.y = element_blank())) +\n  plot_annotation(title = \"Region-specific means\",\n                  subtitle = \"re_formula = NULL; existing region(s) included in newdata\",\n                  theme = theme_clean())\n\n\n\n\n\n\n\nAnd here are the actual medians and credible intervals:\n\nall_regions_autonomy_ame %&gt;% median_hdi()\n## # A tibble: 6 × 8\n##   contrast     region                         .value .lower .upper .width .point .interval\n##   &lt;fct&gt;        &lt;fct&gt;                           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 TRUE - FALSE Eastern Europe and Central As…  0.231  0.155  0.303   0.95 median hdi      \n## 2 TRUE - FALSE Latin America and the Caribbe…  0.241  0.161  0.314   0.95 median hdi      \n## 3 TRUE - FALSE Middle East and North Africa    0.250  0.172  0.330   0.95 median hdi      \n## 4 TRUE - FALSE Sub-Saharan Africa              0.250  0.173  0.330   0.95 median hdi      \n## 5 TRUE - FALSE Western Europe and North Amer…  0.243  0.162  0.320   0.95 median hdi      \n## 6 TRUE - FALSE Asia and Pacific                0.239  0.166  0.314   0.95 median hdi\n\nThe average marginal effect changes depending on region, but not by much. Across all regions, having party autonomy is associated with a 0.24 to 0.25-point increase in the media freedom index, on average. Importantly, these average effects incorporate the uncertainty that comes from the regional multilevel structure that we built in the model.\nContinuous effect\nNext we’ll look at the continuous region-specific average marginal effect for civil liberties across all six regions:\n\n# Posterior predictions across civil_liberties and regions\nall_regions_civlib_dist &lt;- model_basic %&gt;% \n  epred_draws(newdata = expand_grid(party_autonomy = FALSE,\n                                    civil_liberties = seq(0, 1, by = 0.05),\n                                    region = levels(vdem_2015$region)), \n              re_formula = NULL)\n\nplot_all_regions_civlib &lt;- ggplot(all_regions_civlib_dist, \n                                  aes(x = civil_liberties, y = .epred)) +\n  stat_lineribbon() +\n  scale_fill_brewer(palette = \"Reds\") +\n  labs(x = \"Civil liberties index\", y = \"Predicted media freedom index\",\n       fill = \"Credible interval\") +\n  facet_wrap(vars(region)) +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n# emtrends()-based AMEs across region\nall_regions_civlib_ame &lt;- model_basic %&gt;% \n  emtrends(~ civil_liberties + region,\n           var = \"civil_liberties\",\n           at = list(party_autonomy = FALSE,\n                     civil_liberties = c(0.2, 0.8),\n                     region = levels(vdem_2015$region)),\n           epred = TRUE, re_formula = NULL) %&gt;% \n  gather_emmeans_draws()\n\nplot_all_regions_civlib_ame &lt;- ggplot(all_regions_civlib_ame,\n                                     aes(x = .value / 10, fill = factor(civil_liberties))) +\n  stat_halfeye(slab_alpha = 0.75) +\n  scale_fill_okabe_ito(order = c(3, 4)) +\n  labs(x = \"Average marginal effect of a\\n0.1-point increase in the civil liberties index\",\n       y = \"Density\", fill = \"Civil liberties index\") +\n  facet_wrap(vars(region)) +\n  theme_clean() + \n  theme(legend.position = \"bottom\")\n\n(plot_all_regions_civlib / plot_all_regions_civlib_ame) +\n  plot_annotation(title = \"Region-specific means\",\n                  subtitle = \"re_formula = NULL; existing region(s) included in newdata\",\n                  theme = theme_clean())\n\n\n\n\n\n\n\nAnd here are the actual medians for these effects:\n\nall_regions_civlib_ame %&gt;% median_hdi()\n## # A tibble: 12 × 8\n##    civil_liberties region                     .value .lower .upper .width .point .interval\n##              &lt;dbl&gt; &lt;fct&gt;                       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1             0.2 Eastern Europe and Centra…  0.327  0.246  0.418   0.95 median hdi      \n##  2             0.2 Latin America and the Car…  0.380  0.290  0.475   0.95 median hdi      \n##  3             0.2 Middle East and North Afr…  0.492  0.377  0.638   0.95 median hdi      \n##  4             0.2 Sub-Saharan Africa          0.479  0.373  0.590   0.95 median hdi      \n##  5             0.2 Western Europe and North …  0.609  0.469  0.729   0.95 median hdi      \n##  6             0.2 Asia and Pacific            0.366  0.267  0.479   0.95 median hdi      \n##  7             0.8 Eastern Europe and Centra…  0.871  0.709  1.02    0.95 median hdi      \n##  8             0.8 Latin America and the Car…  0.865  0.725  1.01    0.95 median hdi      \n##  9             0.8 Middle East and North Afr…  0.808  0.687  0.924   0.95 median hdi      \n## 10             0.8 Sub-Saharan Africa          0.819  0.708  0.926   0.95 median hdi      \n## 11             0.8 Western Europe and North …  0.724  0.585  0.885   0.95 median hdi      \n## 12             0.8 Asia and Pacific            0.865  0.712  1.00    0.95 median hdi\n\nAs expected, the average marginal effect for civil liberties varies across regions. For the Middle East and North Africa, for instance, it is 0.49 for low values of civil liberties (0.2), and 0.8 for high values (0.8), while in Eastern Europe and Central Asia, it is 0.328 for low values and 0.865 for high values.\nNeat!\nConditional effects for a single new hypothetical group, either typical or brand new\nSo far we’ve calculated a grand mean with no regional effects and a bunch of conditional means for specific regions. But what if we want a single mean that also includes regional effects?\nWe have a few really nuanced options for doing this. In general, we’ll create a brand new hypothetical region and make predictions based on it. Where the nuance comes into play is how we construct this region and think about its variation. We can build its variance based on an average of the existing regions’ variances, or we can simulate a brand new kind of variance all together.\nTo do this, we need to include region in the new data we feed brms, but we need to include a name of a region that doesn’t exist (or alternatively, you can feed it region = NA). It will yell at you initially because that group doesn’t exist, but you can allow it to make these extrapolated predictions by including allow_new_levels = TRUE. We can include the random effects for region by setting re_formula = NULL (or re_formula = ~ (1 | region) if we want to be super explicit).\nFinally, we have to specify how to handle the regional variation for this imaginary new group. We can either draw from the variation in all the other groups by setting sample_new_levels = \"uncertainty\" (this is the default), or we can randomly simulate a whole new kind of regional uncertainty based on the model’s existing parameters using sample_new_levels = \"gaussian\". The documentation for all this is included in ?brms::prepare_predictions.\nIn summary, we have these two general approaches:\n\nCreate a hypothetical region that is based on the observed variation in the existing regions with region = \"Something new\" in the new data and re_formula = NULL, allow_new_levels = TRUE, sample_new_levels = \"uncertainty\" in the prediction function\nCreate a hypothetical region that is completely brand new with region = \"Something new\" in the new data and re_formula = NULL, allow_new_levels = TRUE, sample_new_levels = \"gaussian\" in the prediction function\n\nFor the sake of space I’ll only show marginal effects for the binary party_autonomy variable here. Follow the same process as all the examples above, but set re_formula = NULL, allow_new_levels = TRUE, and either sample_new_levels = \"uncertainty\" or sample_new_levels = \"gaussian\" when making the predictions.\nTechnically there are other ways to create simulated groups or regions, like sample_new_levels = \"old_levels\". Isabella Ghement has a great comprehensive table summarizing them all here. But we’re not going to worry about that here.\nBinary effect for amalgamated hypothetical region\nFirst we’ll create predictions for a hypothetical region that draws its group characteristics and variance from all the other regions in the data. To do this, we include region = \"Something new\" in the new data and re_formula = NULL, allow_new_levels = TRUE, sample_new_levels = \"uncertainty\" in the prediction function.\n\name_hypothetical_amalgamated &lt;- model_basic %&gt;% \n  emmeans(~ party_autonomy + region,\n          at = list(civil_liberties = 0.5, \n                    region = \"Generic world region\"),\n          epred = TRUE, re_formula = NULL, \n          allow_new_levels = TRUE, sample_new_levels = \"uncertainty\") %&gt;% \n  contrast(method = \"revpairwise\") %&gt;% \n  gather_emmeans_draws()\n\name_hypothetical_amalgamated %&gt;% median_hdi()\n## # A tibble: 1 × 7\n##   contrast                                    .value .lower .upper .width .point .interval\n##   &lt;chr&gt;                                        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 TRUE Generic world region - FALSE Generic …  0.243  0.164  0.320   0.95 median hdi\n\nplot_ame_hypothetical_amalgamated &lt;- ggplot(ame_hypothetical_amalgamated, \n                                            aes(x = .value, y = \"Generic world region\")) +\n  stat_halfeye(fill = palette_okabe_ito(order = 7)) +\n  labs(x = \"Average marginal effect of party autonomy\", y = NULL,\n       title = \"Marginal effect in a generic world region\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\nplot_ame_hypothetical_amalgamated\n\n\n\n\n\n\n\nIn this case, the distribution is pretty smooth and normal looking, but if there’s a lot of variation in regional distributions, that variation will transfer to these predictions too.\nFor instance, in an earlier working version of this post, I had region-specific average predictions that varied substantially across regions. The resulting new hypothetical region reflected those different components—because the new generic world region is an amalgamation of uncertainty of all the existing regions, it includes both high values and low values (thanks to TJ Mahr for this insight!):\n\n\n\n\n\n\n\n\nBinary effect for completely new hypothetical region\nAlternatively, we can make it so that the new fake region uses a multivariate normal distribution implied by the model’s group-level standard deviations and correlations. This creates a brand new region that would be plausible in the universe, but that is not sampled directly from the existing regions. As a result, there’s no possibility of a weird bimodal blip—the overall shape of the distribution will be much smoother and consistent.\n\name_hypothetical_new &lt;- model_basic %&gt;% \n  emmeans(~ party_autonomy + region,\n          at = list(civil_liberties = 0.5, \n                    region = \"Atlantis\"),\n          epred = TRUE, re_formula = NULL, \n          allow_new_levels = TRUE, sample_new_levels = \"gaussian\") %&gt;% \n  contrast(method = \"revpairwise\") %&gt;% \n  gather_emmeans_draws()\n\name_hypothetical_new %&gt;% median_hdi()\n## # A tibble: 1 × 7\n##   contrast                       .value .lower .upper .width .point .interval\n##   &lt;chr&gt;                           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 TRUE Atlantis - FALSE Atlantis  0.235  0.141  0.325   0.95 median hdi\n\nplot_ame_hypothetical_new &lt;- ggplot(ame_hypothetical_new, \n                                            aes(x = .value, y = \"Atlantis\")) +\n  stat_halfeye(fill = palette_okabe_ito(order = 7)) +\n  labs(x = \"Average marginal effect of party autonomy\", y = NULL,\n       title = \"Marginal effect in the Atlantis region\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\nplot_ame_hypothetical_new"
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html#which-average-is-best",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html#which-average-is-best",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "Which average is best?",
    "text": "Which average is best?\nYou’ve run a great multilevel model and want to report the average marginal effects for your coefficients of interest. Which average do you report? The global grand mean? Means conditional on existing groups? The mean of a new typical-ish looking region? The mean of a completely brand new region?\nWho knows!\nIt all depends on the story you’re trying to tell about your data, theory, and results."
  },
  {
    "objectID": "blog/2021/11/10/ame-bayes-re-guide/index.html#overall-summary-of-different-approaches",
    "href": "blog/2021/11/10/ame-bayes-re-guide/index.html#overall-summary-of-different-approaches",
    "title": "A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models",
    "section": "Overall summary of different approaches",
    "text": "Overall summary of different approaches\nPhew. We did a lot here with a ton of really neat moving parts. Here’s a summary of all the different average effects you can calculate with multilevel models and how to do them:\n\n\n\n\n\n\n\n\n\n\n\nType of average\nDescription\nnewdata\nre_formula\nOther options\n\n\n\nGlobal grand mean\nAverage predicted outcome ignoring group-specific deviations in intercept or slope\nOmit group from newdata\n\nre_formula = NA\n—\n\n\nConditional effects for existing groups\nAverage predicted outcomes for existing groups, incorporating group-specific deviations in intercept or slope\nInclude existing group(s) in newdata\n\n\nre_formula = NULL or actual group term, like re_formula = ~ (1 | group)\n\n—\n\n\nConditional effects for a single typical hypothetical group\nAverage predicted outcome for a new group that is based on variation of existing groups\nInclude new group in newdata\n\n\nre_formula = NULL or actual group term, like re_formula = ~ (1 | group)\n\n\nallow_new_levels = TRUE,sample_new_levels = \"uncertainty\"\n\n\n\nConditional effects for a single brand new hypothetical group\nAverage predicted outcome for a new group that is based on random draws from model\nInclude new group in newdata\n\n\nre_formula = NULL or actual group term, like re_formula = ~ (1 | group)\n\n\nallow_new_levels = TRUE,sample_new_levels = \"gaussian\"\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nType of average\nGeneric code\n\n\n\nGlobal grand mean\nposterior_epred(model,\n                newdata = __OMIT GROUP__,\n                re_formula = NA)\n\n\nConditional effects for existing groups\nposterior_epred(model,\n                newdata = __INCLUDE EXISTING GROUP(S)__,\n                re_formula = NULL)\n\n\nConditional effects for a single typical hypothetical group\nposterior_epred(model,\n                newdata = __INCLUDE NEW GROUP__,\n                re_formula = NULL, allow_new_levels = TRUE,\n                sample_new_levels = \"uncertainty\")\n\n\nConditional effects for a single brand new hypothetical group\nposterior_epred(model,\n                newdata = __INCLUDE NEW GROUP__,\n                re_formula = NULL, allow_new_levels = TRUE,\n                sample_new_levels = \"gaussian\")\n\n\n\n\n\nAnd here’s what they all look like!"
  },
  {
    "objectID": "blog/2021/09/07/do-calculus-backdoors/index.html",
    "href": "blog/2021/09/07/do-calculus-backdoors/index.html",
    "title": "Do-calculus adventures! Exploring the three rules of do-calculus in plain language and deriving the backdoor adjustment formula by hand",
    "section": "",
    "text": "I’ve been teaching a course on program evaluation since Fall 2019, and while part of the class is focused on logic models and the more managerial aspects of evaluation, the bulk of the class is focused on causal inference. Ever since reading Judea Pearl’s The Book of Why in 2019, I’ve thrown myself into the world of DAGs, econometrics, and general causal inference, and I’ve been both teaching it and using it in research ever since. I’ve even published a book chapter on it. Fun stuff.\nThis post assumes you have a general knowledge of DAGs and backdoor confounding. Read this post or this chapter if you haven’t heard about those things yet.\nDAGs are a powerful tool for causal inference because they let you map out all your assumptions of the data generating process for some treatment and some outcome. Importantly, these causal graphs help you determine what statistical approaches you need to use to isolate or identify the causal arrow between treatment and outcome. One of the more common (and intuitive) methods for idenfifying causal effects with DAGs is to close back doors, or adjust for nodes in a DAG that open up unwanted causal associtions between treatment and control. By properly closing backdoors, you can estimate a causal quantity using observational data. There’s even a special formula called the backdoor adjustment formula that takes an equation with a \\operatorname{do}(\\cdot) operator (a special mathematical function representing a direct experimental intervention in a graph) and allows you to estimate the effect with do-free quantities:\nP(y \\mid \\operatorname{do}(x)) = \\sum_z P(y \\mid x, z) \\times P(z)\nWhen I teach this stuff, I show that formula on a slide, tell students they don’t need to worry about it too much, and then show how actually do it using regression, inverse probability weighting, and matching (with this guide). For my MPA and MPP students, the math isn’t as important as the actual application of these principles, so that’s what I focus on.\nHowever—confession time—that math is also a bit of a magic black box for me too. I’ve read it in books and assume that it’s correct, but I never really fully understood why.\nCompounding my confusion is the fact that the foundation of Judea Pearl-style DAG-based causal inference is the idea of do-calculus (Pearl 2012): a set of three mathematical rules that can be applied to a causal graph to identify causal relationships. Part of my confusion stems from the fact that most textbooks and courses (including mine!) explain that you can identify causal relationships in DAGs using backdoor adjustment, frontdoor adjustment, or the fancy application of do-calculus rules. When framed like this, it seems like backdoor and frontdoor adjustment are separate things from do-calculus, and that do-calculus is something you do when backdoor and frontdoor adjustments don’t work.\nBut that’s not the case! In 2020, I asked Twitter if backdoor and frontdoor adjustment were connected to do-calculus, and surprisingly Judea Pearl himself answered that they are!\nThey’re both specific consequences of the application of the rules of do-calculus—they just have special names because they’re easy to see in a graph.\nBut how? How do people apply these strange rules of do-calculus to derive these magical backdoor and frontdoor adjustment formulas? The question has haunted me since April 2020.\nBut in the past couple days, I’ve stumbled across a couple excellent resources (this course and these videos + this blog post) that explained do-calculus really well, so I figured I’d finally tackle this question and figure out how exactly do-calculus is used to derive the backdoor adjustment formula. I won’t show the derivation of the frontdoor formula—smarter people than me have done that (here and Section 6.2.1 here, for instance), but I can do the backdoor one now!\nFirst, I’ll explain and illustrate how each of the three rules of do-calculus as plain-language-y as possible, and then I’ll apply those rules to show how the backdoor adjustment formula is created.\nI use the ggdag and dagitty packages in R for all this, so you can follow along too. Here we go!\nlibrary(tidyverse)  # For ggplot2 and friends\nlibrary(patchwork)  # For combining plots\nlibrary(ggdag)      # For making DAGs with ggplot\nlibrary(dagitty)    # For dealing with DAG math\nlibrary(latex2exp)  # Easily convert LaTeX into arcane plotmath expressions\nlibrary(ggtext)     # Use markdown in ggplot labels\n\n# Create a cleaner serifed theme to use throughout\ntheme_do_calc &lt;- function() {\n  theme_dag(base_family = \"Linux Libertine O\") +\n    theme(plot.title = element_text(size = rel(1.5)),\n        plot.subtitle = element_markdown())\n}\n\n# Make all geom_dag_text() layers use these settings automatically\nupdate_geom_defaults(ggdag:::GeomDagText, list(family = \"Linux Libertine O\", \n                                               fontface = \"bold\",\n                                               color = \"black\"))"
  },
  {
    "objectID": "blog/2021/09/07/do-calculus-backdoors/index.html#exploring-the-rules-of-do-calculus",
    "href": "blog/2021/09/07/do-calculus-backdoors/index.html#exploring-the-rules-of-do-calculus",
    "title": "Do-calculus adventures! Exploring the three rules of do-calculus in plain language and deriving the backdoor adjustment formula by hand",
    "section": "Exploring the rules of do-calculus",
    "text": "Exploring the rules of do-calculus\nThe three rules of do-calculus have always been confusing to me since they are typically written as pure math equations and not in plain understandable language. For instance, here’s Judea Pearl’s canonical primer on do-calculus—a short PDF with lots of math and proofs (Pearl 2012). In basically everything I’ve read about do-calculus, there’s inevitably a listing of these three very mathy rules, written for people much smarter than me:\n\n\n\n\nFrom left to right: Lattimore and Rohde (2019), The Stanford Encyclopedia of Philosophy, Pearl (2012), Neal (2020)\n\n\n\nHowever, beneath this scary math, each rule has specific intuition and purpose behind it—I just didn’t understand the plain-language reasons for each rule until reading this really neat blog post. Here’s what each rule actually does:\n\n\nRule 1: Decide if we can ignore an observation\n\nRule 2: Decide if we can treat an intervention as an observation\n\nRule 3: Decide if we can ignore an intervention\n\nWhoa! That’s exceptionally logical. Each rule is designed to help simplify and reduce nodes in a DAG by either ignoring them (Rules 1 and 3) or making it so interventions like \\operatorname{do}(\\cdot) can be treated like observations instead (Rule 2).\nLet’s explore each of these rules in detail. In all these situations, we’re assuming that there’s a DAG with 4 nodes: W, X, Y, and Z. Y is always the outcome; X is always the main treatment. In each rule, our goal is to get rid of Z by applying the rule. When talking about interventions in a graph, there’s a special notation with overlines and underlines:\n\nAn overline like G_{\\overline{X}} means that you delete all the arrows going into X\nAn underline like G_{\\underline{X}} means that you delete all the arrows coming out of X\n\nI imagine this line like a wall:\n\nIf the wall is on top of X like \\overline{X}, you can’t draw any arrows going into it, so you delete anything going in\nIf the wall is on the bottom of X like \\underline{X}, you can’t draw any arrows going out of it, so you delete anything going out\n\nRule 1: Ignoring observations\nAccording to Rule 1, we can ignore any observational node if it doesn’t influence the outcome through any path, or if it is d-separated from the outcome. Here’s the formal definition:\n\nP(y \\mid z, \\operatorname{do}(x), w) = P(y \\mid \\operatorname{do}(x), w) \\qquad \\text{ if } (Y \\perp Z \\mid W, X)_{G_{\\overline{X}}}\n\nThere are a lot of moving parts here, but remember, the focus in this equation is z. Our goal here is to remove or ignore z. Notice how z exists on the left-hand side of the equation and how it is gone on the right-hand side. As long as we meet the cryptic conditions of (Y \\perp Z \\mid W, X)_{G_{\\overline{X}}}, we can get rid of it. But what the heck does that even mean?\nHere, G_{\\overline{X}} means “the original causal graph with all arrows into X removed”, while the Y \\perp Z \\mid W, X part means “Y is independent of Z, given W and X” in the new modified graph. If the Y and Z nodes are d-separated from each other after we account for both W and X, we can get rid of Z and ignore it.\nLet’s look at this graphically to help make better sense of this. We’ll use the dagify() function from ggdag to build a couple DAGs: one complete one (G) and one with all the arrows into X deleted (G_{\\overline{X}}). X causes both X and Y, while W confounds X, Y, and Z.\n\nrule1_g &lt;- dagify(\n  Y ~ X + W,\n  X ~ W,\n  Z ~ X + W,\n  coords = list(x = c(X = 1, Y = 2, Z = 1.25, W = 1.5),\n                y = c(X = 1, Y = 1, Z = 2, W = 2))\n)\n\nrule1_g_x_over &lt;- dagify(\n  Y ~ X + W,\n  Z ~ X + W,\n  coords = list(x = c(X = 1, Y = 2, Z = 1.25, W = 1.5),\n                y = c(X = 1, Y = 1, Z = 2, W = 2))\n) \n\n\nplot_rule1_g &lt;- ggplot(rule1_g, aes(x = x, y = y, \n                                    xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G$\"),\n       subtitle = \"Original DAG\") +\n  theme_do_calc()\n\nplot_rule1_g_x_over &lt;- ggplot(rule1_g_x_over, aes(x = x, y = y, \n                                                  xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G_{\\\\bar{X}}$\"),\n       subtitle = \"DAG with arrows *into* X deleted\") +\n  theme_do_calc()\n\nplot_rule1_g | plot_rule1_g_x_over\n\n\n\n\n\n\n\nIf we want to calculate the causal effect of X on Y, do we need to worry about Z here, or can we ignore it? Let’s apply Rule 1. If we look at the modified G_{\\overline{X}}, Y and Z are completely d-separated if we account for both W and X—there’s no direct arrow between them, and there’s no active path connecting them through W or X, since we’re accounting for (or condition on) those nodes. Y and Z are thus d-separated and Y \\perp Z \\mid W, X. We can confirm this with the impliedConditionalIndependencies() function from the dagitty package:\n\nimpliedConditionalIndependencies(rule1_g_x_over)\n## W _||_ X\n## Y _||_ Z | W, X\n\nAnd there it is! The second independency there is Y \\perp Z \\mid W, X. That means that we can apply Rule 1 and ignore Z, meaning that\n\nP(y \\mid z, \\operatorname{do}(x), w) = P(y \\mid \\operatorname{do}(x), w)\n\nThis makes sense but is a little too complicated for me, since we’re working with four different nodes. We can simplify this and pretend that \\operatorname{do}(x) is nothing and that X doesn’t exist. That leaves us with just three nodes—W, Y, and Z—and this DAG:\n\nrule1_g_simple &lt;- dagify(\n  Y ~ W,\n  Z ~ W,\n  coords = list(x = c(Y = 2, Z = 1, W = 1.5),\n                y = c(Y = 1, Z = 1, W = 2))\n)\n\nplot_rule1_g_simple &lt;- ggplot(rule1_g_simple, aes(x = x, y = y, \n                                                  xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G$\"),\n       subtitle = \"Simplified DAG without X\") +\n  theme_do_calc()\nplot_rule1_g_simple\n\n\n\n\n\n\n\nThe simplified X-free version of Rule 1 looks like this:\n\nP(y \\mid z, w) = P(y \\mid w) \\qquad \\text{ if } (Y \\perp Z \\mid W)_{G}\n\nIn other words, we can ignore Z and remove it from the P(y \\mid z, w) equation if Y and Z are d-separated (or independent of each other) after accounting for W. Once we account for W, there’s no possible connection between Y and Z, so they really are d-separated. We can again confirm this with code:\n\nimpliedConditionalIndependencies(rule1_g_simple)\n## Y _||_ Z | W\n\nThere we go. Because Y \\perp Z \\mid W we can safely ignore Z.\nRule 2: Treating interventions as observations\nRule 1 is neat, but it has nothing to do with causal interventions or the \\operatorname{do}(\\cdot) operator. It feels more like a housekeeping rule—it’s a way of simplifying and removing unnecessary nodes that don’t have to do with the main treatment → outcome relationship.\nWith Rule 2, we start messing with interventions. In an experiment like a randomized controlled trial, a researcher has the ability to assign treatment and either \\operatorname{do}(x) or not \\operatorname{do}(x). With observational data, though, it’s not possible to \\operatorname{do}(x) directly. It would be fantastic if we could take an intervention like \\operatorname{do}(x) and treat it like regular non-interventional observational data. Rule 2 lets us do this.\nAccording to Rule 2, interventions (or do(x)) can be treated as observations (or x) when the causal effect of a variable on the outcome (X \\rightarrow Y) only influences the outcome through directed paths. The official math for this is this complicated thing:\n\nP(y \\mid \\operatorname{do}(z), \\operatorname{do}(x), w) = P(y \\mid z, \\operatorname{do}(x), w) \\qquad \\text{ if } (Y \\perp Z \\mid W, X)_{G_{\\overline{X}, \\underline{Z}}}\n\nFor me, this is super confusing, since there are two different \\operatorname{do}(\\cdot) operators here and when I think of causal graphs, I think of single interventions. Like we did with Rule 1, we can simplify this and pretend that there’s no intervention \\operatorname{do}(x) (we’ll do the full rule in a minute, don’t worry). Again, this is legal because each of these rules are focused on messing with the Z variable: ignoring it or treating it as an observation. That leaves us with this slightly simpler (though still cryptic) equation:\n\nP(y \\mid \\operatorname{do}(z), w) = P(y \\mid z, w) \\qquad \\text{ if } (Y \\perp Z \\mid W)_{G_{\\underline{Z}}}\n\nNotice how the left-hand side has the interventional \\operatorname{do}(z), while the right-hand side has the observed z. As long as we meet the condition (Y \\perp Z \\mid W)_{G_{\\underline{Z}}}, we can transform \\operatorname{do}(z) into z and work only with observational data. Once again, though, what does this (Y \\perp Z \\mid W)_{G_{\\underline{Z}}} condition even mean?\nHere, G_{\\underline{Z}} means “the original causal graph with all arrows out of Z removed”, while the Y \\perp Z \\mid W part means “Y is independent of Z, given W” in the new modified graph. Similar to Rule 1, if the Y and Z nodes are d-separated from each other after we account for W, we can legally treat \\operatorname{do}(z) like z.\nAs we did with Rule 1, we’ll build a couple basic DAGs: a complete one (G) and one with all the arrows out of Z deleted (G_{\\underline{Z}}).\n\nrule2_g_simple &lt;- dagify(\n  Y ~ Z + W,\n  Z ~ W,\n  coords = list(x = c(Y = 2, Z = 1, W = 1.5),\n                y = c(Y = 1, Z = 1, W = 2))\n)\n\nrule2_g_simple_z_under &lt;- dagify(\n  Y ~ W,\n  Z ~ W,\n  coords = list(x = c(Y = 2, Z = 1, W = 1.5),\n                y = c(Y = 1, Z = 1, W = 2))\n) \n\n\nplot_rule2_g_simple &lt;- ggplot(rule2_g_simple, \n                              aes(x = x, y = y, \n                                  xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G$\"),\n       subtitle = \"Original DAG\") +\n  theme_do_calc()\n\nplot_rule2_g_simple_z_under &lt;- ggplot(rule2_g_simple_z_under, \n                                      aes(x = x, y = y, \n                                          xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G_{\\\\underline{Z}}$\"),\n       subtitle = \"DAG with arrows *out of* Z deleted\") +\n  theme_do_calc()\n\nplot_rule2_g_simple | plot_rule2_g_simple_z_under\n\n\n\n\n\n\n\nSo, can we treat Z here like an observational node instead of a interventional \\operatorname{do}(\\cdot) node? Let’s apply Rule 2. If we look at the modified G_{\\underline{Z}} graph, Z and Y are completely d-separated if we account for W—there’s no direct arrow between them, and there’s no active path connecting them through W since we’re conditioning on W. We can thus say that Y \\perp Z \\mid W. We can confirm this with code too:\n\nimpliedConditionalIndependencies(rule2_g_simple_z_under)\n## Y _||_ Z | W\n\nWoohoo! Because Y \\perp Z \\mid W in that modified G_{\\underline{Z}} graph, we can legally convert the interventional \\operatorname{do}(z) to just a regular old observational z:\n\nP(y \\mid \\operatorname{do}(z), w) = P(y \\mid z, w)\n\nSo far we’ve applied Rule 2 to a simplified DAG with three nodes, but what does it look like if we’re using the full four-node graph that is used in the formal definition of Rule 2?\n\nP(y \\mid \\operatorname{do}(z), \\operatorname{do}(x), w) = P(y \\mid z, \\operatorname{do}(x), w) \\qquad \\text{ if } (Y \\perp Z \\mid W, X)_{G_{\\overline{X}, \\underline{Z}}}\n\nHere’s one graphical representation of a graph with the four nodes W, X, Y, and Z (but it’s definitely not the only possible graph! These do-calculus rules don’t assume any specific relationships between the nodes). Here, Y is caused by both X and Z, and we’ll pretend that they’re both interventions (so \\operatorname{do}(x) and \\operatorname{do}(z)). X is causally linked to Z, and W confounds all three: X, Y, and Z. Graph G shows the complete DAG; Graph G_{\\overline{X}, \\underline{Z}} shows a modified DAG with all arrows into X deleted (\\overline{X}) and all arrows out of Z deleted (\\underline{Z}).\n\nrule2_g &lt;- dagify(\n  Y ~ X + W + Z,\n  X ~ W,\n  Z ~ X + W,\n  coords = list(x = c(X = 1, Y = 2, Z = 1.25, W = 1.5),\n                y = c(X = 1, Y = 1, Z = 2, W = 2))\n)\n\nrule2_g_modified &lt;- dagify(\n  Y ~ X + W,\n  Z ~ X + W,\n  coords = list(x = c(X = 1, Y = 2, Z = 1.25, W = 1.5),\n                y = c(X = 1, Y = 1, Z = 2, W = 2))\n) \n\n\nplot_rule2_g &lt;- ggplot(rule2_g, aes(x = x, y = y, \n                                    xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G$\"),\n       subtitle = \"Original DAG\") +\n  theme_do_calc()\n\nplot_rule2_modified &lt;- ggplot(rule2_g_modified, \n                              aes(x = x, y = y, \n                                  xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G_{\\\\bar{X}, \\\\underline{Z}}$\"),\n       subtitle = \"DAG with arrows *into* X and *out of* Z deleted\") +\n  theme_do_calc()\n\nplot_rule2_g | plot_rule2_modified\n\n\n\n\n\n\n\nOkay. Our goal here is to check if we can treat \\operatorname{do}(z) like a regular observational z. We can legally do this if Y and Z are d-separated in that modified graph, after accounting for both W and X, or Y \\perp Z \\mid W, X. And that is indeed the case! There’s no direct arrow connecting Y and Z in the modified graph, and once we condition on (or account for) W and X, no pathways between Y and Z are active—Y and Z are independent and d-separated. We can confirm this with code:\n\nimpliedConditionalIndependencies(rule2_g_modified)\n## W _||_ X\n## Y _||_ Z | W, X\n\nThe second independency there is that Y \\perp Z \\mid W, X, which is exactly what we want to see. We can thus legally transform \\operatorname{do}(z) to z:\n\nP(y \\mid \\operatorname{do}(z), \\operatorname{do}(x), w) = P(y \\mid z, \\operatorname{do}(x), w)\n\nWhat’s really neat is that Rule 2 is a generalized version of the backdoor criterion. More on that below after we explore Rule 3.\nRule 3: Ignoring interventions\nRule 3 is the trickiest of the three, conceptually. It tells us when we can completely remove a \\operatorname{do}(\\cdot) expression rather than converting it to an observed quantity. Here it is in all its mathy glory:\n\nP(y \\mid \\operatorname{do}(z), \\operatorname{do}(x), w) = P(y \\mid \\operatorname{do}(x), w) \\qquad \\text{ if } (Y \\perp Z \\mid W, X)_{G_{\\overline{X}, \\overline{Z(W)}}}\n\nIn simpler language, this means that we can ignore an intervention (or a \\operatorname{do}(\\cdot) expression) if it doesn’t influence the outcome through any uncontrolled path—we can remove \\operatorname{do}(z) if there is no causal association (or no unblocked causal paths) flowing from Z to Y.\nThis rule is tricky, though, because it depends on where the Z node (i.e. the intervention we want to get rid of) appears in the graph. Note the notation for the modified graph here. With the other rules, we used things like G_{\\overline{X}} or G_{\\underline{Z}} to remove arrows into and out of specific nodes in the modified graph. Here, though, we have the strange G_{\\overline{Z(W)}}. This Z(W) is weird! It means “any Z node that isn’t an ancestor of W”. We thus only delete arrows going into a Z node in the modified graph if that Z node doesn’t precede W.\nHere’s one version of what that could look like graphically:\n\nrule3_g &lt;- dagify(\n  Y ~ X + W,\n  W ~ Z,\n  Z ~ X,\n  coords = list(x = c(X = 1, Y = 2, Z = 1.25, W = 1.5),\n                y = c(X = 1, Y = 1, Z = 2, W = 1.75))\n)\n\n\nplot_rule3_g &lt;- ggplot(rule3_g, \n                       aes(x = x, y = y, \n                           xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G$\"),\n       subtitle = \"Original DAG\") +\n  theme_do_calc()\n\nplot_rule3_g_modified &lt;- ggplot(rule3_g, \n                                aes(x = x, y = y, \n                                    xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G_{\\\\bar{X}, \\\\bar{Z(W)}}$\"),\n       subtitle = \"DAG with arrows *into* Z deleted as long as Z isn't an&lt;br&gt;ancestor of W + all arrows *into* X deleted\") +\n  theme_do_calc()\n\nplot_rule3_g | plot_rule3_g_modified\n\n\n\n\n\n\n\nNotice how these two graphs are identical. Because we only delete arrows going into Z if Z is not an ancestor of W, in this case G = G_{\\overline{X}, \\overline{Z(W)}}.\nRemember that our original goal is to get rid of \\operatorname{do}(z), which we can legally do if Y and Z are d-separated and independent in our modified graph, or if Y \\perp Z \\mid W, X. That is once again indeed the case here: there’s no direct arrow between Y and Z, and if we condition on W and X, there’s no way to pass association between Y and Z, meaning that Y and Z are d-separated. Let’s confirm it with code:\n\nimpliedConditionalIndependencies(rule3_g)\n## W _||_ X | Z\n## Y _||_ Z | W, X\n\nThat second independency is our Y \\perp Z \\mid W, X, so we can safely eliminate \\operatorname{do}(z) from the equation. We can ignore it because it doesn’t influence the outcome Y through any possible path. Goodbye \\operatorname{do}(z)!:\n\nP(y \\mid \\operatorname{do}(z), \\operatorname{do}(x), w) = P(y \\mid \\operatorname{do}(x), w)\n\nIn this case, the alternative graph G_{\\overline{X}, \\overline{Z(W)}} was the same as the original graph because of the location of Z—Z was an ancestor of W, so we didn’t delete any arrows. If Z is not an ancestor, though, we get to actually modify the graph. For instance, consider this DAG:\n\nrule3_g_alt &lt;- dagify(\n  Y ~ X + W,\n  Z ~ W,\n  X ~ Z,\n  coords = list(x = c(X = 1, Y = 2, Z = 1.25, W = 1.5),\n                y = c(X = 1, Y = 1, Z = 2, W = 1.75))\n)\n\nrule3_g_alt_modified &lt;- dagify(\n  Y ~ X + W,\n  Z ~ 0,\n  X ~ 0,\n  coords = list(x = c(X = 1, Y = 2, Z = 1.25, W = 1.5),\n                y = c(X = 1, Y = 1, Z = 2, W = 1.75))\n) \n\n\nplot_rule3_g_alt &lt;- ggplot(rule3_g_alt, \n                           aes(x = x, y = y, \n                               xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G$\"),\n       subtitle = \"Original DAG\") +\n  theme_do_calc()\n\nplot_rule3_g_alt_modified &lt;- ggplot(rule3_g_alt_modified, \n                                    aes(x = x, y = y, \n                                        xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G_{\\\\bar{X}, \\\\bar{Z(W)}}$\"),\n       subtitle = \"DAG with arrows *into* Z deleted as long as Z isn't an&lt;br&gt;ancestor of W + all arrows *into* X deleted\") +\n  theme_do_calc()\n\nplot_rule3_g_alt | plot_rule3_g_alt_modified\n## Warning: Removed 1 rows containing missing values (`geom_dag_point()`).\n## Warning: Removed 1 rows containing missing values (`geom_dag_text()`).\n\n\n\n\n\n\n\nPhew. In this case, our DAG surgery for making the modified graph G_{\\overline{X}, \\overline{Z(W)}} actually ended up completely d-separating Z from all nodes. Because Z isn’t an ancestor of W (but is instead a descendant), we get to delete arrows going into it, and we get to delete arrows going into X as well. We can remove \\operatorname{do}(z) from the equation as long as Y \\perp Z \\mid W, X in this modified graph. That is most definitely the case here. And once again, code confirms it (ignore the 0s here—they’re only there so that the DAG plots correctly):\n\nimpliedConditionalIndependencies(rule3_g_alt_modified)\n## 0 _||_ W\n## 0 _||_ Y | X\n## W _||_ X\n## W _||_ Z\n## X _||_ Z | 0\n## Y _||_ Z | 0\n## Y _||_ Z | X\n\nAnd once again, we can legally get rid of \\operatorname{do}(z):\n\nP(y \\mid \\operatorname{do}(z), \\operatorname{do}(x), w) = P(y \\mid \\operatorname{do}(x), w)\n\nSummary\nPhew. Let’s look back at the three main rules and add their corresponding mathy versions, which should make more sense now:\n\n\nRule 1: Decide if we can ignore an observation\n\nP(y \\mid z, \\operatorname{do}(x), w) = P(y \\mid \\operatorname{do}(x), w) \\qquad \\text{ if } (Y \\perp Z \\mid W, X)_{G_{\\overline{X}}}\n\n\n\nRule 2: Decide if we can treat an intervention as an observation\n\nP(y \\mid \\operatorname{do}(z), \\operatorname{do}(x), w) = P(y \\mid z, \\operatorname{do}(x), w) \\qquad \\text{ if } (Y \\perp Z \\mid W, X)_{G_{\\overline{X}, \\underline{Z}}}\n\n\n\nRule 3: Decide if we can ignore an intervention\n\nP(y \\mid \\operatorname{do}(z), \\operatorname{do}(x), w) = P(y \\mid \\operatorname{do}(x), w) \\qquad \\text{ if } (Y \\perp Z \\mid W, X)_{G_{\\overline{X}, \\overline{Z(W)}}}"
  },
  {
    "objectID": "blog/2021/09/07/do-calculus-backdoors/index.html#deriving-the-backdoor-adjustment-formula-from-do-calculus-rules",
    "href": "blog/2021/09/07/do-calculus-backdoors/index.html#deriving-the-backdoor-adjustment-formula-from-do-calculus-rules",
    "title": "Do-calculus adventures! Exploring the three rules of do-calculus in plain language and deriving the backdoor adjustment formula by hand",
    "section": "Deriving the backdoor adjustment formula from do-calculus rules",
    "text": "Deriving the backdoor adjustment formula from do-calculus rules\nThat was a lot of math, but hopefully each of these do-calculus rules make sense in isolation now. Now that I finally understand what each of these are doing, we can apply these rules to see where the pre-derived / canned backdoor adjustment formula comes from. Somehow by applying these rules, we can transform the left-hand side of this formula into the do-free right-hand side:\n\nP(y \\mid \\operatorname{do}(x)) = \\sum_z P(y \\mid x, z) \\times P(z)\n\nLet’s go through the derivation of the backdoor adjustment formula step-by-step to see how it works. We’ll use this super simple DAG that shows the causal effect of treatment X on outcome Y, confounded by Z:\n\nbackdoor_g &lt;- dagify(\n  Y ~ X + Z,\n  X ~ Z,\n  coords = list(x = c(Y = 2, X = 1, Z = 1.5),\n                y = c(Y = 1, X = 1, Z = 2))\n)\n\nplot_backdoor_g &lt;- ggplot(backdoor_g, aes(x = x, y = y, \n                                          xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G$\"),\n       subtitle = \"Basic backdoor confounding\") +\n  theme_do_calc()\nplot_backdoor_g\n\n\n\n\n\n\n\nMarginalizing across z\n\nWe’re interested in the causal effect of X on Y, or P(y \\mid \\operatorname{do}(x)). If this were an experiment like a randomized controlled trial, we’d be able to delete all arrows going into X, which would remove all confounding from Z and allow us to measure the exact causal effect of X on Y. However, with observational data, we can’t delete arrows like that. But, we can condition the X → Y relationship on Z, given that it influences both X and Y.\nWe thus need to calculate the joint probability of P(y \\mid \\operatorname{do}(x)) across all values of Z. Using the rules of probability marginalization and the chain rule for joint probabilities, we can write this joint probability like so:\n\nP(y \\mid \\operatorname{do}(x)) = \\sum_z P(y \\mid \\operatorname{do}(x), z) \\times P(z \\mid \\operatorname{do}(x))\n\nThe right-hand side of that equation is what we want to be able to estimate using only observational data, but right now it has two \\operatorname{do}(\\cdot) operators in it, marked in red and purple:\n\n\\sum_z P(y \\mid {\\color{#FF4136} \\operatorname{do}(x)}, z) \\times P(z \\mid {\\color{#B10DC9} \\operatorname{do}(x)})\n\nWe need to get rid of those.\nApplying Rule 2\nFirst let’s get rid of the red \\color{#FF4136} \\operatorname{do}(x) that’s in P(y \\mid {\\color{#FF4136} \\operatorname{do}(x)}, z). This chunk of the equation involves all three variables: treatment, outcome, and confounder. Accordingly, we don’t really want to ignore any of these variables by using something like Rule 1 or Rule 3. Instead, we can try to treat that \\color{#FF4136} \\operatorname{do}(x) as an observational \\color{#FF4136} x using Rule 2.\nAccording to Rule 2, we can treat an interventional \\operatorname{do}(\\cdot) operator as observational if we meet specific criteria in a modified graph where we remove all arrows out of X:\n\nP(y \\mid {\\color{#FF4136} \\operatorname{do}(x)}, z) = P(y \\mid {\\color{#FF4136} x}, z) \\qquad \\text{ if } (Y \\perp X \\mid Z)_{G_{\\underline{X}}}\n\nHere’s the modified G_{\\underline{X}} graph:\n\nbackdoor_g_underline_x &lt;- dagify(\n  Y ~ Z,\n  X ~ Z,\n  coords = list(x = c(Y = 2, X = 1, Z = 1.5),\n                y = c(Y = 1, X = 1, Z = 2))\n)\n\nplot_backdoor_g_underline_x &lt;- ggplot(backdoor_g_underline_x, \n                                      aes(x = x, y = y, \n                                          xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G_{\\\\underline{X}}$\"),\n       subtitle = \"DAG with arrows *out of* X deleted\") +\n  theme_do_calc()\n\nplot_backdoor_g | plot_backdoor_g_underline_x\n\n\n\n\n\n\n\nFollowing Rule 2, we can treat \\color{#FF4136} \\operatorname{do}(x) like a regular observational \\color{#FF4136} x as long as X and Y are d-separated in this modified G_{\\underline{X}} graph when conditioning on Z. And that is indeed the case: there’s no direct arrow between X and Y, and by conditioning on Z, there’s no active pathway between X and Y through Z. Let’s see if code backs us up:\n\nimpliedConditionalIndependencies(backdoor_g_underline_x)\n## X _||_ Y | Z\n\nPerfect! Because Y \\perp X \\mid Z, we can treat \\color{#FF4136} \\operatorname{do}(x) like \\color{#FF4136} x.\nApplying Rule 3\nAfter applying Rule 2 to the first chunk of the equation, we’re still left with the purple \\color{#B10DC9} \\operatorname{do}(x) in the second chunk:\n\n\\sum_z P(y \\mid {\\color{#FF4136} x}, z) \\times P(z \\mid {\\color{#B10DC9} \\operatorname{do}(x)})\n\nThis second chunk doesn’t have the outcome y in it and instead refers only to the treatment and confounder. Since it’s not connected with the outcome, it would be neat if we could get rid of that \\color{#B10DC9} \\operatorname{do}(x) altogether. That’s what Rule 3 is for—ignoring interventions.\nAccording to Rule 3, we can remove a \\operatorname{do}(\\cdot) operator as long as it doesn’t influence the outcome through any uncontrolled or unconditioned path in a modified graph. Because we’re dealing with a smaller number of variables here, the math for Rule 3 is a lot simpler:\n\nP(z \\mid {\\color{#B10DC9} \\operatorname{do}(x)}) = P(z \\mid {\\color{#B10DC9} \\text{nothing!}}) \\qquad \\text{ if } (X \\perp Z)_{G_{\\overline{X}}}\n\nHere’s the simplified G_{\\overline{X}} graph:\n\nbackdoor_g_overline_x &lt;- dagify(\n  Y ~ X + Z,\n  coords = list(x = c(Y = 2, X = 1, Z = 1.5),\n                y = c(Y = 1, X = 1, Z = 2))\n)\n\nplot_backdoor_g_overline_x &lt;- ggplot(backdoor_g_overline_x, \n                                     aes(x = x, y = y, \n                                         xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 10) +\n  geom_dag_text() +\n  labs(title = TeX(\"$G_{\\\\bar{X}}$\"),\n       subtitle = \"DAG with arrows *into* X deleted\") +\n  theme_do_calc()\n\nplot_backdoor_g | plot_backdoor_g_overline_x\n\n\n\n\n\n\n\nAs long as X and Z are d-separated and independent, we can remove that \\color{#B10DC9} \\operatorname{do}(x) completely. According to this graph, there’s no direct arrow connecting them, and there’s no active pathway through Y, since Y is a collider in this case and doesn’t pass on causal association. As always, let’s verify with code:\n\nimpliedConditionalIndependencies(backdoor_g_overline_x)\n## X _||_ Z\n\nHuzzah! X \\perp Z, which means we can nuke the \\color{#B10DC9} \\operatorname{do}(x).\nFinal equation\nAfter marginalizing across z, applying Rule 2, and applying Rule 3, we’re left with the following formula for backdoor adjustment:\n\nP(y \\mid \\operatorname{do}(x)) = \\sum_z P(y \\mid x, z) \\times P(z)\n\nThat’s exactly the same formula as the general backdoor adjustment formula—we successfully derived it using do-calculus rules!\nMost importantly, there are no \\operatorname{do}(\\cdot) operators anywhere in this equation, making this estimand completely do-free and estimable using non-interventional observational data! As long as we close the backdoor confounding by adjusting for Z (however you want, like through inverse probability weighting, matching, fancy machine learning stuff, or whatever else—see this chapter, or this blog post, or this guide for examples of how to do this), we can estimate the causal effect of X on Y (or P(y \\mid \\operatorname{do}(x))) with only observational data.\nHere’s the derivation all at once:\n\n\\begin{aligned}\n& [\\text{Marginalization across } z + \\text{chain rule for conditional probabilities}] \\\\\nP(y \\mid \\operatorname{do}(x)) =& \\sum_z P(y \\mid {\\color{#FF4136} \\operatorname{do}(x)}, z) \\times P(z \\mid {\\color{#B10DC9} \\operatorname{do}(x)}) \\\\\n& [\\text{Use Rule 2 to treat } {\\color{#FF4136} \\operatorname{do}(x)} \\text{ as } {\\color{#FF4136} x}] \\\\\n=& \\sum_z P(y \\mid {\\color{#FF4136} x}, z) \\times P(z \\mid {\\color{#B10DC9} \\operatorname{do}(x)}) \\\\\n& [\\text{Use Rule 3 to nuke } {\\color{#B10DC9} \\operatorname{do}(x)}] \\\\\n=& \\sum_z P(y \\mid {\\color{#FF4136} x}, z) \\times P(z \\mid {\\color{#B10DC9} \\text{nothing!}}) \\\\\n& [\\text{Final backdoor adjustment formula!}] \\\\\n=& \\sum_z P(y \\mid x, z) \\times P(z)\n\\end{aligned}\n\nThat’s so so cool!\nThe frontdoor adjustment formula can be derived in a similar process—see the end of this post for an example (with that, you apply Rules 2 and 3 repeatedly until all the \\operatorname{do}(\\cdot) operators disappear)\nAnd in cases where there’s no pre-derived backdoor or frontdoor adjustment formula, you can still apply these three do-calculus rules to attempt to identify the relationship between X and Y. Not all DAGs are fully estimable, but if they are estimable, the rules of do-calculus can be applied to derive the estimate. Fancier tools like Causal Fusion help with this and automate the process."
  },
  {
    "objectID": "blog/2021/08/25/twfe-diagnostics/index.html",
    "href": "blog/2021/08/25/twfe-diagnostics/index.html",
    "title": "Exploring Pamela Jakiela’s simple TWFE diagnostics with R",
    "section": "",
    "text": "The world of econometrics has been roiled over the past couple years with a bunch of new papers showing how two-way fixed effects (TWFE; situations with nested levels of observations, like country-year, state-month, etc.) estimates of causal effects from difference-in-differences-based natural experiments can be biased when treatment is applied at different times. There are a ton of these papers, like de Chaisemartin and d’Haultfoeuille (2020), Goodman-Bacon (2021), Sun and Abraham (2020), Callaway and Sant’Anna (2020), and Baker, Larcker, and Wang (2021). And they’re all really technical and scary.\nI’ve been peripherally following these discussions on Twitter, since I teach a class on causal inference and include a couple sessions and assignments on difference-in-differences approaches, and since I do lots of research with country-year panel data (see all my posts on marginal structural models for more on that), but I’ve admittedly been lost in the mathy details of all these papers and I’ve been slightly terrified of trying to work through these papers and figuring out what this differential timing bias actually looks like in real life. I’m not a real economist or econometrician (I just teach microeconomics and econometrics lolz), so this world is still pretty intimidating for me.\nBut Pamela Jakiela recently posted a working paper called “Simple Diagnostics for Two-Way Fixed Effects” (Jakiela 2021) where she presents an easy-to-follow example of all this newfangled TWFE work, so I figured I’d finally officially plunge into this new TWFE world and try to figure out what it all means. Her paper is fantastic—you should read it if you want a quick introduction to the issues of differential timing and TWFE and if you want some easy ways to see how bad these situations might bias causal estimates.\nIn the spirit of Vincent Arel-Bundock, who publicly codes through more technical papers to understand them better (see this or this or this), this post is my attempt at translating Jakiela’s paper from conceptual math and Stata code into R. Here we go!\nlibrary(tidyverse)     # For ggplot2, dplyr, and friends\nlibrary(haven)         # For reading Stata files\nlibrary(fixest)        # One way of doing fixed effects regression\nlibrary(estimatr)      # Another way of doing fixed effects regression\nlibrary(broom)         # For converting model objects to data frames\nlibrary(kableExtra)    # For pretty tables\nlibrary(modelsummary)  # For pretty regression tables\nlibrary(patchwork)     # For combining plots\nlibrary(scales)        # For nice formatting functions\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Barlow+Semi+Condensed\ntheme_clean &lt;- function() {\n  theme_minimal(base_family = \"Barlow Semi Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(family = \"Barlow Semi Condensed Medium\"),\n          strip.text = element_text(family = \"Barlow Semi Condensed\",\n                                    face = \"bold\", size = rel(1), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          plot.caption = element_text(hjust = 0))\n}"
  },
  {
    "objectID": "blog/2021/08/25/twfe-diagnostics/index.html#a-different-way-of-thinking-about-ols-coefficients",
    "href": "blog/2021/08/25/twfe-diagnostics/index.html#a-different-way-of-thinking-about-ols-coefficients",
    "title": "Exploring Pamela Jakiela’s simple TWFE diagnostics with R",
    "section": "A different way of thinking about OLS coefficients",
    "text": "A different way of thinking about OLS coefficients\nBefore diving into the issues that come with TWFE estimation (and Jakiela’s proposed diagnostics), we first have to think about regression coefficients in a slightly different way (that was completely new to me!)\nTo do this, let’s make some simulated data to play with. We have two variables: (1) an outcome that ranges from ≈400–2000, and (2) a continuous treatment that ranges from ≈2–70. For the sake of simplicity, we’ll pretend that the outcome represents weekly income, and the treatment is some sort of social policy (test scores from some job training program? idk—imagine something neat here). We’ll also pretend that this treatment is experimental so we can talk about causation here.\n\nset.seed(1234)  # For reproducibility\n\nn_rows &lt;- 500\nfake_data &lt;- tibble(treatment = rbeta(n_rows, shape1 = 3, shape2 = 7)) %&gt;%\n  mutate(treatment = round(treatment * 100)) %&gt;%\n  # Build the outcome based on some baseline level of outcome + a boost in\n  # outcome that happens because of the treatment + some noise\n  mutate(outcome_baseline = rnorm(n_rows, mean = 800, sd = 200),\n         treatment_boost = 10 * treatment,\n         outcome = outcome_baseline + treatment_boost + rnorm(n_rows, 200, 100)) %&gt;%\n  select(outcome, treatment)\n\nhead(fake_data)\n## # A tibble: 6 × 2\n##   outcome treatment\n##     &lt;dbl&gt;     &lt;dbl&gt;\n## 1    514.        13\n## 2   1550.        35\n## 3   1562.        52\n## 4   1037.         4\n## 5   1137.        38\n## 6   1277.        39\n\nFor reference, here’s what the distributions of these variables look like, along with the relationship between treatment and outcome:\n\nCodehist_out &lt;- ggplot(fake_data, aes(x = outcome)) +\n  geom_histogram(binwidth = 100, color = \"white\",\n                 boundary = 0, fill = \"#FFA615\") +\n  scale_x_continuous(labels = dollar_format()) +\n  labs(title = \"Distribution of outcome\",\n       x = \"Outcome\", y = \"Count\") +\n  coord_cartesian(ylim = c(0, 80)) +\n  theme_clean() +\n  theme(panel.grid.major.x = element_blank())\n\nhist_trt &lt;- ggplot(fake_data, aes(x = treatment)) +\n  geom_histogram(binwidth = 5, color = \"white\",\n                 boundary = 0, fill = \"#A4BD0A\") +\n  labs(title = \"Distribution of treatment\",\n       x = \"Treatment\", y = \"Count\") +\n  coord_cartesian(ylim = c(0, 80)) +\n  theme_clean() +\n  theme(panel.grid.major.x = element_blank())\n\nplot_trt_out &lt;- ggplot(fake_data, aes(x = treatment, y = outcome)) +\n  geom_point(size = 0.75) +\n  geom_smooth(method = \"lm\", color = \"#0599B0\") +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(title = \"Effect of treatment on outcome\",\n       x = \"Treatment\", y = \"Outcome\") +\n  theme_clean()\n\n(hist_out + hist_trt) / plot_spacer() / plot_trt_out +\n  plot_layout(heights = c(0.28, 0.02, 0.7))\n\n\n\n\n\n\n\nWe can find the average treatment effect (ATE) of this imaginary program on the outcome using a simple univariate regression model:\n\\[\n\\color{gray}{\\overbrace{\\color{#FFA615}{Y}}^{\\text{Outcome}}_{\\underbrace{\\color{#FFA615}{i}}_{\\text{An individual}}}} \\color{black}{=} \\color{gray}{\\overbrace{\\color{black}{\\alpha}}^{\\text{Intercept}}} \\color{black}{+} \\color{gray}{\\underbrace{\\color{#0599B0}{\\beta}}_{\\text{ATE}}} \\color{gray}{\\overbrace{\\color{#A4BD0A}{D_i}}^{\\text{Treatment}}} \\color{black}{+} \\color{gray}{\\overbrace{\\color{black}{\\epsilon_i}}^{\\text{Error}}}\n\\]\n\nmodel_effect &lt;- lm(outcome ~ treatment, data = fake_data)\ntidy(model_effect)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)  1001.      22.5        44.6 5.70e-176\n## 2 treatment       9.69     0.672      14.4 1.26e- 39\n\nThis is just regular old OLS regression. Based on this model, a 1-unit increase in treatment causes a $9.69 increase in the outcome. Lovely.\nJakiela (2021, 4) shows an alternate way of calculating \\(\\beta\\), though, based on the Frisch-Waugh-Lovell theorem, which is an econometrics idea that I’ve never heard of (since I’m defs not an economist). According to this theorem, we can rewrite the OLS estimate of \\(\\beta\\) based on the residuals (\\(\\tilde{D}_i\\)) of a model that predicts treatment:\n\\[\n\\color{#0599B0}{\\beta}\\ \\color{black}{=}\\ \\sum_i \\color{#FFA615}{Y_i} \\color{black}\\left( \\frac{\\color{#353D03}{\\tilde{D}_i}}{\\sum_i \\color{#353D03}{\\tilde{D}_i}^2} \\right)\n\\]\nSince this is a univariate model, the residuals here are really just the deviations from the average value of the treatment, or \\(D_i - \\bar{D}\\). We can confirm this by running an intercept-only model and comparing it to \\(D_i - \\bar{D}\\):\n\ntrt_resid &lt;- lm(treatment ~ 1, data = fake_data)\n\nfake_data_resid &lt;- fake_data %&gt;%\n  mutate(treatment_resid = residuals(trt_resid)) %&gt;%\n  mutate(mean_treat = mean(treatment),\n         diff = treatment - mean_treat)\n\nhead(fake_data_resid)\n## # A tibble: 6 × 5\n##   outcome treatment treatment_resid mean_treat   diff\n##     &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n## 1    514.        13          -17.2        30.2 -17.2 \n## 2   1550.        35            4.82       30.2   4.82\n## 3   1562.        52           21.8        30.2  21.8 \n## 4   1037.         4          -26.2        30.2 -26.2 \n## 5   1137.        38            7.82       30.2   7.82\n## 6   1277.        39            8.82       30.2   8.82\n\nThe treatment_resid and diff columns here are identical. Now that we have \\(\\tilde{D}_i\\), we can use that fancy rewritten formula and calculate \\(\\beta\\):\n\nfake_data_resid %&gt;%\n  summarize(beta = sum(outcome * (treatment_resid / sum(treatment_resid^2))))\n## # A tibble: 1 × 1\n##    beta\n##   &lt;dbl&gt;\n## 1  9.69\n\nWHAAAAAT it’s the same ATE that we get from running a regular OLS model! That’s magical!\nThe reason Jakiela reparameterizes \\(\\beta\\) this way is because looking at residuals like this creates inherent weights for each observation. Essentially, \\(\\beta\\) is a weighted sum of the outcome variable, with the weights calculated with \\(\\frac{\\tilde{D}_{it}}{\\sum_{it} \\tilde{D}_{it}^2}\\), or (treatment_resid / sum(treatment_resid^2)). We can calculate the weights for each observation:\n\nfake_data_with_weights &lt;- fake_data_resid %&gt;%\n  mutate(treatment_weight = treatment_resid / sum(treatment_resid^2))\nhead(fake_data_with_weights)\n## # A tibble: 6 × 6\n##   outcome treatment treatment_resid mean_treat   diff treatment_weight\n##     &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;\n## 1    514.        13          -17.2        30.2 -17.2        -0.000168 \n## 2   1550.        35            4.82       30.2   4.82        0.0000471\n## 3   1562.        52           21.8        30.2  21.8         0.000213 \n## 4   1037.         4          -26.2        30.2 -26.2        -0.000255 \n## 5   1137.        38            7.82       30.2   7.82        0.0000764\n## 6   1277.        39            8.82       30.2   8.82        0.0000861\n\nIn this case, given that this is overly perfect simulated data, the weights are really small. In theory, the weights should sum up to 0. Let’s check that really quick:\n\nfake_data_with_weights %&gt;%\n  summarize(total_weights = sum(treatment_weight))\n## # A tibble: 1 × 1\n##   total_weights\n##           &lt;dbl&gt;\n## 1     -4.61e-19\n\nWe can visualize the distribution of weights too:\n\nggplot(fake_data_with_weights, aes(x = treatment_weight)) +\n  geom_histogram(binwidth = 0.00005, color = \"white\",\n                 boundary = 0, fill = \"#F6C848\") +\n  geom_vline(xintercept = 0, color = \"#FF2E00\", linewidth = 1) +\n  labs(x = \"Residualized treatment weight\", y = \"Count\") +\n  scale_x_continuous(labels = comma_format()) +\n  theme_clean() +\n  theme(panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\nThere’s a pattern to which observations get positive or negative weights. According to Jakiela (2021, 4),\n\nAs in any univariate OLS regression of an outcome on a continuous measure of treatment intensity, observations with below mean treatment intensity receive negative weight, and may be thought of as part of the comparison group.\n\nSince treatment here is continuous, there’s no clear division between treatment and control groups, so mathematically, that threshold becomes the average value of treatment: observations where the treatment value is less than the average (or \\(D_i &lt; \\bar{D}\\)) receive negative weight and can conceptually be considered part of the control/comparison group, while observations where \\(D_i &gt; \\bar{D}\\) are in the treatment group. That’s apparent in the data too. Look at the first few rows of fake_data_with_weights above—observations where treatment_resid is negative have negative weights."
  },
  {
    "objectID": "blog/2021/08/25/twfe-diagnostics/index.html#residualized-weights-with-twfe-models",
    "href": "blog/2021/08/25/twfe-diagnostics/index.html#residualized-weights-with-twfe-models",
    "title": "Exploring Pamela Jakiela’s simple TWFE diagnostics with R",
    "section": "Residualized weights with TWFE models",
    "text": "Residualized weights with TWFE models\nThis idea of weighting the outcome variable by treatment status (with treated units getting positive weight and untreated units getting negative weight) is central to the rest of Jakiela’s argument and diagnostics (as well as all the other neat new diff-in-diff papers). Fundamentally, the main reason TWFE estimates get weird and biased with differently-timed treatments is because of issues with weights—in TWFE settings, treated observations often get negative weights and vice versa. This isn’t always bad, and Jakiela explains why and when it’s okay. That’s essentially the whole point of her paper—she provides diagnostics to help determine if there are serious issues with these residualized treatment weights.\nLoad and clean data\nTo demonstrate issues with TWFE models, weights, and differential timing, Jakiela looks at the effect of eliminating primary school fees on school enrollment in 15 African countries, based on data from the World Bank. You can get her data and Stata code at GitHub. First let’s load the data and clean it up a little.\n\nfpe_raw &lt;- read_dta(\"WDI-FPE-data.dta\")\n\n# Remove rows where primary school enrollment is missing\nfpe_primary &lt;- fpe_raw %&gt;%\n  filter(!is.na(primary))\n\n# Remove rows where secondary school enrollment is missing\nfpe_secondary &lt;- fpe_raw %&gt;%\n  filter(!is.na(secondary))\n\nhead(fpe_primary)\n## # A tibble: 6 × 8\n##    year country ccode primary    id secondary fpe_year treatment\n##   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1  1981 Benin   BEN      60.3     2      15.5     2006         0\n## 2  1982 Benin   BEN      64.7     2      18.5     2006         0\n## 3  1983 Benin   BEN      66.2     2      21.2     2006         0\n## 4  1984 Benin   BEN      64.2     2      20.1     2006         0\n## 5  1985 Benin   BEN      64.3     2      19.0     2006         0\n## 6  1986 Benin   BEN      62.4     2      16.3     2006         0\n\nWe have 8 columns to work with:\n\n\nyear: The year of the observation\n\ncountry & ccode & id: The country name, ISO3 country code, and id number for each country (Stata apparently struggles with string-based variables, so id works better as a country identifier there)\n\nprimary: Gross enrollment in primary schools\n\nsecondary: Gross enrollment in secondary schools\n\nfpe_year: Year the country eliminated fees for primary schools\n\ntreatment: Indicator variable that is 1 when year &gt; fpe_year and 0 otherwise\nModel specification\nWe can estimate the causal effect of eliminating primary school fees (treatment) on primary and secondary school enrollment (primary and secondary) treatment with a TWFE diff-in-diff model:\n\\[\n\\color{gray}{\\overbrace{\\color{#FFA615}{Y}}^{\\text{Outcome}}_{\\underbrace{\\color{#FFA615}{it}}_{\\substack{i: \\text{ country} \\\\ t: \\text{ year}}}}} \\color{black} = \\color{gray}{\\overbrace{\\color{black}{\\alpha}}^{\\text{Intercept}}} + \\color{gray}{\\overbrace{\\color{black}{\\lambda_i}}^{\\substack{\\text{Country} \\\\ \\text{fixed} \\\\ \\text{effects}}}} + \\color{gray}{\\overbrace{\\color{black}{\\gamma_t}}^{\\substack{\\text{Year} \\\\ \\text{fixed} \\\\ \\text{effects}}}} + \\color{gray}{\\underbrace{\\color{#0599B0}{\\beta}}_{\\text{ATE}}} \\color{gray}{\\overbrace{\\color{#A4BD0A}{D_{it}}}^{\\text{Treatment}}} + \\color{gray}{\\overbrace{\\color{black}{\\epsilon_{it}}}^{\\text{Error}}}\n\\]\nOr without all the annotations:\n\\[\n\\color{#FFA615}{Y_{it}} \\color{black}{\\ = \\alpha + \\lambda_i + \\gamma_t +} \\color{#0599B0}{\\beta} \\color{#A4BD0A}{D_{it}} \\color{black}{\\ + \\epsilon_{it}}\n\\]\nTWFE models with R\nThere are a few different ways to do fixed effects regression with clustered robust standard errors in R. In Stata you do this:\nreg primary treatment i.year i.id, cluster(id)\nIn R, you can use the standard lm() function, but it treats country and year as regular explanatory variables, so it includes them in the results from summary() or tidy(). If you don’t want overly long and detailed results tables, you have to filter those results out.\n\nmodel_lm &lt;- lm(primary ~ treatment + country + factor(year),\n               data = fpe_primary)\n\ntidy(model_lm) %&gt;%\n  filter(!str_detect(term, \"country\"), !str_detect(term, \"year\"))\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)     72.4      4.63     15.6  4.23e-44\n## 2 treatment       20.4      2.75      7.43 5.82e-13\nglance(model_lm)\n## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC deviance\n##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n## 1     0.768         0.742  14.7      29.8 1.30e-110    49 -1985. 4073. 4287.   94828.\n## # ℹ 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nThese observations are clustered by country, so we should probably cluster our standard errors to capture within-country errors. Using lm() doesn’t let you automatically create robust or clustered standard errors. You can use lmtest::coeftest() to do that after the fact, though. We can copy Stata’s standard errors precisely if we (1) specify the variance-covariance matrix using the vcovCl() function from the sandwich library, and (2) specify the degrees of freedom to use, based on the number of countries in the data, minus 1.\n\ndf_primary &lt;- fpe_primary %&gt;% distinct(country) %&gt;% nrow()\n\nmodel_lm_clustered &lt;- lmtest::coeftest(model_lm,\n                                       vcov = sandwich::vcovCL,\n                                       cluster = ~country,\n                                       df = df_primary - 1,\n                                       # Keep original model so modelsummary shows R2\n                                       save = TRUE)\n\ntidy(model_lm_clustered) %&gt;%\n  filter(!str_detect(term, \"country\"), !str_detect(term, \"year\"))\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic       p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n## 1 (Intercept)     72.4      5.83     12.4  0.00000000601\n## 2 treatment       20.4      9.12      2.24 0.0418\n\nYou can also use fancier regression functions that can handle fixed effects more systematically. The lm_robust() function from the estimatr package works really well for defining special fixed effects that are automatically omitted from results tables and returning robust and optionally clustered standard errors:\n\nmodel_lm_robust &lt;- lm_robust(primary ~ treatment,\n                             fixed_effects = ~ country + year,\n                             data = fpe_primary,\n                             clusters = country, se_type = \"stata\")\n\ntidy(model_lm_robust)\n##        term estimate std.error statistic p.value conf.low conf.high df outcome\n## 1 treatment     20.4      9.12      2.24  0.0418    0.867        40 14 primary\nglance(model_lm_robust)\n##   r.squared adj.r.squared statistic p.value df.residual nobs se_type\n## 1     0.768         0.742        NA      NA          14  490   stata\n\nThe feols() function from the fixest package also works well, though you need to change one of the default settings to get the same SEs as Stata when you have a small sample with nested fixed effects like we have here. By default, feols() will nest the fixed effect errors (which is also what Stata’s reghdfe package does), but you can tell it to use the full set of country and year fixed effect errors with the dof argument (thanks to Grant McDermott for pointing this out!):\n\nmodel_feols &lt;- feols(primary ~ treatment | country + year,\n                     data = fpe_primary,\n                     cluster = ~ country,\n                     dof = dof(fixef.K = \"full\"))\n## Warning in dof(fixef.K = \"full\"): The function 'dof' is deprecated. Please use function\n## 'ssc' instead.\n## Warning: feols(fml = pri...: dof is not a valid argument for function feols.\ntidy(model_feols)\n## # A tibble: 1 × 5\n##   term      estimate std.error statistic p.value\n##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 treatment     20.4      8.98      2.28  0.0391\nglance(model_feols)\n## # A tibble: 1 × 9\n##   r.squared adj.r.squared within.r.squared pseudo.r.squared sigma  nobs   AIC   BIC logLik\n##       &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1     0.768         0.742            0.111               NA  14.7   490 4071. 4280. -1985.\n\nWe can show these all in a side-by-side table using the modelsummary package. Conveniently, modelsummary() also lets you adjust standard errors on the fly with the vcov argument, so we could theoretically handle all the clustering here instead of inside lmtest::coeftest(), lm_robust(), or feols(). But since we already specified the clusters above, we’ll just use those.\n\nCode# Look at secondary schools too\nmodel_lm_sec &lt;- lm(secondary ~ treatment + country + factor(year),\n                   data = fpe_secondary)\n\ndf_secondary &lt;- fpe_secondary %&gt;% distinct(country) %&gt;% nrow()\n\nmodel_lm_clustered_sec &lt;- lmtest::coeftest(model_lm_sec,\n                                           vcov = sandwich::vcovCL,\n                                           cluster = ~country,\n                                           df = df_secondary - 1,\n                                           save = TRUE)\n\nmodel_lm_robust_sec &lt;- lm_robust(secondary ~ treatment,\n                                 fixed_effects = ~ country + year,\n                                 data = fpe_secondary,\n                                 clusters = country, se_type = \"stata\")\n\nmodel_feols_sec &lt;- feols(secondary ~ treatment | country + year,\n                         data = fpe_secondary,\n                         cluster = ~ country,\n                         dof = dof(fixef.K = \"full\"))\n\n# Define the goodness-of-fit stats to include\ngof_stuff &lt;- tribble(\n  ~raw, ~clean, ~fmt,\n  \"nobs\", \"N\", 0,\n  \"r.squared\", \"R²\", 3\n)\n\n# Define extra rows at the end of the table\nextra_rows &lt;- tribble(\n  ~term, ~a, ~b, ~c, ~d, ~e, ~f,\n  \"Country fixed effects\", \"•\", \"•\", \"•\", \"•\", \"•\", \"•\",\n  \"Year fixed effects\", \"•\", \"•\", \"•\", \"•\", \"•\", \"•\"\n)\n\nmodelsummary(list(\"&lt;code&gt;lm()&lt;/code&gt;\" = model_lm_clustered,\n                  \"&lt;code&gt;lm_robust()&lt;/code&gt;\" = model_lm_robust,\n                  \"&lt;code&gt;feols()&lt;/code&gt;\" = model_feols,\n                  \"&lt;code&gt;lm()&lt;/code&gt;\" = model_lm_clustered_sec,\n                  \"&lt;code&gt;lm_robust()&lt;/code&gt;\" = model_lm_robust_sec,\n                  \"&lt;code&gt;feols()&lt;/code&gt;\" = model_feols_sec),\n             coef_rename = c(\"treatment\" = \"Treatment\"),\n             estimate = \"{estimate}\",\n             statistic = c(\"s.e. = {std.error}\", \"p = {p.value}\"),\n             coef_omit = \"^country|^factor|Intercept\",\n             gof_map = gof_stuff,\n             add_rows = extra_rows,\n             escape = FALSE, output = \"kableExtra\") %&gt;%\n  add_header_above(c(\" \" = 1, \"Primary enrollment\" = 3, \"Secondary enrollment\" = 3)) %&gt;%\n  kable_styling(htmltable_class = \"table table-sm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimary enrollment\n\n\nSecondary enrollment\n\n\n\n\nlm()\nlm_robust()\nfeols()\nlm()\nlm_robust()\nfeols()\n\n\n\n\nTreatment\n20.428\n20.428\n20.428\n−0.468\n−0.468\n−0.468\n\n\n\ns.e. = 9.120\ns.e. = 9.120\ns.e. = 8.979\ns.e. = 3.081\ns.e. = 3.081\ns.e. = 3.016\n\n\n\np = 0.042\np = 0.026\np = 0.039\np = 0.881\np = 0.879\np = 0.879\n\n\nN\n490\n490\n490\n369\n369\n369\n\n\nR²\n\n0.768\n0.768\n\n0.942\n0.942\n\n\nCountry fixed effects\n•\n•\n•\n•\n•\n•\n\n\nYear fixed effects\n•\n•\n•\n•\n•\n•\n\n\n\n\n\nAll these models show the same result: eliminating primary school fees caused primary school enrollment to increase by 20.4 percentage points. That’s astounding! Removing these fees doesn’t have any effect on secondary enrollment though.\nTWFE estimate with residualized treatment weights\nThese regressions are simple enough to run with lm() or lm_robust(), but there’s an issue with differential timing here. These 15 countries each passed laws eliminating fees in different years:\n\nplot_fpe_start &lt;- fpe_raw %&gt;%\n  filter(year == fpe_year) %&gt;%\n  arrange(fpe_year, country) %&gt;%\n  mutate(country = fct_inorder(country))\n\nggplot(plot_fpe_start, aes(y = fct_rev(country))) +\n  geom_segment(aes(x = fpe_year, xend = 2015, yend = country),\n               size = 3, color = \"#D6EB52\") +\n  geom_text(aes(x = fpe_year + 0.2), label = \"▶\", family = \"Arial Unicode MS\",\n            size = 8, color = \"#A4BD0A\") +\n  labs(x = \"Year free primary education implemented\", y = NULL) +\n  theme_clean()\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nThis timing could cause problems. To see these issues, let’s calculate the ATE (\\(\\beta\\)) with that fancy schmancy Frisch-Waugh-Lovell theorem with residualized treatment weights. There’s one key difference here when calculating the residuals though. With the simple model earlier, the treatment residuals were just \\(D_i - \\bar{D}\\), or the residuals from lm(treatment ~ 1). In TWFE situations, though, treatment residuals need to account for both country and year fixed effects, or lm(treatment ~ country + year):\n\\[\n\\color{#0599B0}{\\beta^{\\text{TWFE}}} \\color{black}{= \\sum_{it}} \\color{#FFA615}{Y_{it}} \\color{black} \\left( \\frac{\\color{#353D03}{\\tilde{D}_{it}}}{\\sum_{it} \\color{#353D03}{\\tilde{D}_{it}}^2} \\right)\n\\]\nOr, with code:\n\ntrt_resid_primary &lt;- lm(treatment ~ country + factor(year), data = fpe_primary)\ntrt_resid_secondary &lt;- lm(treatment ~ country + factor(year), data = fpe_secondary)\n\nfpe_primary_weights &lt;- fpe_primary %&gt;%\n  mutate(treatment_resid = residuals(trt_resid_primary)) %&gt;%\n  mutate(treatment_weight = treatment_resid / sum(treatment_resid^2))\n\nfpe_secondary_weights &lt;- fpe_secondary %&gt;%\n  mutate(treatment_resid = residuals(trt_resid_secondary)) %&gt;%\n  mutate(treatment_weight = treatment_resid / sum(treatment_resid^2))\n\nfpe_primary_weights %&gt;%\n  summarize(twfe_beta_primary = sum(primary * treatment_weight))\n## # A tibble: 1 × 1\n##   twfe_beta_primary\n##               &lt;dbl&gt;\n## 1              20.4\n\nfpe_secondary_weights %&gt;%\n  summarize(twfe_beta_secondary = sum(secondary * treatment_weight))\n## # A tibble: 1 × 1\n##   twfe_beta_secondary\n##                 &lt;dbl&gt;\n## 1              -0.468\n\nWhoa! It still works this way. This still blows my mind every time."
  },
  {
    "objectID": "blog/2021/08/25/twfe-diagnostics/index.html#jakielas-diagnostics",
    "href": "blog/2021/08/25/twfe-diagnostics/index.html#jakielas-diagnostics",
    "title": "Exploring Pamela Jakiela’s simple TWFE diagnostics with R",
    "section": "Jakiela’s diagnostics",
    "text": "Jakiela’s diagnostics\nHowever, because of the differential treatment timing, there are some issues with weighting. Remember from the simple example with fake data that observations in the treatment group typically have positive treatment weights, while those in the comparison/control group have negative weights. With TWFE, some observations’ weights switch directions. There are systematic reasons for this. According to Jakiela (2021, 5), negative weights in treated observations are more likely in (1) early adopter countries, since the country-level treatment mean is high, and (2) later years, since the year-level treatment mean is higher.\nHaving negative weights on treated observations isn’t necessarily bad! It’s often just a mathematical artefact, and if you have (1) enough never-treated observations and (2) enough pre-treatment data, and if (3) the treatment effects are homogenous across all countries, it won’t be a problem. But if you don’t have enough data, your results will be biased and distorted for later years and for early adopters.\nStarting in section 3 of her paper, Jakiela presents a set of diagnostics to see how bad these distortions might be. We need to answer two questions:\n\nDo any treated units get negative weight when calculating \\(\\beta^{\\text{TWFE}}\\)? Check this by looking at the weights\nCan we reject the hypothesis that the treatment effects are homogenous? Check this by looking at the relationship between \\(\\tilde{Y}_{it}\\) and \\(\\tilde{D}_{it}\\). The slope shouldn’t be different.\n\n3.1: Do treated observations receive negative weights?\nFor this first diagnostic, we need to investigate patterns in the residualized weights. Some of the treated country-year observations have negative weight:\n\n# Total treated in primary data\nn_treated_primary &lt;- fpe_primary_weights %&gt;%\n  filter(treatment == 1) %&gt;%\n  nrow()\n\n# Total treated in secondary data\nn_treated_secondary &lt;- fpe_secondary_weights %&gt;%\n  filter(treatment == 1) %&gt;%\n  nrow()\n\n# Negatively weighted treated observations in the primary data\nn_treated_negative_primary &lt;- fpe_primary_weights %&gt;%\n  filter(treatment_weight &lt; 0 & treatment == 1) %&gt;%\n  nrow()\n\nn_treated_negative_primary\n## [1] 50\nn_treated_negative_primary / n_treated_primary\n## [1] 0.259\n\n# Negatively weighted treated observations in the secondary data\nn_treated_negative_secondary &lt;- fpe_secondary_weights %&gt;%\n  filter(treatment_weight &lt; 0 & treatment == 1) %&gt;%\n  nrow()\n\nn_treated_negative_secondary\n## [1] 36\nn_treated_negative_secondary / n_treated_secondary\n## [1] 0.261\n\nRoughly a quarter of the treated observations in both the primary and secondary enrollment data are negatively weighted. That might be bad.\nWe can look at the distribution of these weights too:\n\nCodeplot_weights_primary &lt;- fpe_primary_weights %&gt;%\n  mutate(treatment_fct = factor(treatment, labels = c(\"Untreated\", \"Treated\"), ordered = TRUE)) %&gt;%\n  mutate(oh_no = (treatment == 1 & treatment_weight &lt; 0) | (treatment == 0 & treatment_weight &gt; 0))\n\nplot_weights_secondary &lt;- fpe_secondary_weights %&gt;%\n  mutate(treatment_fct = factor(treatment, labels = c(\"Untreated\", \"Treated\"), ordered = TRUE)) %&gt;%\n  mutate(oh_no = (treatment == 1 & treatment_weight &lt; 0) | (treatment == 0 & treatment_weight &gt; 0))\n\nhist_primary &lt;- ggplot(plot_weights_primary,\n                       aes(x = treatment_weight, fill = treatment_fct, alpha = oh_no)) +\n  geom_histogram(binwidth = 0.002, color = \"white\", boundary = 0,\n                 position = position_identity()) +\n  geom_vline(xintercept = 0, color = \"#FF4136\", size = 0.5) +\n  scale_alpha_manual(values = c(0.6, 1)) +\n  scale_fill_viridis_d(option = \"rocket\", begin = 0.2, end = 0.8) +\n  labs(x = \"Residualized treatment\", y = \"Count\",\n       title = \"Primary school enrollment\") +\n  guides(fill = \"none\", alpha = \"none\") +\n  facet_wrap(vars(treatment_fct), ncol = 1) +\n  theme_clean()\n\nhist_secondary &lt;- ggplot(plot_weights_secondary,\n                         aes(x = treatment_weight, fill = treatment_fct, alpha = oh_no)) +\n  geom_histogram(binwidth = 0.002, color = \"white\", boundary = 0,\n                 position = position_identity()) +\n  geom_vline(xintercept = 0, color = \"#FF4136\", size = 0.5) +\n  scale_alpha_manual(values = c(0.6, 1)) +\n  scale_fill_viridis_d(option = \"rocket\", begin = 0.2, end = 0.8) +\n  labs(x = \"Residualized treatment\", y = \"Count\",\n       title = \"Secondary school enrollment\") +\n  guides(fill = \"none\", alpha = \"none\") +\n  facet_wrap(vars(treatment_fct), ncol = 1) +\n  theme_clean()\n\nhist_primary + hist_secondary\n\n\n\n\n\n\n\nThese histograms confirm the proportions we found earlier—about 25% of the treated observations have negative weights.\nWhich treated country-years are getting these negative weights, though? According to Jakiela, early adopters and later years are more likely to have this switch happen. Let’s make another plot:\n\nCodeplot_waffle_primary &lt;- fpe_primary_weights %&gt;%\n  mutate(weight_fill = case_when(\n    treatment_resid &lt; 0 & treatment ~ \"Treatment observations, negative weight\",\n    treatment_resid &gt; 0 & treatment ~ \"Treatment observations, positive weight\",\n    !treatment ~ \"Comparison observations\"\n  )) %&gt;%\n  arrange(desc(fpe_year), desc(country)) %&gt;%\n  mutate(country = fct_inorder(country)) %&gt;%\n  mutate(weight_fill = fct_inorder(weight_fill))\n\nplot_waffle_secondary &lt;- fpe_secondary_weights %&gt;%\n  mutate(weight_fill = case_when(\n    treatment_resid &lt; 0 & treatment ~ \"Treatment observations, negative weight\",\n    treatment_resid &gt; 0 & treatment ~ \"Treatment observations, positive weight\",\n    !treatment ~ \"Comparison observations\"\n  )) %&gt;%\n  arrange(desc(fpe_year), desc(country)) %&gt;%\n  mutate(country = fct_inorder(country)) %&gt;%\n  mutate(weight_fill = fct_inorder(weight_fill))\n\nwaffle_primary &lt;- ggplot(plot_waffle_primary,\n                         aes(x = year, y = country, fill = weight_fill)) +\n  geom_tile(color = \"white\", size = 0.5) +\n  scale_fill_manual(values = c(\"grey50\", \"#0074D9\", \"#FF4136\"),\n                    guide = guide_legend(reverse = TRUE),\n                    name = NULL) +\n  scale_x_continuous(expand = expansion(add = 0.5),\n                     breaks = seq(1980, 2015, 5)) +\n  labs(x = NULL, y = NULL, title = \"Primary school enrollment\") +\n  coord_equal() +\n  theme_clean() +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank(),\n        legend.key.size = unit(0.8, \"lines\"))\n\nwaffle_secondary &lt;- ggplot(plot_waffle_secondary,\n                         aes(x = year, y = country, fill = weight_fill)) +\n  geom_tile(color = \"white\", size = 0.5) +\n  scale_fill_manual(values = c(\"grey50\", \"#0074D9\", \"#FF4136\"),\n                    guide = guide_legend(reverse = TRUE),\n                    name = NULL) +\n  scale_x_continuous(expand = expansion(add = 0.5),\n                     breaks = seq(1980, 2015, 5)) +\n  labs(x = NULL, y = NULL, title = \"Secondary school enrollment\") +\n  coord_equal() +\n  theme_clean() +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank(),\n        legend.key.size = unit(0.8, \"lines\"))\n\nwaffle_primary / waffle_secondary\n\n\n\n\n\n\n\nAnd that is indeed the case! Malawi, Ethiopia, Ghana, Uganda, and Cameroon all eliminated fees before 2000, and they start getting negative weights after 2005.\n3.2: Testing the homogeneity assumption directly\nWe thus have issues with negative weighting here for early adopting countries and later country-years. However, as long as the treatment effects are homogenous—or that the elimination of school fees has the same effect across time and country—this negative weighting isn’t an issue. If treatment effects are heterogenous—especially if the effects change over time within treated countries—the TWFE estimate will be severely biased.\nJakiela’s second diagnostic tests this assumption of homogeneity based on the mathematical relationship between the residuals of the outcome variable ($\\tilde{Y}_{it}$) and the residuals of the treatment variable ($\\tilde{D}_{it}$). Essentially, if there’s no difference in slopes across treated and untreated observations in a regression of $\\tilde{Y}_{it} = \\tilde{D}_{it}$, there’s no evidence for heterogeneity and all is well.\nThis wasn’t intuitive for me since I’m not a real econometrician, so I had to dig into the math to try to wrap my head around it, and I’m still only like 80% sure I’ve got it :)\nWe can think of the outcome \\(Y_{it}\\) as a combination of three different moving parts: (1) an initial baseline value for each country, or \\(\\mu_{i}\\), (2) a constant change in each additional year in the absence of treatment (i.e. what would naturally happen over time regardless of an intervention), or \\(\\eta_t\\), and (3) a homogenous treatment effect, or \\(\\delta\\) (doesn’t have subscripts for \\(i\\) or \\(t\\)—it’s the same across year and country):\n\\[\n\\color{gray}{\\overbrace{\\color{#FFA615}{Y_{it}}}^{\\text{Outcome}}} = \\color{gray}{\\overbrace{\\color{black}{\\mu_i}}^{\\substack{\\text{Baseline} \\\\ \\text{outcome} \\\\ \\text{at } t = 1}}} + \\color{black} \\sum_{\\tau = 1}^t  \\color{gray}{\\overbrace{\\color{black}{\\eta_t}}^{\\substack{\\text{Global} \\\\ \\text{time} \\\\ \\text{trend}}}} + \\color{gray}{\\overbrace{\\color{black}{\\delta}}^{\\substack{\\text{Homogenous} \\\\ \\text{treatment} \\\\ \\text{effect}}}} \\color{#A4BD0A}{D_{it}}\n\\]\nThis all makes perfect sense is is nice and intuitive. It’s how I typically generate fake data too (see the outcome_baseline up at the beginning of this post, for instance) and it’s a neat way of thinking about the separate components of a data generating process.\nAccording to Jakiela, if the treatment effect \\(\\delta\\) is truly homogenous over time and country, the residuals of the outcome (i.e. the residuals from a model like lm(outcome ~ country + year), or \\(\\tilde{Y}_{it}\\)) should have a linear relationship to the residualized treatment (i.e. the residuals from a model like lm(treatment ~ country + year), or \\(\\delta\\tilde{D}_{it}\\)).\nAgain, due to my non-econometricianness, this idea that \\(\\tilde{Y}_{it}\\) should be related to \\(\\delta\\tilde{D}_{it}\\) didn’t click for me until I thought about this formula in more regression-y terms. Consider this pseudo-regression equation:\n\\[\n\\overbrace{Y_{it}}^{\\text{Outcome}} = \\overbrace{\\alpha_i}^{\\text{Intercept}} + \\overbrace{\\beta_t}^{\\text{Slope / trend}} + \\overbrace{\\epsilon_{it}}^{\\text{Residuals}}\n\\]\nHere, if we run a regression like lm(outcome ~ country + year), the intercept \\(\\alpha_i\\) is analogous to \\(\\mu_i\\), the slope \\(\\beta_t\\) is like \\(\\eta_t\\), and whatever’s leftover (i.e. \\(\\epsilon_{it}\\), or the residualized outcome \\(\\tilde{Y}_{it}\\)) is similar to \\(\\delta D_{it}\\). \\(\\delta D_{it}\\) isn’t the same as \\(\\delta \\tilde{D}_{it}\\), but we can calculate the residualized treatment the same way by removing country and year effects (like lm(treatment ~ country + year)). Basically, by residualizing both the treatment and outcome, and thus removing country and time effects (or the baseline country effect \\(\\mu_i\\) and the time trend \\(\\eta_t\\)) from each, the residuals that remain are related to \\(\\delta\\).\nAs long as \\(\\delta\\) is constant across time and country—or as long as the effect is homogenous—it should show up equally in the residuals for both the outcome and treatment for both treated and untreated observations. (I THINK! I’M NOT SURE ABOUT THIS)\nWe already calculated the treatment residuals (or \\(\\tilde{D}_{it}\\)) when we looked at treatment weights earlier, so now we just need to calculate the outcome residuals (\\(\\tilde{Y}_{it}\\)). We can then see if the relationship between \\(\\tilde{D}_{it}\\) and \\(\\tilde{D}_{it}\\) looks the same for treated and untreated observations—the slopes in the two groups should be statistically indistinguishable from each other.\n\n# Build models for the residualized outcomes\nout_resid_primary &lt;- lm(primary ~ country + factor(year), data = fpe_primary)\nout_resid_secondary &lt;- lm(secondary ~ country + factor(year), data = fpe_secondary)\n\n# Add residuals to data with weights\nfpe_primary_weights &lt;- fpe_primary_weights %&gt;%\n  mutate(out_resid = residuals(out_resid_primary))\n\nfpe_secondary_weights &lt;- fpe_secondary_weights %&gt;%\n  mutate(out_resid = residuals(out_resid_secondary))\n\nWe can check the similarity of these slopes a couple different ways. First, let’s plot the residuals:\n\nCodeplot_out_trt_primary &lt;- ggplot(fpe_primary_weights,\n                               aes(x = treatment_resid, y = out_resid, color = factor(treatment))) +\n  geom_point(size = 0.75, alpha = 0.5) +\n  geom_smooth(aes(linetype = \"Loess\"), method = \"loess\", size = 1, se = FALSE, alpha = 0.5) +\n  geom_smooth(aes(linetype = \"OLS\"), method = \"lm\", se = FALSE) +\n  scale_color_viridis_d(option = \"rocket\", begin = 0.2, end = 0.8,\n                        labels = c(\"Untreated\", \"Treated\")) +\n  scale_linetype_manual(values = c(\"OLS\" = \"solid\", \"Loess\" = \"21\"),\n                        guide = guide_legend(override.aes = list(color = \"grey30\"))) +\n  labs(x = \"Residualized treatment\", y = \"Residualized outcome\",\n       title = \"Primary school enrollment\", color = NULL, linetype = NULL) +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\nplot_out_trt_secondary &lt;- ggplot(fpe_secondary_weights,\n                                 aes(x = treatment_resid, y = out_resid, color = factor(treatment))) +\n  geom_point(size = 0.75, alpha = 0.5) +\n  geom_smooth(aes(linetype = \"Loess\"), method = \"loess\", size = 1, se = FALSE, alpha = 0.5) +\n  geom_smooth(aes(linetype = \"OLS\"), method = \"lm\", se = FALSE) +\n  scale_color_viridis_d(option = \"rocket\", begin = 0.2, end = 0.8,\n                        labels = c(\"Untreated\", \"Treated\")) +\n  scale_linetype_manual(values = c(\"OLS\" = \"solid\", \"Loess\" = \"21\"),\n                        guide = guide_legend(override.aes = list(color = \"grey30\"))) +\n  labs(x = \"Residualized treatment\", y = \"Residualized outcome\",\n       title = \"Secondary school enrollment\", color = NULL, linetype = NULL) +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\nplot_out_trt_primary | plot_out_trt_secondary\n\n\n\n\n\n\n\nFor primary enrollment, the lines for treated and untreated observations look like they have similar slopes. When using Loess lines, the two groups don’t align perfectly, and the relationship between residualized treatment and residualized outcome doesn’t look perfectly linear. When secondary enrollment is the outcome, the lines for the treated and untreated groups seem to switch directions—the slope is negative for untreated observations but positive for treated. This is a bad sign for the homogeneity assumption.\nWe can statistically test if the slopes in two groups are the same or not by using an interaction term in a regression model:\n\ncheck_slopes_primary &lt;- lm(out_resid ~ treatment_resid * factor(treatment),\n                           data = fpe_primary_weights)\n\ncheck_slopes_secondary &lt;- lm(out_resid ~ treatment_resid * factor(treatment),\n                           data = fpe_secondary_weights)\n\nmodelsummary(list(\"Primary enrollment\" = check_slopes_primary,\n                  \"Secondary enrollment\" = check_slopes_secondary),\n             coef_rename = c(\"treatment_resid\" = \"Residualized treatment\",\n                             \"factor(treatment)1\" = \"Treatment group\",\n                             \"treatment_resid:factor(treatment)1\" = \"Treatment group × residualized treatment\",\n                             \"(Intercept)\" = \"Intercept\"),\n             estimate = \"{estimate}\",\n             statistic = c(\"s.e. = {std.error}\", \"p = {p.value}\"),\n             gof_map = tribble(\n               ~raw, ~clean, ~fmt,\n               \"nobs\", \"N\", 0,\n               \"r.squared\", \"R²\", 3\n             ),\n             escape = FALSE, output = \"kableExtra\") %&gt;%\n  kable_styling(htmltable_class = \"table table-sm\")\n\n\n\n\nPrimary enrollment\nSecondary enrollment\n\n\n\nIntercept\n0.320\n−0.202\n\n\n\ns.e. = 0.894\ns.e. = 0.276\n\n\n\np = 0.721\np = 0.466\n\n\nResidualized treatment\n23.761\n−2.902\n\n\n\ns.e. = 3.968\ns.e. = 1.357\n\n\n\np =\np = 0.033\n\n\nTreatment group\n0.341\n−0.189\n\n\n\ns.e. = 1.506\ns.e. = 0.473\n\n\n\np = 0.821\np = 0.690\n\n\nTreatment group × residualized treatment\n−7.806\n5.248\n\n\n\ns.e. = 6.073\ns.e. = 1.993\n\n\n\np = 0.199\np = 0.009\n\n\nN\n490\n369\n\n\nR²\n0.114\n0.019\n\n\n\n\n\nThe coefficient for “Treatment group × residualized treatment” shows the change in slope between the two groups. For primary enrollment, being in the treatment group reduces the slope by 7.8 (so from 23.761 to 15.961), but the standard errors around that change are huge and the p-value is not significant (p = 0.199). For secondary enrollment, though, being in the treatment group increases the slope by 5.3 (from -2.9 to positive 2.4), and that change is statistically significant (p = 0.009).\nSo for primary enrollment, there’s not enough evidence to reject the hypothesis that the slopes are the same (or the hypothesis that the effect is homogenous), so we’ll treat the effect as homogenous. Yay! That means we don’t need to worry so much about the negatively-weighted treated country-years. For secondary enrollment, though, there’s pretty strong evidence that the slopes aren’t the same, which means the treatment effect is likely heterogenous, which also means that the treated country-years with negative weights are biasing the results substantially and we should thus be worried.\nWhen there are heterogenous treatment effects, we don’t need to give up! This is what Callaway and Sant’Anna (2020) and Sun and Abraham (2020) show how to do in their papers (and see Baker, Larcker, and Wang (2021) for a general overview of those approaches). If treatment effects are indeed homogenous, there’s no need to turn to these fancier TWFE estimators—but if they are heterogenous, there are new tools that help."
  },
  {
    "objectID": "blog/2021/08/25/twfe-diagnostics/index.html#jakielas-robustness-checks",
    "href": "blog/2021/08/25/twfe-diagnostics/index.html#jakielas-robustness-checks",
    "title": "Exploring Pamela Jakiela’s simple TWFE diagnostics with R",
    "section": "Jakiela’s robustness checks",
    "text": "Jakiela’s robustness checks\nNow that we’ve run those two diagnostics (checked for negative weights in treated units and checked for treatment effect homogeneity), we can do some neat robustness checks to see how badly these negative weight issues bias the TWFE estimate.\nAs long as we assume that treatment effects are homogenous (remember that \\(\\delta\\) in the equation above doesn’t have subscripts for country or year), we can safely drop some observations from the data and still find the same result. We can also strategically drop observations to check if negative treatment weights influence the results. Jakiela presents three different robustness checks that all involve dropping different categories of observations:\n\nExclude later years\nLimit how many post-treatment years are kept\nExclude individual countries\n\nExclude later years\nSince negative treatment weights for treated country-years are more likely to appear in later years in the data, we can drop those later years and see how the treatment effect changes.\nFor primary schools, if we cut the data off at 2000, the treatment effect is 31.8 instead of the 20.4 we’ve been seeing with the full data. That estimate is higher, but it also has wider errors. Both the coefficient and the errors shrink as we add more years to the data, and the estimate eventually settles on ≈20 by 2005, which is also when negatively weighted treated rows start appearing.\nFor secondary schools, the treatment effect hovers around 0 is never statistically significant regardless of the cutoff year.\n\nCodedifferent_max_years_primary &lt;- tibble(end_year = 2000:2015) %&gt;%\n  # Nest a filtered dataset in a cell for each year\n  mutate(data = map(end_year, ~filter(fpe_primary, year &lt;= .))) %&gt;%\n  # Calculate the TWFE estimate for each dataset\n  mutate(model = map(data, ~lm_robust(primary ~ treatment,\n                                      fixed_effects = ~ country + year,\n                                      data = .,\n                                      clusters = country, se_type = \"stata\"))) %&gt;%\n  # Extract a data frame of the results\n  mutate(tidied = map(model, ~tidy(., conf.int = TRUE))) %&gt;%\n  # Calculate residuals and treatment weights\n  mutate(model_resid = map(data, ~lm(treatment ~ country + factor(year), data = .)),\n         treatment_resid = map(model_resid, ~residuals(.)),\n         treatment_weight = map(treatment_resid, ~ . / sum(.^2)))\n\n# Calculate how many treated observations have negative weights\nprop_negative_primary &lt;- different_max_years_primary %&gt;%\n  unnest(c(data, treatment_resid, treatment_weight)) %&gt;%\n  group_by(end_year) %&gt;%\n  summarize(n_treated_negative_weight = sum(treatment_weight &lt; 0 & treatment == 1),\n            n_treated = sum(treatment == 1),\n            prop_treated_negative_weight = n_treated_negative_weight / n_treated) %&gt;%\n  mutate(prop_nice = percent(prop_treated_negative_weight, accuracy = 1))\n\n# Extract tidied results for plotting\ncoefs_to_plot_primary &lt;- different_max_years_primary %&gt;%\n  unnest(tidied)\n\ndifferent_max_years_secondary &lt;- tibble(end_year = 2000:2015) %&gt;%\n  mutate(data = map(end_year, ~filter(fpe_secondary, year &lt;= .))) %&gt;%\n  mutate(model = map(data, ~lm_robust(secondary ~ treatment,\n                                      fixed_effects = ~ country + year,\n                                      data = .,\n                                      clusters = country, se_type = \"stata\"))) %&gt;%\n  mutate(tidied = map(model, ~tidy(., conf.int = TRUE))) %&gt;%\n  mutate(model_resid = map(data, ~lm(treatment ~ country + factor(year), data = .)),\n         treatment_resid = map(model_resid, ~residuals(.)),\n         treatment_weight = map(treatment_resid, ~ . / sum(.^2)))\n\nprop_negative_secondary &lt;- different_max_years_secondary %&gt;%\n  unnest(c(data, treatment_resid, treatment_weight)) %&gt;%\n  group_by(end_year) %&gt;%\n  summarize(n_treated_negative_weight = sum(treatment_weight &lt; 0 & treatment == 1),\n            n_treated = sum(treatment == 1),\n            prop_treated_negative_weight = n_treated_negative_weight / n_treated) %&gt;%\n  mutate(prop_nice = percent(prop_treated_negative_weight, accuracy = 1))\n\ncoefs_to_plot_secondary &lt;- different_max_years_secondary %&gt;%\n  unnest(tidied)\n\n# Coefficient plot\nate_primary &lt;- ggplot(coefs_to_plot_primary, aes(x = end_year, y = estimate)) +\n  geom_hline(yintercept = 0, color = \"#FF2E00\", size = 1) +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high),\n                  color = \"#0599B0\", size = 1, fatten = 2) +\n  labs(x = NULL, y = \"TWFE-based treatment effect\",\n       title = \"Primary school enrollment\") +\n  theme_clean() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank())\n\nate_secondary &lt;- ggplot(coefs_to_plot_secondary, aes(x = end_year, y = estimate)) +\n  geom_hline(yintercept = 0, color = \"#FF2E00\", size = 1) +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high),\n                  color = \"#0599B0\", size = 1, fatten = 2) +\n  labs(x = NULL, y = \"TWFE-based treatment effect\",\n       title = \"Secondary school enrollment\") +\n  theme_clean() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank())\n\n# Bar plot\nprop_neg_primary &lt;- ggplot(filter(prop_negative_primary,\n                                  prop_treated_negative_weight &gt; 0),\n                           aes(x = end_year, y = prop_treated_negative_weight)) +\n  geom_col(fill = \"#FF4136\") +\n  geom_text(aes(label = prop_nice),\n            nudge_y = 0.02, size = 2.5,\n            family = \"Barlow Semi Condensed Bold\", color = \"#FF4136\") +\n  coord_cartesian(xlim = c(2000, 2015)) +\n  labs(x = \"Last year included in data\", y = NULL,\n       subtitle = \"% of treated observations with negative weight\") +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank())\n\nprop_neg_secondary &lt;- ggplot(filter(prop_negative_secondary,\n                                    prop_treated_negative_weight &gt; 0),\n                             aes(x = end_year, y = prop_treated_negative_weight)) +\n  geom_col(fill = \"#FF4136\") +\n  geom_text(aes(label = prop_nice),\n            nudge_y = 0.02, size = 2.5,\n            family = \"Barlow Semi Condensed Bold\", color = \"#FF4136\") +\n  coord_cartesian(xlim = c(2000, 2015)) +\n  labs(x = \"Last year included in data\", y = NULL,\n       subtitle = \"% of treated observations with negative weight\") +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank())\n\n((ate_primary / prop_neg_primary) + plot_layout(heights = c(0.8, 0.2))) |\n  ((ate_secondary / prop_neg_secondary) + plot_layout(heights = c(0.8, 0.2)))\n\n\n\n\n\n\n\nChange how many post-treatment years are kept\nDropping rows based on end years like this is neat, but it doesn’t take into account the differential treatment timing. Countries like Mozambique, which eliminated its fees in 2005, don’t even show up in the first few models of exclude-later-years robustness check, since those models end in 2000–2004. As an alternative, we can keep specific numbers of years post-treatment for each country. For instance, we can look at Mozambique 2, 3, 4, or 5 (and so on) years after treatment. Doing this centers all countries at \\(t = 0\\), rather than \\(t = \\text{start year}\\).\nFor primary schools in this situation, the effect is smaller and not significant when only a few post-treatment years are kept, but it stabilizes at around 20 after only 5 post-treatment years, which suggests that \\(\\delta\\) is indeed homogenous—we don’t need a ton of post-treatment data to find it. For secondary schools, though, the effect remains negative and insigificant throughout every specification.\n\nCodefpe_primary_years_since &lt;- fpe_primary %&gt;%\n  mutate(years_after = year - fpe_year)\n\nfpe_secondary_years_since &lt;- fpe_secondary %&gt;%\n  mutate(years_after = year - fpe_year)\n\ndifferent_years_after_primary &lt;- tibble(post_trt_years = 2:22) %&gt;%\n  # Nest a filtered dataset in a cell for each year\n  mutate(data = map(post_trt_years, ~filter(fpe_primary_years_since, years_after &lt;= .))) %&gt;%\n  # Calculate the TWFE estimate for each dataset\n  mutate(model = map(data, ~lm_robust(primary ~ treatment,\n                                      fixed_effects = ~ country + year,\n                                      data = .,\n                                      clusters = country, se_type = \"stata\"))) %&gt;%\n  # Extract a data frame of the results\n  mutate(tidied = map(model, ~tidy(., conf.int = TRUE))) %&gt;%\n  # Calculate residuals and treatment weights\n  mutate(model_resid = map(data, ~lm(treatment ~ country + factor(year), data = .)),\n         treatment_resid = map(model_resid, ~residuals(.)),\n         treatment_weight = map(treatment_resid, ~ . / sum(.^2)))\n\n# Calculate how many treated observations have negative weights\nprop_negative_primary &lt;- different_years_after_primary %&gt;%\n  unnest(c(data, treatment_resid, treatment_weight)) %&gt;%\n  group_by(post_trt_years) %&gt;%\n  summarize(n_treated_negative_weight = sum(treatment_weight &lt; 0 & treatment == 1),\n            n_treated = sum(treatment == 1),\n            prop_treated_negative_weight = n_treated_negative_weight / n_treated) %&gt;%\n  mutate(prop_nice = percent(prop_treated_negative_weight, accuracy = 1))\n\n# Extract tidied results for plotting\ncoefs_to_plot_primary &lt;- different_years_after_primary %&gt;%\n  unnest(tidied)\n\ndifferent_years_after_secondary &lt;- tibble(post_trt_years = 2:22) %&gt;%\n  mutate(data = map(post_trt_years, ~filter(fpe_secondary_years_since, years_after &lt;= .))) %&gt;%\n  mutate(model = map(data, ~lm_robust(secondary ~ treatment,\n                                      fixed_effects = ~ country + year,\n                                      data = .,\n                                      clusters = country, se_type = \"stata\"))) %&gt;%\n  mutate(tidied = map(model, ~tidy(., conf.int = TRUE))) %&gt;%\n  mutate(model_resid = map(data, ~lm(treatment ~ country + factor(year), data = .)),\n         treatment_resid = map(model_resid, ~residuals(.)),\n         treatment_weight = map(treatment_resid, ~ . / sum(.^2)))\n\nprop_negative_secondary &lt;- different_years_after_secondary %&gt;%\n  unnest(c(data, treatment_resid, treatment_weight)) %&gt;%\n  group_by(post_trt_years) %&gt;%\n  summarize(n_treated_negative_weight = sum(treatment_weight &lt; 0 & treatment == 1),\n            n_treated = sum(treatment == 1),\n            prop_treated_negative_weight = n_treated_negative_weight / n_treated) %&gt;%\n  mutate(prop_nice = percent(prop_treated_negative_weight, accuracy = 1))\n\ncoefs_to_plot_secondary &lt;- different_years_after_secondary %&gt;%\n  unnest(tidied)\n\n# Coefficient plot\nate_primary &lt;- ggplot(coefs_to_plot_primary, aes(x = post_trt_years, y = estimate)) +\n  geom_hline(yintercept = 0, color = \"#FF2E00\", size = 1) +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high),\n                  color = \"#0599B0\", size = 1, fatten = 2) +\n  labs(x = NULL, y = \"TWFE-based treatment effect\",\n       title = \"Primary school enrollment\") +\n  theme_clean() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank())\n\nate_secondary &lt;- ggplot(coefs_to_plot_secondary, aes(x = post_trt_years, y = estimate)) +\n  geom_hline(yintercept = 0, color = \"#FF2E00\", size = 1) +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high),\n                  color = \"#0599B0\", size = 1, fatten = 2) +\n  labs(x = NULL, y = \"TWFE-based treatment effect\",\n       title = \"Secondary school enrollment\") +\n  theme_clean() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank())\n\n# Bar plot\nprop_neg_primary &lt;- ggplot(filter(prop_negative_primary,\n                                  prop_treated_negative_weight &gt; 0),\n                           aes(x = post_trt_years, y = prop_treated_negative_weight)) +\n  geom_col(fill = \"#FF4136\") +\n  geom_text(aes(label = prop_nice),\n            nudge_y = 0.02, size = 2.5,\n            family = \"Barlow Semi Condensed Bold\", color = \"#FF4136\") +\n  coord_cartesian(xlim = c(2, 22)) +\n  labs(x = \"Post-treatment years included\", y = NULL,\n       subtitle = \"% of treated observations with negative weight\") +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank())\n\nprop_neg_secondary &lt;- ggplot(filter(prop_negative_secondary,\n                                    prop_treated_negative_weight &gt; 0),\n                             aes(x = post_trt_years, y = prop_treated_negative_weight)) +\n  geom_col(fill = \"#FF4136\") +\n  geom_text(aes(label = prop_nice),\n            nudge_y = 0.02, size = 2.5,\n            family = \"Barlow Semi Condensed Bold\", color = \"#FF4136\") +\n  coord_cartesian(xlim = c(2, 22)) +\n  labs(x = \"Last year included in data\", y = NULL,\n       subtitle = \"% of treated observations with negative weight\") +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank())\n\n((ate_primary / prop_neg_primary) + plot_layout(heights = c(0.8, 0.2))) |\n  ((ate_secondary / prop_neg_secondary) + plot_layout(heights = c(0.8, 0.2)))\n\n\n\n\n\n\n\nExclude individual countries\nFinally, since early adopter countries are more likely to have negative treatment weights, we can remove individual countries from the panel to see what their omission does to the overall TWFE estimate. For primary school enrollment, the ATE remains positive and significant across most specifications, except when Malawi, Uganda, or Namibia are excluded. But Malawi and Uganda were early adopters and thus have negatively weighted treatment observations in later years, as we saw earlier, which give them strange leverage in the the overall TWFE ATE calculation. For secondary schools, the effect remains basically zero and insignificant across all specifications except when Malawi is omitted, but that’s again because Malawi was an early adopter.\n\nCodefpe_omit_countries_primary &lt;- fpe_primary %&gt;%\n  arrange(fpe_year, country) %&gt;%\n  mutate(country_start = paste0(country, \" (\", fpe_year, \")\"),\n         country_start = fct_inorder(country_start)) %&gt;%\n  distinct(country, country_start) %&gt;%\n  mutate(data = map(country, ~filter(fpe_primary, country != .))) %&gt;%\n  mutate(model = map(data, ~lm_robust(primary ~ treatment,\n                                      fixed_effects = ~ country + year,\n                                      data = .,\n                                      clusters = country, se_type = \"stata\"))) %&gt;%\n  mutate(tidied = map(model, ~tidy(., conf.int = TRUE)))\n\ncoefs_to_plot_primary &lt;- fpe_omit_countries_primary %&gt;%\n  unnest(tidied)\n\nfpe_omit_countries_secondary &lt;- fpe_secondary %&gt;%\n  arrange(fpe_year, country) %&gt;%\n  mutate(country_start = paste0(country, \" (\", fpe_year, \")\"),\n         country_start = fct_inorder(country_start)) %&gt;%\n  distinct(country, country_start)  %&gt;%\n  mutate(data = map(country, ~filter(fpe_secondary, country != .))) %&gt;%\n  mutate(model = map(data, ~lm_robust(secondary ~ treatment,\n                                      fixed_effects = ~ country + year,\n                                      data = .,\n                                      clusters = country, se_type = \"stata\"))) %&gt;%\n  mutate(tidied = map(model, ~tidy(., conf.int = TRUE)))\n\ncoefs_to_plot_secondary &lt;- fpe_omit_countries_secondary %&gt;%\n  unnest(tidied)\n\nate_primary &lt;- ggplot(coefs_to_plot_primary, aes(x = estimate, y = fct_rev(country_start))) +\n  geom_vline(xintercept = 0, color = \"#FF2E00\", size = 1) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high),\n                  color = \"#0599B0\", size = 1, fatten = 2) +\n  labs(x = \"TWFE-based treatment effect\", y = NULL,\n       title = \"Primary school enrollment\",\n       subtitle = \"Each point represents the ATE when omitting the specified country\",\n       caption = \"Year that fees were eliminated is shown in parentheses\") +\n  coord_cartesian(xlim = c(-10, 50)) +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank())\n\nate_secondary &lt;- ggplot(coefs_to_plot_secondary, aes(x = estimate, y = fct_rev(country_start))) +\n  geom_vline(xintercept = 0, color = \"#FF2E00\", size = 1) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high),\n                  color = \"#0599B0\", size = 1, fatten = 2) +\n  labs(x = \"TWFE-based treatment effect\", y = NULL,\n       title = \"Secondary school enrollment\") +\n  coord_cartesian(xlim = c(-10, 50)) +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank())\n\nate_primary | ate_secondary"
  },
  {
    "objectID": "blog/2021/08/25/twfe-diagnostics/index.html#tldr-conclusion",
    "href": "blog/2021/08/25/twfe-diagnostics/index.html#tldr-conclusion",
    "title": "Exploring Pamela Jakiela’s simple TWFE diagnostics with R",
    "section": "tl;dr: Conclusion",
    "text": "tl;dr: Conclusion\nPhew. That was a ton of code, but coding my way through this paper and translating Jakiela’s diagnostics and robustness checks from math into R was incredibly helpful for me to start understanding all this new TWFE stuff.\nHere are the main tl;dr takeaways from her paper:\n\nWe can think of OLS as a weighted sum of outcome values - these weights are typically negative for untreated observations and positive for treated observations\nIn TWFE situations, these weights take country and year into account\nBecause of mathy reasons, treated observations can actually get negative weights—especially countries that are early adopters, and country-year observations in later years\nWe can see if these negative weights on treated observations are an issue by using a couple simple diagnostics: see how many and which treated observations have negative weights, and see if the treatment effect is homogenous across treated countries\nWe can look at which treated observations have negative weights by making some plots and exploring the data\nWe can check for the homogeniety of the treatment effect by running a regression that uses the residualized treatment and treatment status to explain the residualized outcome. If the slopes for treated and comparison observations are indistinguishable, we can safely assume treatment homogeneity.\nFinally, we can check how robust the TWFE estimate is to negative treatment weights and the assumption of homogeneity by dropping specific types of observations and checking the ATE across these modified datasets"
  },
  {
    "objectID": "blog/2021/07/20/afc-richmond-ted-lasso-cross-stitch/index.html",
    "href": "blog/2021/07/20/afc-richmond-ted-lasso-cross-stitch/index.html",
    "title": "AFC Richmond / Ted Lasso cross stitch pattern",
    "section": "",
    "text": "Downloads\n\n\n\nJump to the downloads and get your own free pattern and template files!\n\n\nApparently I now only produce cross stitch content. Thanks, pandemic.\nIn preparation for season 2 of the incredible Ted Lasso, I made a cross stitch version of the AFC Richmond crest, and I’m really happy with how it turned out!\n\n\n\nA cross stitched AFC Richmond crest\n\n\nFor the world’s enjoyment, and to spread the cheer of Ted Lasso, I’ve provided the template—along with the raw Illustrator file—for free! (With a Creative Commons license.)\nBelieve!\nDownload everything here:\n\nAFC Richmond crest cross stitch PDF (v1.0, 2021-07-20)\nAFC Richmond crest cross stitch Illustrator file\n\n\n\n\nAFC Richmond crest, from Ted Lasso\n\n\n\n\n\nCitationBibTeX citation:@online{heiss2021,\n  author = {Heiss, Andrew},\n  title = {AFC {Richmond} / {Ted} {Lasso} Cross Stitch Pattern},\n  date = {2021-07-20},\n  url = {https://www.andrewheiss.com/blog/2021/07/20/afc-richmond-ted-lasso-cross-stitch/},\n  doi = {10.59350/qvp0m-tr543},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHeiss, Andrew. 2021. “AFC Richmond / Ted Lasso Cross Stitch\nPattern.” July 20, 2021. https://doi.org/10.59350/qvp0m-tr543."
  },
  {
    "objectID": "blog/2021/01/26/bayesian-cross-stitch-sampler/index.html",
    "href": "blog/2021/01/26/bayesian-cross-stitch-sampler/index.html",
    "title": "Bayesian (cross stitch) sampler",
    "section": "",
    "text": "Downloads\n\n\n\nJump to the downloads and get your own free pattern!\n\n\nFor my latest pandemic art medium (on this, the three hundred and nineteenth day of sheltering in place), I decided to teach myself how to cross stitch. I did a ton of needlepoint as a kid and teen, but was always afraid of cross stitch because it was so much smaller and more delicate.\nI somehow stumbled across this lesson repository created by The Carpentries, an organization that normally teaches workshops on reproducible research and data scientific software like R and Python. This nifty Cross Stitch Carpentry workshop is incredibly helpful and easy to follow, and after downloading a pattern from Etsy, I successfully made my first cross stitched thing: a miniature porg.\nI next wanted to make something related to my work with data and stats. In July, I made a linocut print of the R logo, so I considered doing that with cross stitch too, but that seemed like a lot of thread. Also, I couldn’t find a good way to make a good pattern. There’s a neat Python program named ih that automatically creates embroidery and cross stitch patterns from images, and there are plenty of (sketchy?) online resources for converting images to patterns (they probably rely on ih behind the scenes?), but I like having more control over the output.\nAfter lots of googling, I found that you can use Adobe Illustrator to create cross stitch patterns (there’s an archived Facebook Live video showing the process here), which is perfect, since I use Illustrator all the time.\nSo I drew the formula for Bayes’ theorem with little boxes, added some flourishes and borders, and made my first cross stitch pattern, which I then made into an actual cross stitch: a Bayesian sampler!\n\n\n\nA cross stitched Bayesian sampler\n\n\nThe color palette is inspired from the jewel colors of the 2021 inauguration (via Cianna Bedford-Petersen’s R package). The density plots in the corners and center are stylized prior and posterior distributions. The borders on the top and bottom are stylized trace plots showing intertwined MCMC chains.\nI’ve polished the pattern in Illustrator and released it for free! (with a Creative Commons license). I’ve also provided the original Illustrator file so that you can make your own designs—just delete the stuff on the Text and Pattern layers and add little rectangles filled with some color + a 0.5 point white stroke.\nDownload everything here!\n\n“Bayesian Sampler” PDF (v1.0, 2021-01-25)\n“Bayesian Sampler” paginated PDF\n“Bayesian Sampler” Illustrator file\n\n\n\n\nA Bayesian sampler\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{heiss2021,\n  author = {Heiss, Andrew},\n  title = {Bayesian (Cross Stitch) Sampler},\n  date = {2021-01-26},\n  url = {https://www.andrewheiss.com/blog/2021/01/26/bayesian-cross-stitch-sampler/},\n  doi = {10.59350/pdqkh-czk27},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHeiss, Andrew. 2021. “Bayesian (Cross Stitch) Sampler.”\nJanuary 26, 2021. https://doi.org/10.59350/pdqkh-czk27."
  },
  {
    "objectID": "blog/2020/12/03/ipw-tscs-msm/index.html",
    "href": "blog/2020/12/03/ipw-tscs-msm/index.html",
    "title": "Generating inverse probability weights for marginal structural models with time-series cross-sectional panel data",
    "section": "",
    "text": "In my post on generating inverse probability weights for both binary and continuous treatments, I mentioned that I’d eventually need to figure out how to deal with more complex data structures and causal models where treatments, outcomes, and confounders vary over time. Instead of adjusting for DAG confounding with inverse probability weights, we need to use something called marginal structural models (MSMs) to make adjustments that account for treatment and outcome history and other time structures. This is complex stuff and social science hasn’t done much with it (but it’s been a common approach in epidemiology).\nThis post is my first attempt at teaching myself how to do this stuff. As I note at the end in the caveats section, there might be (surely are!) mistakes. Please correct them!\nlibrary(tidyverse)\nlibrary(lme4)  # For mixed models\nlibrary(fixest)  # For fixed effects models\nlibrary(broom)\nlibrary(broom.mixed)  # For tidying mixed models\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(ipw)"
  },
  {
    "objectID": "blog/2020/12/03/ipw-tscs-msm/index.html#dags-and-time-series-cross-sectional-tscs-data",
    "href": "blog/2020/12/03/ipw-tscs-msm/index.html#dags-and-time-series-cross-sectional-tscs-data",
    "title": "Generating inverse probability weights for marginal structural models with time-series cross-sectional panel data",
    "section": "DAGs and time-series cross-sectional (TSCS) data",
    "text": "DAGs and time-series cross-sectional (TSCS) data\nLet’s pretend that we’re interested in the causal effect of a policy in a country on a country’s happiness. We’ll work with two different policies: whether a country implements a 6-hour workday (like Finland has been considering), which is binary, and the number of mandated vacation days a country provides, which is continuous. Both the policy and national happiness are influenced and confounded by a few different variables: general country-specific trends, GDP per capita, level of democratization, and level of political corruption.\nIn the absence of time, this causal model is fairly straightforward:\n\nsimple_dag &lt;- dagify(happiness ~ policy + gdp_cap + democracy + corruption + country,\n                     policy ~ gdp_cap + democracy + corruption + country,\n                     coords = list(x = c(policy = 1, happiness = 5, gdp_cap = 2, \n                                         democracy = 3, corruption = 4, country = 3),\n                                   y = c(policy = 2, happiness = 2, gdp_cap = 3, \n                                         democracy = 3.3, corruption = 3, country = 1)),\n                     exposure = \"policy\",\n                     outcome = \"happiness\")\n\nggdag_status(simple_dag, text_col = \"black\") +\n  guides(color = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\nIn a regular DAG setting, we can isolate the arrow between policy and happiness by statistically adjusting for all the nodes that open up backdoor relationships between them, or confound them. We can use do-calculus logic for that, or we can use R:\n\nadjustmentSets(simple_dag)\n## { corruption, country, democracy, gdp_cap }\n\nAdjusting for the four confounders here is thus sufficient for closing all the backdoors and isolating the causal effect of the policy on national happiness. A standard approach to this kind of adjustment is inverse probability weighting, and I have a whole post about how to do that with both binary and continuous treatments (as well as another post and a textbook chapter with even more details and examples).\nHowever, in reality, time also influences policies, happiness, and other confounders. The number of vacation days a country offers in 2019 depends a lot on the number of vacation days offered in 2018, and 2017, and 2016, and so on. Also, a country’s GDP, level of democracy, and level of corruption all depend on earlier values. Countries aren’t just getting random levels of democracy each year! Not all confounders vary with time—country remains the same every year, as do things like region and continent.\nOn top of all that, happiness in a previous year could influence the policy in the current year. If a country has lower aggregate happiness in 2016, that could influences politicians’ choice to mandate a 6-hour workday or increase vacation days in 2017 or 2018.\nWe need to incorporate time into our simple DAG. Because we’re adding a bunch more nodes, I’m going to collapse the time-varying confounders (GDP per capita, democracy, and corruption) and time-invariant confounders (just country here) into single separate nodes. To account for time, I add \\(t\\) subscripts: \\(t\\) represents the current year, \\(t - 1\\) (t_m1 in the graph) represents the previous year, \\(t - 2\\) represents two years earlier, and so on.\nHere’s what this looks like:\n\ntime_dag &lt;- dagify(happiness_t ~ policy_t + varying_confounders_t + happiness_tm1 + nonvarying_confounders,\n                   policy_t ~ varying_confounders_t + happiness_tm1 + policy_tm1 + nonvarying_confounders,\n                   varying_confounders_t ~ happiness_tm1 + varying_confounders_tm1 + nonvarying_confounders,\n                   happiness_tm1 ~ policy_tm1 + varying_confounders_tm1 + nonvarying_confounders,\n                   policy_tm1 ~ varying_confounders_tm1 + nonvarying_confounders,\n                   varying_confounders_tm1 ~ nonvarying_confounders,\n                   coords = list(x = c(happiness_t = 4, policy_t = 3, varying_confounders_t = 3, \n                                       happiness_tm1 = 2, policy_tm1 = 1, varying_confounders_tm1 = 1,\n                                       nonvarying_confounders = 2.5),\n                                 y = c(happiness_t = 3, policy_t = 2, varying_confounders_t = 4, \n                                       happiness_tm1 = 3, policy_tm1 = 2, varying_confounders_tm1 = 4,\n                                       nonvarying_confounders = 1)),\n                   exposure = \"policy_t\",\n                   outcome = \"happiness_t\")\n\nggdag_status(time_dag, text_col = \"black\") +\n  guides(color = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\nPhew. That’s bananas. And that’s just for one time period. Technically there are also nodes from \\(t - 2\\) and \\(t - 3\\) and so on that influence \\(t - 1\\). Figure 2 from Blackwell and Glynn (2018) shows a similar structure with previous time periods (though they don’t have an arrow from \\(Y_{t-1}\\) to \\(Y\\)):\n\n\nFigure 2 from Blackwell and Glynn (2018)\n\nAll we care about in this situation is the single arrow between policy_t and happiness_t. (There are ways of looking at other arrows, like the effect of policy_tm1 on happiness_t, but we won’t try to measure those here. Blackwell and Glynn (2018) show how to do that.)\nWe can use do-calculus logic to see what nodes need to be adjusted for to isolate that arrow:\n\nadjustmentSets(time_dag)\n## { happiness_tm1, nonvarying_confounders, varying_confounders_t }\n\nAccording to this, we should adjust for time variant confounders in the current year, happiness in the previous year, and nonvarying confounders like country. However, this won’t be completely accurate because the previous history matters. In general, situations where treatments, confounders, and outcomes vary over time, adjustment approaches like inverse probability weighting will be biased and incorrect."
  },
  {
    "objectID": "blog/2020/12/03/ipw-tscs-msm/index.html#marginal-structural-models",
    "href": "blog/2020/12/03/ipw-tscs-msm/index.html#marginal-structural-models",
    "title": "Generating inverse probability weights for marginal structural models with time-series cross-sectional panel data",
    "section": "Marginal structural models",
    "text": "Marginal structural models\nTo account for this time structure, we can instead use something called marginal structural models (MSMs) to make DAG adjustments. These have been used widely in epidemiology, and there are some really great and accessible overviews of the method here:\n\nChapter 12 in Miguel A. Hernán and James M. Robins, Causal Inference: What If (Hernán and Robins 2020)\n\nFelix Thoemmes and Anthony D. Ong, “A Primer on Inverse Probability of Treatment Weighting and Marginal Structural Models” (Thoemmes and Ong 2016)\n\nStephen R. Cole and Miguel A. Hernán, “Constructing Inverse Probability Weights for Marginal Structural Models” (Cole and Hernán 2008)\n\nKosuke Imai and Marc Ratkovic, “Robust Estimation of Inverse Probability Weights for Marginal Structural Models” (Imai and Ratkovic 2015)\n\nJames M. Robins, Miguel Ángel Hernán, and Babette Brumback, “Marginal Structural Models and Causal Inference in Epidemiology” (Robins, Hernán, and Brumback 2000)\n\n\nIn my world of public policy and political science, though, MSMs are far rarer, even though tons of the data we use is time-series cross-sectional (TSCS) data, or panel data where each row represents a country and year (e.g. row 1 is Afghanistan in 2008, row 2 is Afghanistan in 2009, etc.) or state and year (e.g. Alabama 2015, Alabama 2016, etc.). The only paper I’ve really seen that uses MSMs in the political science world is Blackwell and Glynn (2018), which is an introduction to the topic and a call for using them more:\n\nMatthew Blackwell and Adam N. Glynn, “How to Make Causal Inferences with Time-Series Cross-Sectional Data under Selection on Observables,” (Blackwell and Glynn 2018)\n\n\nThe basic intuition behind MSMs is similar to simpler inverse probability weighting:\n\nCalculate weights using confounders and the time structure\nCalculate the average treatment effect using the weights and the time structure\n\nThe formula for calculating weights differs depending on if the treatment is binary or continuous, and they’re written slightly differently across those different resources listed above.\nHere’s my version of how to calculate stabilized inverse probability weights with a binary treatment:\n\\[\n\\text{Binary stabilized IPW}_{it} = \\prod^t_{t = 1} \\frac{P[X_{it} | \\bar{X}_{i, t-1}, V_i]}{P[X_{it} | \\bar{X}_{i, t-1}, Y_{i, t-1}, C_{it}, V_i]}\n\\]\nThere are a ton of variables in this equation. Let’s go through them one at a time:\n\n\n\\(i\\) stands for an individual (person, country, etc.)\n\n\\(t\\) stands for a time period (year, month, day, etc.)\n\n\\(X\\) stands for the observed treatment status; \\(X_{it}\\) stands for the observed treatment status of an individual at a given time. This is often written more specifically as \\(X_{it} = x_{it}\\) (see equation 1 in p. 46 in Thoemmes and Ong (2016), and the equation at the beginning of this tutorial here, for instance), but for simplicity I’ll just write it as \\(X_{it}\\).\n\n\\(\\bar{X}\\) stands for the individual’s history of treatment assignment (e.g. all \\(X\\) values in previous time periods)\n\n\\(Y\\) stands for the outcome; \\(Y_{it}\\) stands for the outcome of an individual at a given time.\n\n\\(C\\) stands for time varying confounders; because these change over time, \\(C\\) gets a \\(t\\) subscript: \\(C_{it}\\)\n\n\n\\(V\\) stands for time invarying confounders; that’s why there’s no \\(t\\) in \\(V_i\\)\n\nFinally \\(P[\\cdot]\\) stands for the probability distribution\n\nHere’s a more human explanation:\n\nThe numerator contains the probability of the observed treatment status (\\(X\\)) at each time given the previous history of treatment (\\(\\bar{X}\\)) and time invariant confounders (\\(V_i\\))\nThe denominator contains the probability of the observed treatment status (\\(X\\)) at each time given the previous history of treatment (\\(\\bar{X}\\)), previous outcomes (\\(Y_{i, t-1}\\)), time varying confounders (\\(C_{it}\\)) and time invariant confounders (\\(V_i\\)). The previous outcomes part (\\(Y_{i, t-1}\\)) is optional; if you think that the outcome’s previous values influence current values, and the DAG shows an arrow from \\(Y_{t-1}\\) and \\(Y_t\\), include it.\n\nImportantly, time varying confounders (\\(C_{it}\\)) are included in the denominator only, not the numerator. The lagged outcome (\\(Y_{i, t-1}\\)), if used, also only goes in the denominator.\nTechnically the numerator can just be 1 instead of the whole \\(P[\\cdot]\\) thing, but that creates unstable weights. Using \\(P[\\cdot]\\) in the numerator creates stabilized weights.\nThe equation for continuous weights looks really similar:\n\\[\n\\text{Continuous stabilized IPW}_{it} = \\prod^t_{t = 1} \\frac{f_{X | \\bar{X}, V}[(X_{it} | \\bar{X}_{i, t-1}, V_i); \\mu_1, \\sigma^2_1]}{f_{X | \\bar{X}, Y, C, V}[(X_{it} | \\bar{X}_{i, t-1}, Y_{i, t-1}, C_{it}, V_i), \\mu_2, \\sigma^2_2]}\n\\]\nYikes. This is looks really complicated (and it is!), but again we can separate it into individual parts:\n\n\n\\(X\\), \\(Y\\), \\(V\\), \\(C\\), \\(i\\), and \\(t\\) are all the same as the binary version of the formula\nThe numerator is still the treatment, treatment history, and time invariant confounders\nThe denominator is still the treatment, treatment history, previous outcome, time varying confounders, and time invariant confounders\nThe \\(f_{\\cdot}(\\cdot)\\) functions are new and stand for a probability density function with a mean of \\(\\mu\\) and a variance of \\(\\sigma^2\\)\n\n\nThat’s a ton of information and it’s all really abstract. Let’s try this out with some simulated data"
  },
  {
    "objectID": "blog/2020/12/03/ipw-tscs-msm/index.html#simulated-time-series-cross-sectional-data",
    "href": "blog/2020/12/03/ipw-tscs-msm/index.html#simulated-time-series-cross-sectional-data",
    "title": "Generating inverse probability weights for marginal structural models with time-series cross-sectional panel data",
    "section": "Simulated time-series cross-sectional data",
    "text": "Simulated time-series cross-sectional data\nFor this example, we’ll use some data I generated with the fabricatr package, which makes it really easy to build multilevel and nested structures like country- and year-level variables. The actual code to generate this is a little long, mostly because it’s heavily annotated and has a ton of intermediate variables. You can download the data here if you want to follow along with the rest of the code:\n\n happiness_data.csv\n happiness_simulation.R\n\nIt contains a bunch of different columns:\n\n\ncountry: The country name (generated as a pronouncable 5-letter sequence (proquint) with the ids package)\n\nyear: The year\n\nvacation_days: The number of mandated vacation days. This is a treatment variable.\n\npolicy: An indicator for whether a country has passed a policy that mandates a 6-hour workday. This is another treatment variable\n\nhappiness_vacation: The level of happiness in a country, on a scale of 1–100 (more happiness = higher values). This is the outcome when using vacation_days as the treatment.\n\nhappiness_policy: The level of happiness in a country. This is the outcome when using policy as the treatment.\n\nlog_populuation: Logged population\n\nlog_gdp: Logged GDP\n\ngdp: GDP\n\npopulation: Population\n\ngdp_cap: GDP per capita\n\nlog_gdp_cap: Logged GDP per capita\n\ndemocracy: The country’s level of democracy, on a scale of 1–100 (more democratic = higher values)\n\ncorruption: The level of political corruption in a country, on a scale of 1–100 (more corrupt = higher values)\n\nlag_*: Lagged versions of a bunch of different columns\n\nAnd here’s what the actual data looks like:\n\nhappiness_data &lt;- read_csv(\"happiness_data.csv\")\n\nglimpse(happiness_data)\n## Rows: 1,520\n## Columns: 18\n## $ country                &lt;chr&gt; \"Mimim\", \"Mimim\", \"Mimim\", \"Mimim\", \"Mimim\", \"Mimim\", \"Mi…\n## $ year                   &lt;dbl&gt; 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 201…\n## $ vacation_days          &lt;dbl&gt; 12, 14, 16, 17, 18, 20, 21, 22, 24, 25, 9, 11, 13, 14, 16…\n## $ policy                 &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ happiness_vacation     &lt;dbl&gt; 43.2, 45.1, 52.7, 52.7, 53.5, 61.4, 63.1, 66.1, 71.4, 73.…\n## $ happiness_policy       &lt;dbl&gt; 36.9, 40.6, 44.3, 46.3, 54.3, 58.4, 54.7, 59.1, 67.6, 59.…\n## $ log_population         &lt;dbl&gt; 17.4, 17.4, 17.5, 17.5, 17.6, 17.6, 17.7, 17.7, 17.8, 17.…\n## $ log_gdp                &lt;dbl&gt; 23.1, 23.2, 23.3, 23.4, 23.5, 23.6, 23.7, 23.8, 23.9, 24.…\n## $ gdp                    &lt;dbl&gt; 1.06e+10, 1.18e+10, 1.27e+10, 1.48e+10, 1.57e+10, 1.78e+1…\n## $ population             &lt;dbl&gt; 36049651, 37745007, 39520093, 41378659, 43324629, 4536211…\n## $ gdp_cap                &lt;dbl&gt; 293, 313, 321, 358, 361, 392, 434, 446, 483, 528, 5750, 6…\n## $ log_gdp_cap            &lt;dbl&gt; 5.68, 5.74, 5.77, 5.88, 5.89, 5.97, 6.07, 6.10, 6.18, 6.2…\n## $ democracy              &lt;dbl&gt; 56.9, 59.8, 77.5, 71.0, 76.2, 83.1, 87.3, 92.2, 100.0, 99…\n## $ corruption             &lt;dbl&gt; 63.4, 62.9, 62.0, 60.7, 61.9, 60.4, 60.4, 57.9, 58.0, 58.…\n## $ lag_policy             &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, …\n## $ lag_happiness_policy   &lt;dbl&gt; 36.8, 36.9, 40.6, 44.3, 46.3, 54.3, 58.4, 54.7, 59.1, 67.…\n## $ lag_vacation_days      &lt;dbl&gt; 12, 12, 14, 16, 17, 18, 20, 21, 22, 24, 9, 9, 11, 13, 14,…\n## $ lag_happiness_vacation &lt;dbl&gt; 41.5, 43.2, 45.1, 52.7, 52.7, 53.5, 61.4, 63.1, 66.1, 71.…\n\nWe’ll use this data explore two different questions:\n\nBinary treatment: What is the effect of a 6-hour workday policy on national happiness?\nContinuous treatment: What is the effect of the number of mandated vacation days on national happiness?"
  },
  {
    "objectID": "blog/2020/12/03/ipw-tscs-msm/index.html#marginal-structural-model-with-a-binary-treatment",
    "href": "blog/2020/12/03/ipw-tscs-msm/index.html#marginal-structural-model-with-a-binary-treatment",
    "title": "Generating inverse probability weights for marginal structural models with time-series cross-sectional panel data",
    "section": "Marginal structural model with a binary treatment",
    "text": "Marginal structural model with a binary treatment\nBefore we do anything with the binary treatment, we need to filter the data a little. Because of the nature of the data, some of the fake countries never implement the policy and have all 0s in the policy column. Weird things happen with the math of logistic regression if there are countries that have all 0s or all 1s for the outcome, since it’s technically impossible to predict their outcomes. That’s why zero-one inflated beta (ZOIB) models or hurdle models are a thing—they’re two step models that first model if you do the policy at all, then model the probability of the policy if it does happen. Rather than deal with ZOIB stuff here, I made it so that all countries start with 0 for the policy (i.e. no country has the policy in the first year), and then here we filter out any countries that don’t ever implement the policy.\n\nhappiness_binary &lt;- happiness_data %&gt;% \n  group_by(country) %&gt;% \n  mutate(never_policy = all(policy == 0)) %&gt;% \n  ungroup() %&gt;% \n  filter(!never_policy)\n\nNaive estimate without weights\nBefore playing with MSMs, let’s look at what the effect of the policy is on happiness without doing any inverse probability weighting for DAG adjustment. This is what most political science and international relations and public policy papers do. This is what I did in my dissertation and what I’ve done in a bunch of working papers. The wrongness of this approach is why I’m writing this post :)\nThis is just a regular linear regression model. I could run it with lm(), but then a ton of country and year coefficients would be included by default in the results, so I use feols() from the delightful fixest package to include country and year as fixed effects. The results from feols() and lm() are identical here; feols() is cleaner and faster.\n\nmodel_naive &lt;- feols(happiness_policy ~ policy + log_gdp_cap + democracy + \n                       corruption + lag_happiness_policy + lag_policy | country + year,\n                data = happiness_binary)\ntidy(model_naive)\n## # A tibble: 6 × 5\n##   term                 estimate std.error statistic  p.value\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 policy                  6.76     0.397      17.0  1.36e-35\n## 2 log_gdp_cap             3.80     1.85        2.05 4.24e- 2\n## 3 democracy               0.146    0.0218      6.71 4.78e-10\n## 4 corruption             -0.158    0.0252     -6.26 4.62e- 9\n## 5 lag_happiness_policy    0.172    0.0449      3.82 2.02e- 4\n## 6 lag_policy             -1.54     0.511      -3.01 3.11e- 3\n\nAccording to this, implementing a 6-hour workday is associated with a 6.8-point increase in national happiness. This is wrong though! We need to generate and use time-adjusted inverse probability weights to adjust for these confounders.\nManual weights\nWe’ll follow this formula to use confounders and previous treatments and outcomes to generate stabilized weights:\n\\[\n\\text{Binary stabilized IPW}_{it} = \\prod^t_{t = 1} \\frac{P[X_{it} | \\bar{X}_{i, t-1}, V_i]}{P[X_{it} | \\bar{X}_{i, t-1}, Y_{i, t-1}, C_{it}, V_i]}\n\\]\nThe numerator predicts the treatment using the previous treatment and time invariant confounders. We’ll use logistic regression here, but I’m like 90% sure you can do fancier things like multilevel models or machine learning or Bayes stuff:\n\nmodel_num &lt;- glm(policy ~ lag_policy + country, \n                 data = happiness_binary, family = binomial(link = \"logit\"))\n\nThe denominator predicts the treatment using time-varying confounders, previous outcome, previous treatment, and time invariant confounders. Again we’ll use logistic regression here, but you can probably do fancier stuff too:\n\n# There's a warning that fitted probabiltiies of 0 or 1 occurred, likely because\n# my data is too perfect. Oh well---we'll live with it.\nmodel_denom &lt;- glm(policy ~ log_gdp_cap + democracy + corruption + \n                     lag_happiness_policy + lag_policy + country, \n                   data = happiness_binary, family = binomial(link = \"logit\"))\n\n# This also works if you use fixest::feglm() for country fixed effects\n# model_denom &lt;- feglm(policy ~ log_gdp_cap + democracy + corruption +\n#                        lag_happiness_policy + lag_policy | country,\n#                      data = happiness_binary, family = binomial(link = \"logit\"))\n\nFinally we need to use the results from the numerator and denominator to construct the weights following the equation:\n\nhappiness_binary_weights &lt;- happiness_binary %&gt;% \n  # Propensity scores from the models\n  mutate(propensity_num = model_num$fitted.values,\n         propensity_denom = model_denom$fitted.values) %&gt;% \n  # Probability of observed outcome\n  mutate(propensity_num_outcome = ifelse(policy == 1, propensity_num, 1 - propensity_num),\n         propensity_denom_outcome = ifelse(policy == 1, propensity_denom, 1 - propensity_denom)) %&gt;% \n  # Numerator / denominator\n  mutate(weights_no_time = propensity_num_outcome / propensity_denom_outcome) %&gt;% \n  # Calculate the cumulative product of the weights within each country\n  group_by(country) %&gt;% \n  mutate(ipw = cumprod(weights_no_time)) %&gt;% \n  ungroup()\n\nhappiness_binary_weights %&gt;% \n  select(country, year, policy, happiness_policy, ipw) %&gt;% \n  head()\n## # A tibble: 6 × 5\n##   country  year policy happiness_policy   ipw\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt;\n## 1 Mimim    2010      0             36.9 0.800\n## 2 Mimim    2011      0             40.6 0.640\n## 3 Mimim    2012      0             44.3 0.516\n## 4 Mimim    2013      0             46.3 0.486\n## 5 Mimim    2014      1             54.3 0.116\n## 6 Mimim    2015      1             58.4 0.116\n\nFinally we’ll use those weights in a regression model to estimate the average treatment effect (ATE) of the policy on happiness. We need to use a model that accounts for the year and country panel structure for this. In every tutorial I’ve seen online, people use geeglm() from the geepack package, which lets you specify country and year dimensions in generalized estimating equations. These feel an awful lot like mixed models with random country/year effects. There’s some useful discussion and useful links about the differences between GEE models and multilevel models in this Twitter thread here. For the sake of this example, I’ll use multilevel models since I’m more familiar with them, and because you can build Bayesian ones with the brms package; I have yet to find a Bayesian flavor of GEEs.\nIn the outcome model, we include the previous treatment history and the invariant confounders (country, which I include as a random effect). To account for the time structure in the data, I also include a year random effect.\n\nmodel_ate_binary &lt;- lmer(happiness_policy ~ policy + lag_policy + \n                           (1 | country) + (1 | year), \n                  data = happiness_binary_weights, weights = ipw)\ntidy(model_ate_binary, effects = \"fixed\")\n## # A tibble: 3 × 5\n##   effect term        estimate std.error statistic\n##   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed  (Intercept)    51.9      1.26      41.2 \n## 2 fixed  policy          7.64     0.510     15.0 \n## 3 fixed  lag_policy     -1.30     0.448     -2.91\n\nVoila! After adjusting for time-varying confounders and previous treatment history, the 6-hour workday policy causes an increase of 7.6 happiness points, on average. This is actually the effect that I built into the data. It worked!\nHowever, I’m still not 100% confident that it did work. There are a lot of different moving parts here, and I’m not sure I have the right covariates in the right place (like in the outcome model, I’m fairly certain the model should be happiness_policy ~ policy + lag_policy, but I’m not sure).\nAlso the standard errors in this outcome model are wrong and have to be adjusted, either with fancy math or with bootstrapping (Blackwell and Glynn (2018) use boostrapping).\nBut still, this is really neat.\nWeights with the ipw package\nInstead of manually doing all the math to generate the weights, we can use the ipwtm() function from the ipw package to do it for us. We still specify a numerator and denominator, but the function takes care of the rest of the math. The numbers are the same.\n\n# ipwtm() can't handle tibbles! Force the data to be a data.frame\nweights_binary_ipw &lt;- ipwtm(\n  exposure = policy,\n  family = \"binomial\",\n  link = \"logit\",\n  # Time invariant stuff\n  numerator = ~ lag_policy + country,\n  # All confounders\n  denominator = ~ log_gdp_cap + democracy + corruption + \n    lag_happiness_policy + lag_policy + country,\n  id = country,\n  timevar = year,\n  type = \"all\",\n  data = as.data.frame(happiness_binary)\n)\n\n# They're the same!\nhead(weights_binary_ipw$ipw.weights)\n## [1] 0.800 0.640 0.516 0.486 0.116 0.116\nhead(happiness_binary_weights$ipw)\n## [1] 0.800 0.640 0.516 0.486 0.116 0.116\n\nThis weights_binary_ipw object contains a bunch of other information too, but all we really care about here is what’s in the ipw.weights slot. We can add those weights as a column in a dataset and run the outcome model, which will give us the same ATE as before (unsurprisingly, since they’re identical). Technically we don’t need to add a new column with the weights—the model will work if they’re a standalone vector—but I don’t like mixing data frames and standalone vectors and prefer to keep everything in one nice object.\n\nhappiness_binary_ipw &lt;- happiness_binary %&gt;% \n  mutate(ipw = weights_binary_ipw$ipw.weights)\n\nmodel_ate_binary_ipw &lt;- lmer(happiness_policy ~ policy + lag_policy + \n                               (1 | country) + (1 | year), \n                             data = happiness_binary_ipw, weights = ipw)\ntidy(model_ate_binary_ipw, effects = \"fixed\")\n## # A tibble: 3 × 5\n##   effect term        estimate std.error statistic\n##   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed  (Intercept)    51.9      1.26      41.2 \n## 2 fixed  policy          7.64     0.510     15.0 \n## 3 fixed  lag_policy     -1.30     0.448     -2.91"
  },
  {
    "objectID": "blog/2020/12/03/ipw-tscs-msm/index.html#marginal-structural-model-with-a-continuous-treatment",
    "href": "blog/2020/12/03/ipw-tscs-msm/index.html#marginal-structural-model-with-a-continuous-treatment",
    "title": "Generating inverse probability weights for marginal structural models with time-series cross-sectional panel data",
    "section": "Marginal structural model with a continuous treatment",
    "text": "Marginal structural model with a continuous treatment\nHere our main question is what the causal effect of mandated vacation time is on national happiness. This treatment is continuous—days of vacation. We don’t need to worry about having all 1s or all 0s and worry about zero-one inflated models or anything, since the treatment varies a lot across all countries and years.\nNaive estimate without weights\nAs before, we’ll look at the effect of vacation time is on happiness without any weights. Again, this is the approach in like a billion political science papers.\n\nmodel_naive &lt;- feols(happiness_vacation ~ vacation_days + log_gdp_cap + democracy + \n                       corruption + lag_happiness_vacation + lag_vacation_days | country + year,\n                data = happiness_data)\ntidy(model_naive)\n## # A tibble: 6 × 5\n##   term                   estimate std.error statistic  p.value\n##   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 vacation_days            2.12      0.129      16.4  1.94e-35\n## 2 log_gdp_cap              1.35      0.226       5.98 1.54e- 8\n## 3 democracy                0.0516    0.0156      3.31 1.17e- 3\n## 4 corruption              -0.0624    0.0224     -2.78 6.10e- 3\n## 5 lag_happiness_vacation   0.559     0.148       3.76 2.38e- 4\n## 6 lag_vacation_days       -1.35      0.382      -3.54 5.32e- 4\n\nHere we see that an additional day of vacation is associated with a 2.1-point increase in national happiness. Once again, this is wrong and biased, since there’s no weighting adjustment that deals with time-based confounding.\nManual weights\nWe’ll follow the formula for continuous stabilized weights:\n\\[\n\\text{Continuous stabilized IPW}_{it} = \\prod^t_{t = 1} \\frac{f_{X | \\bar{X}, V}[(X_{it} | \\bar{X}_{i, t-1}, V_i); \\mu_1, \\sigma^2_1]}{f_{X | \\bar{X}, Y, C, V}[(X_{it} | \\bar{X}_{i, t-1}, Y_{i, t-1}, C_{it}, V_i), \\mu_2, \\sigma^2_2]}\n\\]\nThe numerator predicts the treatment using the previous treatment and time invariant confounders. We’ll use regular old linear regression here, but again, I’m like 90% sure you can do fancier things like multilevel models or machine learning or Bayes stuff:\n\nmodel_num &lt;- lm(vacation_days ~ lag_vacation_days + country, \n                data = happiness_data)\n\n# This multilevel model works too\n# model_num &lt;- lmer(vacation_days ~ lag_vacation_days + (1 | country), \n#                   data = happiness_data)\n\n# Calculate the probability distribution\nnum &lt;- dnorm(happiness_data$vacation_days,\n             predict(model_num),\n             sd(residuals(model_num)))\n\nThe denominator predicts the treatment using time-varying confounders, previous outcome, previous treatment, and time invariant confounders. Again we’ll use linear regression, but you can probably do fancier stuff too:\n\nmodel_denom &lt;- lm(vacation_days ~ log_gdp_cap + democracy + corruption + \n                    lag_happiness_vacation + lag_vacation_days + country, \n                  data = happiness_data)\n\n# This multilevel model works too\n# model_denom &lt;- lmer(vacation_days ~ log_gdp_cap + democracy + corruption + \n#                     lag_happiness_vacation + lag_vacation_days + (1 | country), \n#                   data = happiness_data)\n\n# Calculate the probability distribution\nden &lt;- dnorm(happiness_data$vacation_days,\n             predict(model_denom),\n             sd(residuals(model_denom)))\n\nFinally we need to use the results from the numerator and denominator to build the inverse weights and calculate the cumulative product over time within each country:\n\n# Finally, we make actual IPW weights by building the fraction\nhappiness_data_weights &lt;- happiness_data %&gt;% \n  mutate(weights_no_time = num / den) %&gt;% \n  group_by(country) %&gt;% \n  mutate(ipw = cumprod(weights_no_time)) %&gt;% \n  ungroup()\n\nhappiness_data_weights %&gt;% \n  select(country, year, vacation_days, happiness_vacation, ipw) %&gt;% \n  head()\n## # A tibble: 6 × 5\n##   country  year vacation_days happiness_vacation   ipw\n##   &lt;chr&gt;   &lt;dbl&gt;         &lt;dbl&gt;              &lt;dbl&gt; &lt;dbl&gt;\n## 1 Mimim    2010            12               43.2 0.142\n## 2 Mimim    2011            14               45.1 1.50 \n## 3 Mimim    2012            16               52.7 1.13 \n## 4 Mimim    2013            17               52.7 0.941\n## 5 Mimim    2014            18               53.5 0.838\n## 6 Mimim    2015            20               61.4 0.457\n\nNow we can use the weights to find the ATE, just like we did with the binary version. Again, I’m using a multilevel model instead of a GEE model, which I think is theoretically fine and legal.\n\nmodel_ate &lt;- lmer(happiness_vacation ~ vacation_days + lag_vacation_days + \n                    (1 | country) + (1 | year), \n                  data = happiness_data_weights, weights = ipw)\ntidy(model_ate, effects = \"fixed\")\n## # A tibble: 3 × 5\n##   effect term              estimate std.error statistic\n##   &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed  (Intercept)          23.4     1.82        12.9\n## 2 fixed  vacation_days         3.48    0.0908      38.4\n## 3 fixed  lag_vacation_days    -1.19    0.0957     -12.4\n\nAfter correctly adjusting for all the time-varying confounding, the causal effect of an additional vacation day is 3.48 happiness points, which is bigger than the naive estimate of 2.1 that we found earlier.\nHOWEVER, this isn’t what I built into the data?? In the simulated data, I made the vacation effect be 1.7. So either I did the simulation wrong and built the effect incorrectly and it’s not actually 1.7, or I’m misspecifying the model here. I’m pretty sure that the weights themselves are fine and correct—I copied the equation and code directly from Blackwell and Glynn (2018)’s replication data, and the weights and ATE are basically the same when using ipwtm(). I don’t know what’s going on. :(\nWeights with the ipw package\nIt’s also possible to use the ipwtm() function with continuous weights, but it runs incredibly slowly since it uses geeglm() behind the scenes to build the weights.\n\n# This takes forever! like multiple minutes\nweights_ipw_continuous &lt;- ipwtm(\n  exposure = vacation_days,\n  family = \"gaussian\",\n  corstr = \"ar1\",\n  numerator = ~ lag_vacation_days + country,  # Time invariant stuff\n  denominator = ~ log_gdp_cap + democracy + corruption + \n    lag_happiness_vacation + lag_vacation_days + country,  # All confounders\n  id = country,\n  timevar = year,\n  type = \"all\",\n  data = as.data.frame(happiness_data)\n)\n\nBecause it uses GEE models for the numerator and denominator and accounts for autoregressive time structures in the data (that’s what the costr = \"ar1\" argument is for), the weights are not exactly the same as the ones we found using manual math, but they’re super close:\n\n# Pretty close!\nhead(weights_ipw_continuous$ipw.weights)\n## [1] 0.142 1.505 1.126 0.941 0.838 0.457\nhead(happiness_data_weights$ipw)\n## [1] 0.142 1.505 1.126 0.941 0.838 0.457\n\nFinally we can use the weights to find the ATE. It’s basically identical to the effect we found with the manual math. (BUT STILL NOT 1.7 FOR WHATEVER REASON.)\n\nhappiness_ipw &lt;- happiness_data %&gt;% \n  mutate(ipw = weights_ipw_continuous$ipw.weights)\n\nmodel_ate_ipw &lt;- lmer(happiness_vacation ~ vacation_days + lag_vacation_days + \n                        (1 | country) + (1 | year), \n                      data = happiness_ipw, weights = ipw)\ntidy(model_ate_ipw, effects = \"fixed\")\n## # A tibble: 3 × 5\n##   effect term              estimate std.error statistic\n##   &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed  (Intercept)          23.4     1.82        12.9\n## 2 fixed  vacation_days         3.48    0.0908      38.4\n## 3 fixed  lag_vacation_days    -1.19    0.0957     -12.4"
  },
  {
    "objectID": "blog/2020/12/03/ipw-tscs-msm/index.html#important-caveats",
    "href": "blog/2020/12/03/ipw-tscs-msm/index.html#important-caveats",
    "title": "Generating inverse probability weights for marginal structural models with time-series cross-sectional panel data",
    "section": "Important caveats!",
    "text": "Important caveats!\nThis is just a quick practical overview of how to actually build IPWs and use MSMs. I didn’t cover any of the math behind MSMs or the assumptions behind them, their limitations, diagnostics you should do, etc. Like, weights should generally have an average of 1 and not have values that are too extreme (and if values are too extreme, you can/should truncate them).\nALSO I likely have something wrong here. If so, let me know! Download the simulated data, play with it, fix the MSMs and weights, and tell me what’s wrong. Please!"
  },
  {
    "objectID": "blog/2020/03/12/emergency-online-teaching-resources/index.html",
    "href": "blog/2020/03/12/emergency-online-teaching-resources/index.html",
    "title": "Emergency online teaching resources",
    "section": "",
    "text": "This is written for instructors in the Department of Public Management and Policy at the Andrew Young School of Policy Studies at Georgia State University, but it’s hopefully widely applicable too.\nWith more than 100 universities moving their teaching online (including Emory just last night), it’s looking more and more inevitable that GSU will make a similar switch any time now.\nBelow, I’ve described some resources to help the switch to online teaching go smoother. This is by no means comprehensive! Here are a bunch of other resources you can/should consult:"
  },
  {
    "objectID": "blog/2020/03/12/emergency-online-teaching-resources/index.html#we-are-in-triage-mode",
    "href": "blog/2020/03/12/emergency-online-teaching-resources/index.html#we-are-in-triage-mode",
    "title": "Emergency online teaching resources",
    "section": "We are in triage mode",
    "text": "We are in triage mode\nIn this time of wild uncertainty and anxiety, remember to keep these principles in mind first:\n\nLower your expectations for the class\nDrop everything that’s not essential\nKeep it simple\nCommunicate clearly\nBe flexible\nBe generous\nAsk how you can help\n\nConsider taking a few minutes at the beginning of class to address student concerns about COVID-19, clear up misconceptions, and provide basic information about the disease. I’ve found that many students don’t have complete information about the pandemic, and talking through it and conveying basic information is reassuring.\nLook at the first few slides of my lecture here or this lecture here for some examples.\nflattenthecurve.com is an excellent resource. Direct your students to it (and rely on it yourself). The Johns Hopkins COVID-19 map and the New York Times COVID-19 map are both reliable sources for tracking the spread of the disease"
  },
  {
    "objectID": "blog/2020/03/12/emergency-online-teaching-resources/index.html#synchronous-teaching",
    "href": "blog/2020/03/12/emergency-online-teaching-resources/index.html#synchronous-teaching",
    "title": "Emergency online teaching resources",
    "section": "Synchronous teaching",
    "text": "Synchronous teaching\nPerhaps the easiest option for moving your classes online with minimal extra work is to continue to hold your classes at their regularly scheduled time and have students participate remotely via synchronous video. I’ve been doing this for the past two weeks and it’s caused minimal disruption and extra work from my end.\n\nWebex\nWebex is corporate video conferencing software that GSU has a subscription to. It is similar to Zoom (which you have have heard of or used elsewhere; Zoom was started by a former Webex employee after Webex was bought up by Cisco a few years ago), and provides all sorts of tools for having productive video conferences.\n\nEquipment you need\n\nComputer\nWebcam\n\nYou can use the one that’s built in to your laptop, or you can use an external one if you have one already (the one in your laptop is more than sufficient)\n\nMicrophone\n\nYou can use the one that’s built in to your laptop, or you can use an external one. The one in your laptop is generally sufficient if you’re planning on sitting in front of your computer, but if you’re walking around a room, it might struggle to pick up your sound. It might be a good idea to get an external microphone. There are a billion options out there—I got this one which transmits wirelessly to my computer through USB, and includes both a lapel mic and a headset mic. If you’re only planning on recording at a desk, these articles (link and link) have great suggestions for good mics.\n\n\n\n\nGetting started\nIf you haven’t downloaded Webex on your computer, go to https://gsumeetings.webex.com/ and log in with your GSU account. Once you’re logged in, you can click on the big green “Start a Meeting” button to start.\n\n\n\nPersonal room screen\n\n\nYou’ll be prompted to download the Cisco WebEx Meetings application if you haven’t already. Follow all the prompts to install it.\nFrom now on, you can just open the Cisco Webex Meetings app directly on your computer instead of going to https://gsumeetings.webex.com/.\n\n\nMeeting rooms and URLs\nBecause Webex was invented for remote meetings, the whole application is based on that metaphor. When you teach and have students join via video, they’ll be joining a meeting. Participants join meetings by visiting specific URLs. You can use a permanent URL that will work every time, or you can generate meeting-specific URLs for specific classes\n\nYour permanent personal meeting URL\nWebex gives you a permanent meeting URL—in the screenshot above, you can see that mine is https://gsumeetings.webex.com/meet/aheiss. If you send that link to your students, they’ll join you via video after visiting the URL. If you’re not there (e.g. they click on the link 30 minutes before class and you haven’t started streaming anything yet), they’ll see this:\n\n\n\nPersonal room waiting screen\n\n\nThey can wait on that screen until you start the meeting from your end.\nThe nice thing about using your general permanent meeting URL is that it doesn’t change. You can send it to students who want to meet with you during office hours, or use it for committee meetings, or use it for each of your classes. The downside, though, is that it’s the same for everyone. If you’re meeting with a student during office hours, and a student from another class clicks on the link, they’ll automatically join your one-on-one meeting (which could be awkward!)\n\n\nMeeting-specific URLs\nTo avoid having unwanted guests join your class or office hours or committee meetings, you can generate meeting-specific URLs by scheduling future meetings.\nFrom either the Webex Meetings application or from https://gsumeetings.webex.com/, click on “Schedule” and enter the details for your class or meeting, including the start and end time. It makes you choose a password, but it doesn’t need to be highly secure or anything (I’ve used stuff like “asdf” or “1234” in the past).\n\n\n\nMeeting scheduling\n\n\nIf you include e-mail addresses in the “Attendees” section, Webex will send calendar invitations to your students with instructions on how to connect, including the meeting-specific URL and the meeting password. If you don’t include e-mail addresses there, it’s okay—you can communicate that information to them separately.\nAfter you click on “Save”, you’ll get a confirmation e-mail from Webex, and you’ll see the meeting listed in the “Meetings” section of the Webex webpage:\n\n\n\nSchedule information\n\n\nAt the bottom of page, you’ll see a “Meeting Information” section that includes the meeting-specific URL and the password for the meeting. Send those details to your students if you didn’t invite them through Webex’s system. You don’t really need to to worry about the meeting number or host key. The meeting number is only needed if students join the call/conference/meeting/class over the phone (there are instructions for doing that in the confirmation e-mail that Webex sends to you and t to them), and the host key is only needed if you need to host the meeting by phone.\n\n\n\nWebex basics\nOnce you start a video call, you’ll have a toolbar at the bottom with a bunch of different options:\n\n\n\nWebex toolbar\n\n\n\nMute/unmute: It’s often wise to mute yourself during breaks or while students are talking (or if you take a drink of water)\nStart/stop video: You can turn your video on/off here (the meeting will continue without video, so don’t worry about it accidentally ending)\nShare screen: By default, Webex will show whatever your camera is pointing at (i.e. you), but you can have it stream other things, like your computer screen. You can choose to share specific applications, like PowerPoint or Excel, or you can share your entire screen. If you’re plugged into a projector or screen (like when teaching), you can share “Screen 2” so students can see the projected PowerPoint (and not see your notes in presenter view)\n\n\n\nWebex sharing panel\n\n\nIf you scroll down to the bottom of the share content window, you can also share a whiteboard, which lets you draw on screen with your mouse:\n\n\n\niPad and whiteboard sharing\n\n\nIf you plug an iPad or iPhone into your computer, you can also share that screen, which is an even easier way to have a whiteboard. Find some free/cheap drawing app for your iPad (if you have one) and stream that:\n\n\n\niPad sharing\n\n\nTo stop sharing your screen, move your mouse to the top of the screen and you’ll see a toolbar pop out. Click on “Stop Sharing” there.\nRecord meeting: If you click on the “Record meeting” button, Webex will record the class session. When you finish the meeting, it will process all the video and send you an e-mail with a link that students can use to view later. I’d recommend doing this every time you teach for students who might not be able to participate synchronously.\nView/mute participants: If you click on this button, you’ll see a list of everyone on the call, along with indicators showing if they’ve turned off their video or muted themselves. All students should always keep themselves muted by default, otherwise it gets really noisy and you’ll get lots of audio feedback. If students forget to mute themselves, you can mute them from here. When students want to ask a question or make a comment or respond to a question, they should unmute themselves. Remembering to unmute before speaking takes a little while to get used too, but it really makes the class go a lot smoother.\nChat: You can chat with students too, which is often helpful if you want to have students virtually raise their hand or chime in or ask questions while you’re talking.\nEnd call: This, um, ends the meeting.\n\n\n\nWebex in iCollege\nThere’s a way to embed Webex meeting rooms inside iCollege courses, but I haven’t played with this yet, mostly because I use Webex for so many other things (office hours meetings, nonprofit board meetings, coauthor meetings), that I’m used to using it on its own. You might want to play with the iCollege integration for fun, though.\n\n\n\nOther ways to teach synchronously\nYou don’t have to use Webex if you don’t want to! You can use Zoom, FaceTime, Skype, Google Hangouts, Facebook Messenger, WhatsApp, or any other live video calling service. If you don’t care about video and audio interaction with your students, you can stream your class live on YouTube or Twitch and have students participate through chat. If you want to be extra fancy, you can use the free OBS application to switch between screens, video cameras, tables, etc. while streaming."
  },
  {
    "objectID": "blog/2020/03/12/emergency-online-teaching-resources/index.html#distribution-of-course-materials",
    "href": "blog/2020/03/12/emergency-online-teaching-resources/index.html#distribution-of-course-materials",
    "title": "Emergency online teaching resources",
    "section": "Distribution of course materials",
    "text": "Distribution of course materials\nStudents need to be able to take quizzes or tests, turn in assignments, and access course materials remotely. iCollege is the best avenue for this, since it’s GSU’s main learning management software. If you need help, CETL has a lot of resources.\nIf you’re familiar with web hosting, you can be extra fancy and post slides and class materials to a public website too (like this)."
  },
  {
    "objectID": "blog/2020/03/12/emergency-online-teaching-resources/index.html#meeting-with-students-gras-and-colleagues",
    "href": "blog/2020/03/12/emergency-online-teaching-resources/index.html#meeting-with-students-gras-and-colleagues",
    "title": "Emergency online teaching resources",
    "section": "Meeting with students, GRAs, and colleagues",
    "text": "Meeting with students, GRAs, and colleagues\nIf campus closes (or if you voluntarily start practicing social distancing and work from home), you need a way to continue to meet with your students, GRAs, coauthors, colleagues, committee members, and others. The easiest way to do this is probably through Webex (though you can use whatever video application you feel most comfortable with! I meet with different coauthors over the phone, through FaceTime, through Zoom, and through Google Hangouts). You can either use your permanent private meeting room, or schedule a specific meeting with a password and meeting-specific URL.\nCoordinating student meeting times can be a hassle and you’ll want to avoid the inevitable e-mail tag that happens when trying to schedule stuff. I’ve found it easiest to use a free online service like Calendly or YouCanBook.me. These connect to your Google or Outlook calendars and let you set aside times that you are available. Students can then sign up for times to meet with you during those available slots. If you schedule other things during those times, those slots will become unavailable and people won’t be able to schedule meetings then.\nYou can see what this looks like here: https://calendly.com/andrewheiss/meeting/. I’ve had enormous success with this under non-pandemic conditions—I don’t typically have set office hours during the week (like Mondays from 11-1 or something); instead, I have several chunks of time available for students and GRAs to schedule meetings, and they choose whatever slots are most convenient for them."
  },
  {
    "objectID": "blog/2020/03/12/emergency-online-teaching-resources/index.html#asynchronous-teaching",
    "href": "blog/2020/03/12/emergency-online-teaching-resources/index.html#asynchronous-teaching",
    "title": "Emergency online teaching resources",
    "section": "Asynchronous teaching",
    "text": "Asynchronous teaching\nAt this point, given that we’re halfway through the semester, it’s unlikely that you’ll need to switch to asynchronous teaching. Developing a fully asynchronous class is hard and time-consuming and expensive—there’s a reason why we have an instructional designer on staff (Mya!) and why GSU offers immense support for developing online classes.\nIf you want to switch to a completely asynchronous class, consult with Mya!\nIf you want to have some asynchronous elements, like pre-recorded screencasts of how to do something in Excel, Stata, or R, or a miniature chunk of a lecture, or something similar, there are several useful resources:\n\nKaltura: GSU has a subscription to this software. This is essentially like Webex and allows you to record yourself or your screen. It does not stream the recording live, however—it’s best for recording screencasts or recording yourself giving a lecture.\nPowerPoint: You can record yourself using PowerPoint and upload the resulting video file to iCollege\nQuickTime Player: On macOS, you can use the QuickTime Player app to record audio, video, or your screen if you don’t want to use Kaltura\n\nIf you want to edit or trim video that you create, you can use applications like these:\n\niMovie: macOS comes with the free iMovie, which lets you do basic video editing\nPhotos: Windows 10 comes with the free Photos app, which also lets you do basic video editing.\nAdobe Premiere or Adobe Premiere Rush: If you want to be extra fancy, you can use Adobe Premiere (which you have access to through Adobe Creative Cloud, which you get for free from GSU)"
  },
  {
    "objectID": "blog/2020/01/10/makefile-subdirectory-zips/index.html",
    "href": "blog/2020/01/10/makefile-subdirectory-zips/index.html",
    "title": "Automatically zip up subdirectories with Make",
    "section": "",
    "text": "See this notebook on GitHub. You can (and should) download the project from there if you want to follow along and try this out.\ntl;dr: Skip to the completed example.\nI use blogdown to generate the websites for all the courses I teach, and it’s delightful to not have to worry about databases and server configurations. I use a Makefile to run the requisite commands with make deploy, which creates a magical incantation: R, blogdown, and Hugo parse R Markdown files and generate a complete HTML site, which then gets synced to my server, all with minimal input from me.\nThere are occasional points of friction, though. One that I’ve suffered through for the past few years is the creation and distribution of zipped files. I teach students R, and R projects rarely consist of a single file. To make it easier to distribute problem sets and projects to students, I zip these subfolders into single files like problem_set_1.zip, so they just have to download one thing. Creating .zip files on macOS is trivial—right click on a folder, choose “Compress {foldername}”, and you’re done.\nBut it’s tedious when you have lots of folders to zip, and even more tedious to remember to rezip folders where you’ve made changes. SO MANY TIMES I’ve fixed errors in R scripts or data but then have forgotten to rezip the project, and students end up downloading the uncorrected version of projects. Moreover, macOS includes hidden .DS_Store files when it zips up folders, and R and RStudio create their own invisible files and folders, like .Rhistory and .Rproj.user/. To avoid shipping these out to students, I typically go to the terminal, manually delete the unwanted invisible files, and then zip up the directory. But once again, I regularly forget to do this and end up including unwanted files.\nI figured that since I’m already using a Makefile to generate and upload the course website, I’d try to use the magical power of Make to automatically zip up project folders as I build the site and update them only if there are any changes. And it turns out that it’s possible, though the end result looks really cryptic (as do all Makefiles, really).\nWhat follows here is a step-by-step didactic explanation of how I built a set of Makefile recipes to automatically zip up all the directories within a given directory, excluding invisible files (i.e. any file or directory that begins with a .), and only zipping up directories that have been modified since the last time Make was run. You can also skip to the end to see the finished Makefile."
  },
  {
    "objectID": "blog/2020/01/10/makefile-subdirectory-zips/index.html#folder-structure",
    "href": "blog/2020/01/10/makefile-subdirectory-zips/index.html#folder-structure",
    "title": "Automatically zip up subdirectories with Make",
    "section": "Folder structure",
    "text": "Folder structure\nThe example here assumes the project subdirectories live in a folder called static/projects/, since that mimics blogdown/Hugo (any files in the static/ directory do not get processed by knitr when you build the site). Here’s the example folder structure with three problem set projects:\nstatic\n└── projects\n    ├── problem_set_1\n    │   ├── data\n    │   │   └── stuff.csv\n    │   ├── problem_set_1.Rproj\n    │   └── work.Rmd\n    ├── problem_set_2\n    │   ├── data\n    │   │   └── other_stuff.csv\n    │   ├── problem_set_2.Rproj\n    │   └── work.Rmd\n    └── problem_set_3\n        ├── code.R\n        ├── data\n        │   └── more_stuff.csv\n        ├── problem_set_3.Rproj\n        └── work.Rmd\nIn the end, what I want is something like this, with .zip files for each of the problem set folders (starred):\nstatic\n└── projects\n    ├── problem_set_1\n    │   ├── data\n    │   │   └── stuff.csv\n    │   ├── problem_set_1.Rproj\n    │   └── work.Rmd\n    ├── ⭐️ problem_set_1.zip\n    ├── problem_set_2\n    │   ├── data\n    │   │   └── other_stuff.csv\n    │   ├── problem_set_2.Rproj\n    │   └── work.Rmd\n    ├── ⭐️ problem_set_2.zip\n    ├── problem_set_3\n    │   ├── code.R\n    │   ├── data\n    │   │   └── more_stuff.csv\n    │   ├── problem_set_3.Rproj\n    │   └── work.Rmd\n    └── ⭐️ problem_set_3.zip"
  },
  {
    "objectID": "blog/2020/01/10/makefile-subdirectory-zips/index.html#basic-approach",
    "href": "blog/2020/01/10/makefile-subdirectory-zips/index.html#basic-approach",
    "title": "Automatically zip up subdirectories with Make",
    "section": "Basic approach",
    "text": "Basic approach\nThe basic syntax for Makefile recipes is fairly simple:\nfile_to_create: dependencies\n    stuff to run to create file using dependencies\nIf I want to create a zipped file of one problem set project, I can add this to a file named Makefile (no extension), and then run make static/projects/problem_set_1.zip from the terminal:\nstatic/projects/problem_set_1.zip:\n    zip -r static/projects/problem_set_1.zip static/projects/problem_set_1\nThe -r flag means that all subdirectories in static/projects/problem_set_1 will be included. If I type make static/projects/problem_set_1.zip from the terminal, it should compress the folder and all its subfolders into a single .zip file.\n&gt; make static/projects/problem_set_1.zip\n\nzip -r static/projects/problem_set_1.zip static/projects/problem_set_1\n  adding: static/projects/problem_set_1/ (stored 0%)\n  adding: static/projects/problem_set_1/work.Rmd (stored 0%)\n  adding: static/projects/problem_set_1/.DS_Store (deflated 97%)\n  adding: static/projects/problem_set_1/problem_set_1.Rproj (deflated 28%)\n  adding: static/projects/problem_set_1/data/ (stored 0%)\n  adding: static/projects/problem_set_1/data/stuff.csv (stored 0%)"
  },
  {
    "objectID": "blog/2020/01/10/makefile-subdirectory-zips/index.html#problems-with-the-basic-approach",
    "href": "blog/2020/01/10/makefile-subdirectory-zips/index.html#problems-with-the-basic-approach",
    "title": "Automatically zip up subdirectories with Make",
    "section": "Problems with the basic approach",
    "text": "Problems with the basic approach\nHowever, there are a few issues:\n\nIf I run this again, zip will add files to the .zip. If any were deleted from the actual folder, they’ll stay inside the .zip file (i.e. the folder and the .zip file won’t be synchronized).\nThis will include files and folders that begin with . (like .Rhistory and .Rproj.user/), and I don’t want those.\nThe command will include all the parent folders in the zipped file. After I extract it, the actual files will be nested inside static/projects/.\nThere are no dependencies, so this will happen every time I run make static/projects/problem_set_1.zip, even if nothing changed.\nThis is for a single target only. In theory, I’d need to manually add separate entries for each folder in static/projects, and that’s tedious."
  },
  {
    "objectID": "blog/2020/01/10/makefile-subdirectory-zips/index.html#tweaking-the-zip-recipe",
    "href": "blog/2020/01/10/makefile-subdirectory-zips/index.html#tweaking-the-zip-recipe",
    "title": "Automatically zip up subdirectories with Make",
    "section": "Tweaking the zip recipe",
    "text": "Tweaking the zip recipe\nWe can fix the first two issues with some adjustments to the zip command. Adding the -FS flag (file sync) will ensure that zip syncs the files between the source directory and the zipped files, and including the -x flag allows us to exclude patterns of files from the zipped file. Doing this will both sync files and exclude files/folders with a . prefix:\nstatic/projects/problem_set_1.zip:\n    zip -FSr static/projects/problem_set_1.zip static/projects/problem_set_1 -x static/projects/problem_set_1/.\\*\n&gt; make static/projects/problem_set_1.zip\n\nzip -FSr static/projects/problem_set_1.zip static/projects/problem_set_1 -x static/projects/problem_set_1/.\\*\n  adding: static/projects/problem_set_1/ (stored 0%)\n  adding: static/projects/problem_set_1/work.Rmd (stored 0%)\n  adding: static/projects/problem_set_1/problem_set_1.Rproj (deflated 28%)\n  adding: static/projects/problem_set_1/data/ (stored 0%)\n  adding: static/projects/problem_set_1/data/stuff.csv (stored 0%)\nThe third issue with parent folders can’t be solved directly with zip. The problem occurs because we’re running make from the root of the project, so it’s including the whole nested file structure. To solve this, we can have make navigate to static/projects before zipping anything, which then simplifies the filenames in the command:\nstatic/projects/problem_set_1.zip:\n    cd static/projects && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\*\n&gt; make static/projects/problem_set_1.zip\n\ncd static/projects && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\*\n  adding: problem_set_1/ (stored 0%)\n  adding: problem_set_1/work.Rmd (stored 0%)\n  adding: problem_set_1/problem_set_1.Rproj (deflated 28%)\n  adding: problem_set_1/data/ (stored 0%)\n  adding: problem_set_1/data/stuff.csv (stored 0%)\nNo more nested parent folders!"
  },
  {
    "objectID": "blog/2020/01/10/makefile-subdirectory-zips/index.html#adding-dependencies",
    "href": "blog/2020/01/10/makefile-subdirectory-zips/index.html#adding-dependencies",
    "title": "Automatically zip up subdirectories with Make",
    "section": "Adding dependencies",
    "text": "Adding dependencies\nRight now, there are no dependencies, which means that make will run zip regardless of whether it needs to. Even if nothing is changed in the problem_set_1 folder, a new .zip file will get created (and it’ll get reuploaded to the server because it’ll have a new creation date, which means there will be a lot of unnecessary uploading of potentially large files).\nNormally, we’d have to define dependencies like this:\nstatic/projects/problem_set_1.zip: {DEPENDENT FILES}\n    cd static/projects && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\*\nThere’s no way to use a folder as a dependency, which means we need to list all the possible files in the project:\nstatic/projects/problem_set_1.zip: static/projects/problem_set_1/work.Rmd static/projects/problem_set_1/problem_set_1.Rproj static/projects/problem_set_1/data/stuff.csv\n    cd static/projects && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\*\nAnd that’s horrifically long and awful.\nInstead of manually typing out all the dependencies, we can use a little bash trick to get a list of all the files in the folder and generate the list of dependencies using $(shell ...). To see this in action more easily, it’s helpful to assign this shell command to a variable and create a new temporary target to show it\nFILES_IN_FOLDER = $(shell find static/projects/problem_set_1 -type f)\n\ndebug:\n    @echo $(FILES_IN_FOLDER)\nNow if you run make debug, you should see a list of all the files in the given folder (since that list was stored as FILES_IN_FOLDER; the @ in front of echo suppresses the actual call to echo, so you should just see the output of the command).\n&gt; make debug\n\nstatic/projects/problem_set_1/work.Rmd static/projects/problem_set_1/.DS_Store static/projects/problem_set_1/problem_set_1.Rproj static/projects/problem_set_1/data/stuff.csv\nYou might see that it found invisible files like static/projects/problem_set_1/.DS_Store. We don’t want to include those as dependencies (especially files in .Rproj.user, since those get modified every time you do anything in RStudio), so we can modify the find command to exclude files that start with .:\nFILES_IN_FOLDER = $(shell find static/projects/problem_set_1 -type f ! -path \"static/projects/problem_set_1/.*\")\n\ndebug:\n    @echo $(FILES_IN_FOLDER)\n&gt; make debug\n\nstatic/projects/problem_set_1/work.Rmd static/projects/problem_set_1/problem_set_1.Rproj static/projects/problem_set_1/data/stuff.csv\nWith that $(shell ...) magic working, we can include it in the recipe as a dependency:\nstatic/projects/problem_set_1.zip: $(shell find static/projects/problem_set_1 -type f ! -path \"static/projects/problem_set_1/.*\")\n    cd static/projects && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\*\nHOWEVER, this still won’t quite work. Put simply (and likely incorrectly, but it makes sense) make expands $() variables after it determines dependency rules, so it will treat $(shell ...) like an actual dependency instead of treating the output of $(shell ...) as dependencies. To get around this, we can enable secondary expansion (see also), which delays the creation of dependency rules until after $(shell ...) is run. To enable it, include .SECONDEXPANSION: somewhere and then use $$() instead of $() when expanding variables:\n.SECONDEXPANSION:\n\nstatic/projects/problem_set_1.zip: $$(shell find static/projects/problem_set_1 -type f ! -path \"static/projects/problem_set_1/.*\")\n    cd static/projects && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\*\n&gt; make static/projects/problem_set_1.zip\n\ncd static/projects && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\*\n  adding: problem_set_1/ (stored 0%)\n  adding: problem_set_1/work.Rmd (stored 0%)\n  adding: problem_set_1/problem_set_1.Rproj (deflated 28%)\n  adding: problem_set_1/data/ (stored 0%)\n  adding: problem_set_1/data/stuff.csv (stored 0%)\nPhew. Now all the non-invisible files inside problem_set_1/ are dependencies. If I run it again without changing anything, nothing should get zipped:\n&gt; make static/projects/problem_set_1.zip\n\nmake: `static/projects/problem_set_1.zip' is up to date."
  },
  {
    "objectID": "blog/2020/01/10/makefile-subdirectory-zips/index.html#automatic-variables",
    "href": "blog/2020/01/10/makefile-subdirectory-zips/index.html#automatic-variables",
    "title": "Automatically zip up subdirectories with Make",
    "section": "Automatic variables",
    "text": "Automatic variables\nEverything’s working great so far! We’ve addressed the first 4 of the 5 issues we found before. The only thing we have left is automating this so we don’t have to make another target recipe for static/projects/problem_set_2.zip and static/projects/problem_set_3.zip and so on. In the end, we want to be able to type make zip_projects and have make compress each of the subfolders automatically, based on changes in dependencies. To do this, we can use automatic variables (see also) to generate targets on the fly. Here’s how we do this.\nFirst, we can generate a list of all the subdirectories we want to compress, and then modify that list so that it becomes a list of all the targets we want to create (i.e. problem_set_1.zip, problem_set_2.zip, and so on). We’ll use some built-in make functions for manipulating text and searching for files to create some variables. Again, it’s easiest to see what these variables actually are if you create a temporary target like debug. Check the documentation for $(filter ...), $(wildcard ...), $(patsubst ...), and $(addsuffix ...) for more details about what these functions do.\nTO_ZIP_DIRS = $(filter %/, $(wildcard static/projects/*/))  # Find all directories in static/projects\nTO_ZIP_NAMES = $(patsubst %/,%,$(TO_ZIP_DIRS))  # Remove trailing /\nZIP_TARGETS = $(addsuffix .zip,$(TO_ZIP_NAMES))  # Add .zip\n\ndebug:\n    @echo $(TO_ZIP_DIRS)\n    @echo $(TO_ZIP_NAMES)\n    @echo $(ZIP_TARGETS)\n&gt; make debug\n\nstatic/projects/problem_set_1/ static/projects/problem_set_2/ static/projects/problem_set_3/\nstatic/projects/problem_set_1 static/projects/problem_set_2 static/projects/problem_set_3\nstatic/projects/problem_set_1.zip static/projects/problem_set_2.zip static/projects/problem_set_3.zip\nNeat. $(ZIP_TARGETS) now has a list of zipped files that we want to create. We can use that list as an actual target in a recipe. For now, we’ll just use echo so we can see what’s going on with the variable names. Note how I created a new target called zip_projects—this is what we’ll actually run in the terminal. It will look at the list in $(ZIP_TARGETS) and run the appropriate recipe for each one (which for now means it’ll just print the name of the file). Also notice the $@. This represents the name of the target that is passed to the recipe. Run make zip_projects and you should see a list of future filenames:\n$(ZIP_TARGETS):\n    @echo $@\n\nzip_projects: $(ZIP_TARGETS)\n&gt; make zip_projects\n\nstatic/projects/problem_set_1.zip\nstatic/projects/problem_set_2.zip\nstatic/projects/problem_set_3.zip\nNext we need to modify the big hairy cd static/projects && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\* command to use the automatic names in $(ZIP_TARGETS). We have access to the full target name (static/projects/problem_set_1.zip) as the special $@ variable. We can use other built-in functions to extract pieces of that string. Specifically, we need these things:\n\nstatic/projects, or the level below the base name of the future .zip file\nproblem_set_1.zip, or the parent-directory-less name of the future .zip file\nproblem_set_1, or the parent-directory-less and extension-less name of the future .zip file\n\nWe can extract each of those with different file name functions:\n\n$(basename $@) will lead to static/projects/problem_set_1. We can navigate to this folder with cd and then move back a level with .. to get static/projects\n$(notdir $@) will lead to problem_set_1.zip\n$(notdir $(basename $@)) will lead to problem_set_1\n\nYou can check all these by adding them to the recipe temporarily:\n$(ZIP_TARGETS):\n    @echo $@\n    @echo $(basename $@)\n    @echo $(notdir $@)\n    @echo $(notdir $(basename $@))\n\nzip_projects: $(ZIP_TARGETS)\n&gt; make zip_projects\n\nstatic/projects/problem_set_1.zip\nstatic/projects/problem_set_1\nproblem_set_1.zip\nproblem_set_1\nstatic/projects/problem_set_2.zip\nstatic/projects/problem_set_2\nproblem_set_2.zip\nproblem_set_2\nstatic/projects/problem_set_3.zip\nstatic/projects/problem_set_3\nproblem_set_3.zip\nproblem_set_3\nWith all those pieces of filenames, we can replace the hardcoded values of our hairy zip command with automatic versions:\n$(ZIP_TARGETS):\n    cd $(basename $@)/.. && zip -FSr $(notdir $@) $(notdir $(basename $*)) -x $(notdir $(basename $*))/.\\*\n&gt; make zip_projects\n\ncd static/projects/problem_set_1/.. && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\*\n  adding: problem_set_1/ (stored 0%)\n  adding: problem_set_1/work.Rmd (stored 0%)\n  adding: problem_set_1/problem_set_1.Rproj (deflated 28%)\n  adding: problem_set_1/data/ (stored 0%)\n  adding: problem_set_1/data/stuff.csv (stored 0%)\ncd static/projects/problem_set_2/.. && zip -FSr problem_set_2.zip problem_set_2 -x problem_set_2/.\\*\n  adding: problem_set_2/ (stored 0%)\n  adding: problem_set_2/work.Rmd (stored 0%)\n  adding: problem_set_2/problem_set_2.Rproj (deflated 28%)\n  adding: problem_set_2/data/ (stored 0%)\n  adding: problem_set_2/data/other_stuff.csv (stored 0%)\ncd static/projects/problem_set_3/.. && zip -FSr problem_set_3.zip problem_set_3 -x problem_set_3/.\\*\n  adding: problem_set_3/ (stored 0%)\n  adding: problem_set_3/work.Rmd (stored 0%)\n  adding: problem_set_3/code.R (stored 0%)\n  adding: problem_set_3/problem_set_3.Rproj (deflated 28%)\n  adding: problem_set_3/data/ (stored 0%)\n  adding: problem_set_3/data/more_stuff.csv (stored 0%)\nHoly moly! It zipped each individual folder! We’re almost done! The only thing we’re missing is dependencies—right now, if we run this again, it’ll rezip everything again, which we don’t want. We need to add our $$(shell ..) incantation as a dependency, but we need to make it specific to each target: i.e. it needs to generate a list of dependent files for each of the future zip files (only the contents of problem_set_1 when making problem_set_1.zip, etc.). To make that work, we can use a % wildcard in the dependency definition:\n$(ZIP_TARGETS): %.zip : $$(shell find % -type f ! -path \"%/.*\")\n    cd $(basename $@)/.. && zip -FSr $(notdir $@) $(notdir $(basename $@)) -x $(notdir $(basename $@))/.\\*\nIf we manually delete any zipped files and run make zip_projects, it’ll generate them as expected. Where this is magical, though, is if I edit one of the files (like word.Rmd in problem_set_1) and then rerun make zip_projects, it will only rezip that project:\n&gt; make zip_projects\n\ncd static/projects/problem_set_1/.. && zip -FSr problem_set_1.zip problem_set_1 -x problem_set_1/.\\*\nupdating: problem_set_1/work.Rmd (stored 0%)\nAnd that’s it! We’re done!\nThe final recipe is extraordinarily cryptic, but because we built it up slowly, we should know what each of the pieces ($@, %, $(ZIP_TARGETS), $(basename $@), etc.) are.\n\ntl;dr final Makefile\nHere’s the complete final Makefile without all the intermediate steps:\nTO_ZIP_DIRS = $(filter %/, $(wildcard static/projects/*/))  # Find all directories in static/projects\nTO_ZIP_NAMES = $(patsubst %/,%,$(TO_ZIP_DIRS))  # Remove trailing /\nZIP_TARGETS = $(addsuffix .zip,$(TO_ZIP_NAMES))  # Add .zip\n\n.SECONDEXPANSION:\n\n$(ZIP_TARGETS): %.zip : $$(shell find % -type f ! -path \"%/.*\")\n    cd $(basename $@)/.. && zip -FSr $(notdir $@) $(notdir $(basename $@)) -x $(notdir $(basename $@))/.\\*\n\nzip_projects: $(ZIP_TARGETS)\nYou can incorporate this into any other Makefile, like this one that I use for building a course website. There, the zip_projects target is a dependency of the build target, so I just have to run make build to automatically zip everything up and build the site with blogdown.\nPerfection. Any subfolder in static/projects/ will automatically be zipped up with no input from me. No more accidentally forgetting to zip up things or include invisible files!"
  },
  {
    "objectID": "blog/2019/10/09/convert-md-rtf-macos-services/index.html",
    "href": "blog/2019/10/09/convert-md-rtf-macos-services/index.html",
    "title": "Convert Markdown to rich text (with syntax highlighting!) in any macOS app",
    "section": "",
    "text": "GSU uses Microsoft’s Office365 for e-mail, which is fine. My previous institutions—Duke and BYU—both use it too, and it’s pretty standard. GSU also enforces 2-factor authentication (2FA) with Duo, which is also fine. Everybody should use some sort of 2FA for all their important accounts!\nHowever, for whatever reason, GSU’s version of Duo’s 2FA doesn’t allow you to generate app-specific passwords for things like e-mail. Instead, any e-mail client I use has to have support for Microsoft’s special Modern Authentication system, which opens up a popup window to handle the 2FA and logging in and everything. The issue with this is that very few e-mail clients support Modern Authentication. In the macOS world, the only program that supports it is Apple Mail. That’s all.\nThis means I’ve had to move away from my favorite e-mail client ever: Airmail. Airmail is fast, looks nice, and has great search features. Most importantly for me, though, is that it let you write e-mails in Markdown and then converted the Markdown text to HTML when you clicked send. This is the coolest thing ever if you use Markdown everywhere normally, but it’s even better when teaching code-heavy classes. I could respond to student questions by typing stuff like:\n…and Airmail would convert that to nicely formatted HTML. So convenient!\nApple Mail can’t do this.\nHowever, through the magic of macOS services, Bash scripting, and AppleScript, I’ve found a way to convert Markdown text to richly formatted text, and it’s delightful!\nHere’s how to do it."
  },
  {
    "objectID": "blog/2019/10/09/convert-md-rtf-macos-services/index.html#macos-services",
    "href": "blog/2019/10/09/convert-md-rtf-macos-services/index.html#macos-services",
    "title": "Convert Markdown to rich text (with syntax highlighting!) in any macOS app",
    "section": "macOS services",
    "text": "macOS services\nmacOS comes with Automator, a program that lets you create workflows for repeated tasks. One kind of workflow is called a Service (or Quick Action), which can take text (or a file), do stuff to it, and spit out new text. Here’s a super basic example that takes selected text, converts it to uppercase with the tr bash command, copies it to the system-wide clipboard using the pbcopy shell script, gets the contents of the clipboard, and then replaces the selected text:\n\n\n\nWorkflow to make text uppercase\n\n\nIf you save this as a Quick Action, macOS will put it in a folder named ~/Library/Services. Once it’s there, it’ll be accessible in any program that lets you type, like TextEdit or Mail. Type some text in TextEdit, select it, and go to the TextEdit → Services menu. You should see the “Make uppercase” service. If you click on it, your text will be converted to uppercase. Magic.\n\n\n\nServices menu in TextEdit\n\n\nYou can make these easier to run by assigning them keyboard shortcuts. Go to System Preferences → Keyboard → Shortcuts → Services, scroll down the list until you find the “Make uppercase” service, and add a shortcut for it. Now you can convert text to upper case in any application by selecting it and pressing the keyboard shortcut. Super magic.\n\n\n\nAssigning a keyboard shortcut to a service"
  },
  {
    "objectID": "blog/2019/10/09/convert-md-rtf-macos-services/index.html#markdown-to-rtf-basic",
    "href": "blog/2019/10/09/convert-md-rtf-macos-services/index.html#markdown-to-rtf-basic",
    "title": "Convert Markdown to rich text (with syntax highlighting!) in any macOS app",
    "section": "Markdown to RTF, basic",
    "text": "Markdown to RTF, basic\nRather than converting text to uppercase, we can make a service that pipes Markdown text through pandoc, converts it to nicely styled RTF, and replaces the selected text with the nicely styled text.\nMake a new Quick Action in Automator that looks like this (I named it md2rtf):\n\n\n\nSimple md2rtf workflow\n\n\nThe shell script should look like this:\n# !/bin/bash\nexport LC_CTYPE=UTF-8\n/usr/local/bin/pandoc -t rtf -s | pbcopy\nThis will run your text through pandoc, convert it to RTF, and copy the results to the clipboard. The “Get Contents of Clipboard” will then grab the formatted text from the clipboard and replace the selected text with it.\nWatch it in action here:\n\n\nWith this service, I can type in Markdown in Mail, convert it all to rich text, and then send, which is really convenient!"
  },
  {
    "objectID": "blog/2019/10/09/convert-md-rtf-macos-services/index.html#markdown-to-rtf-with-syntax-highlighting",
    "href": "blog/2019/10/09/convert-md-rtf-macos-services/index.html#markdown-to-rtf-with-syntax-highlighting",
    "title": "Convert Markdown to rich text (with syntax highlighting!) in any macOS app",
    "section": "Markdown to RTF, with syntax highlighting",
    "text": "Markdown to RTF, with syntax highlighting\nHowever, it’s not quite perfect. RTF doesn’t support syntax highlighting, so if I convert any code, it’ll format it in monospaced Courier font (which is good!) that is just plain black (which is less good!). HTML output does support syntax highlighting, though, so it’d be nice if there was a way to take Markdown text and replace it with converted HTML.\nJust changing the pandoc script to /usr/local/bin/pandoc -t html -s | pbcopy won’t work, though. It’ll convert the file to HTML, as expected, but it’ll replace your text with all the HTML tags instead of rendered HTML, which is less than ideal.\n\n\n\nIncorrect unrendered raw HTML\n\n\nSo instead, we need to convert to HTML, somehow render that HTML to rich text, and then replace the text with that instead. Fortunately someone asked a similar question on StackOverflow in 2012, and there’s a solution we can adapt! We basically convert HTML to raw hex code, then convert the hex code to HTML with AppleScript, which renders the HTML correctly. It seems (and is) convoluted, but it works!\nChange the shell script in the Automator workflow to this:\n# !/bin/bash\nexport LC_CTYPE=UTF-8\nhex=`/usr/local/bin/pandoc -t html -s --highlight-style pygments | hexdump -ve '1/1 \"%.2x\"'`\nosascript -e \"set the clipboard to «data HTML${hex}»\"\n\n\n\nBetter md2rtf workflow\n\n\nThis will take the selected text, convert it to HTML, convert the raw HTML to hex codes, convert the hex code to rendered HTML, and replace the selected text with that.\nHere’s what it looks like:\n\n\nThis is almost perfect! The only minor issue is that the non-code text switched from Helvetica (TextEdit’s and Apple Mail’s default) to Times New Roman, which isn’t great. It’d be fantastic if the converted HTML used Helvetica instead of Times.\nFortunately there’s a way to fix that. The text is getting converted to Times because the rendered HTML defaults to Times in the absence of any CSS styles telling it to be something else. If we can insert some custom CSS into the converted HTML file with pandoc, we should be able to get the correct font.\nThere’s an argument for pandoc that lets you insert files into the head of the HTML, -H. (There’s also a --css argument, but it doesn’t play well with standalone HTML files, so it’s easier to insert stuff directly into the converted HTML). Create an HTML file somewhere on your computer with this:\n&lt;style type=\"text/css\"&gt;\nbody {\n    font-family: Helvetica, sans-serif;\n}\n&lt;/style&gt;\n(This isn’t raw CSS—it’s CSS wrapped in HTML. We have to do that because pandoc will take that whole file and insert it as HTML in the converted document, so we have to treat it as HTML.)\nChange your Automator workflow one last time so that it injects the custom CSS:\n# !/bin/bash\nexport LC_CTYPE=UTF-8\nhex=`/usr/local/bin/pandoc -t html -s --highlight-style pygments -H ~/path/to/your/css/thing/md2rtf_styles.html | hexdump -ve '1/1 \"%.2x\"'`\nosascript -e \"set the clipboard to «data HTML${hex}»\"\n\n\n\nBest md2rtf workflow\n\n\nWith that addition, the workflow will now take your selected text, convert it to HTML that is styled with Helvetica, convert that to hex code, convert that to rendered HTML, and finally replace your text with impeccable style:\n\n\nI’ve added ⌘⌥^⇧P as the shortcut for this (so I essentially mash down the whole bottom left corner of my keyboard and hit P), and it makes using Apple Mail with Markdown and code quite convenient!"
  },
  {
    "objectID": "blog/2019/01/29/diff-means-half-dozen-ways/index.html",
    "href": "blog/2019/01/29/diff-means-half-dozen-ways/index.html",
    "title": "Half a dozen frequentist and Bayesian ways to measure the difference in means in two groups",
    "section": "",
    "text": "(See this notebook on GitHub)\nTaking a sample from two groups from a population and seeing if there’s a significant or substantial difference between them is a standard task in statistics. Measuring performance on a test before and after some sort of intervention, measuring average GDP in two different continents, measuring average height in two groups of flowers, etc.—we like to know if any group differences we see are attributable to chance / measurement error, or if they’re real.\nClassical frequentist statistics typically measures the difference between groups with a t-test, but t-tests are 100+ years old and statistical methods have advanced a lot since 1908. Nowadays, we can use simulation and/or Bayesian methods to get richer information about the differences between two groups without worrying so much about the assumptions and preconditions for classical t-tests.\nMostly as a resource to future me, here are a bunch of different ways to measure the difference in means in two groups. I’ve done them all in real life projects, but I’m tired of constantly searching my computer for the code to do them:)\nThese ways can all be adapted to different situations (i.e. difference in proportions, one-sample difference in means, etc.). The process for simulation and Bayesian approaches will be roughly the same, while for frequentist approaches, you’ll need to walk through a statistical test workflow to find the appropriate test.\nAlso, this is long and really kind of meant as a general reference. Here’s a tl;dr table of contents:"
  },
  {
    "objectID": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#data",
    "href": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#data",
    "title": "Half a dozen frequentist and Bayesian ways to measure the difference in means in two groups",
    "section": "Data",
    "text": "Data\nFirst we need some data to play with. We’ll use the ggplot2movies package, which contains information about almost 60,000 movies from IMDB from 1893 to 2005. For this example, we want to see if there is a significant/substantial/real difference in the average IMDB rating for action movies and comedies. Are people more likely to rate comedies higher than action movies?\nInstead of working with all 20,407 action movies and comedies, we take a random sample of 200 each. (This is just so we can have some variation in the group averages—if we work with all 20,000, the confidence intervals for each group average basically disappear since there are so many observations.)\n# Load libraries\nlibrary(tidyverse)  # ggplot, dplyr, and friends\nlibrary(ggridges)   # Ridge plots\nlibrary(ggstance)   # Horizontal pointranges and bars\nlibrary(patchwork)  # Lay out multiple ggplot plots; install from https://github.com/thomasp85/patchwork\nlibrary(scales)     # Nicer formatting for numbers\nlibrary(broom)      # Convert model results to tidy data frames\nlibrary(infer)      # Statistical inference with simulation\nlibrary(rstan)      # R interface to Stan\nlibrary(brms)       # Run Stan-based models with standard R syntax\nlibrary(ggplot2movies)  # Lots of movies from IMDB\n\n\n# Clean up data\nset.seed(1234)  # Set seed so we get the same sampled rows every time\nmovies_clean &lt;- movies %&gt;% \n  # Make a binary column for genre\n  select(title, year, rating, Action, Comedy) %&gt;% \n  filter(!(Action == 1 & Comedy == 1)) %&gt;% \n  mutate(genre = case_when(Action == 1 ~ \"Action\",\n                           Comedy == 1 ~ \"Comedy\",\n                           TRUE ~ \"Neither\")) %&gt;%\n  filter(genre != \"Neither\") %&gt;%\n  # Make a numeric version of genre, where action = 1, comedy = 2\n  mutate(genre_numeric = as.numeric(factor(genre))) %&gt;% \n  # Make genre a factor\n  mutate(genre = factor(genre)) %&gt;% \n  select(-Action, -Comedy) %&gt;% \n  # Randomly select 200 movies in each genre\n  group_by(genre) %&gt;% \n  sample_n(200) %&gt;% \n  ungroup()\nTo get a sense of the data, we’ll do some quick exploratory data analysis with a bunch of different graph types.\n# Make a custom theme\n# I'm using Asap Condensed; download from \n# https://fonts.google.com/specimen/Asap+Condensed\ntheme_fancy &lt;- function() {\n  theme_minimal(base_family = \"Asap Condensed\") +\n    theme(panel.grid.minor = element_blank())\n}\n\neda_boxplot &lt;- ggplot(movies_clean, aes(x = genre, y = rating, fill = genre)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) + \n  scale_y_continuous(breaks = seq(1, 10, 1)) +\n  labs(x = NULL, y = \"Rating\") +\n  theme_fancy()\n\neda_histogram &lt;- ggplot(movies_clean, aes(x = rating, fill = genre)) +\n  geom_histogram(binwidth = 1, color = \"white\") +\n  scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) + \n  scale_x_continuous(breaks = seq(1, 10, 1)) +\n  labs(y = \"Count\", x = \"Rating\") +\n  facet_wrap(~ genre, nrow = 2) +\n  theme_fancy() +\n  theme(panel.grid.major.x = element_blank())\n\neda_ridges &lt;- ggplot(movies_clean, aes(x = rating, y = fct_rev(genre), fill = genre)) +\n  stat_density_ridges(quantile_lines = TRUE, quantiles = 2, scale = 3, color = \"white\") + \n  scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) + \n  scale_x_continuous(breaks = seq(0, 10, 2)) +\n  labs(x = \"Rating\", y = NULL,\n       subtitle = \"White line shows median rating\") +\n  theme_fancy()\n\n(eda_boxplot | eda_histogram) / \n    eda_ridges + \n  plot_annotation(title = \"Do comedies get higher ratings than action movies?\",\n                  subtitle = \"Sample of 400 movies from IMDB\",\n                  theme = theme(text = element_text(family = \"Asap Condensed\"),\n                                plot.title = element_text(face = \"bold\",\n                                                          size = rel(1.5))))\n\n\nExploratory data analysis\n\nInitially, it looks like there might be a difference in average rating. Comedies tend to have higher ratings. We can calculate the difference with some dplyr group_by() %&gt;% summarize():\ngroup_diffs &lt;- movies_clean %&gt;% \n  group_by(genre) %&gt;% \n  summarize(avg_rating = mean(rating, na.rm = TRUE)) %&gt;% \n  mutate(diff_means = avg_rating - lead(avg_rating))\ngroup_diffs\n\n## # A tibble: 2 x 3\n##   genre  avg_rating diff_means\n##   &lt;fct&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n## 1 Action       5.28     -0.682\n## 2 Comedy       5.97     NA\nYep. There’s a -0.6825 point difference in ratings. Action movies score 0.7 points lower than comedies, on average.\nBut how certain are we that that difference is real and not just due to sampling error? It’s time for inference!"
  },
  {
    "objectID": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#classical-frequentist-t-tests",
    "href": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#classical-frequentist-t-tests",
    "title": "Half a dozen frequentist and Bayesian ways to measure the difference in means in two groups",
    "section": "Classical frequentist t-tests",
    "text": "Classical frequentist t-tests\nt-test, assuming equal variances\nWe can use a standard frequentist t-test to check if the group means are different. We can assume that the variances in the two groups are the same and run t.test():\n# Assume equal variances\nt_test_eq &lt;- t.test(rating ~ genre, data = movies_clean, var.equal = TRUE)\nt_test_eq\n\n## \n##  Two Sample t-test\n## \n## data:  rating by genre\n## t = -4.4753, df = 398, p-value = 0.000009977\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.9823168 -0.3826832\n## sample estimates:\n## mean in group Action mean in group Comedy \n##               5.2845               5.9670\nThe default output is helpful—the p-value is really tiny, which means there’s a tiny chance that we’d see a difference that big in group means in a world where there’s no difference. However, in this format, it’s hard to extract any of these values for later use, like in plotting. We can use the tidy() function from the broom library to convert these t-test results to a nice data frame.\nt_test_eq_tidy &lt;- tidy(t_test_eq) %&gt;% \n  # Calculate difference in means, since t.test() doesn't actually do that\n  mutate(estimate = estimate1 - estimate2) %&gt;%\n  # Rearrange columns\n  select(starts_with(\"estimate\"), everything())\n\nt_test_eq_tidy\n\n## # A tibble: 1 x 10\n##   estimate1 estimate2 estimate statistic p.value parameter conf.low\n##       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1      5.28      5.97   -0.682     -4.48 9.98e-6       398   -0.982\n## # … with 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n## #   alternative &lt;chr&gt;\nBy default, though, R does not assume that the variance in the two groups’ populations is equal, which is probably a reasonable thing to do. There may be structural differences in how comedies and action movies are produced, which in turn leads to structural differences in how they’re rated.\nThere are several systematic ways to check if the two groups have equal variance. For all these tests, the null hypothesis is that the two groups have similar (homogeneous) variances. If the p-value is less than 0.05, we can assume that they have unequal or heterogeneous variances. (Here’s a helpful overview of this process.)\n\n\nBartlett test: Check homogeneity of variances based on the mean\nbartlett.test(rating ~ genre, data = movies_clean)\n\n## \n##  Bartlett test of homogeneity of variances\n## \n## data:  rating by genre\n## Bartlett's K-squared = 0.10006, df = 1, p-value = 0.7518\n\n\nLevene test: Check homogeneity of variances based on the median, so it’s more robust to outliers\n# Install the car package first\ncar::leveneTest(rating ~ genre, data = movies_clean)\n\n## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value Pr(&gt;F)\n## group   1  0.4917 0.4836\n##       398\n\n\nFligner-Killeen test: Check homogeneity of variances based on the median, so it’s more robust to outliers\nfligner.test(rating ~ genre, data = movies_clean)\n\n## \n##  Fligner-Killeen test of homogeneity of variances\n## \n## data:  rating by genre\n## Fligner-Killeen:med chi-squared = 0.78337, df = 1, p-value = 0.3761\n\n\nKruskal-Wallis test: Check homogeneity of distributions nonparametrically\nkruskal.test(rating ~ genre, data = movies_clean)\n\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  rating by genre\n## Kruskal-Wallis chi-squared = 19.787, df = 1, p-value = 0.000008655\n\n\nPhew. In all these tests except the Kruskall-Wallis test, we don’t have enough evidence to conclude that the variances are different, so we’re probably safe leaving var.equal = TRUE on.\nt-test, assuming unequal variance\nWe can run a t-test assuming that the two groups have unequal variances by setting var.equal = FALSE, or just leaving it off. I generally just do this instead of going through all the tests for equal variance.\n# Assume unequal variances\nt_test_uneq &lt;- t.test(rating ~ genre, data = movies_clean)\nt_test_uneq_tidy &lt;- tidy(t_test_uneq) %&gt;% \n  mutate(estimate = estimate1 - estimate2) %&gt;% \n  select(starts_with(\"estimate\"), everything())\nt_test_uneq_tidy\n\n## # A tibble: 1 x 10\n##   estimate estimate1 estimate2 statistic p.value parameter conf.low\n##      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1   -0.682      5.28      5.97     -4.48 9.98e-6      398.   -0.982\n## # … with 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n## #   alternative &lt;chr&gt;"
  },
  {
    "objectID": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#simulation-based-tests",
    "href": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#simulation-based-tests",
    "title": "Half a dozen frequentist and Bayesian ways to measure the difference in means in two groups",
    "section": "Simulation-based tests",
    "text": "Simulation-based tests\nInstead of dealing with all the assumptions of the data and finding the exact statistical test written by some dude in the 1940s, we can use the power of bootstrapping, permutation, and simulation to construct a null distribution and calculate confidence intervals. According to Allen Downey, there is actually only one statistical test and that at their core, all statistical tests follow the same universal pattern:\n\n\nStep 1: Calculate a sample statistic, or \\(\\delta\\). This is the main measure you care about: the difference in means, the average, the median, the proportion, the difference in proportions, the chi-squared value, etc.\n\nStep 2: Use simulation to invent a world where \\(\\delta\\) is null. Simulate what the world would look like if there was no difference between two groups, or if there was no difference in proportions, or where the average value is a specific number.\n\nStep 3: Look at \\(\\delta\\) in the null world. Put the sample statistic in the null world and see if it fits well.\n\nStep 4: Calculate the probability that \\(\\delta\\) could exist in null world. This is the p-value, or the probability that you’d see a \\(\\delta\\) at least that high in a world where there’s no difference.\n\nStep 5: Decide if \\(\\delta\\) is statistically significant. Choose some evidentiary standard or threshold (like 0.05) for deciding if there’s sufficient proof for rejecting the null world.\n\nI have a fully commented example of how to do this with the infer package here. Once you get the hang of it, the infer workflow is fairly intuitive and flexible and far more delightful than worrying about classical tests.\nFirst we calculate the difference in means in the actual data:\n# Calculate the difference in means\ndiff_means &lt;- movies_clean %&gt;% \n  specify(rating ~ genre) %&gt;%\n  # Order here means we subtract comedy from action (Action - Comedy)\n  calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\ndiff_means\n\n## # A tibble: 1 x 1\n##     stat\n##    &lt;dbl&gt;\n## 1 -0.682\nThen we can generate a bootstrapped distribution of the difference in means based on our sample and calculate the confidence interval:\nboot_means &lt;- movies_clean %&gt;% \n  specify(rating ~ genre) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\nboostrapped_confint &lt;- boot_means %&gt;% get_confidence_interval()\n\nboot_means %&gt;% \n  visualize() + \n  shade_confidence_interval(boostrapped_confint,\n                            color = \"#8bc5ed\", fill = \"#85d9d2\") +\n  geom_vline(xintercept = diff_means$stat, size = 1, color = \"#77002c\") +\n  labs(title = \"Bootstrapped distribution of differences in means\",\n       x = \"Action − Comedy\", y = \"Count\",\n       subtitle = \"Red line shows observed difference; shaded area shows 95% confidence interval\") +\n  theme_fancy()\n\n\nBootstrapped confidence interval\n\nNeat. We have a simulation-based confidence interval, and it doesn’t contain zero, so we can have some confidence that there’s a real difference between the two groups.\nWe can go through the testing process more thoroughly by following Downey’s process. We’ve already done step one (calculate \\(\\delta\\)); next we generate a world where there’s no difference by shuffling all the action/comedy labels through permutation\n# Step 2: Invent a world where δ is null\ngenre_diffs_null &lt;- movies_clean %&gt;% \n  specify(rating ~ genre) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 5000, type = \"permute\") %&gt;% \n  calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\n# Step 3: Put actual observed δ in the null world and see if it fits\ngenre_diffs_null %&gt;% \n  visualize() + \n  geom_vline(xintercept = diff_means$stat, size = 1, color = \"#77002c\") +\n  scale_y_continuous(labels = comma) +\n  labs(x = \"Simulated difference in average ratings (Action − Comedy)\", y = \"Count\",\n       title = \"Simulation-based null distribution of differences in means\",\n       subtitle = \"Red line shows observed difference\") +\n  theme_fancy()\n\n\nSimulated p-value\n\nThat red line is pretty far to the left and seems like it wouldn’t fit very well in a world where there’s no actual difference between the groups. We can calculate the probability of seeing that red line in a null world (step 4) with get_p_value() (and we can use the cool new pvalue() function in the scales library to format it as &lt; 0.001):\n# Step 4: Calculate probability that observed δ could exist in null world\ngenre_diffs_null %&gt;% \n  get_p_value(obs_stat = diff_means, direction = \"both\") %&gt;% \n  mutate(p_value_clean = pvalue(p_value))\n\n## # A tibble: 1 x 2\n##   p_value p_value_clean\n##     &lt;dbl&gt; &lt;chr&gt;        \n## 1       0 &lt;0.001\nBecause the p-value is so small, it passes pretty much all evidentiary thresholds (p &lt; 0.05, p &lt; 0.01, etc), so we can safely say that there’s a difference between the two groups. Action movies are rated lower, on average, than comedies."
  },
  {
    "objectID": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#bayesian-regression",
    "href": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#bayesian-regression",
    "title": "Half a dozen frequentist and Bayesian ways to measure the difference in means in two groups",
    "section": "Bayesian regression",
    "text": "Bayesian regression\nNext we can leave the world of frequentist statistics and null hypothesis testing and do some Bayesian analysis. Frequentist null hypothesis significance testing (NHST) determines the probability of the data given a null hypothesis (i.e. \\(P(\\text{data} | H)\\), yielding results that are often unwieldy, phrased as the probability of rejecting the null if it is true (hence all that talk of “null worlds” earlier). In contrast, Bayesian analysis determines the probability of a hypothesis given the data (i.e.\\(P(H | \\text{data})\\)), resulting in probabilities that are directly interpretable.\nBayesian analysis is way more computationally intensive and requires more upfront work (e.g prior specification). Because of this, R itself doesn’t have robust support for the actual Monte Carlo sampling and other computational heavy lifting involved in Bayesian stuff. Instead, it can connect to external sampling programs that are optimized for this kind of repetitive simulation work. BUGS and JAGS were popular sampling software tools, but nowadays all the cool kids are using Stan.\nWriting Stan code by hand is tricky since it requires that you essentially learn another programming language. Fortunately, you technically can do Bayesian analysis without writing a single line of Stan code. There are two different packages that precompile Stan code for you:\n\n\nrstanarm: This is written by the Stan team itself and provides functions like stan_lm() and stan_glm() that are drop-in replacements for R’s standard lm() and glm(). The code is highly optimized and precompiled and runs really fast, but there’s no good way to inspect the underlying Stan code. Also, it’s more limited in the types of regression it supports.\n\nbrms: This provides a more universal front end for Stan and supports a wider variety of models. It doesn’t have drop-in lm() replacements, but the brm() function is fairly intuitive after poring through the documentation and examples. It’s slower than rstanarm because it has to compile the Stan scripts it generates with C++, but you can inspect the Stan code behind each of the models with stancode() and better see what’s going on behind the scenes.\n\nThere aren’t really any cool front end packages for running non-regression analyses with Stan from R (there is Rasmus Bååth’s Bayesian First Aid package, but it uses JAGS and hasn’t been updated in a while and I want to use Stan like the cool kids), but you can actually calculate differences in means with regression, which means we can approximate frequentist t-tests without writing any Stan code.\nThis blog post by Matti Vuorre is a masterpiece and clearly explains the rationale behind how and why you can use regression for t-tests. This code here is adapted from his stuff.\nFirst, we’ll set a bunch of Monte Carlo parameters:\nCHAINS &lt;- 4\nITER &lt;- 2000\nWARMUP &lt;- 1000\nBAYES_SEED &lt;- 1234\noptions(mc.cores = parallel::detectCores())  # Use all cores\nRegression, assuming equal variances\nThen we can run the model. We set a couple priors for the intercept (normal, with a mean of 0 and standard deviation of 5) and the beta/slope (normal, with a mean of 0 and a standard deviation of 1). In this model, rating ~ genre, we assume equal variances between the groups.\nWe use tidyMCMC from broom to calculate the medians of the posterior distributions and create confidence intervals.\nbrms_eq &lt;- brm(\n  # bf() is an alias for brmsformula() and lets you specify model formulas\n  bf(rating ~ genre), \n  # Reverse the levels of genre so that comedy is the base case\n  data = mutate(movies_clean, genre = fct_rev(genre)),\n  prior = c(set_prior(\"normal(0, 5)\", class = \"Intercept\"),\n            set_prior(\"normal(0, 1)\", class = \"b\")),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  file = \"cache/brms_eq\"\n)\nbrms_eq_tidy &lt;- \n  tidyMCMC(brms_eq, conf.int = TRUE, conf.level = 0.95, \n           estimate.method = \"median\", conf.method = \"HPDinterval\")\nbrms_eq_tidy\n\n## # A tibble: 3 x 5\n##   term          estimate std.error conf.low conf.high\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 b_Intercept      5.96     0.108     5.74      6.16 \n## 2 b_genreAction   -0.666    0.152    -0.968    -0.374\n## 3 sigma            1.53     0.0529    1.43      1.64\nHere the intercept represents the mean comedy score, while the coefficient for action represents the difference from that mean, or the effect that we care about. These findings match what we got with the frequentist work from earlier, only now we can change the interpretation: We’re 95% confident that the true population-level difference in rating is between -0.968 and -0.374, with a median of -0.666.\nA quick digression into priors\nIf you don’t explicitly set any priors, brms chooses sensible defaults for you. You can see what priors you can potentially set with get_prior():\nget_prior(bf(rating ~ genre), data = movies_clean)\n\n##                 prior     class        coef group resp dpar nlpar bound\n## 1                             b                                        \n## 2                             b genreComedy                            \n## 3 student_t(3, 6, 10) Intercept                                        \n## 4 student_t(3, 0, 10)     sigma\nYou can use this table to select which prior you want to set. For instance, to specify the prior distribution for the intercept, you can use set_prior(\"normal(0, 5)\", class = \"Intercept\"), which matches the table. More complicated formulas will have values in the dpar and nlpar columns, and you can use those to drill down to specific priors for those terms (e.g. set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\")).\nAlso, you can plot the different distributions to get a sense of their shapes with either base R or with ggplot:\n# Normal distribution: normal(0, 5)\ncurve(expr = dnorm(x, mean = 0, sd = 5), from = -20, to = 20)\n\n\n\n\nNormal distribution\n\n\n\n# Cauchy distribution: cauchy(0, 1)\ncurve(expr = dcauchy(x, location = 0, scale = 1), from = -5, to = 5)\n\n\n\n\nCauchy distribution\n\n\n\n# Or do this with ggplot\nnorm_ggplot &lt;- ggplot(data = tibble(x = c(-20, 20)), aes(x = x)) +\n  stat_function(fun = dnorm, n = 500, args = list(mean = 0, sd = 5)) +\n  labs(title = \"normal(0, 5)\") +\n  theme_fancy()\n\ncauchy_ggplot &lt;- ggplot(data = tibble(x = c(-5, 5)), aes(x = x)) +\n  stat_function(fun = dcauchy, n = 500, args = list(location = 0, scale = 1)) +\n  labs(title = \"cauchy(0, 1)\") +\n  theme_fancy()\n\nnorm_ggplot / cauchy_ggplot\n\n\n\n\nNormal and Cauchy distributions with ggplot\n\n\n\nRegression, assuming unequal variances\nWe can also use regression to estimate the difference in means under the assumption that group variances are different. Here, following Matti Vuorre, we need to model the variance (or sigma) in each group, and we need to specify a prior for that term (I chose cauchy(0, 1). We do this by specifying two formulas in brm().\nbrms_uneq &lt;- brm(\n  bf(rating ~ genre, sigma ~ genre), \n  data = mutate(movies_clean, genre = fct_rev(genre)),\n  prior = c(set_prior(\"normal(0, 5)\", class = \"Intercept\"),\n            set_prior(\"normal(0, 1)\", class = \"b\"),\n            set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\")),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  file = \"cache/brms_uneq\"\n)\nbrms_uneq_tidy &lt;- \n  tidyMCMC(brms_uneq, conf.int = TRUE, conf.level = 0.95, \n           estimate.method = \"median\", conf.method = \"HPDinterval\")\nbrms_uneq_tidy\n\n## # A tibble: 4 x 5\n##   term                estimate std.error conf.low conf.high\n##   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 b_Intercept           5.96      0.111     5.73      6.17 \n## 2 b_sigma_Intercept     0.433     0.0494    0.335     0.528\n## 3 b_genreAction        -0.665     0.156    -0.979    -0.371\n## 4 b_sigma_genreAction  -0.0199    0.0702   -0.161     0.113\nFor mathy reasons (again, see Matti Vourre’s post), the sigma terms are on a log scale, so we need to exponentiate them back to the scale of the data.\nbrms_uneq_tidy %&gt;% \n  mutate_at(vars(estimate, std.error, conf.low, conf.high),\n            funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\n\n## # A tibble: 4 x 5\n##   term                estimate std.error conf.low conf.high\n##   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 b_Intercept            5.96      0.111    5.73      6.17 \n## 2 b_sigma_Intercept      1.54      1.05     1.40      1.70 \n## 3 b_genreAction         -0.665     0.156   -0.979    -0.371\n## 4 b_sigma_genreAction    0.980     1.07     0.851     1.12\nHere, the interpretation for the intercept and action coefficients are the same as before: the intercept the average comedy rating; the action coefficient is the change from the average comedy rating, or the effect we care about. The sigmas work similarly: the intercept sigma is the standard deviation for comedies; the intercept sigma + action sigma is the standard deviation for action movies.\nRegression, BEST\nA more robust way of estimating group differences Bayesianly is to use John Kruschke’s Bayesian Estimation Supersedes the t Test (BEST) method. I’m 100% not going into the nitty gritty details of this (Matti Vuorre has, though). In the most simplest terms, the only difference between BEST and the unequal variance regression above is that we model the data with a t distribution, which means we have a new parameter, \\(\\nu\\) (nu), that changes the normality of the distribution (i.e. the degrees of freedom parameter in a t distribution). Kruschke uses an exponential prior with a rate of 1/29 in his paper, so we do too. It looks like this:\nggplot(data = tibble(x = c(0, 100)), aes(x = x)) +\n  stat_function(fun = dexp, n = 500, args = list(rate = 1/29)) +\n  labs(title = \"exponential(1/29)\") +\n  theme_fancy()\n\n\n\n\nExponential distribution\n\n\n\nAdd family = student and set the prior for nu and we’re ready to go:\nbrms_uneq_robust &lt;- brm(\n  bf(rating ~ genre, sigma ~ genre), \n  family = student,\n  data = mutate(movies_clean, genre = fct_rev(genre)),\n  prior = c(set_prior(\"normal(0, 5)\", class = \"Intercept\"),\n            set_prior(\"normal(0, 1)\", class = \"b\"),\n            set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\"),\n            set_prior(\"exponential(1.0/29)\", class = \"nu\")),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  file = \"cache/brms_uneq_robust\"\n)\nbrms_uneq_robust_tidy &lt;- \n  tidyMCMC(brms_uneq_robust, conf.int = TRUE, conf.level = 0.95, \n           estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;% \n  # Rescale sigmas\n  mutate_at(vars(estimate, std.error, conf.low, conf.high),\n            funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\nbrms_uneq_robust_tidy\n\n## # A tibble: 5 x 5\n##   term                estimate std.error conf.low conf.high\n##   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 b_Intercept            5.98      0.104    5.77      6.17 \n## 2 b_sigma_Intercept      1.47      1.06     1.31      1.66 \n## 3 b_genreAction         -0.680     0.148   -0.966    -0.394\n## 4 b_sigma_genreAction    0.999     1.08     0.860     1.16 \n## 5 nu                    31.9      27.5      5.62     95.1\nNeato. Just like the other models, the coefficient for action shows us the difference in means (and it’s super similar to everything else). We can exponentiate the sigma coefficients and see the group standard deviations, and we have a coefficient for \\(\\nu\\), or the degrees of freedom for the underlying distribution."
  },
  {
    "objectID": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#bayesian-analysis-directly-with-stan",
    "href": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#bayesian-analysis-directly-with-stan",
    "title": "Half a dozen frequentist and Bayesian ways to measure the difference in means in two groups",
    "section": "Bayesian analysis, directly with Stan",
    "text": "Bayesian analysis, directly with Stan\nUpdate: Mike DeCrescenzo made a fascinating point on Twitter that relying on regression for t-test-like analysis with Bayesianism actually has important theoretical implications. When specifying a model as y ~ a + bx as we did above with rating ~ genre, the coefficient for genre (b) is actually the difference in means and doesn’t directly reflect the rating itself. In theory, if we’re thinking about two groups with two different variances, we should model the distribution of each group, not the distribution of the differences in groups. Analyzing the distributions of the two groups separately and then calculating the difference should yield more transparent results. And that’s why we can/should do the analysis in Stan by hand like this.\nAnother update: According to TJ Mahr, you can get around this beta-for-the-difference issue by running a model like rating ~ 0 + genre, which suppresses the intercept and estimates coefficients for both groups directly. You have to calculate the difference in coefficients/group means by hand after, but at least you’ll be estimating the right things.\nInstead of using regression models and combining coefficients to figure out group means and differences, we can be brave and write actual Stan code that can return any value we want. We don’t have to be constrained by regression formulas, but we do have to figure out how to code in Stan, which is intimidating.\nThere are lots of resources out there for Stan (and Bayesianism in general), including Richard McElreath’s Statistical Rethinking, which has all its code in raw Stan and brms syntax. John Kruschke’s Doing Bayesian Data Analysis book includes code in both JAGS and Stan (click on the link in step 5 here; the code for BEST analysis is in the file named Stan-Ymet-Xnom2grp-MrobustHet.R).\nBut the resource that made the structure of Stan files (mostly) click for me was a recorded class (specifically the session from 2017-01-30) by Mike Lawrence (a Canadian researcher) that I stumbled on while searching the internet for ways to run BEST with Stan. Though the video is super long (you can skip around and/or watch at 2x speed; the two-sample example starts at minute 52), Mike clearly lays out the process for writing a Stan file and it’s the coolest thing ever. EVEN BETTER is the fact that he provides all the code from his class on Dropbox (see the code from 2017-01-30 for t-test related stuff)\nThrough a combination of Mike Lawrence’s video, Michael Clark’s BEST implementation here, John Kruschke’s Stan code, and lots of debugging, I created this Stan code. Explaining the whole logic behind it goes way beyond the scope of this blog post, but I heavily commented it so it should be easy-ish to follow if you understand Stan files (which I still don’t completely ¯\\_(ツ)_/¯).\nDownload fancy-best.stan and put it in the same folder as whatever script you’re typing in (or change the path to match wherever you put it). Note how the data argument in sampling() doesn’t just take a data frame like in brm(). We have to pass Stan specific pieces of data, all of which are specified in the data{} block in the Stan script.\nstan_best &lt;- \n  sampling(stan_model(\"imdb_best.stan\"), \n           # Make a list of data to send to Stan\n           data = list(N = nrow(movies_clean), \n                       n_groups = length(unique(movies_clean$genre)), \n                       group_id = movies_clean$genre_numeric, \n                       y = movies_clean$rating),\n           chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED)\nstan_best_tidy &lt;- \n  tidyMCMC(stan_best, conf.int = TRUE, conf.level = 0.95, \n           estimate.method = \"median\", conf.method = \"HPDinterval\")\nstan_best_tidy\n\n## # A tibble: 8 x 5\n##   term     estimate std.error conf.low conf.high\n##   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 mu[1]       5.29     0.108     5.09      5.50 \n## 2 mu[2]       5.99     0.104     5.79      6.19 \n## 3 sigma[1]    1.47     0.0838    1.30      1.63 \n## 4 sigma[2]    1.47     0.0885    1.30      1.65 \n## 5 nu         29.0     20.1       6.35     76.2  \n## 6 mu_diff    -0.693    0.147    -0.992    -0.422\n## 7 cohen_d    -0.573    0.123    -0.825    -0.349\n## 8 cles        0.343    0.0318    0.280     0.402\nOne advantage of running the analysis with raw Stan instead of forcing it into regression form is that we can have Stan calculate all sorts of stuff for us. The averages for both groups get returned (mu[1] and mu[2]), along with mu_diff, so there’s no need to combine slope and intercept terms. Also note how the sigma coefficients are already in the right scale. I also (following Kruschke and others) made it so that the Stan code returned information about effect sizes, including Cohen’s d, which is the standardized difference in means, and the common language effect size (CLES), which is the probability that a rating sampled at random from one group will be greater than a rating sampled from the other group.\nHere, the difference in means is the same as all the other methods, but the effect size isn’t terribly huge. According to Cohen’s d, we have a medium effect size, and there’s a 34% chance that we could randomly select an action rating from the comedy distribution."
  },
  {
    "objectID": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#regression-best-with-priors-on-variables-instead-of-difference",
    "href": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#regression-best-with-priors-on-variables-instead-of-difference",
    "title": "Half a dozen frequentist and Bayesian ways to measure the difference in means in two groups",
    "section": "Regression, BEST, with priors on variables instead of difference",
    "text": "Regression, BEST, with priors on variables instead of difference\n(I added this section later)\nIn the Twitter discussion that followed this post, Mike DeCrescenzo made a fascinating point that relying on regression for t-test-like analysis with Bayesianism actually has important theoretical implications.\nWhen specifying a model as y ~ a + bx as we did above with rating ~ genre, the coefficient for genre (b) is actually the difference in means and doesn’t directly reflect the rating itself. In theory, if we’re thinking about two groups with two different variances, we should model the distribution of each group, not the distribution of the differences in groups. Analyzing the distributions of the two groups separately and then calculating the difference should yield more transparent results. Plus it’s easier to think about setting priors on two variables that were measured in real life (e.g. ratings for comedies and action movies), rather than setting priors on whatever the difference between those looks like.\nThe raw Stan code above does this correctly by feeding two groups into the model rather than the difference in means. TJ Mahr and Solomon Kurz’s translation of Kruschke’s code into brms (see near the bottom of chapter 16) both show that we can do this with brms by changing the formula slightly. If we suppress the intercept by running a model like ratiing ~ 0 + genre, brms returns coefficients for each of the groups (no more base case!), and these coefficients represent group means.\nHere’s an intercept-free version of the brms-based BEST regression from earlier. Note how we’re modeling both the rating and the group sigmas without intercepts (rating ~ 0 + genre, sigma ~ 0 + genre), and that we no longer specify a prior for the intercept (if we do, brm() yells at us). Also note that instead of modeling the beta coefficient as a normal distribution centered around zero (since that represented the difference in means), we specify the distribution of action and comedy ratings themselves. Because we’re dealing with actual ratings, we can make them fairly well informed and constrained. For instance, no movie is rated below 1 or above 10, and I’m guessing from past experience looking at ratings on Amazon and IMDB and elsewhere that people tend to inflate their ratings. I’d guess that the distribution of ratings looks something like this: normally distributed with a mean of 6, standard deviation of 2, and truncated at 1 and 10.\n# Cool extra thing I discovered: the msm library has rtnorm, dtnorm, and family\n# which let you plot and draw from a truncated normal distribution\n# msm::rtnorm(1000, mean = 6, sd = 2, lower = 1, upper = 10)\nggplot(data = tibble(x = c(1, 10)), aes(x = x)) +\n  stat_function(fun = dnorm, n = 500, args = list(mean = 6, sd = 2)) +\n  labs(title = \"normal(6, 2); truncated at 1 and 10\") +\n  theme_fancy()\n\n\n\n\nNormal distribution, truncated\n\n\n\nNow we can run the model like normal:\nbrms_uneq_robust_groups &lt;- brm(\n  bf(rating ~ 0 + genre, sigma ~ 0 + genre), \n  family = student,\n  data = mutate(movies_clean, genre = fct_rev(genre)),\n  prior = c(\n    # Set group mean prior\n    set_prior(\"normal(6, 2)\", class = \"b\", lb = 1, ub = 10),\n    # Ser group variance priors. We keep the less informative cauchy(0, 1).\n    set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\"),\n    set_prior(\"exponential(1.0/29)\", class = \"nu\")),\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  file = \"cache/brms_uneq_robust_groups\"\n)\nWe still need to exponentiate the sigma coefficients when we’re done.\nbrms_uneq_robust_groups_tidy &lt;- \n  tidyMCMC(brms_uneq_robust_groups, conf.int = TRUE, conf.level = 0.95, \n           estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;% \n  # Rescale sigmas\n  mutate_at(vars(estimate, std.error, conf.low, conf.high),\n            funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\nbrms_uneq_robust_groups_tidy\n\n## # A tibble: 5 x 5\n##   term                estimate std.error conf.low conf.high\n##   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 b_genreComedy           5.99     0.109     5.77      6.19\n## 2 b_genreAction           5.30     0.107     5.09      5.50\n## 3 b_sigma_genreComedy     1.47     1.06      1.30      1.65\n## 4 b_sigma_genreAction     1.47     1.06      1.31      1.62\n## 5 nu                     29.9     28.1       6.00     92.7\nBecause we calculated the group means themselves, we need to do an extra few steps to get the difference in means. It’s fairly easy: we extract the posterior samples for each of the groups, subtract them from each other, and then calculate the credible interval.\nbrms_uneq_robust_groups_post &lt;- posterior_samples(brms_uneq_robust_groups) %&gt;% \n  # We can exponentiate here!\n  mutate_at(vars(contains(\"sigma\")), funs(exp)) %&gt;% \n  # For whatever reason, we need to log nu?\n  mutate(nu = log10(nu)) %&gt;% \n  mutate(diff_means = b_genreAction - b_genreComedy,\n         diff_sigma = b_sigma_genreAction - b_sigma_genreComedy) %&gt;% \n  # Calculate effect sizes, just for fun\n  mutate(cohen_d = diff_means / sqrt((b_sigma_genreAction + b_sigma_genreComedy)/2),\n         cles = dnorm(diff_means / sqrt((b_sigma_genreAction + b_sigma_genreComedy)), 0, 1))\n\nbrms_uneq_robust_groups_tidy_fixed &lt;- \n  tidyMCMC(brms_uneq_robust_groups_post, conf.int = TRUE, conf.level = 0.95, \n           estimate.method = \"median\", conf.method = \"HPDinterval\")\nbrms_uneq_robust_groups_tidy_fixed\n\n## # A tibble: 9 x 5\n##   term                estimate std.error conf.low conf.high\n##   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 b_genreComedy        5.99       0.109     5.77      6.19 \n## 2 b_genreAction        5.30       0.107     5.09      5.50 \n## 3 b_sigma_genreComedy  1.47       0.0882    1.30      1.64 \n## 4 b_sigma_genreAction  1.47       0.0826    1.31      1.62 \n## 5 nu                   1.48       0.287     0.963     2.04 \n## 6 diff_means          -0.690      0.151    -1.01     -0.415\n## 7 diff_sigma           0.00100    0.111    -0.212     0.217\n## 8 cohen_d             -0.571      0.126    -0.818    -0.327\n## 9 cles                 0.368      0.0132    0.341     0.391\nAnd voila! Difference in means based on priors on individual group means rather than differences in groups!"
  },
  {
    "objectID": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#comparing-all-methods",
    "href": "blog/2019/01/29/diff-means-half-dozen-ways/index.html#comparing-all-methods",
    "title": "Half a dozen frequentist and Bayesian ways to measure the difference in means in two groups",
    "section": "Comparing all methods",
    "text": "Comparing all methods\nHoly cow, that’s a lot of code. We can compare the output from all these different methods in a single plot. In this case, since both groups are pretty normally distributed already and there were no outliers, there isn’t much variation at all in the results—all the different methods show essentially the same thing. We can legally interpret the Bayesian results using credible intervals and probabilities; with the classical t-tests, we still have to talk about null hypotheses. But in the end, the results are nearly identical (but that’s definitely not always the case).\n# Make a bunch of data frames that have three columns: \n# estimate, conf.low, and conf.high\n\n# Extract t-test results\nt_test_eq_small &lt;- t_test_eq_tidy %&gt;% \n  select(estimate, conf.low, conf.high)\n\nt_test_uneq_small &lt;- t_test_uneq_tidy %&gt;% \n  select(estimate, conf.low, conf.high)\n\n# Extract simulation results\ninfer_simulation &lt;- tibble(estimate = diff_means$stat,\n                           conf.low = boostrapped_confint$`2.5%`,\n                           conf.high = boostrapped_confint$`97.5%`)\n\n# Extract brms regression results\nbrms_eq_small &lt;- brms_eq_tidy %&gt;% \n  filter(term == \"b_genreAction\") %&gt;% \n  select(estimate, conf.low, conf.high)\n\nbrms_uneq_small &lt;- brms_uneq_tidy %&gt;% \n  filter(term == \"b_genreAction\") %&gt;% \n  select(estimate, conf.low, conf.high)\n\nbrms_uneq_robust_small &lt;- brms_uneq_robust_tidy %&gt;% \n  filter(term == \"b_genreAction\") %&gt;% \n  select(estimate, conf.low, conf.high)\n\nbrms_uneq_robust_groups_small &lt;- brms_uneq_robust_groups_tidy_fixed %&gt;% \n  filter(term == \"diff_means\") %&gt;% \n  select(estimate, conf.low, conf.high)\n\n# Extract Stan results\nstan_best_small &lt;- stan_best_tidy %&gt;% \n  filter(term == \"mu_diff\") %&gt;% \n  select(estimate, conf.low, conf.high)\n\n# Put all these mini dataframes into a list column, then unnest\nmeta_diffs &lt;- tribble(\n  ~package, ~method, ~results,\n  \"t-test\", \"equal variances\", t_test_eq_small,\n  \"t-test\", \"unequal variances\", t_test_uneq_small,\n  \"infer\", \"simulation\", infer_simulation,\n  \"brms\", \"equal variances\", brms_eq_small,\n  \"brms\", \"unequal variances\", brms_uneq_small,\n  \"brms\", \"BEST\", brms_uneq_robust_small,\n  \"brms\", \"BEST; group means\", brms_uneq_robust_groups_small,\n  \"Stan\", \"BEST\", stan_best_small\n) %&gt;% \n  unnest(results) %&gt;% \n  mutate(method = paste0(package, \": \", method)) %&gt;% \n  mutate(method = fct_inorder(method))\n\nggplot(meta_diffs, aes(x = estimate, y = fct_rev(method), color = package)) +\n  geom_pointrangeh(aes(xmin = conf.low, xmax = conf.high), size = 1) +\n  geom_vline(xintercept = 0, size = 1) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.9, guide = FALSE) +\n  labs(x = \"Mean rating for action movies − mean rating for comedies\",\n       y = NULL, caption = \"Sample of 400 movies from IMDB\",\n       title = \"Comedies get higher ratings than action movies\",\n       subtitle = \"Effect is roughly the same regardless of method used\") +\n  expand_limits(x = 0) +\n  theme_fancy() +\n  theme(plot.title = element_text(face = \"bold\", size = rel(1.5)))\n\n\nComparison of all methods"
  },
  {
    "objectID": "blog/2018/12/05/test-any-hypothesis/index.html",
    "href": "blog/2018/12/05/test-any-hypothesis/index.html",
    "title": "How to test any hypothesis with the infer package",
    "section": "",
    "text": "This semester, I used the new ModernDive textbook to teach introductory statistics for executive MPA students at BYU, and it’s been absolutely delightful. The book’s approach to teaching statistics follows a growing trend (led by Mine Çetinkaya-Rundel, Alison Hill, and others) of emphasizing data and simulations instead of classical probability theory and complex statistical tests.\nWhere this approach really shines is with hypothesis testing. This is the core of inferential statistics, but it’s really hard for students to wrap their head around how to reject null hypotheses and interpret p-values. Even seasoned scientists struggle with explaining what p-values mean. This stuff is hard.\nModernDive borrows from Allen Downey’s philosophy that there is only one statistical test and that at their core, all statistical tests (be they t-tests, chi-squared tests, signed Wilcoxon rank tests, etc.) follow the same universal pattern:\n\n\nStep 1: Calculate a sample statistic, or \\(\\delta\\). This is the main measure you care about: the difference in means, the average, the median, the proportion, the difference in proportions, the chi-squared value, etc.\n\nStep 2: Use simulation to invent a world where \\(\\delta\\) is null. Simulate what the world would look like if there was no difference between two groups, or if there was no difference in proportions, or where the average value is a specific number.\n\nStep 3: Look at \\(\\delta\\) in the null world. Put the sample statistic in the null world and see if it fits well.\n\nStep 4: Calculate the probability that \\(\\delta\\) could exist in null world. This is the p-value, or the probability that you’d see a \\(\\delta\\) at least that high in a world where there’s no difference.\n\nStep 5: Decide if \\(\\delta\\) is statistically significant. Choose some evidentiary standard or threshold for deciding if there’s sufficient proof for rejecting the null world. Standard thresholds (from least to most rigorous) are 0.1, 0.05, and 0.01.\n\nThat’s all. Five steps. No need to follow complicated flowcharts to select the best and most appropriate statistical test. No need to run a bunch of tests to see if you need to pool variances or leave them separate. Calculate a number, simulate a null world, and decide if that number is significantly different from what is typically seen in the null world. Voila!\nThe infer package in R makes this process explicit, easy, and intuitive.\nIn December 2018 (just two days ago!), the World Bank announced a new Worldwide Bureaucracy Indicators Database, with dozens of measures of public sector effectiveness. The data includes variables that measure the proportion of women employed in both the private and the public sectors. Is there a difference between these two sectors? Do more women work in the public sector than the private sector? Instead of consulting a flow chart to find the right test that meets our assumptions and data limitations, we just simulate our way to a p-value and test to see if the difference in median proportions in each sector is significantly different from zero.\nWe first download a CSV of the WWBI data from the World Bank. In its raw form, it’s kind of a mess, with a row for each indicator in each country (so Afghanistan has a row for women private sector employment, one for women public sector employment, and so on), and columns for each year. This heavily commented code will load and clean and rearrange the WWBI data into something we can analyze and plot.\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(scales)\nlibrary(infer)\n\nset.seed(1234)  # Make all random draws reproducible\n\n# https://datacatalog.worldbank.org/dataset/worldwide-bureaucracy-indicators\nwwbi &lt;- read_csv(\"WWBIData.csv\")\n\n# Create a list of indicators we want to work with\nindicators &lt;- c(\n  \"BI.PWK.PRVS.FE.ZS\",  # females as share of private paid employees\n  \"BI.PWK.PUBS.FE.ZS\"   # females as share of public paid employees\n)\n\n# Make a small, cleaner subset of the WWBI data\nwwbi_small &lt;- wwbi %&gt;% \n  # Only select the columns we care about\n  select(country = `Country Name`, country_code = `Country Code`, \n         indicator = `Indicator Code`, starts_with(\"20\")) %&gt;% \n  # Keep only the indicators we care about\n  filter(indicator %in% indicators) %&gt;% \n  # Gather all the year-based columns into two long columns\n  gather(year, value, starts_with(\"20\")) %&gt;% \n  # Spread the data back out so that there are columns for each indicator\n  spread(indicator, value) %&gt;% \n  # Make these indicator names human readable\n  rename(share_female_private = `BI.PWK.PRVS.FE.ZS`,\n         share_female_public = `BI.PWK.PUBS.FE.ZS`) %&gt;% \n  # Amid all the gathering and spreading, every column has become a character.\n  # This converts the year and all the share_* variables back to numbers\n  mutate_at(vars(year, starts_with(\"share\")), as.numeric)\n\nwwbi_2012 &lt;- wwbi_small %&gt;% \n  filter(year == 2012) %&gt;% \n  # Get rid of rows that are missing data in the share_* columns\n  drop_na(starts_with(\"share\")) %&gt;% \n  # Make this tidy and long, with a column for private or public\n  gather(sector, proportion, starts_with(\"share\")) %&gt;% \n  # Make these values even nicer\n  mutate(sector = recode(sector,\n                         share_female_private = \"Women in private sector\",\n                         share_female_public = \"Women in public sector\"))\n\nFirst, we’ll use a ridge plot (or overlapping density plots) to see if there’s a difference in the distribution of employment across these two sectors. We include quantile_lines = TRUE and quantiles = 2 to draw the median of each distribution.\n\nggplot(wwbi_2012, aes(x = proportion, y = sector, fill = sector)) +\n  stat_density_ridges(quantile_lines = TRUE, quantiles = 2, scale = 3, color = \"white\") + \n  scale_fill_manual(values = c(\"#98823c\", \"#9a5ea1\"), guide = \"none\") + \n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  labs(x = \"Percent of women employed in the sector\", y = NULL) +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank())\n## Warning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n\n\n\nDistribution of female employment in public and private sectors\n\n\n\nIt looks like there’s a difference here—the median percent of women in the public sector looks like it’s between 40–50%, while the median percent of women in the private sector is 50ish%.\nWe can use the infer package to determine the exact difference in the medians of these distributions. We use a formula in specify() to indicate which variables we want to be our response (or y) and which we want to be our explanatory (or x) variables, and then we calculate() the difference in median values. The order argument tells infer to subtract the public values from the private values.\n\ndiff_prop &lt;- wwbi_2012 %&gt;% \n  specify(proportion ~ sector) %&gt;% \n  calculate(\"diff in medians\", \n            order = c(\"Women in private sector\", \"Women in public sector\"))\ndiff_prop\n## Response: proportion (numeric)\n## Explanatory: sector (factor)\n## # A tibble: 1 × 1\n##     stat\n##    &lt;dbl&gt;\n## 1 -0.152\n\nThe difference in the median proportions here is 15.2%. More women appear to be employed in the public sector. But because of sampling issues, is there a possibility that this 15% difference could potentially be zero? Could it be positive instead of negative?\nTo test this, we simulate a world where the actual difference in medians between these two sectors is zero. We then plot that null distribution, place the observed 15.2% difference in it, and see how well it fits. Here we follow the same specify() %&gt;% calculate() pattern, but introduce two new steps. We first hypothesize() that the two sectors are independent of each other and that there’s no difference between the two. We then generate() a null distribution based on permutation (essentially shuffling the private/public labels within the existing data 5,000 times), and then calculate the difference in medians for the two groups.\n\npub_private_null &lt;- wwbi_2012 %&gt;% \n  specify(proportion ~ sector) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 5000, type = \"permute\") %&gt;% \n  calculate(\"diff in medians\", \n            order = c(\"Women in private sector\", \"Women in public sector\"))\n\n# Conveniently, the visualize() function that comes with infer returns a ggplot\n# object, so we can continue to add ggplot layers to enhance and clean the plot\npub_private_null %&gt;%\n  visualize() + \n  geom_vline(xintercept = diff_prop$stat, color = \"#FF4136\", linewidth = 1) +\n  labs(x = \"Difference in median proportion\\n(Women in private sector − women in public sector)\",\n       y = \"Count\",\n       subtitle = \"Red line shows observed difference in median proportions\") +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank())\n\n\n\nObserved difference in a hypothetical null world\n\n\n\nThis red line is pretty far in the left tail of the distribution and seems atypical. We can calculate the probability of seeing a difference as big as 15.2% with the get_pvalue() function. Because we care about differences in means or medians (and because the difference could be positive if we flipped the order of subtracting the two groups—public−private instead of private−public), we specify direction = \"both\" to get a two-tailed p-value.\n\npub_private_null %&gt;% \n  get_pvalue(obs_stat = diff_prop, direction = \"both\")\n## Warning: Please be cautious in reporting a p-value of 0. This result is an approximation\n## based on the number of `reps` chosen in the `generate()` step. See `?get_p_value()` for\n## more information.\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1       0\n\nThe p-value is basically 0 (it’s not actually 0—it’s something like 0.0000001—but because it is based on simulations, it’ll report 0 and R will show a warning), which means that there’s a 0% chance of seeing a difference at least as large as 15.2% in a world where there’s no difference. That’s pretty strong evidence, and I’d feel confident declaring that there’s a statistically significant difference between sectoral employment patterns for women, with more women working in public sector jobs than in the private sector.\nAnd that’s it! No flow charts. No complex assumption checking. Using the infer package in R, we brute forced-ly simulated our way to a p-value that is actually interpretable, and now we know that women tend to work in the public sector more than in the private sector. Magic!"
  },
  {
    "objectID": "blog/2018/03/08/amelia-broom-huxtable/index.html",
    "href": "blog/2018/03/08/amelia-broom-huxtable/index.html",
    "title": "Show multiply imputed results in a side-by-side regression table with broom and huxtable",
    "section": "",
    "text": "(See this notebook on GitHub)\n\ntl;dr: Use the functions in broomify-amelia.R to use broom::tidy(), broom::glance(), and huxtable::huxreg() on lists of multiply imputed models.\n\nThe whole reason I went into the rabbit hole of the mechanics of merging imputed regression results in the previous post was so I could easily report these results in papers and writeups. In political science and economics (and probably other social science disciplines), it’s fairly standard to report many regression models in a side-by-side table, with a column for each model and rows for each coefficient. R packages like stargazer and huxtable make this fairly straightforward.\n\nlibrary(tidyverse)\nlibrary(Amelia)\nlibrary(stargazer)\nlibrary(huxtable)\nlibrary(broom)\n\n# Use the africa dataset from Ameila\ndata(africa)\n\n# Build some example models\nmodel_original1 &lt;- lm(gdp_pc ~ trade + civlib, data = africa)\nmodel_original2 &lt;- lm(gdp_pc ~ trade + civlib + infl, data = africa)\n\nStargazer takes a list of models:\n\nstargazer(model_original1, model_original2, type = \"text\")\n## \n## ====================================================================\n##                                   Dependent variable:               \n##                     ------------------------------------------------\n##                                          gdp_pc                     \n##                               (1)                      (2)          \n## --------------------------------------------------------------------\n## trade                      18.000***                18.500***       \n##                             (1.270)                  (1.200)        \n##                                                                     \n## civlib                    -665.000***              -589.000***      \n##                            (185.000)                (176.000)       \n##                                                                     \n## infl                                                -6.340***       \n##                                                      (1.620)        \n##                                                                     \n## Constant                    136.000                 166.000*        \n##                            (100.000)                (94.900)        \n##                                                                     \n## --------------------------------------------------------------------\n## Observations                  115                      115          \n## R2                           0.653                    0.695         \n## Adjusted R2                  0.647                    0.687         \n## Residual Std. Error    352.000 (df = 112)      332.000 (df = 111)   \n## F Statistic         106.000*** (df = 2; 112) 84.500*** (df = 3; 111)\n## ====================================================================\n## Note:                                    *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\nAs does huxtable’s huxreg():\n\nhuxreg(model_original1, model_original2) %&gt;%\n  print_screen()\n##                    ────────────────────────────────────────────────────\n##                                            (1)              (2)        \n##                                     ───────────────────────────────────\n##                      (Intercept)         136.063          166.435      \n##                                         (100.409)         (94.871)     \n##                      trade                18.027 ***       18.494 ***  \n##                                           (1.272)          (1.204)     \n##                      civlib             -665.428 ***     -588.722 **   \n##                                         (185.436)        (175.717)     \n##                      infl                                  -6.336 ***  \n##                                                            (1.620)     \n##                                     ───────────────────────────────────\n##                      N                   115              115          \n##                      R2                    0.653            0.695      \n##                      logLik             -836.136         -828.709      \n##                      AIC                1680.272         1667.418      \n##                    ────────────────────────────────────────────────────\n##                      *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.           \n## \n## Column names: names, model1, model2\n\nStargazer has support for a ton of different model types (see ?stargazer for details), but they’re all hardcoded into stargazer’s internal code and adding more is tricky. Huxtable, on the other hand, doesn’t rely on hardcoded model processing, but instead will display any model that works with broom::tidy() and broom::glance(). The broom package supports way more models than stargazer (including models created with rstan and rstanarm!), and because of this, huxtable is far more extensible—if you can create a tidy() and a glance() function for a type of model, huxtable can use it.\nAlso, stargazer was written before R Markdown was really a thing, so it has excellent support for HTML and LaTeX output, but that’s it. Including stargazer tables in an R Markdown document is a hassle, especially if you want to be able to convert it to Word (I’ve written a Python script for doing this—that’s how much extra work it takes). Huxtable, though, was written after the R Markdown and tidyverse revolutions, so it supports piping and can output to HTML, LaTeX, and Markdown (with huxtable::print_md()).\nThis history is important because it means that models based on multiple imputation will not work with stargazer. Melding all the coefficients across imputations creates nice data frames of model results, but it doesn’t create a model that stargazer can work with. This is unfortunate, especially given how much I use stargazer. However, if we could make a tidy() and a glance() function that could work with a list of multiply imputed models, huxtable would solve all our problems.\nSo here’s how to solve all your problems :)\nFirst, we’ll impute the missing data in the Africa data set, nest the imputed data in a larger data frame, and run a model on each imputed dataset:\n\nset.seed(1234)\nimp_amelia &lt;- amelia(x = africa, m = 5, cs = \"country\", ts = \"year\", \n                     logs = \"gdp_pc\", p2s = 0)\n\nmodels_imputed_df &lt;- bind_rows(unclass(imp_amelia$imputations), .id = \"m\") %&gt;%\n  group_by(m) %&gt;%\n  nest() %&gt;% \n  mutate(model = data %&gt;% map(~ lm(gdp_pc ~ trade + civlib, data = .)))\n\nmodels_imputed_df\n## # A tibble: 5 × 3\n## # Groups:   m [5]\n##   m     data               model \n##   &lt;chr&gt; &lt;list&gt;             &lt;list&gt;\n## 1 imp1  &lt;tibble [120 × 7]&gt; &lt;lm&gt;  \n## 2 imp2  &lt;tibble [120 × 7]&gt; &lt;lm&gt;  \n## 3 imp3  &lt;tibble [120 × 7]&gt; &lt;lm&gt;  \n## 4 imp4  &lt;tibble [120 × 7]&gt; &lt;lm&gt;  \n## 5 imp5  &lt;tibble [120 × 7]&gt; &lt;lm&gt;\n\nBefore we do anything with the models in models_imputed_df$model, first we can define a few functions to extend broom. R’s S3 object system means that a function named whatever.blah() will automatically work when called on objects with the class blah. This is how broom generally works—there are functions named tidy.anova(), tidy.glm(), tidy.lm(), etc. that will do the correct tidying when run on anova, glm, and lm objects. Huxtable also takes advantage of this S3 object system—it will call the appropriate tidy and glance functions based on the class of the models passed to it.\nTo make a list of models work with broom, we need to invent a new class of model. In this example I’ve named it melded, but it could be anything. Here are three functions designed to work on melded objects (the code for these is largely based on the previous post about melding coefficients). These functions are also found in broomify-amelia.R, which you can add to your project (maybe someday this could be an actual package, but I don’t see a reason for it yet).\n\ntidy.melded &lt;- function(x, conf.int = FALSE, conf.level = 0.95) {\n  # Get the df from one of the models\n  model_degrees_freedom &lt;- glance(x[[1]])$df.residual\n  \n  # Create matrices of the estimates and standard errors\n  params &lt;- tibble(models = unclass(x)) %&gt;%\n    mutate(m = 1:n(),\n           tidied = models %&gt;% map(tidy)) %&gt;% \n    unnest(tidied) %&gt;%\n    select(m, term, estimate, std.error) %&gt;%\n    gather(key, value, estimate, std.error) %&gt;%\n    mutate(term = fct_inorder(term)) %&gt;%  # Order the terms so that spread() keeps them in order\n    spread(term, value)\n  \n  just_coefs &lt;- params %&gt;% filter(key == \"estimate\") %&gt;% select(-m, -key)\n  just_ses &lt;- params %&gt;% filter(key == \"std.error\") %&gt;% select(-m, -key)\n  \n  # Meld the coefficients with Rubin's rules\n  coefs_melded &lt;- mi.meld(just_coefs, just_ses)\n  \n  # Create tidy output\n  output &lt;- as.data.frame(cbind(t(coefs_melded$q.mi),\n                                t(coefs_melded$se.mi))) %&gt;%\n    magrittr::set_colnames(c(\"estimate\", \"std.error\")) %&gt;%\n    mutate(term = rownames(.)) %&gt;%\n    select(term, everything()) %&gt;%\n    mutate(statistic = estimate / std.error,\n           p.value = 2 * pt(abs(statistic), model_degrees_freedom, lower.tail = FALSE))\n  \n  # Add confidence intervals if needed\n  if (conf.int & conf.level) {\n    # Convert conf.level to tail values (0.025 when it's 0.95)\n    a &lt;- (1 - conf.level) / 2\n    \n    output &lt;- output %&gt;% \n      mutate(conf.low = estimate + std.error * qt(a, model_degrees_freedom),\n             conf.high = estimate + std.error * qt((1 - a), model_degrees_freedom))\n  }\n  \n  # tidy objects only have a data.frame class, not tbl_df or anything else\n  class(output) &lt;- \"data.frame\"\n  output\n}\n\nglance.melded &lt;- function(x) {\n  # Because the properly melded parameters and the simple average of the\n  # parameters of these models are roughly the same (see\n  # https://www.andrewheiss.com/blog/2018/03/07/amelia-tidy-melding/), for the\n  # sake of simplicty we just take the average here\n  output &lt;- tibble(models = unclass(x)) %&gt;%\n    mutate(glance = models %&gt;% map(glance)) %&gt;%\n    unnest(glance) %&gt;%\n    summarize_at(vars(r.squared, adj.r.squared, sigma, statistic, p.value, df, \n                      logLik, AIC, BIC, deviance, df.residual),\n                 list(mean)) %&gt;%\n    mutate(m = as.integer(length(x)))\n  \n  # glance objects only have a data.frame class, not tbl_df or anything else\n  class(output) &lt;- \"data.frame\"\n  output\n}\n\nnobs.melded &lt;- function(x, ...) {\n  # Take the number of observations from the first model\n  nobs(x[[1]])\n}\n\nWith these three functions, we can now use glance() and tidy() on a list of models with the class melded, like so:\n\n# Extract the models into a vector and make it a \"melded\" class\nmodels_imputed &lt;- models_imputed_df$model\n# Without this, R won't use our custom tidy.melded() or glance.melded() functions\nclass(models_imputed) &lt;- \"melded\"\nglance(models_imputed)\n##   r.squared adj.r.squared sigma statistic  p.value df logLik  AIC  BIC deviance\n## 1     0.657         0.651   348       112 9.68e-28  2   -871 1750 1761 14154815\n##   df.residual m\n## 1         117 5\ntidy(models_imputed)\n##                    term estimate std.error statistic  p.value\n## (Intercept) (Intercept)    120.6     96.67      1.25 2.15e-01\n## trade             trade     18.1      1.24     14.63 3.45e-28\n## civlib           civlib   -637.8    181.13     -3.52 6.13e-04\n\nEven better, though, is that we can use these imputed models in a huxtable regression table. And, because I included a column named m in glance.melded(), we can also include it in the regression output!\n\nhuxreg(model_original1, model_original2, models_imputed,\n       statistics = c(N = \"nobs\", R2 = \"r.squared\", `Adj R2` = \"adj.r.squared\", \n                      \"logLik\", \"AIC\", \"m\")) %&gt;% \n  print_screen()\n##                ────────────────────────────────────────────────────────────\n##                                    (1)            (2)            (3)       \n##                              ──────────────────────────────────────────────\n##                  (Intercept)    136.063        166.435        120.591      \n##                                (100.409)       (94.871)       (96.669)     \n##                  trade           18.027 ***     18.494 ***     18.089 ***  \n##                                  (1.272)        (1.204)        (1.237)     \n##                  civlib        -665.428 ***   -588.722 **    -637.832 ***  \n##                                (185.436)      (175.717)      (181.130)     \n##                  infl                           -6.336 ***                 \n##                                                 (1.620)                    \n##                              ──────────────────────────────────────────────\n##                  N              115            115            120          \n##                  R2               0.653          0.695          0.657      \n##                  Adj R2           0.647          0.687          0.651      \n##                  logLik        -836.136       -828.709       -870.954      \n##                  AIC           1680.272       1667.418       1749.908      \n##                  m                                              5.000      \n##                ────────────────────────────────────────────────────────────\n##                  *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.                   \n## \n## Column names: names, model1, model2, model3"
  },
  {
    "objectID": "blog/2018/02/15/derivatives-r-fun/index.html",
    "href": "blog/2018/02/15/derivatives-r-fun/index.html",
    "title": "Fun with empirical and function-based derivatives in R",
    "section": "",
    "text": "(See this notebook on GitHub)\n\ntl;dr: Use functions like Deriv::Deriv(), splinefun(), approxfun(), and uniroot() to do things with derivatives in R, both with actual functions and with existing empirical data\n\nA typical microeconomics problem involves finding the optimal price and quantity of a product, given its demand and cost across different quantities. You can optimize this price and quantity and maximize profit by finding the point where the marginal cost and the marginal revenue (or the first derivatives of the cost and revenue functions) are equal to each other.\nFor instance, the demand for some product can be defined as \\(Q = 10 - 2P\\) (where \\(Q =\\) quantity and \\(P =\\) price). The revenue you get from selling that product is defined as \\(R = PQ\\) (just multiplying price × quantity), so through some algebraic trickery and rearranging of Ps and Qs, you can create a revenue function for this demand curve: \\(R = 5Q - 0.5Q^2\\). The cost function for this product can be defined as \\(C = 0.25Q + 0.5Q^2\\).\nTo figure out the optimal profit, we set the marginal cost and marginal revenue equations equal to each other and solve for Q. Here, \\(\\frac{dC}{dQ} = MC = 0.25 + 0.5Q\\) and \\(\\frac{dR}{dQ} = MR = 5 - Q\\), so with algebra we can find the optimal point:\n\\[\n\\begin{aligned}\nMC &= MR \\\\\n0.25 + 0.5Q &= 5 - Q \\\\\n1.5Q &= 4.75 \\\\\nQ &= 3.1\\overline{66}\n\\end{aligned}\n\\]\nPhew. Calculus.\nDoing this in R is fairly straightforward and far more flexible and far less algebra-intensive. First, define the functions:\n\nlibrary(tidyverse)\nlibrary(Deriv)\nlibrary(pander)\n\ndemand &lt;- function(q) 5 - (0.5 * q)\nrevenue &lt;- function(q) (5 - 0.5 * q) * q\n\ncost &lt;- function(q) (0.25 * q) + (0.5 * q)^2\n\nPlotting these functions is easy with geom_function():\n\nggplot(data = tibble(x = 0:10), aes(x = x)) +\n  geom_function(fun = cost, linewidth = 1, aes(color = \"Total cost\")) +\n  geom_function(fun = revenue, linewidth = 1, aes(color = \"Total revenue\")) +\n  labs(x = \"Quantity\", y = \"Price\") +\n  scale_y_continuous(labels = scales::dollar) +\n  scale_color_manual(values = c(\"Total cost\" = \"red\", \"Total revenue\" = \"blue\"),\n                     name = \"Function\") +\n  theme_light() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nThen, using Deriv::Deriv(), create derivative functions for the marginal cost and marginal revenue equations:\n\nmr &lt;- Deriv(revenue, \"q\")\nmc &lt;- Deriv(cost, \"q\")\n\nWe can also plot these:\n\nggplot(data = tibble(x = 0:10), aes(x = x)) +\n  geom_function(fun = mc, linewidth = 1, aes(color = \"Marginal cost\")) +\n  geom_function(fun = mr, linewidth = 1, aes(color = \"Marginal revenue\")) +\n  labs(x = \"Quantity\", y = \"Price\") +\n  scale_y_continuous(labels = scales::dollar) +\n  scale_color_manual(values = c(\"Marginal cost\" = \"red\", \"Marginal revenue\" = \"blue\"),\n                     name = \"Function\") +\n  coord_cartesian(ylim = c(0, 6)) +\n  theme_light() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFinally, use the uniroot() function to look for the point where mc and mr intersect within a given range (here I’m looking between 1 and 10 since the demand curve goes negative after \\(Q =\\) 10):\n\noptimal_q &lt;- uniroot(function(x) mc(x) - mr(x), c(1, 10))\noptimal_q$root\n## [1] 3.166667\n\nIt’s the same answer!\nWe can then plug optimal_q$root back into the marginal revenue and demand functions to find the optimal price (in a competitive market, the price should be equal to the marginal revenue, but this happens to be a monopoly, so the actual price is higher, but that’s totally unrelated to the topic here):\n\nmr(optimal_q$root)\n## [1] 1.833333\ndemand(optimal_q$root)\n## [1] 3.416667\n# oh noes monopolies\n\nHowever! Wait! Stop! This is all well and fine if you have precise formulas for demand and cost. But real life is far messier than this. What if you don’t know the underlying equations?\nOften in economics, you have a set of quantities and prices based on empirical data. Market research and surveys can estimate the demand for a product, and tracking how fixed and variable costs change over time can estimate the costs for a product, but this data is all empirically based and not based in actual formulas.\nFor instance, suppose you have this table of prices, quantities, and costs (which is actually really based on the demand and cost functions from earlier):\n\ncosts_revenues &lt;- tibble(Quantity = seq(0, 10, 1),\n                         Price = demand(Quantity),\n                         `Total Revenue` = revenue(Quantity),\n                         `Total Cost` = cost(Quantity),\n                         Profit = `Total Revenue` - `Total Cost`)\n\n\n\n\n\nQuantity\nPrice\nTotal Revenue\nTotal Cost\nProfit\n\n\n\n0\n$5.00\n$0.00\n$0.00\n$0.00\n\n\n1\n$4.50\n$4.50\n$0.50\n$4.00\n\n\n2\n$4.00\n$8.00\n$1.50\n$6.50\n\n\n3\n$3.50\n$10.50\n$3.00\n$7.50\n\n\n4\n$3.00\n$12.00\n$5.00\n$7.00\n\n\n5\n$2.50\n$12.50\n$7.50\n$5.00\n\n\n6\n$2.00\n$12.00\n$10.50\n$1.50\n\n\n7\n$1.50\n$10.50\n$14.00\n-$3.50\n\n\n8\n$1.00\n$8.00\n$18.00\n-$10.00\n\n\n9\n$0.50\n$4.50\n$22.50\n-$18.00\n\n\n10\n$0.00\n$0.00\n$27.50\n-$27.50\n\n\n\n\n\nWe can still use R to find the optimal quantity, even without actual formulas. R has two base functions for approximating functions based on existing data. approxfun() will try to fit data linearly, and splinefun() will try to fit data with cubic splines (i.e. it can handle curvy lines better than approxfun()).\nFirst, we can plot the revenue and cost columns to see their shape:\n\ncosts_revenues_plot &lt;- costs_revenues %&gt;% \n  select(Quantity, starts_with(\"Total\")) %&gt;% \n  gather(Variable, Price, -Quantity)\n\nggplot(costs_revenues_plot, aes(x = Quantity, y = Price, color = Variable)) +\n  geom_line(linewidth = 1) +\n  scale_y_continuous(labels = scales::dollar) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_light() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nBecause both variables are curvilinear, it’s probably best to approximate their functions using splines with splinefun():\n\ncost_empirical &lt;- splinefun(x = costs_revenues$Quantity, \n                            y = costs_revenues$`Total Cost`)\n\nrevenue_empirical &lt;- splinefun(x = costs_revenues$Quantity, \n                               y = costs_revenues$`Total Revenue`)\n\nIf we compare the empirically-based functions with their real-life counterparts, we can see that the approximation worked great:\n\ncost(1:10)\n##  [1]  0.5  1.5  3.0  5.0  7.5 10.5 14.0 18.0 22.5 27.5\ncost_empirical(1:10)\n##  [1]  0.5  1.5  3.0  5.0  7.5 10.5 14.0 18.0 22.5 27.5\n\nrevenue(1:10)\n##  [1]  4.5  8.0 10.5 12.0 12.5 12.0 10.5  8.0  4.5  0.0\nrevenue_empirical(1:10)\n##  [1]  4.5  8.0 10.5 12.0 12.5 12.0 10.5  8.0  4.5  0.0\n\nDetermining the marginal cost and revenue functions from these approximations is surprisingly easy because splinefun() objects have a built-in mechanism for returning derivatives with a deriv argument:\n\nmc(1:10)\n##  [1] 0.75 1.25 1.75 2.25 2.75 3.25 3.75 4.25 4.75 5.25\ncost_empirical(1:10, deriv = 1)\n##  [1] 0.75 1.25 1.75 2.25 2.75 3.25 3.75 4.25 4.75 5.25\n\nmr(1:10)\n##  [1]  4  3  2  1  0 -1 -2 -3 -4 -5\nrevenue_empirical(1:10, deriv = 1)\n##  [1]  4  3  2  1  0 -1 -2 -3 -4 -5\n\nMagic!\nWe can plot these empirically-approximated marginal functions and see that they intersect, as expected:\n\nggplot(data = tibble(x = 0:10), aes(x = x)) +\n  geom_function(fun = cost_empirical, linewidth = 1, args = list(deriv = 1),\n                aes(color = \"Marginal cost\")) +\n  geom_function(fun = revenue_empirical, linewidth = 1, args = list(deriv = 1),\n                aes(color = \"Marginal revenue\")) +\n  labs(x = \"Quantity\", y = \"Price\") +\n  scale_y_continuous(labels = scales::dollar) +\n  scale_color_manual(values = c(\"Marginal cost\" = \"red\", \"Marginal revenue\" = \"blue\"),\n                     name = \"Empirical function\") +\n  coord_cartesian(ylim = c(0, 6)) +\n  theme_light() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFinally, we can use uniroot() to find where these two functions intersect:\n\noptimal_q_empirical &lt;- uniroot(function(x) cost_empirical(x, deriv = 1) - \n                                 revenue_empirical(x, deriv = 1), c(1, 10))\noptimal_q_empirical$root\n## [1] 3.166667\n\nIt’s the same!\nAnd just like before, we can find the optimal price, given this quantity. But first we have to create an empirical function for the demand. The demand variable is linear here, so we can use approxfun(), but splinefun() works just fine too (and it has built-in derivative capabilities, while approxfun() doesn’t).\n\nrevenue_empirical(optimal_q_empirical$root, deriv = 1)\n## [1] 1.833333\n\ndemand_empricial_spline &lt;- splinefun(x = costs_revenues$Quantity,\n                                     y = costs_revenues$Price)\n\ndemand_empricial_approx &lt;- approxfun(x = costs_revenues$Quantity,\n                                     y = costs_revenues$Price)\n\ndemand_empricial_spline(optimal_q_empirical$root)\n## [1] 3.416667\ndemand_empricial_approx(optimal_q_empirical$root)\n## [1] 3.416667\n# oh noes monopolies again\n\nWe can plot all of these things together:\n\nggplot(data = tibble(x = 0:10), aes(x = x)) +\n  geom_function(fun = demand_empricial_spline, linewidth = 1,\n                aes(color = \"Demand\")) +\n  geom_function(fun = cost_empirical, linewidth = 1, args = list(deriv = 1),\n                aes(color = \"Marginal cost\")) +\n  geom_function(fun = revenue_empirical, linewidth = 1, args = list(deriv = 1),\n                aes(color = \"Marginal revenue\")) +\n  geom_vline(xintercept = optimal_q_empirical$root, \n             color = \"grey50\", linetype = \"dashed\") +\n  geom_hline(yintercept = revenue_empirical(optimal_q_empirical$root, deriv = 1), \n             color = \"grey50\", linetype = \"dashed\") +\n  labs(x = \"Quantity\", y = \"Price\") +\n  scale_y_continuous(labels = scales::dollar) +\n  scale_color_manual(values = c(\"Marginal cost\" = \"red\", \"Marginal revenue\" = \"blue\",\n                                \"Demand\" = \"darkgreen\"),\n                     name = \"Function\") +\n  coord_cartesian(ylim = c(0, 6)) +\n  theme_light() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nIn this case, the empirical solution and the function-based solution are identical, but that’s only because I created the empirical data from the functions. In real life, though, this same process should work on any empirical price, quantity, and cost data."
  },
  {
    "objectID": "blog/2017/09/15/create-supply-and-demand-economics-curves-with-ggplot2/index.html",
    "href": "blog/2017/09/15/create-supply-and-demand-economics-curves-with-ggplot2/index.html",
    "title": "Create supply and demand economics curves with ggplot2",
    "section": "",
    "text": "Tip\n\n\n\nThis is now an R package named reconPlots.\n(Skip to the tl;dr complete example; see this mini project on GitHub)\nSo far, teaching at BYU has been delightful. I’ve been using static course-specific websites for the two classes I’m teaching this semester—data visualization and telling stories with data—and it’s been fantastic. Everything is self-contained and automated and magic and I’m a huge fan of blogdown.\nI’m teaching basic microeconomics for public managers next semester. Economics is full of graphs, with supply curves, demand curves, intersections, lines, and shaded areas galore. However, these graphics are rarely connected to real data—they’re conceptual—which makes them a little harder to plot with statistical graphics packages.\nIn the econ classes I took at BYU and Duke, I either drew problem set graphs by hand on paper or by hand in Illustrator, which was tedious and not very automatable. Since I’m hoping to create another course-specific website with blogdown, I headed out to find an R-based solution for creating conceptual, non-data-based graphs.\nAfter an initial call out on Twitter and searches on Google, I found that the cool kids in econ either use OmniGraffle or Illustrator (which requires manual labor) or tikz to create their graphs. R Markdown and knitr support raw tikz chunks, but only in LaTeX/PDF output (which makes sense, since tikz is essentially TeX). There’s a hacky workaround to get tikz graphics in HTML output, but it looks horrible. Beyond these issues, I didn’t want to learn yet another scripting language, so it was back to looking for R-only solutions.\nTo my delight, I came across this post from is.R() from 2012 where David Sparks essentially did exactly what I want to do—use ggplot to create conceptual non-data-based graphs. I borrowed extensively from David’s original code and updated his system for my own graphs.\nThere are a couple key functions that make this work. First is bezier() from the Hmisc package, which generates a Bézier curve from a set of coordinates. Importantly, though, we don’t need to actually load the Hmisc package, since we only need Hmisc::bezier(). Loading the whole package muddies up the environment—in particular Hmisc::summarize() conflicts with dplyr::summarize() and can cause problems later.\nFirst, we can create a supply curve:\nGenerate supply curve\nWe can adjust the curviness of the curve by moving the x and y coordinates around. For instance:\nLots of supply curves\nWe can make a downward-sloping demand curve the same way. Since we’re using two geom_path() layers here, we remove the data parameter to the main ggplot() function, but keep the aesthetic mapping.\nGenerate demand curve\nThe second key function for plotting these supply and demand graphs is a combination of approxfun() and uniroot(), which we use to find the intersection of the two curves. In his original post, Sparks created an approxIntersection() function to figure out intersections with brute force (i.e. create curves with hundreds of points and then look along the points to find where the coordinates are closest). In his post, he notes:\nSo I wanted to try to find a less hacky solution. In this old e-mail to r-help about finding the intersection of two lines, it was suggested that:\nI’ve used R for years and I’d never heard of either of those functions. But I figured I’d give it a try.\napproxfun() takes a matrix of data and approximates a function to fit that data. For example, we can generate a function for the supply curve and then plug in any x value to calculate the corresponding y. Here are the y values for 2, 6, and 8 (they should match the graphs above):\nMagic.\nThe uniroot() function can take a function and search across an interval for the root of that function (or, in this case, where two functions intersect). As said in the r-help post, we want to the root of the difference of the supply and demand curves. uniroot only accepts a single function, so we create an anonymous function where we calculate the difference between the two (function(x) fun_supply(x) - fun_demand(x)). We also want to search along the whole range of x, which currently goes from 1 to 9:\nThis gives a lot of output, but we only really care about the $root value, which is 4.654. And sure enough, it calculated the correct intersection!\nSupply demand intersection for just x\nTo get the horizontal intersection, we just have to find where the vertical intersection (4.654) shows up in the demand function. We calculate this by plugging the intersection into fun_demand():\nSupply demand intersection for both x and y\nFinding the intersections involves a lot of code, so we can put it all in a single function to make life easier later. This function only works on one intersection—it’ll find the first intersection in the full range of the first curve. Finding multiple intersections requires more complicated logic, but since I’m not planning on plotting anything more complicated, I’m fine with this.\nThe function returns a list with x and y values:\nWe can use this simpler list in the plot. Here, we stop using geom_vline() and geom_hline() and plot segments instead, stopping at the intersection of the curves (with a point at the intersection, just for fun):\nSimple supply demand intersection\nNow that we can quickly calculate the intersection of two curves, we can make more complicated plots, like adding a second demand curve and showing the change in price that results from the shift:\nAdd second demand curve\nSuper magic!\nWe can put a few final touches on it:\nAll together now!\nPerfect!\nThe only thing I have left to figure out is shading areas under the lines and curves to show consumer and producer surplus, but I’ll get to that later (in theory, it should be a matter of using geom_ribbon(), like this.)"
  },
  {
    "objectID": "blog/2017/09/15/create-supply-and-demand-economics-curves-with-ggplot2/index.html#tldr",
    "href": "blog/2017/09/15/create-supply-and-demand-economics-curves-with-ggplot2/index.html#tldr",
    "title": "Create supply and demand economics curves with ggplot2",
    "section": "tl;dr",
    "text": "tl;dr\nAll that explanation above makes the process sound more complicated than it actually is. Here’s a complete example:\nsupply &lt;- Hmisc::bezier(c(1, 8, 9),\n                        c(1, 5, 9)) %&gt;%\n  data.frame()\n\ndemand1 &lt;- Hmisc::bezier(c(1, 3, 9),\n                         c(9, 3, 1)) %&gt;%\n  data.frame()\n\ndemand2 &lt;- Hmisc::bezier(c(3, 5, 11),\n                         c(11, 5, 3)) %&gt;%\n  data.frame()\n\n# Calculate the intersections of the two curves\nintersections &lt;- bind_rows(curve_intersect(supply, demand1),\n                           curve_intersect(supply, demand2))\n\nplot_labels &lt;- data_frame(label = c(\"S\", \"D[1]\", \"D[2]\"),\n                          x = c(8, 1, 5),\n                          y = c(8, 8, 8))\n\nggplot(mapping = aes(x = x, y = y)) +\n  geom_path(data = supply, color = \"#0073D9\", size = 1) +\n  geom_path(data = demand, color = \"#FF4036\", size = 1, linetype = \"dashed\") +\n  geom_path(data = demand2, color = \"#FF4036\", size = 1) +\n  geom_segment(data = intersections,\n               aes(x = x, y = 0, xend = x, yend = y), lty = \"dotted\") +\n  geom_segment(data = intersections,\n               aes(x = 0, y = y, xend = x, yend = y), lty = \"dotted\") +\n  geom_text(data = plot_labels,\n            aes(x = x, y = y, label = label), parse = TRUE,\n            family = \"Source Sans Pro\") +\n  annotate(\"segment\", x = 3.5, xend = 4.5, y = 6, yend = 7,\n           arrow = arrow(length = unit(1, \"lines\")), colour = \"grey50\") +\n  geom_point(data = intersections, size = 3) +\n  scale_x_continuous(expand = c(0, 0), breaks = intersections$x,\n                     labels = expression(Q[1], Q[2])) +\n  scale_y_continuous(expand = c(0, 0), breaks = intersections$y,\n                     labels = expression(P[1], P[2])) +\n  labs(x = \"Quantity\", y = \"Price\",\n       title = \"Rightward shift in demand\",\n       subtitle = \"As demand increases, so does price\") +\n  coord_equal() +\n  theme_classic(base_family = \"Source Sans Pro\") +\n  theme(plot.title = element_text(family = \"Source Sans Pro Semibold\", size = rel(1.3)))\n\n\n\n\nComplete example"
  },
  {
    "objectID": "blog/2017/09/27/working-with-r-cairo-graphics-custom-fonts-and-ggplot/index.html",
    "href": "blog/2017/09/27/working-with-r-cairo-graphics-custom-fonts-and-ggplot/index.html",
    "title": "Working with R, Cairo graphics, custom fonts, and ggplot",
    "section": "",
    "text": "Note\n\n\n\nSkip to instructions for macOS or Windows\nR and ggplot can create fantastic graphs, but the default Arial/Helvetica font is too boring and standard. You can change the font used in a plot fairly easily three different ways:\nFor example:\nExample nice plot\nHowever, there are a couple difficulties when using custom fonts like this:\nFixing both of these issues is relatively easy. On Windows, you can either load fonts into R on the fly with windowsFonts(name_of_font_inside_r = windowsFont(\"Name of actual font\")), or you can use extrafonts::load_fonts() from the extrafonts library to permanently load fonts into R’s internal database. A full example of this is included below.\nEmbedding fonts in PDFs is also fairly easy. Instead of using R’s default PDF-writing engine, you can use the Cairo graphics library (which, nowadays, is conveniently packaged with R). Cairo has full Unicode support and can handle embedding custom fonts just fine. To make ggsave() use the Cairo engine when writing a PDF, specify the device:\nYou can also use Cairo’s PNG engine when writing PNG files. R’s default PNG-writing engine can sometimes have issues with correctly setting the resolution. In theory, if you specify a width and a height and a DPI, ggsave() will generate a file with those dimensions. However, if you place the PNG into Word, PowerPoint, InDesign, or any other programs, the graphic will be too large, for reasons unknown. If you save the graphic with the Cairo library or AGG library though, these programs will respect the size and DPI and place the image correctly.\nUsing the AGG or Cairo PNG libraries makes a significant difference when you use the image in other programs. Notice how the Cairo/AGG-based PNG is actually 4 inches wide in Word, while R’s default PNG takes up the full width of the page and uses a lower resolution:\nCairo vs. not Cairo in Word\nFinally, if you use R Markdown and knitr, you can specify the Cairo device for each output type in the document metadata:\nHere’s how you can use ggplot::ggsave() and Cairo to create PDF with embedded custom fonts and PNGs with correct resolutions:"
  },
  {
    "objectID": "blog/2017/09/27/working-with-r-cairo-graphics-custom-fonts-and-ggplot/index.html#mac",
    "href": "blog/2017/09/27/working-with-r-cairo-graphics-custom-fonts-and-ggplot/index.html#mac",
    "title": "Working with R, Cairo graphics, custom fonts, and ggplot",
    "section": "Full instructions for macOS",
    "text": "Full instructions for macOS\nThe Cairo graphics library should be installed behind the scenes when you install R—you should not need to install any R-specific Cairo libraries or anything for this to work. However, you do need to install an X11 window system first, like XQuartz.\nYou can verify that you have Cairo support by running the capabilities() function; TRUE should show up under cairo:\ncapabilities()\n#&gt;        jpeg         png        tiff       tcltk         X11        aqua\n#&gt;        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE\n#&gt;    http/ftp     sockets      libxml        fifo      cledit       iconv\n#&gt;        TRUE        TRUE        TRUE        TRUE       FALSE        TRUE\n#&gt;         NLS     profmem       cairo         ICU long.double     libcurl\n#&gt;        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE\nR on macOS should automatically see the fonts you have installed on your computer.\nHere’s a full example of loading and using a custom font on macOS:\n# Load libraries\nlibrary(tidyverse)\n\n# Create sample data\nset.seed(1234)  # This makes R run the same random draw\ndf &lt;- data_frame(x = rnorm(100),\n                 y = rnorm(100))\n\n# Create plot\np &lt;- ggplot(df, aes(x = x, y = y)) +\n  geom_point() +\n  labs(title = \"This is a title\",\n       subtitle = \"This is a subtitle\") +\n  annotate(\"text\", x = 0, y = 0, label = \"This is some text\",\n           family = \"Papyrus\", color = \"darkred\", size = 8) +\n  theme_light(base_family = \"Comic Sans MS\")\np\n\n\n\n\nExample plot on macOS\n\n\n\n# Save the plot as a PDF with ggsave and Cairo\n# R will want to autocomplete cairo_pdf to cairo_pdf() (note the parentheses)\n# This will not work with the parentheses; ensure there aren't any\nggsave(p, filename = \"example.pdf\", device = cairo_pdf,\n       width = 4, height = 3, units = \"in\")\n\n# You can also save the plot as a high resolution PNG using \n# AGG or Cairo\n# With {ragg}\nggsave(p, filename = \"whatever.png\",\n       device = ragg::agg_png, res = 300,\n       width = 4, height = 3, units = \"in\")\n\n# With Cairo\nggsave(p, filename = \"whatever.png\",\n       device = png, type = \"cairo\", dpi = 300,\n       width = 4, height = 3, units = \"in\")"
  },
  {
    "objectID": "blog/2017/09/27/working-with-r-cairo-graphics-custom-fonts-and-ggplot/index.html#windows",
    "href": "blog/2017/09/27/working-with-r-cairo-graphics-custom-fonts-and-ggplot/index.html#windows",
    "title": "Working with R, Cairo graphics, custom fonts, and ggplot",
    "section": "Full instructions for Windows",
    "text": "Full instructions for Windows\nThe Cairo graphics library should be installed behind the scenes when you install R—you should not need to install any special Cairo libraries or anything for this to work.\nYou can verify that you have Cairo support by running the capabilities() function; TRUE should show up under cairo:\ncapabilities()\n#&gt;        jpeg         png        tiff       tcltk         X11        aqua\n#&gt;        TRUE        TRUE        TRUE        TRUE       FALSE       FALSE\n#&gt;    http/ftp     sockets      libxml        fifo      cledit       iconv\n#&gt;        TRUE        TRUE        TRUE        TRUE       FALSE        TRUE\n#&gt;         NLS     profmem       cairo         ICU long.double     libcurl\n#&gt;        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE\nR on Windows cannot see the fonts you have installed on your computer. You can see a list of fonts R does have access to with the windowsFonts() function:\nwindowsFonts()\n#&gt; $serif\n#&gt; [1] \"TT Times New Roman\"\n#&gt;\n#&gt; $sans\n#&gt; [1] \"TT Arial\"\n#&gt;\n#&gt; $mono\n#&gt; [1] \"TT Courier New\"\nYou can add all your system fonts to that database by installing the extrafont library and running font_import(). This will take a while, though, and it will only pick up fonts that are currently installed. If you install a font later, R will not see it—you’ll need to run extrafont::font_import() again.\nAlternatively, you can load fonts into R on the fly, without loading the full database, using windowsFonts(name_of_font_inside_r = windowsFont(\"Name of actual font\")):\nwindowsFonts(`Comic Sans MS` = windowsFont(\"Comic Sans MS\"))\nOnce you do this, the font will be loaded:\nwindowsFonts()\n#&gt; $serif\n#&gt; [1] \"TT Times New Roman\"\n#&gt;\n#&gt; $sans\n#&gt; [1] \"TT Arial\"\n#&gt;\n#&gt; $mono\n#&gt; [1] \"TT Courier New\"\n#&gt;\n#&gt; $`Comic Sans MS`\n#&gt; [1] \"Comic Sans MS\"\nThis only takes effect for your current R session, so if you are knitting a document or if you ever plan on closing RStudio, you’ll need to incorporate this font assignment code into your script. If you don’t want to do that, run extrafont::load_fonts() to load all the fonts—once you do this, you won’t need to repeatedly run windowsFonts() to load fonts each time you run a script.\nHere’s a full example of loading and using a custom font on Windows:\n# Load specific fonts into R's internal database\nwindowsFonts(`Comic Sans MS` = windowsFont(\"Comic Sans MS\"))\nwindowsFonts(Papyrus = windowsFont(\"Papyrus\"))\n# Load libraries\nlibrary(tidyverse)\n\n# Create sample data\nset.seed(1234)  # This makes R run the same random draw\ndf &lt;- data_frame(x = rnorm(100),\n                 y = rnorm(100))\n\n# Create plot\np &lt;- ggplot(df, aes(x = x, y = y)) +\n  geom_point() +\n  annotate(\"text\", x = 0, y = 0, label = \"This is some text\",\n           family = \"Papyrus\", color = \"darkred\", size = 8) +\n  labs(title = \"This is a title\",\n       subtitle = \"This is a subtitle\") +\n  theme_light(base_family = \"Comic Sans MS\")\np\n\n\n\n\nExample plot in Windows\n\n\n\n# Save the plot as a PDF with ggsave and Cairo\n# R will want to autocomplete cairo_pdf to cairo_pdf() (note the parentheses)\n# This will not work with the parentheses; ensure there aren't any\nggsave(p, filename = \"example.pdf\", device = cairo_pdf,\n       width = 4, height = 3, units = \"in\")\n\n# You can also save the plot as a high resolution PNG using \n# AGG or Cairo\n# With {ragg}\nggsave(p, filename = \"whatever.png\",\n       device = ragg::agg_png, res = 300,\n       width = 4, height = 3, units = \"in\")\n\n# With Cairo\nggsave(p, filename = \"whatever.png\",\n       device = png, type = \"cairo\", dpi = 300,\n       width = 4, height = 3, units = \"in\")"
  },
  {
    "objectID": "blog/2018/03/07/amelia-tidy-melding/index.html",
    "href": "blog/2018/03/07/amelia-tidy-melding/index.html",
    "title": "Meld regression output from multiple imputations with tidyverse",
    "section": "",
    "text": "(See this notebook on GitHub)\n\nMissing data can significantly influence the results of normal regression models, since the default in R and most other statistical packages is to throw away any rows with missing variables. To avoid unnecessarily throwing out data, it’s helpful to impute missing values. One of the best ways to do this is to build a separate regression model to make predictions that fill in the gaps in data. This isn’t always accurate, so it’s best to make many iterations of predictions (in imputation parlance, \\(m\\) is the number of imputations done to a dataset). After making \\(m\\) datasets, you can use this data by (1) running statistical tests on each imputation individually and then (2) pooling those results into a single number. The excellent Amelia vignette details the theory and mechanics of how to use multiple imputation, and it’s a fantastic resource.\nThere are several packages for dealing with missing data in R, including mi, mice, and Amelia, and Thomas Leeper has a short overview of how to use all three. I’m partial to Amelia, since it’s designed to work well with time series-cross sectional data and can deal with complicated features like country-year observations.\nBecause Amelia is written by Gary King, et al., it works with Zelig, a separate framework that’s designed to simplify modeling in R. With Zelig + Amelia, you can combine all of the \\(m\\) imputations automatically with whatever Zelig uses for printing model results. I’m not a huge fan of Zelig, though, and I prefer using lm(), glm(), stan_glm(), and gang on my own, thank you very much.\nHowever, doing it on my own means there’s a little more work involved with combining coefficients and parameters across imputations. Fortunately, the tidyverse—specifically its ability to store models within data frames—makes it really easy to deal with models based on imputed data. Here’s how to do it using tidy functions. The code for this whole process can be greatly simplified in real life. You technically don’t need all these intermediate steps, though they’re helpful for seeing what’s going on behind the scenes.\nWe’ll start by working with some basic example imputed data frame from Amelia’s built-in data. We create 5 imputed datasets defining countries and years as cross sections and time series, and we log GDP per capita in the predictive model:\n\nlibrary(tidyverse)\nlibrary(Amelia)\nlibrary(broom)\n\nset.seed(1234)\ndata(africa)\nimp_amelia &lt;- amelia(x = africa, m = 5, cs = \"country\", ts = \"year\", \n                     logs = \"gdp_pc\", p2s = 0)\n\nThe resulting object contains a list of data frames, and each imputed dataset is stored in a list slot named “imputations” or imp_amelia$imputations. We can combine these all into one big data frame with bind_rows(), group by the imputation number (\\(m\\)), and nest them into imputation-specific rows:\n\n# unclass() is necessary because bind_rows() will complain when dealing with\n# lists with the \"amelia\" class, which is what amelia() returns\nall_imputations &lt;- bind_rows(unclass(imp_amelia$imputations), .id = \"m\") %&gt;%\n  group_by(m) %&gt;%\n  nest()\n\nall_imputations\n## # A tibble: 5 × 2\n## # Groups:   m [5]\n##   m     data              \n##   &lt;chr&gt; &lt;list&gt;            \n## 1 imp1  &lt;tibble [120 × 7]&gt;\n## 2 imp2  &lt;tibble [120 × 7]&gt;\n## 3 imp3  &lt;tibble [120 × 7]&gt;\n## 4 imp4  &lt;tibble [120 × 7]&gt;\n## 5 imp5  &lt;tibble [120 × 7]&gt;\n\nWith this nested data, we can use purrr::map() to run models and return tidy summaries of those models directly in the data frame:\n\nmodels_imputations &lt;- all_imputations %&gt;%\n  mutate(model = data %&gt;% map(~ lm(gdp_pc ~ trade + civlib, data = .)),\n         tidied = model %&gt;% map(~ tidy(., conf.int = TRUE)),\n         glance = model %&gt;% map(~ glance(.)))\n\nmodels_imputations\n## # A tibble: 5 × 5\n## # Groups:   m [5]\n##   m     data               model  tidied           glance           \n##   &lt;chr&gt; &lt;list&gt;             &lt;list&gt; &lt;list&gt;           &lt;list&gt;           \n## 1 imp1  &lt;tibble [120 × 7]&gt; &lt;lm&gt;   &lt;tibble [3 × 7]&gt; &lt;tibble [1 × 12]&gt;\n## 2 imp2  &lt;tibble [120 × 7]&gt; &lt;lm&gt;   &lt;tibble [3 × 7]&gt; &lt;tibble [1 × 12]&gt;\n## 3 imp3  &lt;tibble [120 × 7]&gt; &lt;lm&gt;   &lt;tibble [3 × 7]&gt; &lt;tibble [1 × 12]&gt;\n## 4 imp4  &lt;tibble [120 × 7]&gt; &lt;lm&gt;   &lt;tibble [3 × 7]&gt; &lt;tibble [1 × 12]&gt;\n## 5 imp5  &lt;tibble [120 × 7]&gt; &lt;lm&gt;   &lt;tibble [3 × 7]&gt; &lt;tibble [1 × 12]&gt;\n\nHaving the models structured like this makes it easy to access coefficients for models from individual imputations, like so:\n\nmodels_imputations %&gt;%\n  filter(m == \"imp1\") %&gt;%\n  unnest(tidied)\n## # A tibble: 3 × 11\n## # Groups:   m [1]\n##   m     data     model  term     estim…¹ std.e…² stati…³  p.value conf.…⁴ conf.…⁵ glance  \n##   &lt;chr&gt; &lt;list&gt;   &lt;list&gt; &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;  \n## 1 imp1  &lt;tibble&gt; &lt;lm&gt;   (Interc…   114.    97.7     1.17 2.44e- 1   -79.0   308.  &lt;tibble&gt;\n## 2 imp1  &lt;tibble&gt; &lt;lm&gt;   trade       18.1    1.25   14.4  9.65e-28    15.6    20.6 &lt;tibble&gt;\n## 3 imp1  &lt;tibble&gt; &lt;lm&gt;   civlib    -631.   182.     -3.46 7.47e- 4  -993.   -270.  &lt;tibble&gt;\n## # … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic, ⁴​conf.low,\n## #   ⁵​conf.high\n\nMore importantly, we can access the coefficients for all the models, which is essential for combining and averaging the coefficients across all five imputations.\nPooling or melding coefficients from many models is a little trickier than just averaging them all together (as delightfully easy as that would be). Donald Rubin (1987) outlines an algorithm/set of rules for combining the results from multiply imputed datasets that reflects the averages and accounts for differences in standard errors. Rubin’s rules are essentially a fancier, more robust way of averaging coefficients and other quantities of interest across imputations.\nAmelia has a built-in function for using Rubin’s rules named mi.meld() that accepts two m-by-k matrices (one for coefficients and one for standard errors) like so:\n      coef1  coef2  coefn\nimp1  x      x      x\nimp2  x      x      x\nimpn  x      x      x\nWe can use some dplyr/tidyr magic to wrangle the regression results into this form:\n\n# Create a wide data frame of just the coefficients and standard errors\nparams &lt;- models_imputations %&gt;%\n  unnest(tidied) %&gt;%\n  select(m, term, estimate, std.error) %&gt;%\n  gather(key, value, estimate, std.error) %&gt;%\n  spread(term, value) %&gt;% \n  ungroup()\nparams\n## # A tibble: 10 × 5\n##    m     key       `(Intercept)` civlib trade\n##    &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n##  1 imp1  estimate          114.   -631. 18.1 \n##  2 imp1  std.error          97.7   182.  1.25\n##  3 imp2  estimate          123.   -626. 18.0 \n##  4 imp2  std.error          96.8   181.  1.24\n##  5 imp3  estimate          114.   -633. 18.2 \n##  6 imp3  std.error          96.5   181.  1.24\n##  7 imp4  estimate          119.   -651. 18.2 \n##  8 imp4  std.error          95.4   180.  1.22\n##  9 imp5  estimate          132.   -648. 18.0 \n## 10 imp5  std.error          95.2   180.  1.22\n\n\n# Extract just the coefficients\njust_coefs &lt;- params %&gt;%\n  filter(key == \"estimate\") %&gt;%\n  select(-m, -key)\njust_coefs\n## # A tibble: 5 × 3\n##   `(Intercept)` civlib trade\n##           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1          114.  -631.  18.1\n## 2          123.  -626.  18.0\n## 3          114.  -633.  18.2\n## 4          119.  -651.  18.2\n## 5          132.  -648.  18.0\n\n\n# Extract just the standard errors\njust_ses &lt;- params %&gt;%\n  filter(key == \"std.error\") %&gt;%\n  select(-m, -key)\njust_ses\n## # A tibble: 5 × 3\n##   `(Intercept)` civlib trade\n##           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1          97.7   182.  1.25\n## 2          96.8   181.  1.24\n## 3          96.5   181.  1.24\n## 4          95.4   180.  1.22\n## 5          95.2   180.  1.22\n\nWe can then use these matrices in mi.meld(), which returns a list with two slots—q.mi and se.mi:\n\ncoefs_melded &lt;- mi.meld(just_coefs, just_ses)\ncoefs_melded\n## $q.mi\n##      (Intercept) civlib trade\n## [1,]         121   -638  18.1\n## \n## $se.mi\n##      (Intercept) civlib trade\n## [1,]        96.7    181  1.24\n\nArmed with these, we can create our regression summary table with some more dplyr wizardry. To calculate the p-value and confidence intervals, we need to extract the degrees of freedom from one of the imputed models\n\nmodel_degree_freedom &lt;- models_imputations %&gt;%\n  unnest(glance) %&gt;%\n  filter(m == \"imp1\") %&gt;%\n  pull(df.residual)\n\nmelded_summary &lt;- as.data.frame(cbind(t(coefs_melded$q.mi),\n                                      t(coefs_melded$se.mi))) %&gt;%\n  magrittr::set_colnames(c(\"estimate\", \"std.error\")) %&gt;%\n  mutate(term = rownames(.)) %&gt;%\n  select(term, everything()) %&gt;%\n  mutate(statistic = estimate / std.error,\n         conf.low = estimate + std.error * qt(0.025, model_degree_freedom),\n         conf.high = estimate + std.error * qt(0.975, model_degree_freedom),\n         p.value = 2 * pt(abs(statistic), model_degree_freedom, lower.tail = FALSE))\n\nmelded_summary\n##                    term estimate std.error statistic conf.low conf.high  p.value\n## (Intercept) (Intercept)    120.6     96.67      1.25    -70.9     312.0 2.15e-01\n## civlib           civlib   -637.8    181.13     -3.52   -996.6    -279.1 6.13e-04\n## trade             trade     18.1      1.24     14.63     15.6      20.5 3.45e-28\n\nHooray! Correctly melded coefficients and standard errors!\nBut what do we do about the other model details, like \\(R^2\\) and the F-statistic? How do we report those?\nAccording to a post on the Amelia mailing list, there are two ways. First, we can use a fancy method for combining \\(R^2\\) and adjusted \\(R^2\\) described by Ofer Harel (2009). Second, we can just take the average of the \\(R^2\\)s from all the imputed models. The results should be roughly the same.\nHarel’s method involves two steps:\n\nIn each complete data set, calculate the \\(R^2\\), take its square root (\\(R\\)), transform \\(R\\) with a Fisher z-transformation (\\(Q = \\frac{1}{2} \\log_{e}(\\frac{1 + R}{1 - R})\\)), and calculate the variance of \\(R^2\\) (which is \\(\\frac{1}{\\text{degrees of freedom}}\\))\nMeld the resulting \\(Q\\) and variance using Rubin’s rules (mi.meld(); this creates \\(Q_a\\)), undo the z-transformation (\\(R_a = (\\frac{-1 + \\exp(2Q_a)}{1 + \\exp(2Q_a)})^2\\)), and square it (\\(R_a^2\\))\n\nThat looks complicated, but it’s fairly easy with some dplyr magic. Here’s how to do it for adjusted \\(R^2\\) (the same process works for regular \\(R^2\\) too):\n\n# Step 1: in each complete data set, calculate R2, take its square root,\n# transform it with Fisher z-transformation, and calculate the variance of R2\\\nr2s &lt;- models_imputations %&gt;%\n  unnest(glance) %&gt;%\n  select(m, adj.r.squared, df.residual) %&gt;%\n  mutate(R = sqrt(adj.r.squared),  # Regular R\n         Q = 0.5 * log((R + 1) / (1 - R)),  # Fisher z-transformation\n         se = 1 / df.residual)  # R2 variance\nr2s\n## # A tibble: 5 × 6\n## # Groups:   m [5]\n##   m     adj.r.squared df.residual     R     Q      se\n##   &lt;chr&gt;         &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n## 1 imp1          0.643         117 0.802  1.10 0.00855\n## 2 imp2          0.648         117 0.805  1.11 0.00855\n## 3 imp3          0.652         117 0.807  1.12 0.00855\n## 4 imp4          0.660         117 0.812  1.13 0.00855\n## 5 imp5          0.654         117 0.808  1.12 0.00855\n\n\n# Step 2: combine the results using Rubin's rules (mi.meld()), inverse transform\n# the value, and square it\n\n# Meld the R2 values with mi.meld()\nQ_melded &lt;- mi.meld(as.matrix(r2s$Q), as.matrix(r2s$se))\n\n# Inverse transform Q to R and square it\nr2_melded &lt;- ((exp(2 * Q_melded$q.mi) - 1) / (1 + exp(2 * Q_melded$q.mi)))^2\nr2_melded\n##       [,1]\n## [1,] 0.651\n\nThe correctly pooled/melded \\(R^2\\) is thus 0.651. Neat.\nHow does this compare to just the average of all the \\(R^2\\)s from all the imputations?\n\nr2s_avg &lt;- models_imputations %&gt;%\n  ungroup() %&gt;% \n  unnest(glance) %&gt;%\n  summarize(adj.r.squared_avg = mean(adj.r.squared)) %&gt;%\n  pull(adj.r.squared_avg)\nr2s_avg\n## [1] 0.651\n\nThe incorrectly averaged \\(R^2\\) is 0.651, which is basically identical to the correctly melded 0.651. This is probably because the models from the five imputed models are already fairly similar—there might be more variance in \\(R^2\\) in data that’s less neat. But for this situation, the two approaches are essentially the same. Other model diagnostics like the F-statistic can probably be pooled just with averages as well. I haven’t found any specific algorithms for melding them with fancy math.\nSo, in summary, combine the coefficients and standard errors from multiply imputed models with mi.meld() and combine other model parameters like \\(R^2\\) either with Harel’s fancy method or by simply averaging them."
  },
  {
    "objectID": "blog/2018/07/30/disposable-supercomputer-future/index.html",
    "href": "blog/2018/07/30/disposable-supercomputer-future/index.html",
    "title": "Create a cheap, disposable supercomputer with R, DigitalOcean, and future",
    "section": "",
    "text": "tl;dr: Skip to complete example\nIn one of my current research projects, I use Bayesian modeling (with Stan and rstanarm) and multiple imputation (with Amelia) to measure how international aid agencies change their funding allocations to countries that impose legal restrictions on NGOs. It’s a fascinating topic and I’m using exciting cutting edge research methods to do it.\nHowever, these cutting edge research methods are really computationally intensive. On my laptop, using all four CPU cores, it takes ≈45 minutes to run one set of models (see h1.barriers.total &lt;- mods.h1.next_year.raw.bayes.nested here, for instance), so with all the different model specifications and robustness checks, it takes hours to run the complete analysis for this project. It’s awful and I hate rerunning it.\nIn the past, I created several DigitalOcean droplets (what they call virtual private servers), installed RStudio Server on each, uploaded my data to each instance, and ran separate models on each. This reduced computation time, since I could have like 5 computers all running different parts of the analysis, but it required a ton of manual work and oversight. Computers are really good at automating stuff, and tools like Slurm and Hadoop and Kubernetes all allow you to orchestrate computing across clusters of machines. However, because these tools are powerful, they’re also really complicated and require a lot of additional configuration, and the tradeoff between teaching myself Kubernetes vs. just doing it all manually wasn’t that favorable.\nSurely, I mused on Twitter last week, there’s a better easier way to manage all this computing.\nThere is! And it’s surprisingly easy with the future package in R!\nYou can create your own disposable supercomputer with remote clusters of computers in just a few intuitive steps.\nA fully worked out, tl;dr example is at the end of this post."
  },
  {
    "objectID": "blog/2018/07/30/disposable-supercomputer-future/index.html#super-quick-introduction-to-future",
    "href": "blog/2018/07/30/disposable-supercomputer-future/index.html#super-quick-introduction-to-future",
    "title": "Create a cheap, disposable supercomputer with R, DigitalOcean, and future",
    "section": "0. Super quick introduction to future",
    "text": "0. Super quick introduction to future\nThe key to all of this working correctly is the future package in R. future allows you to evaluate R commands and expressions in separate processes.\nFor instance, ordinarily, when you run this command, x gets assigned a value immediately:\n\nx &lt;- {\n  cat(\"Something really computationally intensive\\n\")\n  10\n}\n#&gt; Something really computationally intensive\n\nx\n#&gt; [1] 10\n\nThe stuff in cat() gets printed immediately and the value of 10 is assigned to x immediately as well. This is all well and good, but if the command is computationally intensive, you have to wait until it’s done before doing anything else in R.\nfuture includes a special assignment command %&lt;-% that delays evaluation until x is called.\n\nlibrary(future)\n\nx %&lt;-% {\n    cat(\"Something really computationally intensive\\n\")\n    10\n}\n\nx\n#&gt; Something really computationally intensive\n#&gt; [1] 10\n\nNotice how cat() isn’t run until x is run. That’s because the whole expression isn’t actually run yet—it isn’t evaluated until x is called.\nThe magic of future is that this deferred evaluation can automatically happen anywhere else. You specify where evaluation happens with future::plan(). For instance, if you want x to be handled on multiple CPUs on your local computer, you’d do this:\n\nlibrary(future)\nplan(multiprocess)\n\nx %&lt;-% {\n  cat(\"Something really computationally intensive\\n\")\n  10\n}\n\nx\n#&gt; Something really computationally intensive\n#&gt; [1] 10\n\nR now uses multiple cores to create x.\nOr, if you want x to be processed on a cluster of three remote computers, you’d do this:\n\nlibrary(future)\n\nips &lt;- c(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\nplan(remote, workers = ips)\n\nx %&lt;-% {\n    cat(\"Something really computationally intensive\\n\")\n    10\n}\n\nx\n#&gt; Something really computationally intensive\n#&gt; [1] 10\n\nx is now created across three computers automatically.\nfuture has functions like future.apply::future_lapply() that allow you to apply functions to lists, just like lapply() and friends in base R, and the furrr package has futurized functions like future_map() that are equivalent to map() and friends in purrr. The future_* versions of these functions will automatically take advantage of whatever you’ve specified in plan(). If you use plan(multiprocess), future_map() will automatically send chunks of computations to each of the CPU cores; if you use plan(remote), future_map() will automatically send chunks of computations to each of the remote computers.\nThis is seriously magic and incredible. The backend you specify with plan() doesn’t matter and you can change it later to anything you want without having to change your code."
  },
  {
    "objectID": "blog/2018/07/30/disposable-supercomputer-future/index.html#create-remote-computers-that-have-the-correct-r-environment-and-packages-set-up-already",
    "href": "blog/2018/07/30/disposable-supercomputer-future/index.html#create-remote-computers-that-have-the-correct-r-environment-and-packages-set-up-already",
    "title": "Create a cheap, disposable supercomputer with R, DigitalOcean, and future",
    "section": "1. Create remote computers that have the correct R environment and packages set up already",
    "text": "1. Create remote computers that have the correct R environment and packages set up already\n1.1. Create a remote computer (or two or three)\nFirst, you need a remote computer. I prefer to use DigitalOcean, mostly because I already use it for my personal web hosting and other personal projects, and because I find it way more intuitive and easy to use than Amazon’s EC2 (which, like Kubernetes, et al. is incredibly powerful, but incredibly complicated). But you can also do all of this with AWS, Linode, Google Cloud Platform, or any other VPS service (here’s how to do it with AWS and with Google Cloud). The magic of future is that it doesn’t matter what backend you use—the package takes care of everything for you.\nHere’s what to do with DigitalOcean:\n\nCreate a DigitalOcean account (get $10 for free with this link)\nCreate an SSH key pair between your DigitalOcean account and your computer. Follow the instructions here to create SSH keys and then upload the public keys to your account. Setting up SSH keys like this lets you securely access your remote computer without a password, which is nice when running remote computations from R.\nIn your DigitalOcean account, create a new Droplet. For simplicity’s sake, you can use the “One-click apps” tab to create a computer that has Docker pre-installed.\nChoose how big you want the droplet to be. For now, we’ll just make a small $5/month VPS, but you can get as fancy as you want in real life.\nCheck the box near the bottom to add your SSH key to the VPS automatically.\nAll done! Take note of the IP address. You can connect to the machine now with a terminal with ssh root@IPADDRESS if you want, but you don’t need to.\n\nYou can automate all this with the analogsea package in R. Create an API key in your DigitalOcean account:\nThen edit ~/.Rprofile (or create a new file there, if needed) and add this line:\n\nSys.setenv(DO_PAT = \"KEY_GOES_HERE\")\n\nNow, you can use R to create DigitalOcean droplets, like so:\n\nlibrary(tidyverse)\nlibrary(analogsea)\n\n# droplet_create() makes a generic Linux VPS\nremote_computer &lt;- droplet_create(region = \"sfo2\", size = \"1gb\")\n\n# Or...\n# docklet_create() makes a Linux VPS with Docker pre-installed\nremote_computer &lt;- docklet_create(region = \"sfo2\", size = \"1gb\")\n\n1.2. Make sure the remote computer has the correct R environment and packages\nThe computers you specify in plan(remote, workers = \"blah\") need to (1) have R installed, and (2) have all the packages installed that are needed for the computation. You can manually install R and all the needed packages, but that can take a long time and it’s tedious. If you need a cluster of 6 computers, you don’t want to take an hour to install R on each of them (and wait for all the packages to compile).\nThe easiest way to get a ready-to-go R environment is to use Docker. I won’t cover the details of Docker here (since I’ve done that already)—essentially, Docker lets you install pre-configured virtual computers instantly. For example, rocker/rbase is a Linux machine with R preinstalled, and rocker/tidyverse is a Linux machine with R + RStudio server + tidyverse pre-installed. You can also create project-specific environments like this one for my donors-NGOs project, and you can use R packages like containerit to automatically create a Dockerfile out of your current R environment. It’s probably best to make your own custom Docker image for your own projects if you use any packages beyond what’s in the default rbase or tidyverse images.\nTo get a Docker image onto your remote computer, log into the computer with ssh root@IPADDRESS in your terminal and run docker pull rocker/tidyverse.\nIf you’re using analogsea, run this from R:\n\ndroplet(remote_computer$id) %&gt;% docklet_pull(\"rocker/tidyverse\")\n\nYou can do this to create as many remote computers as you want.\nThe first time you run docklet_pull() on a droplet, Docker will download hundreds of megabytes of container files. This is fine for one computer, but if you’re creating a cluster of multiple computers, you might not want to redownload everything every time on each computer (to avoid extra bandwidth charges, for example). Instead of pulling a fresh Docker image on each computer, you can take a snapshot of the first remote droplet and then make new droplets based on the snapshot:\n\n# Create new droplet with rocker/tidyverse\nremote_computer &lt;- docklet_create(region = \"sfo2\", size = \"1gb\")\ndroplet(remote_computer$id) %&gt;% docklet_pull(\"rocker/tidyverse\")\n\n# Create snapshot\ndroplet(remote_computer$id) %&gt;% \n  droplet_power_off() %&gt;% \n  droplet_snapshot(name = \"tidyverse_ready\") %&gt;% \n  droplet_power_on()\n\n# Create a new droplet based on this snapshot. This new computer will already\n# have rocker/tidyverse on it\n#\n# You can see a list of available snapshots and get the name/id with\n# images(private = TRUE)\nremote_computer2 &lt;- droplet_create(image = \"12345678\", \n                                   region = \"sfo2\", size = \"1gb\")\n\n# This won't take long because it already has rocker/tidyverse\n# You should get this message:\n#   Status: Image is up to date for rocker/tidyverse:latest\ndroplet(remote_computer$id) %&gt;% docklet_pull(\"rocker/tidyverse\")\n\nJust make sure you delete the snapshots when you’re done—they cost $0.05 per GB per month."
  },
  {
    "objectID": "blog/2018/07/30/disposable-supercomputer-future/index.html#use-futureplan-to-point-r-to-those-computers",
    "href": "blog/2018/07/30/disposable-supercomputer-future/index.html#use-futureplan-to-point-r-to-those-computers",
    "title": "Create a cheap, disposable supercomputer with R, DigitalOcean, and future",
    "section": "2. Use future::plan() to point R to those computers",
    "text": "2. Use future::plan() to point R to those computers\nWe can now point R to this remote computer (or remote computers) and have future automatically use the Docker-installed R.\nIf we didn’t use Docker and instead installed R on the remote machine itself, all we’d need to do is run this in R:\n\nplan(remote, workers = \"IP ADDRESS HERE\")\n\nHowever, because R lives inside a Docker image, we need to do a tiny bit of extra configuration on the local computer—we have to tell the remote computer how to turn on and access the R Docker image. We do this by defining a cluster. Here’s a heavily commented example of how to do that:\n\n# Public IP for droplet(s); this can also be a vector of IP addresses\nip &lt;- IP_ADDRESS_HERE\n\n# Path to private SSH key that matches key uploaded to DigitalOcean\nssh_private_key_file &lt;- \"/Users/andrew/.ssh/id_rsa\"\n\n# Connect and create a cluster\ncl &lt;- makeClusterPSOCK(\n  ip,\n  \n  # User name; DigitalOcean droplets use root by default\n  user = \"root\",\n  \n  # Use private SSH key registered with DigitalOcean\n  rshopts = c(\n    \"-o\", \"StrictHostKeyChecking=no\",\n    \"-o\", \"IdentitiesOnly=yes\",\n    \"-i\", ssh_private_key_file\n  ),\n  \n  # Command to run on each remote machine\n  # The script loads the tidyverse Docker image\n  # --net=host allows it to communicate back to this computer\n  rscript = c(\"sudo\", \"docker\", \"run\", \"--net=host\", \n              \"rocker/tidyverse\", \"Rscript\"),\n  \n  # These are additional commands that are run on the remote machine. \n  # At minimum, the remote machine needs the future library to work—installing furrr also installs future.\n  rscript_args = c(\n    # Create directory for package installation\n    \"-e\", shQuote(\"local({p &lt;- Sys.getenv('R_LIBS_USER'); dir.create(p, recursive = TRUE, showWarnings = FALSE); .libPaths(p)})\"),\n    # Install furrr and future\n    \"-e\", shQuote(\"if (!requireNamespace('furrr', quietly = TRUE)) install.packages('furrr')\")\n  ),\n  \n  # Actually run this stuff. Set to TRUE if you don't want it to run remotely.\n  dryrun = FALSE\n)\n\nWith this cluster defined, we can now use it in future::plan():\n\nplan(cluster, workers = cl)\n\nAnd that’s it! future is now ready to run commands on the remote computer."
  },
  {
    "objectID": "blog/2018/07/30/disposable-supercomputer-future/index.html#run-r-commands-on-those-computers-with-futurefuture_lapply-or-furrrfuture_map",
    "href": "blog/2018/07/30/disposable-supercomputer-future/index.html#run-r-commands-on-those-computers-with-futurefuture_lapply-or-furrrfuture_map",
    "title": "Create a cheap, disposable supercomputer with R, DigitalOcean, and future",
    "section": "3. Run R commands on those computers with future::future_lapply() or furrr::future_map()\n",
    "text": "3. Run R commands on those computers with future::future_lapply() or furrr::future_map()\n\nNow we can run future-based commands on the remote computer with %&lt;-%. First, let’s check the remote computer’s name:\n\n# Verify that commands run remotely by looking at the name of the remote\n# Create future expression; this doesn't run remotely yet\nremote_name %&lt;-% {\n  Sys.info()[[\"nodename\"]]\n} \n\n# Run remote expression and see that it's running inside Docker, not locally\nremote_name\n#&gt; [1] \"docker-s-1vcpu-2gb-sfo2-01\"\n\nHow many CPUs does it have?\n\n# See how many CPU cores the remote machine has\nn_cpus %&lt;-% {parallel::detectCores()} \nn_cpus\n#&gt; [1] 1\n# Just one, since this is a small machine\n\nWe can now outsource any command we want to the remote computer. We don’t even have to transfer data manually to the remote—future takes care of all of that automatically:\n\n# Do stuff with data locally\ntop_5_worlds &lt;- starwars %&gt;% \n  filter(!is.na(homeworld)) %&gt;% \n  count(homeworld, sort = TRUE) %&gt;% \n  slice(1:5) %&gt;% \n  mutate(homeworld = fct_inorder(homeworld, ordered = TRUE))\n\n# Create plot remotely, just for fun\nhomeworld_plot %&lt;-% { \n  ggplot(top_5_worlds, aes(x = homeworld, y = n)) +\n    geom_bar(stat = \"identity\") + \n    labs(x = \"Homeworld\", y = \"Count\", \n         title = \"Most Star Wars characters are from Naboo and Tatooine\",\n         subtitle = \"It really is a Skywalker/Amidala epic\")\n}\n\n# Run the command remotely and show plot locally\n# Note how we didn't have to load any data on the remote machine. future takes\n# care of all of that for us!\nhomeworld_plot\n\n\nIn addition to %&lt;-%, we can use functions like furrr::future_map() to run functions across a vector of values. Because we’re only running one remote computer, all these calculations happen on that remote image. If we used more, future would automatically dispatch different chunks of this code across different computers and then reassemble them locally. Anything you can do with furrr or future can now be done remotely.\n\n# Multiply the numbers 1-5 by 10000 random numbers\n# All these calculations happen remotely!\nfuture_map(1:5, ~ rnorm(10000) * .x)\n#&gt; [[1]]\n#&gt;     [1] -6.249602e-01  1.949156e+00  2.205669e+00  4.525842e-01\n#&gt; ...\n\nYou can even nest future plans by specifying them in a list, and the package will take care of the different nested layers automatically. You can also place plan(list(...)) in a file named .future.R, which future will source automatically when it is loaded. This allows you to use different plans on different computers without ever changing your code.\n\nplan(list( \n  tweak(cluster, workers = cl),  # Use a cluster of computers locally \n  multiprocess  # Use all the CPUs on remote machines \n)) \n\ncomplicated_remote_stuff %&lt;-% {  \n  # Do complicated stuff on all the remote CPUs with future or furrr functions\n  future_map(1:5, ~ rnorm(10000) * .x)\n} \n\ncomplicated_remote_stuff\n#&gt; [[1]]\n#&gt;     [1] -4.746093e-02  1.874897e+00  1.679342e+00 -5.775777e-01\n#&gt; ..."
  },
  {
    "objectID": "blog/2018/07/30/disposable-supercomputer-future/index.html#throw-away-the-remote-computers-when-youre-done",
    "href": "blog/2018/07/30/disposable-supercomputer-future/index.html#throw-away-the-remote-computers-when-youre-done",
    "title": "Create a cheap, disposable supercomputer with R, DigitalOcean, and future",
    "section": "4. Throw away the remote computers when you’re done",
    "text": "4. Throw away the remote computers when you’re done\nBecause we’ve used Docker-based R installations, spinning up new droplets is trivial—you don’t have to manually install R and all the packages you need by hand. All these remote images are completely disposable.\nOnce you’re done running stuff remotely, you can delete the droplets and save money. Either delete them through the DigitalOcean dashboard, or use analogsea:\n\ndroplet_destroy(remote_computer$id)"
  },
  {
    "objectID": "blog/2018/07/30/disposable-supercomputer-future/index.html#thats-all",
    "href": "blog/2018/07/30/disposable-supercomputer-future/index.html#thats-all",
    "title": "Create a cheap, disposable supercomputer with R, DigitalOcean, and future",
    "section": "5. That’s all!",
    "text": "5. That’s all!\nPhew. That’s all! This is a lot easier than figuring out how to orchestrate Docker images with Kubernetes or figuring out how to create Hadoop clusters. future takes care of all the hard work behind the scenes and this all Just Works™."
  },
  {
    "objectID": "blog/2018/07/30/disposable-supercomputer-future/index.html#full-example",
    "href": "blog/2018/07/30/disposable-supercomputer-future/index.html#full-example",
    "title": "Create a cheap, disposable supercomputer with R, DigitalOcean, and future",
    "section": "Full example",
    "text": "Full example\nHere’s a real-life example of using Stan to estimate a bunch of models on a cluster of two 4-CPU, 8 GB RAM machines. It uses the andrewheiss/docker-donors-ngo-restrictions Docker image because it already has Stan and family pre-installed. Each machine costs $0.06 per hour to run, so it’s essentially a cheap, fast, remote, and disposable supercomputer.\n\n# Load libraries ----------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(analogsea)\nlibrary(broom)\nlibrary(rstanarm)\nlibrary(gapminder)\nlibrary(tictoc)\nlibrary(ggstance)\n\n# Path to private SSH key that matches key on DigitalOcean\nssh_private_key_file &lt;- \"/Users/andrew/.ssh/id_rsa\"\n\n\n# Set up remote machines --------------------------------------------------\n\n# Create two new droplets with Docker pre-installed\n# Here I'm using \"s-4vcpu-8gb\", which has 4 CPUs and 8 GB of RAM.\n# Run analogsea::sizes() to see all the available sizes\ndroplet1 &lt;- docklet_create(region = \"sfo2\", size = \"s-4vcpu-8gb\")\ndroplet2 &lt;- docklet_create(region = \"sfo2\", size = \"s-4vcpu-8gb\")\n\n# Pull the docker image with the environment for this project\n#\n# Here I'm using andrewheiss/docker-donors-ngo-restrictions because it already\n# has rstan and friends installed; none of the rocker R images do that\n#\n# NB: Wait for a minute before running this so that Docker is ready to\n# run on the remote machines\ndroplet(droplet1$id) %&gt;% \n  docklet_pull(\"andrewheiss/docker-donors-ngo-restrictions\")\n\ndroplet(droplet2$id) %&gt;% \n  docklet_pull(\"andrewheiss/docker-donors-ngo-restrictions\")\n\n# Get IP addresses\nip1 &lt;- droplet(droplet1$id)$networks$v4[[1]]$ip_address\nip2 &lt;- droplet(droplet2$id)$networks$v4[[1]]$ip_address\n\nips &lt;- c(ip1, ip2)\n\n\n# Make remote cluster -----------------------------------------------------\n\n# Command to run on each remote machine\n# The script loads the docker-donors-ngo-restrictions Docker image\n# --net=host allows it to communicate back to this computer\nrscript &lt;- c(\"sudo\", \"docker\", \"run\", \"--net=host\", \n             \"andrewheiss/docker-donors-ngo-restrictions\", \"Rscript\")\n\n# Connect and create a cluster\ncl &lt;- makeClusterPSOCK(\n  ips,\n  \n  # User name; DO droplets use root by default\n  user = \"root\",\n  \n  # Use private SSH key registered with DO\n  rshopts = c(\n    \"-o\", \"StrictHostKeyChecking=no\",\n    \"-o\", \"IdentitiesOnly=yes\",\n    \"-i\", ssh_private_key_file\n  ),\n  \n  rscript = rscript,\n  \n  # Things to run each time the remote instance starts\n  rscript_args = c(\n    # Set up .libPaths() for the root user and install future/purrr/furrr packages\n    # Technically future and furrr are already installed on \n    # andrewheiss/docker-donors-ngo-restrictions, so these won't do anything\n    \"-e\", shQuote(\"local({p &lt;- Sys.getenv('R_LIBS_USER'); dir.create(p, recursive = TRUE, showWarnings = FALSE); .libPaths(p)})\"),\n    \"-e\", shQuote(\"if (!requireNamespace('furrr', quietly = TRUE)) install.packages('furrr')\"),\n    # Make sure the remote computer uses all CPU cores with Stan\n    \"-e\", shQuote(\"options(mc.cores = parallel::detectCores())\")\n  ),\n  \n  dryrun = FALSE\n)\n\n# Use the cluster of computers as the backend for future and furrr functions\nplan(cluster, workers = cl)\n\n# We'll use gapminder data to estimate the relationship between health and \n# wealth in each continent using a Bayesian model\n\n# Process and manipulate data locally\n# Nest continent-based data frames into one larger data frame\ngapminder_to_model &lt;- gapminder %&gt;% \n  group_by(continent) %&gt;% \n  nest() %&gt;% \n  # Not enough observations here, so ignore it\n  filter(continent != \"Oceania\")\ngapminder_to_model\n#&gt; # A tibble: 4 x 2\n#&gt;   continent data              \n#&gt;   &lt;fct&gt;     &lt;list&gt;            \n#&gt; 1 Asia      &lt;tibble [396 × 5]&gt;\n#&gt; 2 Europe    &lt;tibble [360 × 5]&gt;\n#&gt; 3 Africa    &lt;tibble [624 × 5]&gt;\n#&gt; 4 Americas  &lt;tibble [300 × 5]&gt; \n\n# Fit a Bayesian model with naive normal priors on the coefficients and\n# intercept on each of the continents. In real life, you'd want to use less\n# naive priors and rescale your data, but this is just an example.\nmodel_to_run &lt;- function(df) {\n  model_stan &lt;- stan_glm(lifeExp ~ gdpPercap + country, \n                         data = df, family = gaussian(),\n                         prior = normal(), prior_intercept = normal(),\n                         chains = 4, iter = 2000, warmup = 1000, seed = 1234)\n  return(model_stan)\n}\n\n# Use future_map to outsource each of the continent-based models to a different\n# remote computer, where it will be run with all 4 remote cores\ntic()\ngapminder_models &lt;- gapminder_to_model %&gt;% \n  mutate(model = data %&gt;% future_map(~ model_to_run(.x)))\ntoc()\n#&gt; 27.786 sec elapsed\n\n# That's so fast!\n\n# It worked!\ngapminder_models\n#&gt; # A tibble: 4 x 3\n#&gt;   continent data               model        \n#&gt;   &lt;fct&gt;     &lt;list&gt;             &lt;list&gt;       \n#&gt; 1 Asia      &lt;tibble [396 × 5]&gt; &lt;S3: stanreg&gt;\n#&gt; 2 Europe    &lt;tibble [360 × 5]&gt; &lt;S3: stanreg&gt;\n#&gt; 3 Africa    &lt;tibble [624 × 5]&gt; &lt;S3: stanreg&gt;\n#&gt; 4 Americas  &lt;tibble [300 × 5]&gt; &lt;S3: stanreg&gt;\n\n\n# Do stuff with the models ------------------------------------------------\n\n# Extract the gdpPercap coefficient from the rstanarm models\ngapminder_models_to_plot &lt;- gapminder_models %&gt;% \n  mutate(tidied = model %&gt;% map(~ tidy(.x, intervals = TRUE, prob = 0.9))) %&gt;% \n  unnest(tidied) %&gt;% \n  filter(term == \"gdpPercap\")\n\n# Plot the coefficients\nggplot(gapminder_models_to_plot, aes(x = estimate, y = continent)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrangeh(aes(xmin = lower, xmax = upper, color = continent), size = 1) +\n  labs(x = \"Coefficient estimate (log GDP per capita)\", y = NULL,\n       caption = \"Bars show 90% credible intervals\") +\n  scale_color_viridis_d(begin = 0, end = 0.9, name = NULL) +\n  theme_grey() + theme(legend.position = \"bottom\")\n\n\n\n# Delete droplets ---------------------------------------------------------\n\ndroplet_delete(droplet1$id)\ndroplet_delete(droplet2$id)"
  },
  {
    "objectID": "blog/2018/12/17/academic-job-market-visualized/index.html",
    "href": "blog/2018/12/17/academic-job-market-visualized/index.html",
    "title": "The academic job search finally comes to an end",
    "section": "",
    "text": "I am so beyond thrilled to announce that I’ll be joining the Andrew Young School of Policy Studies at Georgia State University in Fall 2019 as an assistant professor in the Department of Public Management and Policy. I’ll be teaching classes in statistics/data science, economics, and nonprofit management in beautiful downtown Atlanta, and we’ll be moving back to the South. I am so so excited about this! The Andrew Young School does amazing work in public policy, administration, and nonprofit management, and I’ll be working with phenomenal colleagues and students. I still can’t believe this is real.\nPart of the reason I’m in shock is that for the past 2.5 years, I’ve been ripped apart and destroyed by the academic job market. This job market is a horrendous beast of a thing. It is soul-crushing and dream-shattering and a constant stream of rejection. While facing rejection is good and builds grit etc., etc., in reality it’s awful.\nIn an effort to stay On Brand™, here are a bunch of fancy graphs and numbers showing what it’s been like to apply for nearly 200 jobs since August 2016. Unlike many of my other blog posts, I haven’t included any of the code to generate these. That code is all available in a GitHub repository (see README.Rmd), along with the raw data that I’ve collected over the past few years (for the morbidly curious)."
  },
  {
    "objectID": "blog/2018/12/17/academic-job-market-visualized/index.html#application-count-and-outcomes",
    "href": "blog/2018/12/17/academic-job-market-visualized/index.html#application-count-and-outcomes",
    "title": "The academic job search finally comes to an end",
    "section": "Application count and outcomes",
    "text": "Application count and outcomes\nBetween August 31, 2016 and November 18, 2018, I applied for 186 tenure-track and non-tenure-track academic jobs at R1 schools, liberal arts colleges, and teaching-focused public universities. I was offered one two-year visiting assistant professorship at the Romney Institute of Public Service and Ethics at BYU (where I completed my MPA before getting my PhD), and one tenure-track assistant professorship at the Andrew Young School at Georgia State University.\nThat’s it. That’s a 98.9% rejection rate. Here’s what the weight of rejection looks like:\n\n\n\n\n\n\n\n\nNot every one of these was an outright rejection. The typical academic search goes through a few stages:\n\nScreen hundreds of initial applications\nSkype or call 10-15 possible candidates\nFly out ≈3 candidates\nExtend offer to 1 candidate\n\nI made it through different stages of this process with many of the schools I applied to. In total, I had 27 Skype interviews and 9 flyouts over three years. This waffle plot divides up each of the applications by their final outcome (i.e. Skype, flyout, offer), discipline, and year. smh polisci.\n\n\n\n\n\n\n\n\nI received my PhD in public policy, with an emphasis in political science and international relations. Many faculty at Duke emphasized that having a dual emphasis like this would be great for the academic job market—because I’m trained in both fields, I could theoretically fit comfortably in a traditional political science department or in a public policy or administration department/school. I applied to positions in both disciplines, but I was far less successful in getting any traction in the political science world (even though I attend and present research at ISA and APSA pretty regularly and I have published in the Journal of Politics ¯\\_(ツ)_/¯).\nMy first year on the market, I was split 50/50 between public administration/policy jobs and political science jobs. During my second year, because I had very little response from political science I focused almost entirely on public admin/policy. During this most recent cycle, out of desperation I went back to applying to political science jobs in international relations and comparative politics. In the chart below, my proportion of public admin/policy jobs is actually the lowest this year, but that’s because (1) political science deadlines are way earlier, and (2) I essentially quit applying for jobs. This stopping was both because my mom died suddenly, which completely threw me off the rhythm of the market (i.e. I stopped applying completely until one school graciously e-mailed me to remind me to apply), and because I got an amazing job offer. I was on track for another 50/50 year, though."
  },
  {
    "objectID": "blog/2018/12/17/academic-job-market-visualized/index.html#application-timing",
    "href": "blog/2018/12/17/academic-job-market-visualized/index.html#application-timing",
    "title": "The academic job search finally comes to an end",
    "section": "Application timing",
    "text": "Application timing\nApplying for jobs is also a grueling process. Here’s what a typical job needs for a complete application:\n\nCV\nCover letter tailored for the job, department, and university\nTeaching statement and/or research statement and/or diversity statement (and for a few religious schools I applied to, a belief statement)\nTeaching evaluations and/or sample syllabuses\nTranscripts from graduate school\nWriting sample(s)\n3–5 letters of recommendation\n\nFortunately, once I wrote a version of each of these things, applying to individual schools didn’t take too terribly long. I spent August–September 2016 crafting my teaching/research/diversity statements and general cover letter, polishing my writing samples, and collecting letters of recommendation, and I edited and improved them over the whole 2.5-year process.\nWriting cover letters generally took ≈45 minutes per school to scour each department’s website for relevant programs, centers, faculty, and classes. On some of the days where I sent out 5+ applications, I occasionally forgot to change the recipient address in the cover letter, or writing “I am applying for job X at university Y” while leaving “University Z” from the previous application. This was always horrifying, but I got Skype interviews out of a couple of those, so I think search committees tend to be forgiving about those kinds of mistakes.\nThis plot shows my pace of applying for jobs. Each dot is a job; each column is a week. The fall semester has been the most intense for sending out applications. In 2016–17, my record was 16 jobs in one week; in 2017–18 it was only 6 (since I severely cut back on political science jobs); and in 2018–19, I applied for 30 jobs in one week. With a rough average of 1 hour per application, that week was particularly intense.\n\n\n\n\n\n\n\n\nSo many dots."
  },
  {
    "objectID": "blog/2018/12/17/academic-job-market-visualized/index.html#geography",
    "href": "blog/2018/12/17/academic-job-market-visualized/index.html#geography",
    "title": "The academic job search finally comes to an end",
    "section": "Geography",
    "text": "Geography\nFinally, another unique aspect of the academic job market is the fact that you rarely have control over where you end up. If you want to live in a specific state or city, you have to make sure a university there has an open position in your exact field (and then you have to compete against 300+ other applicants, so lolz to that). Friends, family, and neighbors would always suggest that I send my CV to nearby schools because “surely they’ll find a place for you—you’re smart!”. But that’s not how academia works.\nI applied to positions in 12 different countries, with most in the United States.\n\n\n\n\n\n\n\n\nHere’s a world map showing the locations of all these jobs across the three years. It’s really hard to see any patterns beyond the fact that I only applied for jobs in the Gulf in 2016–17, I guess?\n\n\n\n\n\n\n\n\nSince the bulk of my applications went to schools in the US and Canada, here’s a more zoomed-in map. Because there are occasionally clusters of schools—particularly along the east coast—I put a 15-mile radius around each school, and if any of those buffer zones overlapped, I increased the point size to show how many schools are in that shared area. The code for this is actually pretty magical and ingenius—it’s worth it to check out the R code for this post just for those calculations :).\nI applied to schools in 36 states + DC. I didn’t apply to any schools in Alaska, Delaware, Iowa, Louisiana, Mississippi, Montana, North Dakota, Nebraska, New Hampshire, Nevada, Rhode Island, South Dakota, Vermont, West Virginia, or Wyoming.\n\n\n\n\n\n\n\n\n\nHere’s to being done with the job search! Off to Georgia State next year!"
  },
  {
    "objectID": "blog/2019/02/16/algebra-calculus-r-yacas/index.html",
    "href": "blog/2019/02/16/algebra-calculus-r-yacas/index.html",
    "title": "Chidi’s budget and utility: doing algebra and calculus with R and yacas",
    "section": "",
    "text": "(See this notebook on GitHub)\nA year ago, I wrote about how to use R to solve a typical microeconomics problem: finding the optimal price and quantity of some product given its demand and cost. Doing this involves setting the first derivatives of two functions equal to each other and using algebra to find where they cross. I showed how to use neat functions like Deriv::Deriv() and splinefun() and make fancy plots showing supply and demand and it’s pretty cool. I wrote it mostly because I was teaching an introductory microeconomics course and wanted an easy, generalizable, and manual math-less way to make these plots for my students’ exercises and problem sets, and it works great.\nI’m teaching microeconomics again this year and decided to tackle a trickier problem that involves curvier curves, more variables, and more math. And the results are even cooler and open the door for more doing math and symbolic algebra directly with R.\nAnother typical microeconomics problem is to find the optimal level of consumption of two goods, given their prices, your budget, and your preferences for those goods. It involves (1) constructing a budget line that shows the feasible level of consumption, (2) drawing indifference curves that show the combination of goods that give you the same amount of happiness, or utility, or utils, and (3) finding which indifference curve is tangent line and where that happens. The point where the budget matches the indifference curve is the optimal utility maximizing point. The more mathy approach is that we need to find where the slope of the budget line, or marginal rate of transformation (MRT) is equal to the slope of the indifference curve, or marginal rate of substitution (MRS).\nFor this situation, imagine a mild-mannered professor named Chidi Anagonye who is crippled with decision anxiety (he is also dead). He has to choose the best combination of pizza and frozen yogurt, but doesn’t know what to do! Here’s the information he has:\n\\[\nU(x, y) = x^2\\ 0.25y\n\\]\nArmed with only this information, we can find the optimal number of pizza slices and bowls of yogurt. So buckle up, my little chili babies. Let’s do this with R!"
  },
  {
    "objectID": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#step-0-a-brief-note-about-math-in-r",
    "href": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#step-0-a-brief-note-about-math-in-r",
    "title": "Chidi’s budget and utility: doing algebra and calculus with R and yacas",
    "section": "Step 0: A brief note about math in R",
    "text": "Step 0: A brief note about math in R\nR is phenomenal software for statistics and graphics and all sorts of other things, but it was not invented to do higher level algebra and calculus. Expensive programs like MATLAB and Mathematica were invented for this kind of complicated numerical computing (and Wolfram Alpha is really neat! Try searching for “derive 3x^2” there and see what happens!). There are also a bunch of open source computer algebra systems (CAS) that let you do mathy things with formulas, like SageMath, SymPy, and Yacas.\nMany of these CAS libraries have interfaces with R, like rSymPy and Ryacas. However, they’re fairly undocumented and rSymPy hasn’t been updated since 2010. Additionally, rSymPy requires rJava (for whatever reason), which is a gigantic headache to install. Ryacas, on the other hand, doesn’t have a ton of dependencies, is actively maintained, and is slightly better documented.\nThere are like a billion examples of how to use Ryacas here or on StackOverflow, but here are just a few tiny examples to show what it can do:\nlibrary(Ryacas)\n\n# Define some variables to work with\nx &lt;- Sym(\"x\")\ny &lt;- Sym(\"y\")\n\n# Write a formula as a string or R expression\nyacas(\"x^2 + 5\")\n## expression(x^2 + 5)\n\nyacas(x^2 + 5)\n## expression(x^2 + 5)\n\n# Find the derivative of a complicated formula\nyacas(deriv(x^2 * (5*y)^3), x)\n## expression(2 * (x * (5 * y)^3))\n\n# Find the derivative of an R function (!!!)\nneat_function &lt;- function(x, y) x^2 * (5*y)^3\nyacas(deriv(neat_function(x, y), x))  # d/dx\n## expression(2 * (x * (5 * y)^3))\n\nyacas(deriv(neat_function(x, y), y))  # d/dy\n## expression(x^2 * (15 * (5 * y)^2))\n\n# Solve a complicated formula\nSolve(x^2 + (5*y) == 0, y)\n## Yacas vector:\n## [1] y == -(x^2/5)\n\n# Write these yacas things as LaTeX\ncat(TeXForm(yacas(neat_function(x, y))))\n## x ^{2} \\left( 5 y\\right)  ^{3}\nWhat’s really neat about Ryacas is that because it returns expressions, you can use them as pseudo functions within R using Eval(). You can even build wrapper functions with Eval():\nderiv_neat_function &lt;- function(my_y, my_x) {\n  derived &lt;- yacas(deriv(neat_function(x, y), x))\n  Eval(derived, list(x = my_x, y = my_y))\n}\n\nderiv_neat_function(1, 2)\n## [1] 500\nAgain, this is just scratching the surface of what Ryacas can do. There are also more powerful CAS libraries, like Python’s SymPy, though it doesn’t currently have a nice interface with R (but you could theoretically use reticulate to feed R objects directly to Python, do mathy things with them, and return them to R)."
  },
  {
    "objectID": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#step-1-plot-a-budget-line",
    "href": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#step-1-plot-a-budget-line",
    "title": "Chidi’s budget and utility: doing algebra and calculus with R and yacas",
    "section": "Step 1: Plot a budget line",
    "text": "Step 1: Plot a budget line\nWith that mathy stuff out of the way, we can finally start figuring out Chidi’s optimal level of consumption. First, we’ll load our libraries, create a custom ggplot theme, and set up some variables to work with Ryacas\nlibrary(tidyverse)\nlibrary(Ryacas)\n\n# Make a few minor adjustments to theme_classic()\n# I'm using Encode Sans Condensed: https://fonts.google.com/specimen/Encode+Sans+Condensed\ntheme_econ &lt;- function() {\n  theme_classic(base_family = \"Encode Sans Condensed\") + \n    theme(axis.title.x = element_text(margin = margin(t = 10)),\n          axis.title.y = element_text(margin = margin(r = 10)),\n          axis.title = element_text(family = \"Encode Sans Condensed Medium\"))\n}\n\n# Make all labels automatically use Encode Sans\nupdate_geom_defaults(\"label\", list(family = \"Encode Sans Condensed Bold\"))\n\n# Yacas variables\nx &lt;- Sym(\"x\")\ny &lt;- Sym(\"y\")\nU &lt;- Sym(\"U\")\nTo build the budget line, we need to figure out how many pizzas and yogurts Chidi would buy if he spent all his money on each item. We can do this with some simple math:\ntotal_budget &lt;- 45\nprice_x &lt;- 3\nprice_y &lt;- 1.5\n\nn_pizza &lt;- total_budget / price_x\nn_pizza\n## [1] 15\n\nn_yogurt &lt;- total_budget / price_y\nn_yogurt\n## [1] 30\n\nslope &lt;- -n_yogurt / n_pizza\nslope\n## [1] -2\nThat’s 15 pizzas or 30 yogurts. We can use this information to build a formula. Since we can draw a line using the \\(y = mx + b\\) form, we need to figure out the slope (\\(m\\)) and y-intercept (\\(b\\)). The y-intercept here is 30, or the number of yogurts he’d eat if x is 0, and the slope is the number of yogurts divided by the number of pizzas, or -2. That means the budget line is:\n\\[\ny = -2x + 30\n\\]\nWe’ll write this as an R function, and then we can use stat_function() in ggplot2 to plot it:\nbudget &lt;- function(x) (slope * x) + n_yogurt\n\nggplot() + \n  # Draw the line\n  stat_function(data = tibble(x = 0:15), aes(x = x),\n                fun = budget, color = \"#638ccc\", size = 1.5) +\n  # Add a label\n  annotate(geom = \"label\", x = 2.5, y = budget(2.5), \n           label = \"Budget\", color = \"#638ccc\") +\n  labs(x = \"Slices of pizza\", y = \"Bowls of frozen yogurt\") +\n  # Make the axes go all the way to zero\n  scale_x_continuous(expand = c(0, 0), breaks = seq(0, 15, 5)) +\n  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 30, 10)) +\n  coord_cartesian(xlim = c(0, 16), ylim = c(0, 32)) +\n  theme_econ()\n\n\n\n\nBudget line\n\n\n\nAny point along that line will use up Chidi’s entire budget. He can’t afford any point above the line, and he’s not efficiently spending his money if he chooses any point below the line (like 5 pizzas and 10 yogurts, for instance)."
  },
  {
    "objectID": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#step-2-plot-utility-and-indifference-curves",
    "href": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#step-2-plot-utility-and-indifference-curves",
    "title": "Chidi’s budget and utility: doing algebra and calculus with R and yacas",
    "section": "Step 2: Plot utility and indifference curves",
    "text": "Step 2: Plot utility and indifference curves\nLike we said at the beginning, Chidi gets utility (or happiness points) from his consumption of pizza and yogurt, based on this formula:\n\\[\nU(x, y) = x^2\\ 0.25y\n\\]\nWe can write this as an R function and use it to figure out how happy he’d be with any given amount of pizza and yogurt:\nutility_u &lt;- function(x, y) x^2 * (0.25 * y)\n\n# 5 pizzas, 5 yogurt\nutility_u(5, 5)\n## [1] 31.25\nHe gets 31.25 happiness points from 5 slices of pizza and 5 bowls of frozen yogurt. Neat.\nIf we rearrange this utility formula to be in terms of \\(y\\), we can plot the function to show the combinations of pizza and yogurt that would provide the same level of happiness. Ordinarily, you have to do this by hand with algebra, but the tediousness of that is what led me down this mathy rabbit hole. We can use Ryacas to rearrange this for us! With the Solve() function, we can set Chidi’s utility function to \\(U\\) and solve for \\(y\\) and yacas will rearrange everything for us:\nutility_solved &lt;- Solve(utility_u(x, y) == U, y)\nutility_solved\n## Yacas vector:\n## [1] y == U/(0.25 * x^2)\n\\[\n\\left( y = \\frac{U}{0.25 x ^{2}} \\right)\n\\]\nAnd with Eval(), we can use this as a function and figure out values of \\(y\\) given different \\(x\\)s and \\(U\\)s. It’s even vectorized so you can feed it lots of numbers:\nEval(utility_solved, list(x = 5, U = 10))\n## [1] \"( y == 1.6 )\"\n\n# It's already vectorized\nEval(utility_solved, list(x = 1:5, U = 10))\n## [1] \"( y == 40 )\"               \"( y == 10 )\"              \n## [3] \"( y == 4.44444444444444 )\" \"( y == 2.5 )\"             \n## [5] \"( y == 1.6 )\"\nWe can make this more general by wrapping a function around it, which lets us pass any \\(x\\) or \\(U\\) we want. More importantly, we convert the text that Eval() returns into a number with str_extract(), which uses a regular expression to pull out the number in the string (including decimals and negative signs).\nNote how the arguments for the function aren’t just x and U, but my_x and my_u instead. That’s because of environment issues—using the same x and U variable names causes conflicts, since it’s not clear if the arguments or the global x and U we set earlier get passed to Solve() or Eval(). To avoid this namespace clash, we rename the arguments.\nutility_y &lt;- function(my_x, my_U) {\n  solved &lt;- Solve(utility_u(x, y) == U, y)\n  solution &lt;- Eval(solved, list(x = my_x, U = my_U))\n  # Regex incantation to extract numbers\n  as.numeric(str_extract(solution, \"-?[0-9]\\\\d*(\\\\.\\\\d+)?\"))\n}\n\n# We could avoid regexes by digging into the deeply nested list that yacas()\n# returns, but doing that doesn't handle negatives very well\n# See https://stackoverflow.com/q/8480778/120898\n# unlist(as.list(yacas(solution)$text[[1]][[3]]))\n\nutility_y(5, 10)\n## [1] 1.6\n\n# Still vectorized\nutility_y(1:5, 10)\n## [1] 40.000000 10.000000  4.444444  2.500000  1.600000\nNow that we’ve created this y-based function, we can plot it with stat_function(). Here are a few different indifference curves for different levels of utility:\nggplot() + \n  # Budget line\n  stat_function(data = tibble(x = 0:15), aes(x = x),\n                fun = budget, color = \"#638ccc\", size = 1.5) +\n  annotate(geom = \"label\", x = 2.5, y = budget(2.5), \n           label = \"Budget\", color = \"#638ccc\") +\n  # U = 10\n  stat_function(data = tibble(x = 1:15), aes(x = x),\n                fun = utility_y, args = list(my_U = 10),\n                color = \"#ab62c0\", size = 1.5) +\n  annotate(geom = \"label\", x = 2.5, y = utility_y(2.5, 10), \n           label = \"U = 10\", color = \"#ab62c0\") +\n  # U = 100\n  stat_function(data = tibble(x = 1:15), aes(x = x),\n                fun = utility_y, args = list(my_U = 100),\n                color = \"#ca5670\", size = 1.5) +\n  annotate(geom = \"label\", x = 7, y = utility_y(7, 100), \n           label = \"U = 100\", color = \"#ca5670\") +\n  # U = 500\n  stat_function(data = tibble(x = 1:15), aes(x = x),\n                fun = utility_y, args = list(my_U = 500),\n                color = \"#c57c3c\", size = 1.5) +\n  annotate(geom = \"label\", x = 12.5, y = utility_y(12.5, 500), \n           label = \"U = 500\", color = \"#c57c3c\") +\n  labs(x = \"Slices of pizza\", y = \"Bowls of frozen yogurt\") +\n  scale_x_continuous(expand = c(0, 0), breaks = seq(0, 15, 5)) +\n  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 30, 10)) +\n  coord_cartesian(xlim = c(0, 16), ylim = c(0, 32)) +\n  theme_econ()\n\n\n\n\nThree indifference curves\n\n\n\nEvery combination of pizza and yogurt on these indifference curves generates the same level of utility. Cool cool cool."
  },
  {
    "objectID": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#step-3-find-where-the-indifference-curve-and-budget-line-are-tangent",
    "href": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#step-3-find-where-the-indifference-curve-and-budget-line-are-tangent",
    "title": "Chidi’s budget and utility: doing algebra and calculus with R and yacas",
    "section": "Step 3: Find where the indifference curve and budget line are tangent",
    "text": "Step 3: Find where the indifference curve and budget line are tangent\nHowever, we still don’t know what the best level of consumption is. To find that, we need to figure out the point where an indifference curve is tangent to the budget line. Based on the graph, it’s a combination of \\(x\\) and \\(y\\) that yields somewhere between 100 and 500 utils, but that’s a wide range. The mathy way to find this point is to set the slope of the budget line (the marginal rate of transformation) equal to the slope of the indifference curve (the marginal rate of substitution) and solve for \\(x\\) and \\(y\\). In economics, “marginal” means slope, or first derivative, so we need to differentiate our formulas.\nIn addition to rearranging formulas, we can use Ryacas to derive functions. Last year I showed how to use the Deriv package to calculate the derivative of a function in R. Those formulas only had one variable to worry about. This utility function has two, which means we need to take the partial derivative with respect to \\(x\\) (\\(\\partial u / \\partial x\\)) and divide it by the partial derivative with respect to \\(y\\) (\\(\\partial u / \\partial y\\)).\nFrom my minimal tinkering, neither the base R derivative functions nor Deriv::Deriv() can handle partial derivatives well, but Ryacas::deriv() does. Also, since it returns expressions, we can do math (like division!) with the partial derivatives. Here I find each of the partial derivatives and then use Simplify() to clean up their ratio:\n# Partial derivative with respect to x\nmu_x &lt;- deriv(utility_u(x, y), x)\nmu_x\n## expression(2 * (x * (0.25 * y)))\n\\[\n\\partial u / \\partial x = 2 x 0.25 y\n\\]\n# Partial derivative with respect to y\nmu_y &lt;- deriv(utility_u(x, y), y)\nmu_y\n## expression(0.25 * x^2)\n\\[\n\\partial u / \\partial y = 0.25 x ^{2}\n\\]\n# Cleaned up version\nSimplify(mu_x / mu_y)\n## expression(0.5 * y/(0.25 * x))\n\\[\n\\frac{\\partial u / \\partial x}{\\partial u / \\partial y} = \\frac{2 x 0.25 y}{0.25 x ^{2}} = \\frac{0.5 y}{0.25 x}\n\\]\nThis marginal rate of substitution is the slope of the indifference curve at any combination of \\(x\\) and \\(y\\), but it doesn’t incorporate any information about the prices. Before we can find where happiness meets the budget, we have to incorporate prices into the marginal utility, or marginal rate of substitution. Because of calculus proofs that are beyond my skill, the marginal rate of substitution can also be written as a ratio of prices, or \\(\\frac{\\text{Price}_x}{\\text{Price}_y}\\). If we set marginal utility equal to this price ratio and rearrange the formula to be in terms of y, we’ll have the official price-attuned marginal rate of substitution (or slope of the indifference curve).\nTo do this with R, we can use Solve() again, only this time we can set the equation equal to the price ratio:\n# For unknown reasons, yacas works better with (3 / 1.5) instead of 2, so we\n# build the string with price_x and price_y instead of price_x/price_y\nSolve(paste(Simplify(mu_x / mu_y), \"==\", price_x, \"/\", price_y), y)\n## Yacas vector:\n## [1] y == -(-0.75 * x/0.75)\n\\[\n\\left( y =  - \\frac{-0.75 x}{0.75} \\right)\n\\]\nAs before, we can turn this solved formula into a function so that we can find the slope of the indifference curve at any x.\nmarginal_utility &lt;- function(my_x) {\n  mux_muy &lt;- Simplify(deriv(utility_u(x, y), x) / deriv(utility_u(x, y), y))\n  mux_muy_price &lt;- Solve(paste(mux_muy, \"==\", price_x, \"/\", price_y), y)\n  solution &lt;- Eval(mux_muy_price, list(x = my_x))\n  as.numeric(str_extract(solution, \"-?[0-9]\\\\d*(\\\\.\\\\d+)?\"))\n}\nIf we set this formula equal to the budget line and solve for x and y individually, we’ll find the optimal mix of pizza and yogurt. Instead of doing this by hand with algebra, we can use the built-in uniroot() function to see where the two functions are the same. We have to feed uniroot() a range to look in—here I chose 0–100. There’s probably a way to do this with Ryacas, but I didn’t want to figure it out. And besides, I already made the marginal_utility() function be an official R function, so I figured I could use it like one here.\n# Find best x\noptimal_x &lt;- uniroot(function(x) budget(x) - marginal_utility(x), c(0, 100))$root\noptimal_x\n## [1] 10\n\n# Plug best x into the budget function to find best y\noptimal_y &lt;- budget(optimal_x)  # or marginal_utility(optimal_x)\noptimal_y\n## [1] 10\n\n# Plug optimal x and y into utility function to find maximum utils given the budget\nmax_utility &lt;- utility_u(optimal_x, optimal_y)\nmax_utility\n## [1] 250\nThere! We have an answer. The optimal level of consumption is 10 slices of pizza and 10 bowls of frozen yogurt, which yields 250 utils of happiness.\nWith these final numbers and derived functions, we can combine all this information into a final plot, which shows that the indifference curve for 250 utils is exactly tangent to the budget line at (10, 10). MAGIC.\nggplot() + \n  # Budget line\n  stat_function(data = tibble(x = 0:15), aes(x = x),\n                fun = budget, color = \"#638ccc\", size = 1.5) +\n  annotate(geom = \"label\", x = 2.5, y = budget(2.5), \n           label = \"Budget\", color = \"#638ccc\") +\n  # Best indifference curve\n  stat_function(data = tibble(x = 1:15), aes(x = x),\n                fun = utility_y, args = list(my_U = max_utility),\n                color = \"#ca5670\", size = 1.5) +\n  annotate(geom = \"label\", x = 7, y = utility_y(7, max_utility), \n           label = paste0(\"U = \", max_utility), color = \"#ca5670\") +\n  # Dotted lines to show x and y\n  annotate(geom = \"segment\", x = 0, y = optimal_y, xend = optimal_x, yend = optimal_y,\n           linetype = \"dashed\", color = \"grey50\", size = 0.5) +\n  annotate(geom = \"segment\", x = optimal_x, y = 0, xend = optimal_x, yend = optimal_y,\n           linetype = \"dashed\", color = \"grey50\", size = 0.5) +\n  # Dot at optimal point\n  annotate(geom = \"point\", x = optimal_x, y = optimal_y, size = 3) +\n  labs(x = \"Slices of pizza\", y = \"Bowls of frozen yogurt\") +\n  scale_x_continuous(expand = c(0, 0), breaks = seq(0, 15, 5)) +\n  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 30, 10)) +\n  coord_cartesian(xlim = c(0, 16), ylim = c(0, 32)) +\n  theme_econ()\n\n\n\n\nUtility, budget, and optimal point\n\n\n\nAnd there we go. You all get As. Or Fs. And there is no test. And you all failed it. And you all got As. Who cares. Goodbye."
  },
  {
    "objectID": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#tldr-complete-example",
    "href": "blog/2019/02/16/algebra-calculus-r-yacas/index.html#tldr-complete-example",
    "title": "Chidi’s budget and utility: doing algebra and calculus with R and yacas",
    "section": "tl;dr complete example",
    "text": "tl;dr complete example\nThat was a lot of annotated code. Here’s a complete example without all the explanation.\nlibrary(tidyverse)\nlibrary(Ryacas)\n\nupdate_geom_defaults(\"label\", list(family = \"Encode Sans Condensed Bold\"))\n\n# Define some variables to work with\nx &lt;- Sym(\"x\")\ny &lt;- Sym(\"y\")\nU &lt;- Sym(\"U\")\n\n\n# Prices and budget -------------------------------------------------------\ntotal_budget &lt;- 45\nprice_x &lt;- 3\nprice_y &lt;- 1.5\n\n# Build budget line\nn_pizza &lt;- total_budget / price_x\nn_yogurt &lt;- total_budget / price_y\nslope &lt;- -n_yogurt / n_pizza\n\nbudget &lt;- function(x) (slope * x) + n_yogurt\n\n\n# Utility and indifference ------------------------------------------------\n# U = x^2 0.25y\nutility_u &lt;- function(x, y) x^2 * (0.25 * y)\n\n# Rewrite as y = something\nutility_y &lt;- function(my_x, my_U) {\n  solved &lt;- Solve(utility_u(x, y) == U, y)\n  solution &lt;- Eval(solved, list(x = my_x, U = my_U))\n  as.numeric(str_extract(solution, \"-?[0-9]\\\\d*(\\\\.\\\\d+)?\"))\n}\n\n# Marginal rate of substitution\nmarginal_utility &lt;- function(my_x) {\n  mux_muy &lt;- Simplify(deriv(utility_u(x, y), x) / deriv(utility_u(x, y), y))\n  mux_muy_price &lt;- Solve(paste(mux_muy, \"==\", price_x, \"/\", price_y), y)\n  solution &lt;- Eval(mux_muy_price, list(x = my_x))\n  as.numeric(str_extract(solution, \"-?[0-9]\\\\d*(\\\\.\\\\d+)?\"))\n}\n\n\n# Optimal points ----------------------------------------------------------\n# Find best x\noptimal_x &lt;- uniroot(function(x) budget(x) - marginal_utility(x), c(0, 100))$root\n\n# Plug best x into the budget function to find best y\noptimal_y &lt;- budget(optimal_x)\n\n# Plug optimal x and y into utility function to find maximum utils given the budget\nmax_utility &lt;- utility_u(optimal_x, optimal_y)\n\n\n# Plot everything ---------------------------------------------------------\nggplot() + \n  # Budget line\n  stat_function(data = tibble(x = 0:15), aes(x = x),\n                fun = budget, color = \"#638ccc\", size = 1.5) +\n  annotate(geom = \"label\", x = 2.5, y = budget(2.5), \n           label = \"Budget\", color = \"#638ccc\") +\n  # Best indifference curve\n  stat_function(data = tibble(x = 1:15), aes(x = x),\n                fun = utility_y, args = list(my_U = max_utility),\n                color = \"#ca5670\", size = 1.5) +\n  annotate(geom = \"label\", x = 7, y = utility_y(7, max_utility), \n           label = paste0(\"U = \", max_utility), color = \"#ca5670\") +\n  # Dotted lines to show x and y\n  annotate(geom = \"segment\", x = 0, y = optimal_y, xend = optimal_x, yend = optimal_y,\n           linetype = \"dashed\", color = \"grey50\", size = 0.5) +\n  annotate(geom = \"segment\", x = optimal_x, y = 0, xend = optimal_x, yend = optimal_y,\n           linetype = \"dashed\", color = \"grey50\", size = 0.5) +\n  # Dot at optimal point\n  annotate(geom = \"point\", x = optimal_x, y = optimal_y, size = 3) +\n  labs(x = \"Slices of pizza\", y = \"Bowls of frozen yogurt\") +\n  scale_x_continuous(expand = c(0, 0), breaks = seq(0, 15, 5)) +\n  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 30, 10)) +\n  coord_cartesian(xlim = c(0, 16), ylim = c(0, 32)) +\n  theme_classic(base_family = \"Encode Sans Condensed\") + \n  theme(axis.title.x = element_text(margin = margin(t = 10)),\n        axis.title.y = element_text(margin = margin(r = 10)),\n        axis.title = element_text(family = \"Encode Sans Condensed Medium\"))\n\n\n\n\nComplete example"
  },
  {
    "objectID": "blog/2020/01/01/flexdashboard-dynamic-data/index.html",
    "href": "blog/2020/01/01/flexdashboard-dynamic-data/index.html",
    "title": "Create a dynamic dashboard with R, flexdashboard, and Shiny",
    "section": "",
    "text": "Now that I’m on the tenure track, I’ve been looking for a way to keep track of my different research projects so I can get them all finished and published. Matt Lebo’s “Managing Your Research Pipeline” presents a neat way of quantifying and tracking the progress of your research, and I recently adopted it for my own stuff. I even made a fancy R Markdown + flexdashboard dashboard to show the status of the pipeline interactively. I manage the data for the dashboard in a Google Sheet, knit the dashboard, and create an HTML file with the latest statistics.\nThe combination of R Markdown + flexdashboard + ggplotly makes it incredibly easy to create interactive dashboards, but you have to reknit and recompile the dashboard any time any data changes, which I’m not a fan of doing. Wouldn’t it be amazing if there was a way to get the latest data into a dashboard document without any reknitting?\nFortunately there is a way! flexdashboard’s integration with Shiny makes it possible!\nHere’s a minimal working example of feeding data from a Google Sheet into a Shiny-based flexdashboard. It has to be hosted on a Shiny server somewhere (like shinyapps.io or on your own server), but you shouldn’t have to reknit ever again!"
  },
  {
    "objectID": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-1-create-a-google-sheet",
    "href": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-1-create-a-google-sheet",
    "title": "Create a dynamic dashboard with R, flexdashboard, and Shiny",
    "section": "Step 1: Create a Google Sheet",
    "text": "Step 1: Create a Google Sheet\nGo to Google Drive, create a new spreadsheet, and put this data in it:\n\n\n\nCategory\nCount\n\n\n\n\nA\n4\n\n\nB\n3\n\n\nC\n8\n\n\nD\n2\n\n\n\nYou can keep it private, but then you have to deal with OAuth tokens to get it to work with a Shiny server, so it’s easiest here to make it publicly accessible. If you want to work with private data, check the googlesheets4 documentation on authentication. Click on the “Share” button in the top right corner and make the document accessible via a link. You should also make the sheet public by going to File &gt; Publish to the web and publish as a CSV. You can also just use this sheet here if you don’t want to create your own (but this is read only and you won’t be able to edit it and see your changes live)."
  },
  {
    "objectID": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-2-build-a-static-dashboard",
    "href": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-2-build-a-static-dashboard",
    "title": "Create a dynamic dashboard with R, flexdashboard, and Shiny",
    "section": "Step 2: Build a static dashboard",
    "text": "Step 2: Build a static dashboard\nWe’ll make a barebones dashboard with a couple big numbers at the top showing a count of categories and the sum of the category counts, and an interactive bar chart at the bottom showing the counts in each category. Check the fantastic documentation for flexdashboard for more details and examples.\nSave this as an R Markdown file and knit it—the resulting HTML file is a complete self-contained dashboard.\n---\ntitle: \"Example dashboard with static data from Google\"\noutput:\n  flexdashboard::flex_dashboard:\n    orientation: rows\n    theme: yeti\n---\n\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(flexdashboard)\nlibrary(plotly)\n\n# Make googlesheets4 not try to authenticate, since we're using a public sheet\nsheets_deauth()\n```\n\n```{r get-data, message=FALSE, include=FALSE}\n# The 1RKNn84xVLUanhGyX8DPvDSd8zt4GX_eBmK2ZX2nf0BI comes from the shared link\n# URL from Google Sheets. You can also refer to sheets by name if you're\n# authenticated into your account. See the googlesheets4 documentation for more\n# details\nremote_data &lt;- read_sheet(\"1RKNn84xVLUanhGyX8DPvDSd8zt4GX_eBmK2ZX2nf0BI\")\n```\n\n## Row\n\n### Categories {.value-box}\n\n```{r}\n# Find the number of unique categories\nn_categories &lt;- remote_data %&gt;% distinct(Category) %&gt;% nrow()\n\n# Show the number in a special valueBox (note the {.value-box} CSS class\n# above—that applies the CSS class to the HTML output and makes it render\n# correctly)\nvalueBox(value = n_categories, icon = \"fas fa-users\")\n```\n\n### Total {.value-box}\n\n```{r}\n# Get a total of all the counts\ntotal &lt;- sum(remote_data$Count)\n\n# Show the number in a valueBox\nvalueBox(value = total, icon = \"fas fa-cubes\")\n```\n\n## Row\n\n###\n\n```{r}\n# Make a basic column plot\nmy_plot &lt;- ggplot(remote_data, aes(x = Category, y = Count)) +\n  geom_col(aes(text = Count)) +\n  theme_minimal()\n\n# Show the plot with plotly\nggplotly(my_plot, tooltip = \"text\")\n```"
  },
  {
    "objectID": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-3-make-static-dashboard-dynamic",
    "href": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-3-make-static-dashboard-dynamic",
    "title": "Create a dynamic dashboard with R, flexdashboard, and Shiny",
    "section": "Step 3: Make static dashboard dynamic",
    "text": "Step 3: Make static dashboard dynamic\nThe only problem with this dashboard is that it’s now the same every time you visit it. If you update the data in the Google Sheet, those changes won’t be reflected in the dashboard unless you reknit the document.\nWe can make the dashboard dynamic with some basic shinyfication: we can put the data reading into a function that gets rerun when the page is reloaded, and we can put valueBox() and ggplotly() inside functions that connect them to the more dynamic data. This only requires a few changes—here’s the same static dashboard with the Shiny bits added.\n---\ntitle: \"Example dashboard with dynamic data from Google\"\noutput:\n  flexdashboard::flex_dashboard:\n    orientation: rows\nruntime: shiny  # Make this use a Shiny backend\n---\n\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(flexdashboard)\nlibrary(plotly)\n\n# Make googlesheets4 not try to authenticate, since we're using a public sheet\nsheets_deauth()\n```\n\n```{r get-data, message=FALSE, include=FALSE}\n# Create a function that reads the data from Google. I've seen other examples\n# that make this a reactive(), but I don't really know what that is since I\n# rarely use Shiny :shrug:\n#\n# I'm also not completely sold that this is the right approach, since this feels\n# kind of roundabout (create a function, run the function), but it doesn't work\n# if you just do remote_data &lt;- read_sheet(). Also :shrug: for now.\n#\n# ALSO flexdashboard can use a special global chunk to speed up expensive data\n# loading (https://rmarkdown.rstudio.com/flexdashboard/shiny.html#loading_data),\n# which I assume also includes loading data remotely from Google, but if you\n# name this chunk global, the dynamic data loading stops working. Once again,\n# big :shrug:.\nload_remote_data &lt;- function() {\n  read_sheet(\"1RKNn84xVLUanhGyX8DPvDSd8zt4GX_eBmK2ZX2nf0BI\")\n}\n\nremote_data &lt;- load_remote_data()\n```\n\n## Row\n\n### Categories {.value-box}\n\n```{r}\nn_categories &lt;- remote_data %&gt;% distinct(Category) %&gt;% nrow()\n\n# Put valueBox() inside renderValueBox({})\nrenderValueBox({\n  valueBox(value = n_categories, icon = \"fas fa-users\")\n})\n```\n\n\n### Total {.value-box}\n\n```{r}\ntotal &lt;- sum(remote_data$Count)\n\n# Put valueBox() inside renderValueBox({})\nrenderValueBox({\n  valueBox(value = total, icon = \"fas fa-cubes\")\n})\n```\n\n## Row\n\n###\n\n```{r}\nmy_plot &lt;- ggplot(remote_data, aes(x = Category, y = Count)) +\n  geom_col(aes(text = Count)) +\n  theme_minimal()\n\n# Put ggplotly() inside renderPlotly({})\nrenderPlotly({\n  ggplotly(my_plot, tooltip = \"text\")\n})\n```"
  },
  {
    "objectID": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-4-check-the-dynamicness",
    "href": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-4-check-the-dynamicness",
    "title": "Create a dynamic dashboard with R, flexdashboard, and Shiny",
    "section": "Step 4: Check the dynamicness",
    "text": "Step 4: Check the dynamicness\nEdit the Google Sheet, refresh the Shiny app, and you should see everything change in real time!"
  },
  {
    "objectID": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-5-publish-somewhere",
    "href": "blog/2020/01/01/flexdashboard-dynamic-data/index.html#step-5-publish-somewhere",
    "title": "Create a dynamic dashboard with R, flexdashboard, and Shiny",
    "section": "Step 5: Publish somewhere",
    "text": "Step 5: Publish somewhere\nThe easiest way to publish the Shiny-based dashboard is to click on the “Publish” menu in the top right corner of the RStudio preview window and then go through the menus to publish the document at shinyapps.io. Here’s mine.\n\n\n\nflexdashboard-based Shiny app on shinyapps.io\n\n\n\nAnd that’s it! An R Markdown-based flexdashboard that dynamically loads data from a Google Sheet. There are probably better ways to load the data (like somehow using the global chunk?), but this works well enough!"
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html",
    "title": "Ways to close backdoors in DAGs",
    "section": "",
    "text": "(See this notebook on GitHub)\nI’ve been teaching program evaluation in the MPA/MPP program at GSU for the past semester and a half, and since I was given free rein over how to teach it, I decided to make it as modern and cutting edge as possible. To do this, I’ve infused the class with modern causal inference techniques (commonly known as the “causal revolution”), focused on the language of directed acyclic graphs (DAGs), do-calculus, confounding, colliding, and the rest of Judea Pearl’s world of finding causation in observational data.\nHowever, I was never taught this approach in any of my doctoral classes, and I’m entirely self-taught. I read The Book of Why in March 2019, and I’ve thrown myself into every possible journal article, blog post, and online course I’ve come across in order to understand how to isolate and identify causal effects. It’s been hard, but thanks to an incredibly welcoming community of economists, epidemiologists, and political scientists on Twitter, I’m getting it! (And so are my students!)\nThe purpose of this post isn’t to introduce causal models and DAGs and confounders vs. colliders and all that. For that kind of introduction, consult any (or all!) of these resources:\nInstead, this post is a practical example of how exactly you can isolate causal effects by closing backdoor paths and adjusting for confounders in observational data. At its core, DAG-based causal inference involves isolating relationships—if some variable causes both your treatment and your outcome (thus confounding it), you can deal with that common cause in some statistical way and isolate the treatment–outcome effect. There’s no one right way to statistically deal with confounding, so here I show a few different ways to do it.\nTo make this more concrete and practical, I use simulated data from a hypothetical program, but I use actual variable names rather than the \\(x\\), \\(y\\), and \\(z\\) variables that are common in tutorials and articles about DAGs. Here, our outcome \\(y\\) is a final grade, \\(x\\) is a special math camp, and \\(z\\) is all the possible confounders of the effect of the math camp on the grade.\nLike my post showing a bunch of different ways to test differences in means, this is mostly a resource to my students and to future me. Please feel free to comment and make corrections or additions at GitHub!\nHere’s a tl;dr table of contents since this is a little long:"
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#example-program",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#example-program",
    "title": "Ways to close backdoors in DAGs",
    "section": "Example program",
    "text": "Example program\nWe’ll refer to a hypothetical math camp program throughout all these examples. Many policy schools offer a brief math camp in the weeks before students begin their graduate degrees, with the hope that it will help students be more prepared in math-heavy classes like statistics and microeconomics. For these examples, we’re interested in answering one question: what is the causal effect of attending math camp on final student outcomes?\nWe can use ggdag to draw a simplified causal model that explains what causes final student outcomes (it’s probably wrong, but whatever). I added all the fancy bells and whistles to the graph object here just for the sake of reference. In reality, you don’t need labels or coordinates or the individual geom_dag_*() layers and you can just do ggdag(math_camp_dag) to get a basic graph, but for fun I’ve included everything you need for a publication-worthy graph.\nlibrary(tidyverse)  # ggplot, dplyr, %&gt;%, and friends\nlibrary(ggdag)  # Make DAGs with ggplot\nlibrary(dagitty)  # Do basic DAG math\nlibrary(broom)  # For converting model output to data frames\n\nnode_details &lt;- tribble(\n  ~name, ~label, ~x, ~y,\n  \"math_camp\", \"Math camp\", 2, 1,\n  \"final_grade\", \"Final grade\", 4, 1,\n  \"needs_camp\", \"Needs camp\", 1, 2,\n  \"gre_quant\", \"GRE quantitative\", 2.5, 2,\n  \"gre_verbal\", \"GRE verbal\", 5, 2,\n  \"background\", \"Background\", 2, 3,\n  \"undergraduate_gpa\", \"Undergraduate GPA\", 4, 3\n)\n\nnode_labels &lt;- node_details$label\nnames(node_labels) &lt;- node_details$name\n\nmath_camp_dag &lt;- dagify(final_grade ~ math_camp + gre_quant + gre_verbal + \n                          undergraduate_gpa + background,\n                        math_camp ~ needs_camp, \n                        needs_camp ~ background + undergraduate_gpa + gre_quant,\n                        gre_quant ~ background + undergraduate_gpa,\n                        gre_verbal ~ background + undergraduate_gpa,\n                        undergraduate_gpa ~ background,\n                        exposure = \"math_camp\",\n                        outcome = \"final_grade\",\n                        latent = \"background\",\n                        coords = node_details,\n                        labels = node_labels)\n\n# Turn DAG into a tidy data frame for plotting\nmath_camp_dag_tidy &lt;- math_camp_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  node_status()   # Add column for exposure/outcome/latent\n\nstatus_colors &lt;- c(exposure = \"#0074D9\", outcome = \"#FF4136\", latent = \"grey50\")\n\n# Fancier graph\nggplot(math_camp_dag_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_label_repel(aes(label = label, fill = status), seed = 1234,\n                       color = \"white\", fontface = \"bold\") +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = FALSE, fill = FALSE) + \n  theme_dag()\n\n\n\nDAG for math camp example\n\n\nWe can tell a fairly complex story using this graph. Your final grade in the program is caused by a host of things, including your quantitative and verbal GRE scores (PROBABLY DEFINITELY NOT in real life, but go with it), your undergraduate GPA, and your unmeasured background factors (age, parental income, math anxiety, level of interest in the program, etc.). Your undergraduate GPA is determined by your background, and your GRE scores are determined by both your undergraduate GPA and your background. Because this math camp program is open to anyone, there is self-selection in who chooses to do it. We can pretend that this is decided by your undergraduate GPA, your quantitative GRE score, and your background. If the program was need-based and only offered to people with low GRE scores, we could draw an arrow from GRE quantitative to math camp, but we don’t. Finally, needing the math camp causes people to do it.\nThere is a direct path between our treatment and outcome (math camp → final grade), but there is also some possible backdoor confounding. Both GRE quantitative and undergraduate GPA have arrows pointing to final grade and math camp (through “needs camp”), which means they’re a common cause, and background is both a confounder and unmeasurable. But we don’t need to give up! If we adjust or control for “needs camp,” we can block the association between background, GRE quantitative, and undergraduate GPA. With this backdoor closed, we’ve isolated the math camp → final grade relationship and can find the causal effect.\nHowever, we don’t really have a measure for needing math camp—we can’t read peoples’ minds and see if they need the program—so while it’d be great to just include a needs_camp variable in a regression model, we’ll have to use other techniques to close the backdoor.\nYou can find the backdoors automatically with Dagitty (draw the DAG there and notice red arrows between the unblocked confounders), or with functions in the dagitty R package. If you run paths(math_camp_dag), you can see that the only node pointing back into math_camp is needs_camp, and if you run adjustmentSets(math_camp_dag), you’ll see that needs_camp is the only required adjustment set:\npaths(math_camp_dag)\n## $paths\n##  [1] \"math_camp -&gt; final_grade\"\n##  [2] \"math_camp &lt;- needs_camp &lt;- background -&gt; final_grade\"\n##  [3] \"math_camp &lt;- needs_camp &lt;- background -&gt; gre_quant -&gt; final_grade\"\n##  [4] \"math_camp &lt;- needs_camp &lt;- background -&gt; gre_quant &lt;- undergraduate_gpa -&gt; final_grade\"\n##  [5] \"math_camp &lt;- needs_camp &lt;- background -&gt; gre_quant &lt;- undergraduate_gpa -&gt; gre_verbal -&gt; final_grade\"\n##  [6] \"math_camp &lt;- needs_camp &lt;- background -&gt; gre_verbal -&gt; final_grade\"\n##  ...\n##\n## $open\n##  [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n## [12]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n## [23]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n\nadjustmentSets(math_camp_dag)\n##  { needs_camp }"
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#simulated-data",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#simulated-data",
    "title": "Ways to close backdoors in DAGs",
    "section": "Simulated data",
    "text": "Simulated data\nAssuming this causal graph is correct (it’s probably not, but again, go with it), we can simulate data that reflects these causal relationships. There are a host of R packages for simulating data (like wakefield, simstudy, and fabricatr), but here I do it a little more manually using MASS::mvrnorm() to use a multivariate normal distribution to generate continuous variables that have a specific mean, standard deviation, and relationship with other variables.\nBecause data generation is beyond the scope of this post, the code below is heavily annotated. Importantly, the various mutate() commands that create the math_camp data below create relationships between the confounders, treatment, and outcome. We also create a needs_camp variable that is true if both a student’s quantitative GRE score is less than the average and if their undergraduate GPA is less than the average. We also build in some noncompliance: 80% of those who need math camp do it; 20% of those who don’t need it do it.\nFor the sake of simplicity, the outcome here (final_grade) isn’t GPA or anything—it’s an arbitrary number between 120 and 160 (though we could rescale it to something else).\nMost importantly, we force the math camp program to cause a 10 point increase in students’ final grades. This is our baseline average treatment effect that we want to be able to find using different backdoor adjustment techniques.\n# Make these random draws the same every time\nset.seed(1234)\n\n# Create 2,000 rows\nnum &lt;- 2000\n\n# Create confounder variables that are related to each other\nmu &lt;- c(undergrad_gpa = 3, gre_verbal = 160, gre_quant = 145)\nstddev &lt;- c(undergrad_gpa = 0.5, gre_verbal = 10, gre_quant = 5)\n\n# Correlation matrix: undergrad GPA and verbal GRE have a correlation of 0.8;\n# undergrad GPA and quantitative GRE have a correlation of 0.6, and verbal GRE\n# and quantitative GRE have a correlation of 0.4\ncor_matrix &lt;- matrix(c(1.0, 0.8, 0.6,\n                       0.8, 1.0, 0.4,\n                       0.6, 0.4, 1.0),\n                     ncol = 3)\n\n# Convert correlation matrix to covariance matrix using fancy math\ncov_matrix &lt;- stddev %*% t(stddev) * cor_matrix\n\n# Draw random numbers\nconfounders &lt;- MASS::mvrnorm(n = num, mu = mu, Sigma = cov_matrix, empirical = TRUE) %&gt;%\n  as_tibble() %&gt;%\n  # Truncate values so they're within 130-170 range for GRE and less than 4.0 for GPA\n  mutate_at(vars(gre_verbal, gre_quant),\n            ~case_when(\n              . &gt; 170 ~ 170,\n              . &lt; 130 ~ 130,\n              TRUE ~ .\n            )) %&gt;%\n  mutate(undergrad_gpa = ifelse(undergrad_gpa &gt; 4, 4, undergrad_gpa))\n\n# Make official dataset of simulated values\nmath_camp &lt;- tibble(id = 1:num) %&gt;%\n  bind_cols(confounders) %&gt;%  # Bring in confounders\n  # People need math camp if their GRE and GPA is lower than the average\n  mutate(needs_camp = gre_quant &lt; mean(gre_quant) & \n           undergrad_gpa &lt; mean(undergrad_gpa)) %&gt;%\n  # Build in some noncompliance: 80% of those who need math camp do it; 20% of\n  # those who don't need it do it.\n  mutate(math_camp = case_when(\n    needs_camp == TRUE ~ rbinom(n(), 1, 0.8),\n    needs_camp == FALSE ~ rbinom(n(), 1, 0.2)\n  )) %&gt;%\n  # Create random error in grades\n  mutate(grade_noise = rnorm(num, 15, 5)) %&gt;%\n  # Create final grade based on all the arrows going into the node in the DAG.\n  # There's a 10 point causal effect\n  mutate(final_grade = (0.3 * gre_verbal) + (0.5 * gre_quant) + \n           (0.4 * undergrad_gpa) + (10 * math_camp) + grade_noise) %&gt;%\n  mutate(math_camp = as.logical(math_camp))  # Make true/false\nPhew. That’s a lot of code to make fake data, but it worked! We can look at the first few rows:\nhead(math_camp)\n\n## # A tibble: 6 x 8\n##      id undergrad_gpa gre_verbal gre_quant needs_camp math_camp grade_noise\n##   &lt;int&gt;         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;           &lt;dbl&gt;\n## 1     1          3.90       170       151. FALSE      FALSE            13.5\n## 2     2          3.20       163.      143. FALSE      FALSE            13.2\n## 3     3          2.83       162.      140. TRUE       TRUE             10.4\n## 4     4          2.63       144.      154. FALSE      FALSE            17.0\n## 5     5          3.24       170       148. FALSE      FALSE            16.4\n## 6     6          2.95       167.      146. FALSE      FALSE            19.0\n## # … with 1 more variable: final_grade &lt;dbl&gt;\nAbout 40% of the students participated in math camp:\nmath_camp %&gt;% \n  count(math_camp) %&gt;% \n  mutate(prop = n / sum(n))\n\n## # A tibble: 2 x 3\n##   math_camp     n  prop\n##   &lt;lgl&gt;     &lt;int&gt; &lt;dbl&gt;\n## 1 FALSE      1182 0.591\n## 2 TRUE        818 0.409"
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#incorrect-correlation-is-not-causation-estimate",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#incorrect-correlation-is-not-causation-estimate",
    "title": "Ways to close backdoors in DAGs",
    "section": "Incorrect “correlation is not causation” estimate",
    "text": "Incorrect “correlation is not causation” estimate\nWe can take an initial stab at finding the causal effect of the program by comparing the average final grades for those who did math camp and those who didn’t. At first glance, it looks like math camp participants have a higher grade!\nmath_camp %&gt;% \n  group_by(math_camp) %&gt;% \n  summarize(avg = mean(final_grade))\n\n## # A tibble: 2 x 2\n##   math_camp   avg\n##   &lt;lgl&gt;     &lt;dbl&gt;\n## 1 FALSE      138.\n## 2 TRUE       144.\nThe distribution of scores is higher for those who did the program:\nggplot(math_camp, aes(x = final_grade, fill = math_camp)) +\n  geom_histogram(binwidth = 2, color = \"white\") + \n  guides(fill = FALSE) +\n  facet_wrap(vars(math_camp), ncol = 1) +\n  theme_light()\n\n\n\nNaive, wrong distributions\n\n\nWe can run a simple linear regression model to estimate the exact effect:\nmodel_wrong &lt;- lm(final_grade ~ math_camp, data = math_camp)\ntidy(model_wrong)\n\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)     138.       0.185     744.  0.       \n## 2 math_campTRUE     6.54     0.290      22.6 8.48e-101\nBased on this model, participating in the program is associated with 6.5 more points in your final grade. Neat.\nThis is wrong, though, since there are confounders at play that cause both attendance at math camp and final grade. We need to adjust for those to get the actual causal effect."
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#adjustment-using-forbidden-unmeasured-variable",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#adjustment-using-forbidden-unmeasured-variable",
    "title": "Ways to close backdoors in DAGs",
    "section": "Adjustment using forbidden unmeasured variable",
    "text": "Adjustment using forbidden unmeasured variable\nThe backdoor confounder we have to worry about is needs_camp. We created this variable when we generated the data, so for fun, we can include it in the regression model as a control variable to adjust for it:\nmodel_adj_needs_camp &lt;- lm(final_grade ~ math_camp + needs_camp, data = math_camp)\ntidy(model_adj_needs_camp)\n\n## # A tibble: 3 x 5\n##   term           estimate std.error statistic   p.value\n##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      139.       0.174     798.  0.       \n## 2 math_campTRUE     10.2      0.320      31.9 2.79e-181\n## 3 needs_campTRUE    -6.69     0.329     -20.3 1.28e- 83\nThe coefficient for math_campTRUE is now ≈10, which is what it should be! Adjusting for needing math camp isolated the causal effect.\nBut we can’t do that in real life. We don’t know what needing math camp looks like in actual data, so we need to use other techniques."
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#adjustment-using-educated-guess-based-naive-matching",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#adjustment-using-educated-guess-based-naive-matching",
    "title": "Ways to close backdoors in DAGs",
    "section": "Adjustment using educated-guess-based naive matching",
    "text": "Adjustment using educated-guess-based naive matching\nOne possible approach to guessing the need for math camp is to create groups of students based on what we think might be driving the need for camp. We know from the causal model that quantitative GRE scores and undergraduate GPAs partially cause needing the program. We can assume that people with lower test scores or lower GPAs need the camp and create our own guess about what the threshold might be.\nLet’s look at the distribution of GRE scores and see if there’s any pattern about why people may have chosen to do the program:\nggplot(math_camp, aes(x = gre_quant, fill = math_camp)) +\n  geom_histogram(binwidth = 2, color = \"white\") + \n  guides(fill = FALSE) +\n  facet_wrap(vars(math_camp), ncol = 1) +\n  theme_light()\n\n\n\nQuantitative GRE score across math camp participation\n\n\nThere’s a pretty noticable break in the distribution of GRE scores for those who did the program: lots of people who scored under 145ish did the program, while not a lot of people who scored over 145 did. Without knowing anything about what completely causes math camp need, we can pretend that 145 is some sort of threshold of need and use that as our confounder:\nmath_camp_guessed_need &lt;- math_camp %&gt;% \n  mutate(maybe_needs_camp = gre_quant &lt; 145)\n\nmodel_adj_needs_camp_guess &lt;- lm(final_grade ~ math_camp + maybe_needs_camp, \n                                 data = math_camp_guessed_need)\ntidy(model_adj_needs_camp_guess)\n\n## # A tibble: 3 x 5\n##   term                 estimate std.error statistic   p.value\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)            140.       0.197     708.  0.       \n## 2 math_campTRUE            8.70     0.292      29.8 4.41e-162\n## 3 maybe_needs_campTRUE    -5.33     0.287     -18.6 2.36e- 71\nAfter adjusting for our possible needing camp confounder, the causal effect is now 8.7, which is closer to 10! It’s still not entirely correct, but we’re getting closer."
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#brief-interlude-matching-slightly-simpler-dag",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#brief-interlude-matching-slightly-simpler-dag",
    "title": "Ways to close backdoors in DAGs",
    "section": "Brief interlude: Matching + slightly simpler DAG",
    "text": "Brief interlude: Matching + slightly simpler DAG\nWe just attempted to guess what the “needs camp” node might be based on the nodes flowing into it. If you notice, though, the “needs camp” node is an intermediate node on the path between GPA and GRE scores and actually participating in the math camp program. If we can guess what causes people to enroll in the program, that’s roughly the same as predicting their need for the camp.\nAdditionally, predicting enrollment in the program directly (rather than the desire to enroll) lets us use better matching techniques. Our guess of needing camp was pretty naive—it’d be more accurate if we incorporated other variables (like GPA and background) into our manual grouping. But the intuition of trying to group manually was correct—we looked at the factors that caused needing math camp and guessed that some things make it more likely. We can make this process more formal by building an actual model that predicts the likelihood of treatment.\nTo do this, we can remove the “needs camp” node, since it wasn’t doing much in the model and since we can use confounders like GPA and quantitative GRE to estimate the probability of enrollment in math camp directly. Here’s a slightly simpler version without “needs camp” and “background”:\nnode_details_simpler &lt;- tribble(\n  ~name, ~label, ~x, ~y,\n  \"math_camp\", \"Math camp\", 2, 1,\n  \"final_grade\", \"Final grade\", 4, 1,\n  \"gre_quant\", \"GRE quantitative\", 2.5, 2,\n  \"gre_verbal\", \"GRE verbal\", 5, 2,\n  \"undergraduate_gpa\", \"Undergraduate GPA\", 4, 3\n)\n\nnode_labels_simpler &lt;- node_details_simpler$label\nnames(node_labels_simpler) &lt;- node_details_simpler$name\n\nmath_camp_dag_simpler &lt;- dagify(final_grade ~ math_camp + gre_quant + gre_verbal + \n                                  undergraduate_gpa,\n                                math_camp ~ undergraduate_gpa + gre_quant,\n                                gre_quant ~ undergraduate_gpa,\n                                gre_verbal ~ undergraduate_gpa,\n                                exposure = \"math_camp\",\n                                outcome = \"final_grade\",\n                                coords = node_details,\n                                labels = node_labels)\n\n# Turn DAG into a tidy data frame for plotting\nmath_camp_dag_simpler_tidy &lt;- math_camp_dag_simpler %&gt;% \n  tidy_dagitty() %&gt;%\n  node_status()   # Add column for exposure/outcome\n\nstatus_colors &lt;- c(exposure = \"#0074D9\", outcome = \"#FF4136\", latent = \"grey50\")\n\n# Fancier graph\nggplot(math_camp_dag_simpler_tidy, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_label_repel(aes(label = label, fill = status), seed = 1234,\n                       color = \"white\", fontface = \"bold\") +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = FALSE, fill = FALSE) + \n  theme_dag()\n\n\n\nSimpler math camp DAG"
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#adjustment-using-inverse-probability-weighting-ipw",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#adjustment-using-inverse-probability-weighting-ipw",
    "title": "Ways to close backdoors in DAGs",
    "section": "Adjustment using inverse probability weighting (IPW)",
    "text": "Adjustment using inverse probability weighting (IPW)\nOne common method for matching the assignment to treatment based on confounders that is quite popular in epidemiology is to use inverse probability weighting (IPW). To estimate causal effects with IPW, we follow a two-step process. In the first step, we use the confounders to estimate propensity scores for each observation, or the probability of that observation going to math camp given other characteristics.\nA common way to generate propensity scores is to use logistic regression. Here we build a model estimating participation in math camp based on undergraduate GPA and quantitative GRE scores. We then use augment() to plug the GPA and GRE values for each observation into the model and generate a predicted probability:\nneeds_camp_model &lt;- glm(math_camp ~ undergrad_gpa + gre_quant, data = math_camp, \n                        family = binomial(link = \"logit\"))\n\nmath_camp_propensities &lt;- augment(needs_camp_model, math_camp, type.predict = \"response\") %&gt;%\n  mutate(p_camp = .fitted)  # Rename column\n\nmath_camp_propensities %&gt;% \n  select(id, undergrad_gpa, gre_quant, math_camp, p_camp) %&gt;% \n  head()\n\n## # A tibble: 6 x 5\n##      id undergrad_gpa gre_quant math_camp p_camp\n##   &lt;int&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;      &lt;dbl&gt;\n## 1     1          3.90      151. FALSE     0.0969\n## 2     2          3.20      143. FALSE     0.371 \n## 3     3          2.83      140. TRUE      0.554 \n## 4     4          2.63      154. FALSE     0.291 \n## 5     5          3.24      148. FALSE     0.258 \n## 6     6          2.95      146. FALSE     0.371\nWe have a new column p_camp that shows the probability of going to camp based on grades and test scores. Our first person has a high GPA and high GRE score, so they have a 9% chance of going to math camp, while person 3 has a low GPA and low test score, so they’re more likely to need it.\nIn the second step, we incorporate these predicted probabilities into our causal effect estimation by converting them into weights, which creates a pseduo-population of observations (i.e. some student observations are more important than others for estimating the causal effect). What this means practically is that people with a low likelihood of attending math camp who do attend it anyway should be treated as more important than those who follow expectations, since those with higher weights are more likely to influence the overall causal effect.\nThere are a whole bunch of different weighting techniques, and Lucy D’Agostino McGowan covers them in depth in her excellent blog post here (see also this). For the sake of this example, we’ll calculate weights that are appropriate for estimating two different causal quantities. Here, \\(i\\) represents an individual student, \\(e_i\\) represents the propensity score for an individual needing/participating in math camp, or the treatment \\(Z_i\\):\n\nAverage treatment effect (ATE): overall average effect, or the difference in potential outcomes when the Z = 1 and Z = 0. Formula for weights: \\(\\frac{Z_i}{e_i} + \\frac{1 - Z_i}{1 - e_i}\\)\nAverage treatment effect among the overlap population (ATO): effect of math camp across observations that overlap (i.e. those who are both likely and unlikely to need math camp). Formula for weights: \\((1-e_i)Z_i + e_i(1-Z_i)\\)\n\n\nmath_camp_propensities &lt;- math_camp_propensities %&gt;% \n  mutate(w_ate = (math_camp / p_camp) + ((1 - math_camp) / (1 - p_camp)),\n         w_ato = (1 - p_camp) * math_camp + p_camp * (1 - math_camp))\n\nmath_camp_propensities %&gt;% \n  select(id, p_camp, w_ate, w_ato) %&gt;% \n  head()\n\n## # A tibble: 6 x 4\n##      id p_camp w_ate  w_ato\n##   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1     1 0.0969  1.11 0.0969\n## 2     2 0.371   1.59 0.371 \n## 3     3 0.554   1.80 0.446 \n## 4     4 0.291   1.41 0.291 \n## 5     5 0.258   1.35 0.258 \n## 6     6 0.371   1.59 0.371\nFinally, we can incorporate these weights into a regression model. Note how we’re using math_camp as the only explanatory variable. Because we used undergraduate GPA and quantitative GRE scores to estimate the propensity scores for needing camp (and receiving the program), including the weights should be enough to close the “needs camp” back door:\nmodel_ipw_ate &lt;- lm(final_grade ~ math_camp, \n                    data = math_camp_propensities, weights = w_ate)\ntidy(model_ipw_ate)\n\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      137.      0.222     614.  0.       \n## 2 math_campTRUE     10.9     0.308      35.3 8.75e-213\nmodel_ipw_ato &lt;- lm(final_grade ~ math_camp, \n                    data = math_camp_propensities, weights = w_ato)\ntidy(model_ipw_ato)\n\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      136.      0.203     672.  0.       \n## 2 math_campTRUE     10.0     0.286      35.0 1.02e-209\nBoth of these causal estimates are pretty spot on, with the ATO providing an answer incredibly close to the true value of 10. Neat!\nIf we had other backdoors to adjust for, we could include them in the propensity score model as well. We can do all our backdoor adjustment in the logit model, generate propensity scores, generate inverse probability weights, and then use the weights in a simple regression model to find the actual causal effect."
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#adjustment-using-matching-with-mahalanobis-distance",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#adjustment-using-matching-with-mahalanobis-distance",
    "title": "Ways to close backdoors in DAGs",
    "section": "Adjustment using matching (with Mahalanobis distance)",
    "text": "Adjustment using matching (with Mahalanobis distance)\nWe can use other methods to find matches in the data to estimate the probability of attending math camp. There’s a whole world of other statistical methods for creating matches; inverse probability weights are just one method.\nWhile matching based on propensity scores (i.e. building some model to generate predicted probabilities for the likelihood of treatment and matching observations that have similar propensities) is popular, it can cause problems when you use it for causal identification. Following Gary King’s suggestions, we can match with other techniques. (Again, covering what all these do goes beyond the scope of this post, but there are some excellent resources out there, like this highly accessible video by Gary King.)\nOne popular technique in political science is to match based on Mahalanobis (or Euclidean) distance. We can use matchit() from the MatchIt library to group students with similar needs. Instead of creating a new grouping variable like we did before for needs_camp, because we know that undergrad GPA and quantitative GRE scores cause needing math camp, and that needing math camp is the only path into actually doing the program, we can model the probability of treatment by using undergrad GPA and quantitative GRE.\nlibrary(MatchIt)  # For matching stuff\n\nmatched &lt;- matchit(math_camp ~ undergrad_gpa + gre_quant, data = math_camp,\n                   method = \"nearest\", distance = \"mahalanobis\", replace = TRUE)\nmatched\n\n## \n## Call: \n## matchit(formula = math_camp ~ undergrad_gpa + gre_quant, data = math_camp, \n##     method = \"nearest\", distance = \"mahalanobis\", replace = TRUE)\n## \n## Sample sizes:\n##           Control Treated\n## All          1182     818\n## Matched       366     818\n## Unmatched     816       0\n## Discarded       0       0\nBy matching this way, we found 366 people who didn’t do math camp who look similar to those who did, which means we can arguably say that those who didn’t do it didn’t need to. If we want, we can see which observations were matched:\nhead(matched$match.matrix)\n\n##    1     \n## 3  \"1864\"\n## 7  \"646\" \n## 9  \"586\" \n## 12 \"83\"  \n## 15 \"244\" \n## 20 \"1815\"\nAnd we can extract the details from the match:\nmath_camp_matched &lt;- match.data(matched)\nhead(math_camp_matched)\n\n##    id undergrad_gpa gre_verbal gre_quant needs_camp math_camp grade_noise\n## 3   3      2.828828   161.6843  140.4169       TRUE      TRUE    10.37564\n## 5   5      3.244684   170.0000  147.9607      FALSE     FALSE    16.40707\n## 7   7      3.757950   170.0000  151.4425      FALSE      TRUE    24.01470\n## 9   9      3.085583   162.4533  149.3013      FALSE      TRUE    21.00359\n## 12 12      3.300517   167.0163  152.7640      FALSE      TRUE    15.10616\n## 15 15      3.809366   170.0000  141.3918      FALSE      TRUE    12.68628\n##    final_grade distance   weights\n## 3     140.2209       NA 1.0000000\n## 5     142.6853       NA 0.4474328\n## 7     162.2392       NA 1.0000000\n## 9     155.6245       NA 1.0000000\n## 12    152.9133       NA 1.0000000\n## 15    145.9059       NA 1.0000000\nWe have one new column in this data: weights. Observations are now weighted differently based on how distant they are from their matches—observations who attended math camp that are similar to those who didn’t have a higher weight. This weighting creates a pseudo-population of students (i.e. some student observations are more important than others for estimating the causal effect), just like we did with IPW.\nWe can incorporate these weights into a regression model. Note how we’re using math_camp as the only explanatory variable. Because we used matching to guess the likelihood of needing camp (and receiving the program), including the weights should be enough to close the “needs camp” back door:\nmodel_matched &lt;- lm(final_grade ~ math_camp, data = math_camp_matched, weights = weights)\ntidy(model_matched)\n\n## # A tibble: 2 x 5\n##   term          estimate std.error statistic   p.value\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      134.      0.338     398.  0.       \n## 2 math_campTRUE     10.0     0.407      24.7 7.68e-109\nAnd look at that! The coefficient for math_campTRUE is 10, which is what the true causal effect is.\nMatching with Mahalanobis distance isn’t the only technique available—depending on the context of your actual data (and how complicated you want to get), you could use other algorithms like coarsened exact matching (CEM), optimal matching, or other techniques included in the MatchIt package.\nOne potential downside to matching is that it throws away data. Notice how there are only 1,184 rows in the math_camp_matched dataset, while using inverse probability weights let us use the full 2,000 rows in the original data."
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#someday-when-im-smarter-do-calculus",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#someday-when-im-smarter-do-calculus",
    "title": "Ways to close backdoors in DAGs",
    "section": "Someday when I’m smarter: do-calculus",
    "text": "Someday when I’m smarter: do-calculus\nFinally, Judea Pearl’s most important contribution to the world of causal inference is a complete set of three axioms (or rules) that allow us to convert equations with a \\(do(\\cdot)\\) operator into something estimatable with observational data.\nVery briefly, the do operator lets us define interventions (like programs and policies) in mathematical formulas. For instance, in our ongoing example, we’re interested in \\(Pr(\\text{Grade} | do(\\text{Math camp}))\\), or the probability distribution of final grades given that someone does math camp. Math camp is an intervention, and in a randomized controlled trial, we’d be able to control who got to do it, and thereby estimate the causal effect.\nGiven observational data, though, we’re left only with the ability to calculate \\(Pr(\\text{Grade} | \\text{Math camp})\\), or the probability distribution of final grades given math camp. Because there’s no \\(do(\\cdot)\\) here, we can’t isolate the effect entirely if there are confounders like GRE and GPA. We tried that earlier in the “correlation isn’t causation” section and found an incorrect program effect.\nThe three rules of Pearl’s do-calculus allows us to chop up causal diagrams in systematic ways that can remove the \\(do(\\cdot)\\). The reason closing backdoors by adjusting for confounders works is because the approach follows one of the do-calculus rules that removes \\(do(\\cdot)\\) from \\(Pr(\\text{Grade} | do(\\text{Math camp}))\\). For instance (and apologies for using X, Y, and Z instead of actual variables!), in a DAG with one confounder (Z)…\nbackdoor_dag &lt;- dagify(Y ~ X + Z,\n                       X ~ Z,\n                       exposure = \"X\",\n                       outcome = \"Y\",\n                       coords = list(x = c(X = 1, Y = 3, Z = 2),\n                                     y = c(X = 1, Y = 1, Z = 2)))\n\nggdag(backdoor_dag) + \n  theme_dag()\n\n\n\nTiny XYZ DAG\n\n\n…the do-calculus version of backdoor adjustment is:\n\\[\nP(Y | do(X)) = \\sum_Z P(Y | X, Z) \\times P(Z)\n\\]\nIn other words, we can remove the \\(do(\\cdot)\\) if we multiply the probability distribution of Y given both X and Z by the probability distribution of Z, and then add all those values up for every value of Z. If X, Y, and Z were all binary, we’d be able to write the \\(do\\)-free version of the causal effect like this:\n# P(y|do(x=1)) = P(y|x=1, z=1)*P(z=1) + P(y|x=1, z=0)*P(z=0)\nmean(y[x == 1 & z == 1]) * mean(z == 1) + mean(y[x == 1 & z == 0]) * mean(z == 0)\nThere are fancy algorithms that can determine the exact adjustment formula for a given DAG, and the causaleffect package lets you use these algorithms in R. Here’s the do-calculus version of our math camp example. Unfortunately we have to rewrite the DAG with a different syntax for it to work:\nlibrary(causaleffect)\nlibrary(igraph)\n\nmath_dag &lt;- graph.formula(grade +- undergradGPA, \n                          grade +- GREquant, \n                          grade +- GREverbal,\n                          grade +- mathcamp,\n                          mathcamp +- GREquant,\n                          mathcamp +- undergradGPA,\n                          GREquant +- undergradGPA,\n                          GREverbal +- undergradGPA,\n                      simplify = FALSE)\n# plot(math_dag)  # Plot this\n\n# expr returns a LaTeX formula; simp simplifies the formula through repeated\n# application of the rules of do-calculus\ndo_calc_formula &lt;- causal.effect(y = \"grade\", x = \"mathcamp\",\n                                 G = math_dag, expr = TRUE, simp = TRUE)\nThis produces the following do-calculus-based adjustment formula:\n\n\\[\n\\sum_{GREquant,undergradGPA}P(grade|undergradGPA,GREquant,mathcamp)P(GREquant|undergradGPA)P(undergradGPA)\n\\]\n\nNeat! We still need to adjust for GRE quantitative scores and undergrad GPA, and if we somehow multiply the three probability distributions (final grade given GPA, GRE, and mathcamp, GRE given GPA, and GPA), we’ll have a \\(do(\\cdot)\\)-free version of the causal effect.\nUnfortunately I have absolutely no idea how to do this with R. Continuous variables blow up the math in do-calculus equations and I don’t know how to deal with that."
  },
  {
    "objectID": "blog/2020/02/25/closing-backdoors-dags/index.html#comparison-of-all-methods",
    "href": "blog/2020/02/25/closing-backdoors-dags/index.html#comparison-of-all-methods",
    "title": "Ways to close backdoors in DAGs",
    "section": "Comparison of all methods",
    "text": "Comparison of all methods\nPhew. We just used naive matching, inverse probability weighting, and Mahalanobis matching to estimate the causal effect of a hypothetical math camp program on final grades using only observational data. How’d we do?!\nHere are all the estimates we have, along with a blue dotted line for the truth. Adjusting for backdoor confounders allows us to get far more accurate and identified causal results than when we leave things unadjusted, and we get the most accurate results when we explicitly attempt to match treated and untreated observations (as with do with the IPW ATO and with Mahalanobis matching). That’s likely not always the case—lots of these methods depend on the relationships between the different nodes in the graph, and it’s possible that they don’t work when there are non-linear relationships.\nBut in this case, at least, we can prove causation with observational data, which is really neat!\nlibrary(ggstance)\n\nall_models &lt;- tribble(\n  ~method, ~model,\n  \"Wrong correlation-not-causation\", model_wrong,\n  \"Forbidden needs camp\", model_adj_needs_camp,\n  \"Educated guess needs camp\", model_adj_needs_camp_guess,\n  \"Inverse probability weighting ATE\", model_ipw_ate,\n  \"Inverse probability weighting ATO\", model_ipw_ato,\n  \"Mahalanobis matching\", model_matched\n) %&gt;% \n  # Extract coefficient for math_camp from each model\n  mutate(tidied = map(model, ~tidy(., conf.int = TRUE)),\n         effect = map(tidied, ~filter(., term == \"math_campTRUE\"))) %&gt;% \n  unnest(effect) %&gt;% \n  select(-model, -tidied) %&gt;% \n  mutate(method = fct_inorder(method))\n\nggplot(all_models, aes(x = estimate, y = fct_rev(method), color = method)) +\n  geom_vline(xintercept = 10, size = 1, linetype = \"dashed\", color = \"#0074D9\") +\n  geom_pointrangeh(aes(xmin = conf.low, xmax = conf.high), size = 1) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.9, guide = FALSE) +\n  labs(x = \"Causal effect\", y = NULL, caption = \"Dotted line shows true effect\") +\n  theme_light()\n\n\n\nComparison of all causal effects"
  },
  {
    "objectID": "blog/2020/12/01/ipw-binary-continuous/index.html",
    "href": "blog/2020/12/01/ipw-binary-continuous/index.html",
    "title": "Generating inverse probability weights for both binary and continuous treatments",
    "section": "",
    "text": "My program evaluation class is basically a fun wrapper around topics in causal inference and econometrics. I’m a big fan of Judea Pearl-style “causal revolution” causal graphs (or DAGs), and they’ve made it easier for both me and my students to understand econometric approaches like diff-in-diff, regression discontinuity, and instrumental variables.\nDAGs are also incredibly helpful for doing causal inference with observational data without needing a specific quasi-experimental situation. As I show in this blog post (and in this new textbook chapter!), you can use DAGs to identify confounders that distort the relationship (i.e. open up backdoors) between treatment and outcome. You can then use statistical methods to close those backdoors and adjust for the confounding. In both that blog post and the chapter, I show how to do this with matching and with inverse probability weighting (IPW).\nHowever, those examples assume that the treatment is binary. This is fine—lots of social programs are binary (used program/didn’t use program), and the math for creating inverse probability weights with binary treatment variables is fairly straightforward. However, treatment variables are also often not binary, especially outside of program evaluation.\nIn my own research, I’m working on a couple projects right now where the “treatment” is a count of anti-NGO legal restrictions in a country. I want to be able to use DAGs and inverse probability weighting to adjust for confounders, but I can’t use the IPW stuff I’ve been teaching because that variable isn’t binary! This research project gets even more complicated because it involves time-series cross-sectional (TSCS) data with both time-varying and time-invarying confounders, which opens up a whole other can of worms that I’ll figure out soon following Blackwell and Glynn (2018).\nSo I had to teach myself how to do IPW with continuous variables. This post shows how to calculate IPWs for both binary and continuous treatments, both manually and with a couple different R packages (ipw and WeightIt).\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(scales)\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(truncnorm)\nlibrary(ipw)\nlibrary(WeightIt)"
  },
  {
    "objectID": "blog/2020/12/01/ipw-binary-continuous/index.html#binary-treatments",
    "href": "blog/2020/12/01/ipw-binary-continuous/index.html#binary-treatments",
    "title": "Generating inverse probability weights for both binary and continuous treatments",
    "section": "Binary treatments",
    "text": "Binary treatments\nExample data\nFor this example, we’ll generate a DAG for a hypothetical program where bed net use causes a reduction in malaria risk. That relationship is confounded by both income and health, and income influences health. Income and health both increase the probability of net usage.\nThe treatment here is binary: either people use nets or they don’t.\n\nmosquito_dag &lt;- dagify(mal ~ net + inc + hlth,\n                       net ~ inc + hlth,\n                       hlth ~ inc,\n                       coords = list(x = c(mal = 4, net = 1, inc = 2, hlth = 3),\n                                     y = c(mal = 1, net = 1, inc = 2, hlth = 2)),\n                       exposure = \"net\",\n                       outcome = \"mal\")\n\nggdag_status(mosquito_dag) +\n  guides(color = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\nWe’ll measure these nodes like so:\n\n\nMalaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\n\nNet use: binary 0/1, TRUE/FALSE variable, where 50% of people use nets. Best to use a binomial distribution. However, since we want to use other variables that increase the likelihood of using a net, we’ll generate a latent continuous variable, rescale it to 0–1, and then use it as probabilities in rbinom() and assign people to treatment based on those probabilities.\n\nIncome: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n\nHealth: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 1138 people (just for fun)\nn_people &lt;- 1138\n\nnet_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %&gt;% \n  # Generate health variable: beta, centered around 70ish\n  mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100,\n         # Health increases by 0.02 for every dollar in income\n         health_income_effect = income * 0.02,\n         # Make the final health score and add some noise\n         health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3),\n         # Rescale so it doesn't go above 100\n         health = rescale(health, to = c(min(health), 100))) %&gt;% \n  # Generate net variable based on income, health, and random noise\n  mutate(net_score = (0.5 * income) + (1.5 * health) + rnorm(n_people, mean = 0, sd = 15),\n         # Scale net score down to 0.05 to 0.95 to create a probability of using a net\n         net_probability = rescale(net_score, to = c(0.05, 0.95)),\n         # Randomly generate a 0/1 variable using that probability\n         net = rbinom(n_people, 1, net_probability)) %&gt;% \n  # Finally generate a malaria risk variable based on income, health, net use,\n  # and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100,\n         # Risk goes down by 10 when using a net. Because we rescale things,\n         # though, we have to make the effect a lot bigger here so it scales\n         # down to -10. Risk also decreases as health and income go up. I played\n         # with these numbers until they created reasonable coefficients.\n         malaria_effect = (-30 * net) + (-1.9 * health) + (-0.1 * income),\n         # Make the final malaria risk score and add some noise\n         malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3),\n         # Rescale so it doesn't go below 0,\n         malaria_risk = rescale(malaria_risk, to = c(5, 70))) %&gt;% \n  select(-c(health_base, health_income_effect, net_score, net_probability, \n            malaria_risk_base, malaria_effect))\n\nhead(net_data)\n## # A tibble: 6 × 5\n##      id income health   net malaria_risk\n##   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;\n## 1     1   409.   63.1     0         45.1\n## 2     2   521.   83.5     1         23.4\n## 3     3   581.   73.0     0         36.5\n## 4     4   324.   60.6     0         58.7\n## 5     5   532.   73.4     1         32.7\n## 6     6   538.   42.6     0         52.5\n\nIPW manually, binary treatment\nIf we just look at the effect of nets on malaria risk without any statistical adjustment, we see that nets cause a decrease of 13 points in malaria risk. This is wrong though because there’s confounding.\n\n# Wrong correlation-is-not-causation effect\nmodel_net_naive &lt;- lm(malaria_risk ~ net, data = net_data)\ntidy(model_net_naive)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)     41.9     0.413     102.  0        \n## 2 net            -13.6     0.572     -23.7 2.90e-101\n\nAccording to do-calculus logic, we need to adjust for both income and health:\n\nadjustmentSets(mosquito_dag)\n## { hlth, inc }\n\nWe’ll do that with inverse probability weighting. First we’ll use the health and income confounders to predict the treatment, or net use, and then we’ll generate propensity scores. We’ll then use those propensity scores to generate inverse probability weights following this formula:\n\\[\n\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}\n\\]\nThis formula will calculate weights for the average treatment effect (ATE). Lucy D’Agostino McGowan has formulas for a bunch of different IPWs, including the average treatment on the treated (ATT), average treatment among the controls (ATC), and other effects.\nHere’s how we do that with R:\n\n# Logit model to predict net use\nmodel_predict_net &lt;- glm(net ~ income + health,\n                         family = binomial(link = \"logit\"),\n                         data = net_data)\n\n# Generate propensity scores and IPWs\nnet_data_ipw &lt;- augment_columns(model_predict_net, net_data,\n                                type.predict = \"response\") %&gt;% \n  rename(propensity = .fitted) %&gt;% \n  mutate(ipw = (net / propensity) + ((1 - net) / (1 - propensity)))\n\nnet_data_ipw %&gt;% \n  select(id, income, health, net, malaria_risk, propensity, ipw) %&gt;% \n  head()\n## # A tibble: 6 × 7\n##      id income health   net malaria_risk propensity   ipw\n##   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n## 1     1   409.   63.1     0         45.1      0.380  1.61\n## 2     2   521.   83.5     1         23.4      0.628  1.59\n## 3     3   581.   73.0     0         36.5      0.659  2.93\n## 4     4   324.   60.6     0         58.7      0.266  1.36\n## 5     5   532.   73.4     1         32.7      0.597  1.68\n## 6     6   538.   42.6     0         52.5      0.459  1.85\n\nFinally we’ll use those weights in a regression model to find the ATE. After adjusting for confounding and closing the backdoor paths opened by income and health, the effect of nets is -10.5, which is more accurate than the naive estimate we found before. Yay!\n\nmodel_net_ipw &lt;- lm(malaria_risk ~ net, data = net_data_ipw, weights = ipw)\ntidy(model_net_ipw)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)     40.4     0.409      98.8 0       \n## 2 net            -10.5     0.578     -18.3 1.83e-65\n\nIPW with the ipw package, binary treatment\nInstead of running a logistic regression model and generating propensity scores by hand, we can use the ipw package to generate that ipw column automatically. Specify the confounders in the denominator argument. There’s a numerator argument too that we can use for generating stabilized weights, but we’ll skip that for now.\n\n# ipwpoint() can't handle tibbles! Force net_data to be a data.frame\nweights_ipwpoint &lt;- ipwpoint(\n  exposure = net,\n  family = \"binomial\",  # The treatment is binary\n  link = \"logit\",\n  denominator = ~ income + health,\n  data = as.data.frame(net_data)\n)\n\n# They're the same!\nhead(weights_ipwpoint$ipw.weights)\n## [1] 1.61 1.59 2.93 1.36 1.68 1.85\nhead(net_data_ipw$ipw)\n## [1] 1.61 1.59 2.93 1.36 1.68 1.85\n\nThe resulting weights object here is a standalone object, and you can do other things with it like summary(). We can add the weights back into the main data and then fit the final model (technically we don’t need to—we could just say weights = weights_ipwpoint$ipw.weights and it would work just fine, but I don’t like working with standalone vectors and prefer to have them be columns, just so everything is all together in one place).\nWe get the same ATE of -10.5.\n\nnet_data_ipwpoint &lt;- net_data %&gt;% \n  mutate(ipw = weights_ipwpoint$ipw.weights)\n\nmodel_net_ipwpoint &lt;- lm(malaria_risk ~ net, \n                         data = net_data_ipwpoint, weights = ipw)\ntidy(model_net_ipwpoint)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)     40.4     0.409      98.8 0       \n## 2 net            -10.5     0.578     -18.3 1.83e-65\n\nIPW with the WeightIt package, binary treatment\nWe can also use the WeightIt package to generate weights. It has slightly different syntax and can find all sorts of different estimands beyond the ATE (like most of the ones Lucy has listed). It can also handle a bunch of different methods beyond propensity scores. WeightIt can also handle tibbles, which is nice. It also provides a bunch of other summary information (if you use summary()), like effective sample sizes (ESS) in the treated/untreated groups and covariate balance.\n\nweights_weightit &lt;- weightit(net ~ income + health,  # Model net use with confounders\n                             data = net_data, \n                             estimand = \"ATE\",  # Find the ATE\n                             method = \"ps\")  # Build weights with propensity scores\nweights_weightit\n## A weightit object\n##  - method: \"glm\" (propensity score weighting with GLM)\n##  - number of obs.: 1138\n##  - sampling weights: none\n##  - treatment: 2-category\n##  - estimand: ATE\n##  - covariates: income, health\n\n# See even more details here\n# summary(weights_weightit)\n\n# Same as the other methods!\nhead(weights_weightit$weights)\n## [1] 1.61 1.59 2.93 1.36 1.68 1.85\n\nAs with ipw, we can add the weights to the dataset and run the model to find the same -10.5 ATE:\n\nnet_data_weightit &lt;- net_data %&gt;% \n  mutate(ipw = weights_weightit$weights)\n\nmodel_net_weightit &lt;- lm(malaria_risk ~ net, \n                         data = net_data_weightit, weights = ipw)\ntidy(model_net_weightit)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)     40.4     0.409      98.8 0       \n## 2 net            -10.5     0.578     -18.3 1.83e-65"
  },
  {
    "objectID": "blog/2020/12/01/ipw-binary-continuous/index.html#continuous-treatments",
    "href": "blog/2020/12/01/ipw-binary-continuous/index.html#continuous-treatments",
    "title": "Generating inverse probability weights for both binary and continuous treatments",
    "section": "Continuous treatments",
    "text": "Continuous treatments\nExample data\nInverse probability weights work with continuous treatment variables too, but the math is a little lot trickier. For this example, we’ll generate a DAG for a hypothetical program where poorer families are given cash grants that they can spend on malaria prevention supplies, like mosquito nets, chemical treatments, and medication. It’s a voluntary program—people self select into it, and we’ll assume that people with lower health scores and lower income will sign up. The amount of the grant depends on income.\nThe treatment here is continuous: people get different amounts of anti-malaria grant money. For the sake of simplicity here, everyone gets some grant money. I’m not even going to try multilevel zero-inflated models or anything (though those are cool!).\nThe DAG looks the same as before (since we’re trying to keep things super simple here):\n\ngrant_dag &lt;- dagify(mal ~ grant + inc + hlth,\n                    grant ~ inc + hlth,\n                    hlth ~ inc,\n                    coords = list(x = c(mal = 4, grant = 1, inc = 2, hlth = 3),\n                                  y = c(mal = 1, grant = 1, inc = 2, hlth = 2)),\n                    exposure = \"grant\",\n                    outcome = \"mal\")\n\nggdag_status(grant_dag) +\n  guides(color = \"none\") +\n  theme_dag()\n\n\n\n\n\n\n\nWe’ll measure these nodes like so:\n\n\nMalaria risk: scale from 0–100, mostly around 40, but ranging from 10ish to 80ish. Best to use a Beta distribution.\n\nGrant: amount between 5 and 40, centered around 20ish.\n\nIncome: weekly income, measured in dollars, mostly around 500 ± 300. Best to use a normal distribution.\n\nHealth: scale from 0–100, mostly around 70, but ranging from 50ish to 100. Best to use a Beta distribution.\n\n\n# Make this randomness consistent\nset.seed(1234)\n\n# Simulate 1504 people\nn_people &lt;- 1504\n\ngrant_data &lt;- tibble(\n  # Make an ID column (not necessary, but nice to have)\n  id = 1:n_people,\n  # Generate income variable: normal, 500 ± 300\n  income = rnorm(n_people, mean = 500, sd = 75)\n) %&gt;%\n  # Generate health variable: beta, centered around 70ish\n  mutate(health_base = rbeta(n_people, shape1 = 7, shape2 = 4) * 100,\n         # Health increases by 0.02 for every dollar in income\n         health_income_effect = income * 0.02,\n         # Make the final health score and add some noise\n         health = health_base + health_income_effect + rnorm(n_people, mean = 0, sd = 3),\n         # Rescale so it doesn't go above 100\n         health = rescale(health, to = c(min(health), 100))) %&gt;% \n  # Generate grant variable\n  mutate(grant_base = rtruncnorm(n_people, mean = 18, sd = 10, a = 5, b = 40),\n         # Grants are higher for people with lower incomes; higher for people with lower health\n         grant_effect = (income * -0.25) + (health * -0.5),\n         # Make the final grant amount + noise + rescale it back down\n         grant = grant_base + grant_effect + rnorm(n_people, mean = 0, sd = 8),\n         grant = round(rescale(grant, to = c(5, 40)), 0)) %&gt;% \n  # Finally generate a malaria risk variable based on income, health, grant amount,\n  # and random noise\n  mutate(malaria_risk_base = rbeta(n_people, shape1 = 4, shape2 = 5) * 100,\n         # Risk goes down as grant money goes up. I played with these numbers\n         # until they created reasonable coefficients.\n         malaria_effect = (-40 * grant) + (-25 * health) + (-0.05 * income),\n         # Make the final malaria risk score and add some noise\n         malaria_risk = malaria_risk_base + malaria_effect + rnorm(n_people, 0, sd = 3),\n         # Rescale so it doesn't go below 0,\n         malaria_risk = rescale(malaria_risk, to = c(5, 70))) %&gt;% \n  select(-c(health_base, health_income_effect, grant_base, grant_effect, \n            malaria_risk_base, malaria_effect))\n\nhead(grant_data)\n## # A tibble: 6 × 5\n##      id income health grant malaria_risk\n##   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n## 1     1   409.   70.2    27         26.3\n## 2     2   521.   75.3    18         33.3\n## 3     3   581.   62.6    20         42.3\n## 4     4   324.   83.9    28         11.8\n## 5     5   532.   74.3    20         31.8\n## 6     6   538.   75.9    15         36.2\n\nIPW manually, continuous treatment\nIf we just look at the effect of grants on malaria risk without any adjustment, every extra grant dollar causes a drop of 0.4 malaria risk points. Once again, though, this is wrong because of confounding.\n\n# Wrong correlation-is-not-causation effect\nmodel_grant_naive &lt;- lm(malaria_risk ~ grant, data = grant_data)\ntidy(model_grant_naive)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   44.2      1.35       32.8  2.56e-178\n## 2 grant         -0.417    0.0615     -6.78 1.69e- 11\n\nAccording to do-calculus logic, we again need to adjust for both income and health:\n\nadjustmentSets(grant_dag)\n## { hlth, inc }\n\nHere’s where the math gets tricky. When we worked with a binary treatment, we calculated the propensity score for each observation and then used this formula to generate inverse probability weights:\n\\[\n\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}\n\\]\nWe can’t do that with continuous treatment variables, though, since we don’t really have propensity scores. Instead, we use this hairy-but-not-too-scary formula (from Naimi et al. (2014); ungated version here; also see this for another R example):\n\\[\n\\text{IPW} = \\frac{f_X (X; \\mu_1, \\sigma^2_1)}{f_{X | C} (X | C = c; \\mu_2, \\sigma^2_2)}\n\\]\nPhew. That’s a lot of math, but it’s not too bad if we take it apart:\n\n\n\\(X\\) stands for the continuous exposure or treatment variable\n\n\\(C\\) stands for the confounders\nThe \\(f_\\cdot (\\cdot)\\) function in both the numerator and denominator stands for a probability density function with a mean of \\(\\mu\\) and a variance of \\(\\sigma^2\\)\n\nThe numerator \\(f_X (X; \\mu_1, \\sigma^2_1)\\) refers to the probability distribution of just the treatment variable (technically you could just use 1 as the numerator, but that can lead to unstable weights—using the probability distribution of the treatment helps stabilize the weights)\nThe denominator \\(f_{X | C} (X | C = c; \\mu_2, \\sigma^2_2)\\) refers to the probability distribution of the treatment variable explained by the confounders\n\n(Fun fact: I’m like 85% sure that the \\(\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}\\) formula is just an algebraically rearranged and simplified version of this fancier equation)\nWe can calculate each element of this fraction and then generate the inverse probability weights. Here’s how to do that with R:\n\n# The numerator is the probability distribution of just the treatment variable.\n# We'll use a normal distribution for it (hence dnorm()). We need to feed\n# dnorm() the grant amount for each person, the predicted value from a simple\n# grant ~ 1 model, and the sd of the residuals from that model\nmodel_num &lt;- lm(grant ~ 1, data = grant_data)\nnum &lt;- dnorm(grant_data$grant,\n             predict(model_num),\n             sd(model_num$residuals))\n\n# The denominator is the probability distribution of the treatment variable\n# explained by the confounders. We'll again use a normal distribution for it.\n# We'll feed dnorm() the grant amount, the predicted value from a model that\n# includes the confounders, and the sd of the residuals from that model\nmodel_den &lt;- lm(grant ~ health + income, data = grant_data)\nden &lt;- dnorm(grant_data$grant,\n             predict(model_den),\n             sd(model_den$residuals))\n\n# Finally, we make actual IPW weights by building the fraction\ngrant_data_ipw &lt;- grant_data %&gt;% \n  mutate(ipw = num / den)\n\nhead(grant_data_ipw)\n## # A tibble: 6 × 6\n##      id income health grant malaria_risk   ipw\n##   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n## 1     1   409.   70.2    27         26.3 0.288\n## 2     2   521.   75.3    18         33.3 0.492\n## 3     3   581.   62.6    20         42.3 0.687\n## 4     4   324.   83.9    28         11.8 0.177\n## 5     5   532.   74.3    20         31.8 0.499\n## 6     6   538.   75.9    15         36.2 0.748\n\nNow we can use the weights to find the ATE just like we did with the binary treatment:\n\nmodel_grant_ipw &lt;- lm(malaria_risk ~ grant, data = grant_data_ipw, weights = ipw)\ntidy(model_grant_ipw)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    58.4     1.79        32.6 1.95e-176\n## 2 grant          -1.11    0.0806     -13.7 1.45e- 40\n\nEach dollar of grant money thus causes a drop of -1.1 malaria risk points. Neato!\nIPW with the ipw package, continuous treatment\nManually creating the numerator and denominator can get tedious though. We can use the ipwpoint() function from ipw to generate continuous weights in one step. Instead of specifying a binomial treatment like we did before, we’ll use a Gaussian (normal) family. We also specify both the numerator and denominator. It will generate identical weights.\n\nweights_continuous_ipwpoint &lt;- ipwpoint(\n  exposure = grant,\n  family = \"gaussian\",\n  numerator = ~ 1,\n  denominator = ~ health + income,\n  data = as.data.frame(grant_data)\n)\n\n# Same values!\nhead(grant_data_ipw$ipw)\n## [1] 0.288 0.492 0.687 0.177 0.499 0.748\nhead(weights_continuous_ipwpoint$ipw.weights)\n## [1] 0.288 0.492 0.687 0.177 0.499 0.748\n\nWe can then put those weights into the dataset and run a model with them. We get the same ATE of -1.1:\n\ngrant_data_ipwpoint &lt;- grant_data %&gt;% \n  mutate(ipw = weights_continuous_ipwpoint$ipw.weights)\n\nmodel_grant_ipwpoint &lt;- lm(malaria_risk ~ grant, \n                           data = grant_data_ipwpoint, weights = ipw)\ntidy(model_grant_ipwpoint)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    58.4     1.79        32.6 1.95e-176\n## 2 grant          -1.11    0.0806     -13.7 1.45e- 40\n\nIPW with the WeightIt package, continuous treatment\nThe WeightIt package also handles continuous weights. The syntax is a lot simpler—there’s no need to worry about numerators and denominators.\n\nweights_weightit &lt;- weightit(grant ~ income + health,  # Model grant amount with confounders\n                             data = grant_data, \n                             stabilize = TRUE)\nweights_weightit\n## A weightit object\n##  - method: \"glm\" (propensity score weighting with GLM)\n##  - number of obs.: 1504\n##  - sampling weights: none\n##  - treatment: continuous\n##  - covariates: income, health\n\n# See even more details here\n# summary(weights_weightit)\n\n# Not the same as the other methods :(\nhead(weights_weightit$weights)\n## [1] 0.580 0.990 1.384 0.356 1.005 1.506\n\nHowever(!), for mathy reasons I don’t understand, the weights it generates are not the same as what we get when doing it by hand or with ipwpoint(). In fact, they’re almost exactly twice as large as the manual and ipwpoint() weights:\n\n# Manual weights\nhead(grant_data_ipw$ipw)\n## [1] 0.288 0.492 0.687 0.177 0.499 0.748\n\n# weightit() weights / 2\nhead(weights_weightit$weights) / 2\n## [1] 0.290 0.495 0.692 0.178 0.502 0.753\n\nSurely there’s an argument to weightit() that I’m missing somewhere.\nRegardless, for more mathy reasons I don’t understand, the ATE is identical even though the weights are roughly doubled:\n\ngrant_data_weightit &lt;- grant_data %&gt;% \n  mutate(ipw = weights_weightit$weights)\n\nmodel_grant_weightit &lt;- lm(malaria_risk ~ grant, \n                           data = grant_data_weightit, weights = ipw)\ntidy(model_grant_weightit)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    58.4     1.79        32.6 1.95e-176\n## 2 grant          -1.11    0.0806     -13.7 1.45e- 40"
  },
  {
    "objectID": "blog/2021/01/15/msm-gee-multilevel/index.html",
    "href": "blog/2021/01/15/msm-gee-multilevel/index.html",
    "title": "Marginal structural models for panel data with GEE and multilevel models",
    "section": "",
    "text": "Since my last two blog posts on binary and continuous inverse probability weights (IPWs) and marginal structural models (MSMs) for time-series cross-sectional (TSCS) panel data, I’ve spent a ton of time trying to figure out why I couldn’t recover the exact causal effect I had built in to those examples when using panel data. It was a mystery, and it took weeks to figure out what was happening.\nAfter poring through all sorts of articles on MSMs and TSCS data like Thoemmes and Ong (2016) and Blackwell and Glynn (2018), along with all sorts of articles on the differences between generalized estimating equations (GEEs), which the epidemiology world (and everyone who does MSMs) seems to love, and multilevel models (which I find a lot more intuitive, and which can be done Bayesianly with brms), I finally figured out what was wrong and how to calculate correct IPWs for panel data.\nThe main issue lies in the synthetic data I had created for those earlier blog posts. There, I used the fabricatr package to create a country-year panel, which seemed correct and great. However, it didn’t exactly match the data-generating process in situations where the value in one time period (like \\(X_t\\)) depends on the value from the previous time period (or \\(X_{t-1}\\)), or autocorrelation. As such, I couldn’t quite get the correct causal effects out (since the data didn’t show interdepdency across time).\nAfter lots of struggle, though, I finally figured out a way to explicitly build this autocorrelation in. And thanks to this delightfully accessible article by Thoemmes and Ong (2016) (ungated free version here), I found clear code for MSMs that I could expand to test on more complex panel data.\nIn this post, I’ll do a few things: (1) recreate the two-time-period example from Thoemmes and Ong’s appendix (which is stuck in a PDF and really hard to copy/paste out), (2) redo their two-period example with a tidier approach, and (3) expand their two-period approach to multiple years and replace GEEs with multilevel models.\nHere we go!\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(ipw)  # For automatically calculating IPWs\nlibrary(geepack)  # For GEE models\nlibrary(lme4)  # For mixed/multilevel models\nlibrary(broom.mixed)\nlibrary(kableExtra)"
  },
  {
    "objectID": "blog/2021/01/15/msm-gee-multilevel/index.html#synthetic-data-overview",
    "href": "blog/2021/01/15/msm-gee-multilevel/index.html#synthetic-data-overview",
    "title": "Marginal structural models for panel data with GEE and multilevel models",
    "section": "Synthetic data overview",
    "text": "Synthetic data overview\nIn their paper, Thoemmes and Ong create simulated data based on this DAG, with a time-varying treatment, a non-time-varying confounder, and a time-varying outcome (depression):\n\nCode# I generally prefer the easier-to-read formula syntax in dagify() (i.e. D1 ~ T1\n# + C, etc.), but it doesn't work with subscripted numbers like T[1], so we have\n# to use this dagitty syntax instead\ndepression_dag &lt;- dagitty('dag {\n\"C\" [pos=\"2.5,2\"]\n\"T[1]\" [pos=\"1,1\"]\n\"D[1]\" [pos=\"2,1\"]\n\"T[2]\" [pos=\"3,1\"]\n\"D[2]\" [pos=\"4,1\"]\n\"C\" -&gt; \"D[1]\"\n\"C\" -&gt; \"D[2]\"\n\"C\" -&gt; \"T[1]\"\n\"C\" -&gt; \"T[2]\"\n\"D[1]\" -&gt; \"T[2]\"\n\"T[1]\" -&gt; \"D[1]\"\n\"T[2]\" -&gt; \"D[2]\"\n}') %&gt;% \n  tidy_dagitty()\n\nggplot(depression_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(color = \"grey80\", size = 12) +\n  geom_dag_text(color = \"black\", size = 5, parse = TRUE) +\n  theme_dag()\n\n\n\n\n\n\n\nEssentially earlier treatment (\\(T_1\\)) causes some level of depression (\\(D_1\\)), which causes later treatment (\\(T_2\\)), which causes later depression (\\(D_2\\)). This whole chain is influenced by some non-varying confounder (\\(C\\))."
  },
  {
    "objectID": "blog/2021/01/15/msm-gee-multilevel/index.html#original-two-period-thoemmes-and-ong-results",
    "href": "blog/2021/01/15/msm-gee-multilevel/index.html#original-two-period-thoemmes-and-ong-results",
    "title": "Marginal structural models for panel data with GEE and multilevel models",
    "section": "Original two-period Thoemmes and Ong results",
    "text": "Original two-period Thoemmes and Ong results\nHere’s the original code from the appendix of Thoemmes and Ong (2016) for data with a time-varying continuous treatment and a confounder. It explicitly creates columns for t1, d1, t2, and d2, so it is easy to mathematically make it so that t2 is caused by d1.\nBecause it’s only two time periods, they calculate the inverse probability weights in two formulas and then multiply them together:\n\n\nWeights for the first time period:\n\\[\n  w_1 = \\frac{\\phi(T_{i1})}{\\phi(T_{i1}\\ |\\ C)}\n  \\]\n\n\nWeights for the second time period:\n\\[\n  w_2 = \\frac{\\phi(T_{i2}\\ |\\ T_{i1})}{\\phi(T_{i2}\\ |\\ C, T_{i1}, D_{i1})}\n  \\]\n\n\nFinal weights:\n\\[\n  w = w_1 \\cdot w_2\n  \\]\n\n\nHere’s their code:\n\nCode##################################################################### \n# iptw demo with time-varying continuous treatment and confounder\n#####################################################################\n# Felix Thoemmes, October, 2015 \n#####################################################################\n\n#set seed to replicate results\nset.seed(12345)\n#define sample size\nn &lt;- 2000\n#define confounder c\nc &lt;- rnorm(n,0,1)\n#define treatment at time 1 as function of confounder\nt1 &lt;- .1*c + rnorm(n,0, sqrt(.99))\n#define depression at time 1 as function of confounder and treat1\nd1 &lt;- .1*c + .4*t1 + rnorm(n,0, sqrt(.822))\n#define treatment at time 2 as function of confounder and dep1\nt2 &lt;- .1*c + .4*d1 + .4*t1 + rnorm(n,0, sqrt(.5196))\n#define outcome depression at time 2 as function of confounder, treat1, and dep1 \nd2 &lt;- .1*c + .4*t2 + .4*d1 + rnorm(n,0, sqrt(.4582))\n#add ID variable to do mixed effects models later\nid &lt;- rep(1:length(c))\n\ndf1 &lt;- data.frame(id, c, t1, d1, t2, d2)\n\n#compute the weights for timepoint 1\n#this is a continuous treatment\n#therefore we use densities of normal distributions\n#weights at time 1\nw1 &lt;- dnorm(df1$t1, predict(lm(t1 ~ 1)), sd(lm(t1 ~ 1)$residuals)) / \n  dnorm(df1$t1, predict(lm(t1 ~ c)), sd(lm(t1 ~ c)$residuals))\n\n#weights at time 2\nw2 &lt;- dnorm(df1$t2, predict(lm(t2 ~ t1)), sd(lm(t2 ~ t1)$residuals)) / \n  dnorm(df1$t2, predict(lm(t2 ~ c + d1 + t1)), sd(lm(t2 ~ c + d1 + t1)$residuals)) \n\n#total weights are a product of all time-varying weights\nw &lt;- w1*w2\n\n#truncate weights at 5%\ntw1 &lt;- ifelse(w &lt; quantile(w, probs = .05), quantile(w, probs = 0.05), w)\ntw1 &lt;- ifelse(w &gt; quantile(w, probs = .95), quantile(w, probs = 0.95), tw1)\n\n#truncate weights at 1%\ntw2 &lt;- ifelse(w &lt; quantile(w, probs = .01), quantile(w, probs = 0.01), w)\ntw2 &lt;- ifelse(w &gt; quantile(w, probs = .99), quantile(w, probs = 0.99), tw2)\n\n\nThey they run a bunch of different outcome models with geeglm():\n\nonly_t1 &lt;- geeglm(d2 ~ t1, data = df1, id = rownames(df1))\nonly_t2 &lt;- geeglm(d2 ~ t2, data = df1, id = rownames(df1))\nboth_t &lt;- geeglm(d2 ~ t1 + t2, data = df1, id = rownames(df1))\nboth_t_c &lt;- geeglm(d2 ~ t1 + t2 + c + d1, data = df1, id = rownames(df1))\n\nstab_ipw &lt;- geeglm(d2 ~ t1 + t2, data = df1, id = rownames(df1), weights = w)\nstab_ipw_1p &lt;- geeglm(d2 ~ t1 + t2, data = df1, id = rownames(df1), weights = tw2)\nstab_ipw_5p &lt;- geeglm(d2 ~ t1 + t2, data = df1, id = rownames(df1), weights = tw1)\n\nresults &lt;- tribble(\n  ~`Method`, ~`T&lt;sub&gt;1&lt;/sub&gt; effect`, ~`T&lt;sub&gt;2&lt;/sub&gt; effect`,\n  \"True effect\", 0.160, 0.400,\n  \"Naive, only T1\", filter(tidy(only_t1), term == \"t1\")$estimate, NA,\n  \"Naive, only T2\", NA, filter(tidy(only_t2), term == \"t2\")$estimate,\n  \"Naive, both\", filter(tidy(both_t), term == \"t1\")$estimate, filter(tidy(both_t), term == \"t2\")$estimate,\n  \"Naive, both + controls\", filter(tidy(both_t_c), term == \"t1\")$estimate, filter(tidy(both_t_c), term == \"t2\")$estimate,\n  \"IPW\", filter(tidy(stab_ipw), term == \"t1\")$estimate, filter(tidy(stab_ipw), term == \"t2\")$estimate,\n  \"IPW, 1% truncated\", filter(tidy(stab_ipw_1p), term == \"t1\")$estimate, filter(tidy(stab_ipw_1p), term == \"t2\")$estimate,\n  \"IPW, 5% truncated\", filter(tidy(stab_ipw_5p), term == \"t1\")$estimate, filter(tidy(stab_ipw_5p), term == \"t2\")$estimate\n)\n\n\n\n\n\nMethod\nT1 effect\nT2 effect\n\n\n\nTrue effect\n0.160\n0.400\n\n\nNaive, only T1\n0.423\n—\n\n\nNaive, only T2\n—\n0.649\n\n\nNaive, both\n0.048\n0.623\n\n\nNaive, both + controls\n0.011\n0.399\n\n\nIPW\n0.157\n0.421\n\n\nIPW, 1% truncated\n0.154\n0.434\n\n\nIPW, 5% truncated\n0.131\n0.480\n\n\n\n\n\nAnd here’s all that compared to what they have in the paper:\n\n\n\n\n\n\n\n\nIt’s identical! I’m chalking any tiny differences up to the fact that set.seed() changed with R 4.0 and they used R 3.x in their paper. (Also the T2-only model is wrong, but that’s probably a typo—the “estimated scale parameter” from that model is 0.574 with an sd of 0.018, which lines up perfectly with the coefficient in the published table, so I think maybe the published paper has the wrong number in the table.)\nAlso, there seems to be another typo in that table. The true effect of T1 in the table is 0.140, but in the text of the paper, it says the effect should be 0.160, which makes sense—that’s the product of 0.4 × 0.4, or each of the treatment coefficients (\\(0.4 \\times 0.4 = 0.16\\)).\nRegardless of those super tiny insignificant typos, we did it! We have their exact results using marginal structural models and inverse probability weights."
  },
  {
    "objectID": "blog/2021/01/15/msm-gee-multilevel/index.html#tidier-two-period-results",
    "href": "blog/2021/01/15/msm-gee-multilevel/index.html#tidier-two-period-results",
    "title": "Marginal structural models for panel data with GEE and multilevel models",
    "section": "Tidier two-period results",
    "text": "Tidier two-period results\nThe code to generate that data isn’t very tidyverse-friendly and it creates a ton of intermediate vectors. Here’s a cleaner version with dplyr:\n\nset.seed(12345)\nn &lt;- 2000\n\ndf_nice &lt;- tibble(id = 1:n,\n                  c = rnorm(n, 0, 1)) %&gt;% \n  mutate(t1 = (0.1 * c) + rnorm(n, 0, sqrt(0.99)),\n         d1 = (0.1 * c) + (0.4 * t1) + rnorm(n, 0, sqrt(0.822)),\n         t2 = (0.1 * c) + (0.4 * d1) + (0.4 * t1) + rnorm(n, 0, sqrt(0.5196)),\n         d2 = (0.1 * c) + (0.4 * t2) + (0.4 * d1) + rnorm(n, 0, sqrt(0.4582)))\n\nAlso, this original data is wide, with explicit columns for each time period. Let’s make it tidy and pretend the time periods are years (y) and add some lagged columns:\n\ndf_tidy &lt;- df_nice %&gt;% \n  pivot_longer(cols = c(t1, d1, t2, d2)) %&gt;% \n  separate(name, into = c(\"variable\", \"y\"), sep = 1) %&gt;% \n  pivot_wider(names_from = \"variable\", values_from = \"value\") %&gt;% \n  group_by(id) %&gt;% \n  mutate(across(c(t, d), list(lag = lag))) %&gt;% \n  ungroup()\nhead(df_tidy)\n## # A tibble: 6 × 7\n##      id      c y           t       d  t_lag   d_lag\n##   &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n## 1     1  0.586 1     -0.546  -1.11   NA     NA     \n## 2     1  0.586 2     -0.853  -1.73   -0.546 -1.11  \n## 3     2  0.709 1      1.14    0.361  NA     NA     \n## 4     2  0.709 2     -0.0673 -1.30    1.14   0.361 \n## 5     3 -0.109 1     -0.584  -0.0822 NA     NA     \n## 6     3 -0.109 2     -0.932   0.535  -0.584 -0.0822\n\nThis is how panel data is typically structured, with rows repeated for each time period. This is good because we can figure out how to structure the weight models and outcome model in a way that uses this structure and compare it to the original t1, d1, etc. models.\nBecause there are only two time periods, the lagged columns here are missing a ton of data (since you can’t see what the value is for year 0), but it’ll still work.\nWith the data like this, we can find the weights. We’ll do it both manually and with the ipw package.\nManually, we need to fit two models (a numerator and denominator) and then take the cumulative product of their probability distributions:\n\\[\nw = \\prod^t_{t = 1} \\frac{\\phi(T_{it} | T_{i, t-1}, C_i)}{\\phi(T_{it} | T_{i, t-1}, D_{i, t-1}, C_i)}\n\\]\nHere we go!\n\n# Remove NAs since ipwpoint() will complain\ndf_tidy_sans_na &lt;- df_tidy %&gt;%\n  filter(!is.na(t_lag))\n\n# Manually\n# Numerator is lagged treatment + non-varying confounders\nmodel_num &lt;- lm(t ~ t_lag + c,\n                data = df_tidy_sans_na)\n\n# Denominator is lagged treatment, lagged outcome, time-varying confounders +\n# non-varying confounders\nmodel_denom &lt;- lm(t ~ t_lag + d_lag + c, \n                  data = df_tidy_sans_na)\n\n# Probability distributions\nnum &lt;- dnorm(df_tidy_sans_na$t,\n             predict(model_num),\n             sd(residuals(model_num)))\n\nden &lt;- dnorm(df_tidy_sans_na$t,\n             predict(model_denom),\n             sd(residuals(model_denom)))\n\n# Make num/den fraction and find cumulative product within each id\ndf_weights_manual &lt;- df_tidy_sans_na %&gt;% \n  mutate(weights_no_time = num / den) %&gt;% \n  group_by(id) %&gt;% \n  mutate(ipw = cumprod(weights_no_time)) %&gt;% \n  ungroup()\n\n# Automatically find weights with ipw::ipwpoint()\ntidy_weights_auto &lt;- ipwpoint(\n  exposure = t,\n  family = \"gaussian\", \n  numerator = ~ t_lag + c, \n  denominator = ~ t_lag + d_lag + c, \n  data = as.data.frame(df_tidy_sans_na))\n\ndf_weights_auto &lt;- df_tidy_sans_na %&gt;% \n  mutate(ipw = tidy_weights_auto$ipw.weights)\n\nNow we can use these new weights in the outcome model:\n\nmodel_lags_manual &lt;- geeglm(d ~ t + t_lag, data = df_weights_manual,\n                            id = id, weights = ipw)\n\nmodel_lags_auto &lt;- geeglm(d ~ t + t_lag, data = df_weights_auto, \n                          id = id, weights = ipw)\n\nresults &lt;- tribble(\n  ~`Method`, ~`T&lt;sub&gt;1&lt;/sub&gt; effect`, ~`T&lt;sub&gt;2&lt;/sub&gt; effect`,\n  \"True effect\", 0.160, 0.400,\n  \"IPW, original\", filter(tidy(stab_ipw), term == \"t1\")$estimate, filter(tidy(stab_ipw), term == \"t2\")$estimate,\n  \"IPW, lagged, manual\", filter(tidy(model_lags_manual), term == \"t_lag\")$estimate, filter(tidy(model_lags_manual), term == \"t\")$estimate,\n  \"IPW, lagged, automatic\", filter(tidy(model_lags_auto), term == \"t_lag\")$estimate, filter(tidy(model_lags_auto), term == \"t\")$estimate\n) %&gt;% \n  mutate(across(2:3, ~sprintf(\"%.3f\", round(., 3)))) %&gt;% \n  add_row(`T&lt;sub&gt;1&lt;/sub&gt; effect` = \"&lt;b&gt;lag(T) effect&lt;/b&gt;\", \n          `T&lt;sub&gt;2&lt;/sub&gt; effect` = \"&lt;b&gt;T effect&lt;/b&gt;\", \n          .after = 2)\n\n\n\n\n\nMethod\nT1 effect\nT2 effect\n\n\n\nTrue effect\n0.160\n0.400\n\n\nIPW, original\n0.157\n0.421\n\n\n—\nlag(T) effect\nT effect\n\n\nIPW, lagged, manual\n0.150\n0.441\n\n\nIPW, lagged, automatic\n0.150\n0.441\n\n\n\n\n\nThose results aren’t identical (I don’t know why!), but they’re close-ish, so whatever. It worked! Instead of using explicit t1, d1, etc. columns, we can do the same marginal structural model with just t and d in a long data frame."
  },
  {
    "objectID": "blog/2021/01/15/msm-gee-multilevel/index.html#expansion-to-multiple-period-data",
    "href": "blog/2021/01/15/msm-gee-multilevel/index.html#expansion-to-multiple-period-data",
    "title": "Marginal structural models for panel data with GEE and multilevel models",
    "section": "Expansion to multiple-period data",
    "text": "Expansion to multiple-period data\nSince that’s all working, now we can do something a little/lot trickier—instead of manually specifying \\(T_1\\) and \\(T_2\\), which isn’t really easily scalable to more time periods, we’ll construct the data more generally for any number of years\nTricky interdependent data generating process\nThe trick here is that we want to have any number of time periods, but also maintain the same DAG relationships. In the small data, these are the two general relationships:\n\n\\(\\text{treatment}_t = (0.1 \\cdot \\text{confounder}) + (0.4 \\cdot \\text{depression}_{t-1}) + (0.4 \\cdot \\text{treatment}_{t-1}) + \\text{noise}\\)\n\\(\\text{depression}_t = (0.1 \\cdot \\text{confounder}) + (0.4 \\cdot \\text{treatment}_{t}) + (0.4 \\cdot \\text{depression}_{t-1}) + \\text{noise}\\)\n\nOr in code:\n\nt = (0.1 * c) + (0.4 * lag(d)) + (0.4 * lag(t)) + rnorm(n, 0, sqrt(0.5196))\nd = (0.1 * c) + (0.4 * t) + (0.4 * lag(d)) + rnorm(n, 0, sqrt(0.4582))\n\nGenerating this data in a tidy dplyr way is super tricky, since the t and d columns are interdependent—the data has to be generated rowwise, but we also have to be able to look back a row to get the lagged values of t and d. I’ve seen people like Blackwell and Glynn (2018) get around this by using a for loop to build the data, but I have an aversion to for loops in R since purrr exists, so I spent way too much time trying to figure out a purrr-based way to do this. Others have tried this (like this RStudio Community post), and I asked about it on Twitter, where I got a few cool solutions using purrr::accumulate() and purrr::reduce(), like this fancy accumutate() function from Ian Moran and these solutions from Miles McBain.\nThere are no loops involved in those solutions, but phew that accumulate/reduce syntax is wonky and I can’t fully wrap my head around it (plus it makes it hard to carry over non-accumulated variables). So I resorted to a loop. Oooof.\nIt works, though! Let’s test it on just one ID. We’ll generate just one row with t1, t2, d1, and d2, and we’ll remove the noise from each of the columns:\n\nset.seed(12345)\nn &lt;- 1\n\n# Remove all noise so that we can compare this with the function version\ndf_example &lt;- tibble(id = 1:n,\n                     c = rnorm(n, 0, 1)) %&gt;% \n  mutate(t1 = (0.1 * c),\n         d1 = (0.1 * c) + (0.4 * t1),\n         t2 = (0.1 * c) + (0.4 * d1) + (0.4 * t1),\n         d2 = (0.1 * c) + (0.4 * t2) + (0.4 * d1))\ndf_example\n## # A tibble: 1 × 6\n##      id     c     t1     d1    t2    d2\n##   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 0.586 0.0586 0.0820 0.115 0.137\n\nThis isn’t tidy data, so technically there are two years here. We’ll keep t1 and d1 as the initial values for year 1, then generate values for 5 years. If we do this right, the t and d values for year 2 should be identical to t2 and d2 here.\n\n# Add a bunch of extra years with empty cells for t and d\ndf_multiple_years_empty &lt;- df_example %&gt;% \n  mutate(y = 1) %&gt;% \n  select(id, y, t = t1, d = d1, c) %&gt;% \n  add_row(y = 2:5) %&gt;% \n  # c doesn't vary across time\n  mutate(id = 1, \n         c = df_example$c[1])\ndf_multiple_years_empty\n## # A tibble: 5 × 5\n##      id     y       t       d     c\n##   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n## 1     1     1  0.0586  0.0820 0.586\n## 2     1     2 NA      NA      0.586\n## 3     1     3 NA      NA      0.586\n## 4     1     4 NA      NA      0.586\n## 5     1     5 NA      NA      0.586\n\n# Now we need to build the interdependent values for t and d with &lt;gasp&gt; a loop\n#\n# The loop is inside this function---it skips the initial row and then\n# iteratively adds new rows. We can't use neat things like lag(t), so instead\n# we use df$t[i - 1]\n# \n# We omit the extra noise for now\ndgp &lt;- function(df) {\n  for (i in 2:nrow(df)) {\n    df$t[i] &lt;- (0.1 * df$c[i]) + (0.4 * df$d[i - 1]) + (0.4 * df$t[i - 1])\n    df$d[i] &lt;- (0.1 * df$c[i]) + (0.4 * df$t[i]) + (0.4 * df$d[i - 1])\n  }\n  \n  df\n}\n\n# Generate all 5 years\ndf_multiple_years_empty %&gt;% \n  dgp()\n## # A tibble: 5 × 5\n##      id     y      t      d     c\n##   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1     1     1 0.0586 0.0820 0.586\n## 2     1     2 0.115  0.137  0.586\n## 3     1     3 0.159  0.177  0.586\n## 4     1     4 0.193  0.207  0.586\n## 5     1     5 0.219  0.229  0.586\n\nIf this worked, the t and d values for year 2 should be the same as t2 and d2 here:\n\ndf_example\n## # A tibble: 1 × 6\n##      id     c     t1     d1    t2    d2\n##   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 0.586 0.0586 0.0820 0.115 0.137\n\nThey are!\nSince we know it works, let’s generate a big dataset for playing with multi-year MSMs, this time with noise:\n\nset.seed(1234)\n\n# Make data for the first year\ndf_first_year &lt;- expand_grid(id = 1:2000, y = 1) %&gt;% \n  mutate(c = rnorm(n(), 0, 1),\n         t = (0.1 * c) + rnorm(n(), 0, sqrt(0.99)),\n         d = (0.1 * c) + (0.4 * t) + rnorm(n(), 0, sqrt(0.822)))\ndf_first_year\n## # A tibble: 2,000 × 5\n##       id     y      c        t       d\n##    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n##  1     1     1 -1.21  -1.09    -1.43  \n##  2     2     1  0.277 -0.0714  -0.739 \n##  3     3     1  1.08  -0.00174 -0.0686\n##  4     4     1 -2.35   0.952    1.88  \n##  5     5     1  0.429 -1.60     0.0373\n##  6     6     1  0.506 -0.990    0.535 \n##  7     7     1 -0.575 -1.79    -0.889 \n##  8     8     1 -0.547  0.456   -1.14  \n##  9     9     1 -0.564 -0.500    0.386 \n## 10    10     1 -0.890 -1.92    -1.04  \n## # ℹ 1,990 more rows\n\n# Add empty years 2-5\ndf_panel_empty &lt;- df_first_year %&gt;% \n  bind_rows(expand_grid(id = 1:2000, y = 2:5)) %&gt;% \n  arrange(id, y)\nhead(df_panel_empty, 10)\n## # A tibble: 10 × 5\n##       id     y      c       t      d\n##    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n##  1     1     1 -1.21  -1.09   -1.43 \n##  2     1     2 NA     NA      NA    \n##  3     1     3 NA     NA      NA    \n##  4     1     4 NA     NA      NA    \n##  5     1     5 NA     NA      NA    \n##  6     2     1  0.277 -0.0714 -0.739\n##  7     2     2 NA     NA      NA    \n##  8     2     3 NA     NA      NA    \n##  9     2     4 NA     NA      NA    \n## 10     2     5 NA     NA      NA\n\n# Add noise to the fancy dgp() function\ndgp &lt;- function(df) {\n  for (i in 2:nrow(df)) {\n    df$t[i] &lt;- (0.1 * df$c[i]) + (0.4 * df$d[i - 1]) + (0.4 * df$t[i - 1]) + rnorm(1, 0, sqrt(0.5196))\n    df$d[i] &lt;- (0.1 * df$c[i]) + (0.4 * df$t[i]) + (0.4 * df$d[i - 1]) + rnorm(1, 0, sqrt(0.4582))\n  }\n  \n  df\n}\n\n# Run dgp() within each id\ndf_panel &lt;- df_panel_empty %&gt;% \n  group_by(id) %&gt;% \n  # This doesn't vary with time, so repeat it across all the years\n  mutate(c = c[1]) %&gt;% \n  # Nest the data into a single cell in each row\n  nest() %&gt;% \n  # Run dgp() on the nested cell (in a column named \"data\")\n  mutate(dgp = map(data, dgp)) %&gt;% \n  select(-data) %&gt;% \n  # Unnest the nested dgp()-ed cells\n  unnest(dgp) %&gt;% \n  # Add some lags\n  mutate(across(c(t, d), list(lag = lag))) %&gt;% \n  ungroup()\nhead(df_panel, 10)\n## # A tibble: 10 × 7\n##       id     y      c       t      d   t_lag  d_lag\n##    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n##  1     1     1 -1.21  -1.09   -1.43  NA      NA    \n##  2     1     2 -1.21  -1.11   -1.28  -1.09   -1.43 \n##  3     1     3 -1.21  -1.83   -1.35  -1.11   -1.28 \n##  4     1     4 -1.21  -1.62   -1.20  -1.83   -1.35 \n##  5     1     5 -1.21  -1.62   -0.981 -1.62   -1.20 \n##  6     2     1  0.277 -0.0714 -0.739 NA      NA    \n##  7     2     2  0.277  1.09    0.116 -0.0714 -0.739\n##  8     2     3  0.277  1.43    0.562  1.09    0.116\n##  9     2     4  0.277  0.584  -0.606  1.43    0.562\n## 10     2     5  0.277 -0.881   0.603  0.584  -0.606\n\nMultiple-period marginal structural models\nCool cool cool. We have multi-period data where variables depend on their lagged values and other interdependent columns. Let’s build some MSMs!\nWe’re going to build weights two different ways here. Because the data has a time-series cross-sectional structure (with multiple individuals across multiple years), the weight models need to account for that structure. Everywhere I’ve seen elsewhere uses generalized estimating equations (GEEs) for these models (and that’s what the ipw package uses behind the scenes), but I’m a fan of multilevel models, so I’ll make weights with both to compare their results.\nWe’ll use the same equation as before—the cumulative product of the ratio of two probability distributions:\n\\[\nw = \\prod^t_{t = 1} \\frac{\\phi(T_{it} | T_{i, t-1}, C_i)}{\\phi(T_{it} | T_{i, t-1}, D_{i, t-1}, C_i)}\n\\]\n\n# Again, ipw complains about missing values, so get rid of them (i.e. all year 1)\ndf_panel_sans_na &lt;- df_panel %&gt;%\n  filter(!is.na(t_lag)) %&gt;% \n  mutate(y = as.numeric(y))\n\n# Manually\n# Numerator is lagged treatment + non-varying confounders\n# ipw uses GEE models to account for panel structure, so use them here too\nmodel_num_gee &lt;- geeglm(t ~ t_lag + c, \n                        id = id, waves = y, corstr = \"ar1\",\n                        data = df_panel_sans_na)\n\n# But we can also use mixed models with lmer\nmodel_num_multi &lt;- lmer(t ~ t_lag + c + (1 | id),\n                        data = df_panel_sans_na)\n\n# Denominator is lagged treatment, lagged outcome, time-varying confounders +\n# non-varying confounders\nmodel_denom_gee &lt;- geeglm(t ~ t_lag + d_lag + c, \n                          id = id, waves = y, corstr = \"ar1\",\n                          data = df_panel_sans_na)\n\nmodel_denom_multi &lt;- lmer(t ~ t_lag + d_lag + c + (1 | id),\n                          data = df_panel_sans_na)\n\n# Probability distributions\nnum_gee &lt;- dnorm(df_panel_sans_na$t,\n             predict(model_num_gee),\n             sd(residuals(model_num_gee)))\n\nden_gee &lt;- dnorm(df_panel_sans_na$t,\n                 predict(model_denom_gee),\n                 sd(residuals(model_denom_gee)))\n\nnum_multi &lt;- dnorm(df_panel_sans_na$t,\n                   predict(model_num_multi),\n                   sd(residuals(model_num_multi)))\n\nden_multi &lt;- dnorm(df_panel_sans_na$t,\n                   predict(model_denom_multi),\n                   sd(residuals(model_denom_multi)))\n\ndf_panel_weights_manual &lt;- df_panel_sans_na %&gt;% \n  mutate(weights_no_time_gee = num_gee / den_gee,\n         weights_no_time_multi = num_multi / den_multi) %&gt;% \n  group_by(id) %&gt;% \n  mutate(ipw_gee = cumprod(weights_no_time_gee),\n         ipw_multi = cumprod(weights_no_time_multi)) %&gt;% \n  ungroup()\n\n# Automatically with ipwtm()\npanel_weights_auto &lt;- ipwtm(\n  exposure = t,\n  family = \"gaussian\", \n  numerator = ~ t_lag + c, \n  denominator = ~ t_lag + d_lag + c, \n  id = id,\n  timevar = y,\n  type = \"all\",\n  corstr = \"ar1\",\n  data = as.data.frame(df_panel_sans_na))\n\ndf_panel_weights_auto &lt;- df_panel_sans_na %&gt;% \n  mutate(ipw = panel_weights_auto$ipw.weights)\n\nNow we can build the outcome models, using both GEE and multilevel models.\n\nm_manual_gee &lt;- geeglm(d ~ t + t_lag, \n                       data = df_panel_weights_manual,\n                       id = id, waves = y, \n                       weights = ipw_gee)\n\nm_auto_gee &lt;- geeglm(d ~ t + t_lag, \n                     data = df_panel_weights_auto, \n                     id = id, waves = y, \n                     weights = ipw)\n\nm_lmer_manual_gee_wt &lt;- lmer(d ~ t + t_lag + (1 | id), \n                             data = df_panel_weights_manual, \n                             weights = ipw_gee)\n\nm_lmer_auto_gee_wt &lt;- lmer(d ~ t + t_lag + (1 | id), \n                           data = df_panel_weights_auto, \n                           weights = ipw)\n\nm_lmer_manual_multi_wt &lt;- lmer(d ~ t + t_lag + (1 | id), \n                               data = df_panel_weights_manual, \n                               weights = ipw_multi)\n\nresults &lt;- tribble(\n  ~`Method`, ~`Weights`, ~`lag(T) effect`, ~`T effect`,\n  \"True effect\", NA, 0.160, 0.400,\n  \"Panel with GEE\", \"Manual GEE weights\", filter(tidy(m_manual_gee), term == \"t_lag\")$estimate, filter(tidy(m_manual_gee), term == \"t\")$estimate,\n  \"Panel with GEE\", \"Automatic GEE weights\", filter(tidy(m_auto_gee), term == \"t_lag\")$estimate, filter(tidy(m_auto_gee), term == \"t\")$estimate,\n  \"Panel with multilevel model\", \"Manual GEE weights\", filter(tidy(m_lmer_manual_gee_wt), term == \"t_lag\")$estimate, filter(tidy(m_lmer_manual_gee_wt), term == \"t\")$estimate,\n  \"Panel with multilevel model\", \"Automatic GEE weights\", filter(tidy(m_lmer_auto_gee_wt), term == \"t_lag\")$estimate, filter(tidy(m_lmer_auto_gee_wt), term == \"t\")$estimate,\n  \"Panel with multilevel model\", \"Manual multilevel weights\", filter(tidy(m_lmer_manual_multi_wt), term == \"t_lag\")$estimate, filter(tidy(m_lmer_manual_multi_wt), term == \"t\")$estimate\n)\n\n\n\n\n\nMethod\nWeights\nlag(T) effect\nT effect\n\n\n\nTrue effect\n—\n0.160\n0.400\n\n\nPanel with GEE\nManual GEE weights\n0.230\n0.429\n\n\nPanel with GEE\nAutomatic GEE weights\n0.230\n0.429\n\n\nPanel with multilevel model\nManual GEE weights\n0.195\n0.408\n\n\nPanel with multilevel model\nAutomatic GEE weights\n0.195\n0.408\n\n\nPanel with multilevel model\nManual multilevel weights\n0.196\n0.414\n\n\n\n\n\nPhew.\nAll of the models here are relatively close to the true effect for \\(T_t\\)! (Though they’re all sizably off for \\(T_{t-1}\\), but I don’t know why).\nImportantly, it seems that mixed models work just fine for both weights and outcome models, which means there’s probably no need to use GEE (and this can all be done Bayesianly with brms()!).\nIt works!"
  },
  {
    "objectID": "blog/2021/01/15/msm-gee-multilevel/index.html#questions-for-the-future",
    "href": "blog/2021/01/15/msm-gee-multilevel/index.html#questions-for-the-future",
    "title": "Marginal structural models for panel data with GEE and multilevel models",
    "section": "Questions for the future",
    "text": "Questions for the future\nI still have some lingering things to check (forthcoming in a future blog post):\n\ny isn’t any any of the models: weights numerator, weights denominator, or outcome. If I include it in any of the models, the there’s perfect fit. That may be because there are no time-varying confounders? I think year might need to be in there somewhere, but I’m not sure. Blackwell and Glynn (2018) include year in the numerator and denominator for weights in their Swank Steinmo replication, but not in their Burgoon replication.\nThis example has no time-varying confounders. I need to see how this all works with those.\nThis DAG for this example is pretty simple. I need to see what happens when there are also direct arrows from \\(T_{t-1} \\rightarrow T_{t}\\) and \\(D_{t-1} \\rightarrow D_{t}\\)."
  },
  {
    "objectID": "blog/2021/07/10/hex-cross-stitch/index.html",
    "href": "blog/2021/07/10/hex-cross-stitch/index.html",
    "title": "Hex sticker/logo cross stitch pattern",
    "section": "",
    "text": "Downloads\n\n\n\nJump to the downloads and get your own free pattern and template files!\n\n\nIn the data science world, hex stickers with logos of developers’ favorite packages are all the rage. People collect them at conferences and events and display them on their laptops (or, like me, keep them in a pile on their desk because they’re too afraid to commit to affixing them anywhere permanently). There are even official standard dimensions and templates people can follow.\nJust for fun, I make hex logos for each of my classes (program evaluation/causal inference, microeconomics, and data visualization)—see all my class and package hex logos here—and in non-pandemic times I print them and hand them out to students at the beginning of class. They look awesome.\n\n\n\nCourse-specific hex logos\n\n\nContinuing my pandemic art kick (on this, the four hundred and eighty-fifth day of sheltering in place), and following my foray into cross stitch (like this Bayesian Sampler), I decided to make cross stitch versions of my hex logos. So I stuck my program evaluation logo into Illustrator, pixel-art-ified it by hand, and made the thing.\n\n\n\nA cross stitched hex logo\n\n\nThe design is a generic DAG showing an exposure, an outcome, an indirect effect, a confounder, and a collider; the color palette comes from the viridis inferno scale, generated with viridisLite::viridis(8, option = \"inferno\", begin = 0.1, end = 0.9) in R.\nTo help the world create hex logo cross stitch art, I’ve provided the Illustrator file for free! (with a Creative Commons license). Make your own designs—just delete the stuff on the Text and Pattern layers and add little rectangles filled with some color + a 0.5 point white stroke.\nDownload everything here!\n\nHex logo cross stitch PDF (v1.0, 2021-02-15)\nHex logo cross stitch Illustrator file\n\n\n\n\nA Bayesian sampler\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{heiss2021,\n  author = {Heiss, Andrew},\n  title = {Hex Sticker/Logo Cross Stitch Pattern},\n  date = {2021-07-10},\n  url = {https://www.andrewheiss.com/blog/2021/07/10/hex-cross-stitch/},\n  doi = {10.59350/ha58q-xzf62},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHeiss, Andrew. 2021. “Hex Sticker/Logo Cross Stitch\nPattern.” July 10, 2021. https://doi.org/10.59350/ha58q-xzf62."
  },
  {
    "objectID": "blog/2021/08/21/r2-euler/index.html",
    "href": "blog/2021/08/21/r2-euler/index.html",
    "title": "Exploring R² and regression variance with Euler/Venn diagrams",
    "section": "",
    "text": "Regression is the core of my statistics and program evaluation/causal inference courses. As I’ve taught different stats classes, I’ve found that one of the regression diagnostic statistics that students really glom onto is \\(R^2\\). Unlike lots of regression diagnostics like AIC, BIC, and the joint F-statistic, \\(R^2\\) has a really intuitive interpretation—it’s the percent of variation in the outcome variable explained by all the explanatory variables. For instance, let’s explain global life expectancy using GDP per capita, based on data from the Gapminder project. Here’s the basic model:\n\\[\n\\widehat{\\text{Life expectancy}} = \\beta_0 + \\beta_1 \\text{GDP per capita} + \\epsilon\n\\]\nlibrary(tidyverse)     # For ggplot, dplyr, and friends\nlibrary(broom)         # For converting models into data frames\nlibrary(gapminder)     # For health and wealth data\nlibrary(faux)          # For generating fake data\nlibrary(eulerr)        # For creating Euler and Venn diagrams\n# Just look at 2007\ngapminder_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)\n\nsuper_naive_model &lt;- lm(lifeExp ~ gdpPercap, data = gapminder_2007)\ntidy(super_naive_model)\n## # A tibble: 2 × 5\n##   term         estimate std.error statistic   p.value\n##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept) 59.6      1.01           59.0 9.89e-101\n## 2 gdpPercap    0.000637 0.0000583      10.9 1.69e- 20\nglance(super_naive_model)\n## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC deviance\n##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n## 1     0.461         0.457  8.90      120. 1.69e-20     1  -511. 1028. 1037.   11086.\n## # ℹ 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;\nThis is an awful model on theoretical grounds, but whatever. A $1 increase in GDP per capita is associated with a 0.001 year increase in life expectancy, on average. Look at all those diagnostics though—AIC? BIC? LogLik? Those are important, but definitely not intuitive. The \\(R^2\\) value, on the other hand, is nice and interpretable. GDP per capita explains 46.1% of the variation in life expectancy. That’s soundbite-worthy. Students love it.\nPeople also love to critique models based on it. In my program evaluation class, a model in one of my problem sets has an \\(R^2\\) of 0.02, and without fail, a majority of students dismiss it because of its low explanatory power. It only explains 2% of the variation in the outcome! It must be junk!\nBut low \\(R^2\\)s aren’t necessarily bad. And while saying that a model explains X% of the variation in an outcome, what does that even mean in practice?\nSo how the heck does \\(R^2\\) work anyway? I’ve struggled teaching this, but recently a friend mentioned that she teaches students to imagine \\(R^2\\) as a combination of overlapping circles, or a type of Venn diagram. I’d never heard of this approach before, so I went down a really interesting rabbit hole to discover that this approach has actually been used for decades! Let’s go down that rabbit hole together!"
  },
  {
    "objectID": "blog/2021/08/21/r2-euler/index.html#regression-as-overlapping-circles",
    "href": "blog/2021/08/21/r2-euler/index.html#regression-as-overlapping-circles",
    "title": "Exploring R² and regression variance with Euler/Venn diagrams",
    "section": "Regression as overlapping circles",
    "text": "Regression as overlapping circles\nThe earliest example I found is Cohen and Cohen (1975), who proposed visualizing the shared variance between 2–3 variables as a “ballantine” graph (apparently named after an ale logo?), or what we call nowadays a Venn or Euler diagram. Others refined their approach, like Hunt (1986) who provides all sorts of fancy geometric equations to make them accurate (and provides some neat vintage Pascal code in the appendix!), and Ip (2001), who shows a bunch of different examples and highlights some of the limitations of using this approach.\nFor this ongoing example, we’ll simulate some correlated data using the faux package. We could use real data, but as you’ll see, these diagrams are pretty finicky and fragile and generally need very tame data to work. Here we’ll create data with three variables that are normally distributed with these parameters:\n\n\nY: mean = 10, sd = 2\n\nX1: mean = 9, sd = 1.7\n\nX2: mean = 9, sd = 1.3\n\nWe’ll make Y and X1 be correlated at r = 0.5, Y and X2 correlated at r = 0.3, and X1 and X2 correlated at r = 0.4:\n\nset.seed(1234)\ndf &lt;- rnorm_multi(n = 100,\n                  mu = c(10, 9, 9),\n                  sd = c(2, 1.7, 1.3),\n                  r = c(0.5, 0.3, 0.4),\n                  varnames = c(\"Y\", \"X1\", \"X2\"),\n                  empirical = FALSE)\n\n# Look at the first few rows\nhead(df)\n##       Y    X1    X2\n## 1  8.22  6.73  8.38\n## 2 10.17  9.36 10.12\n## 3 12.03 10.32  9.86\n## 4  5.41  5.80  8.38\n## 5 10.10 10.08 10.09\n## 6 11.13  9.11  9.94\n\n# Check if the correlations worked\ncor(df)\n##        Y    X1    X2\n## Y  1.000 0.453 0.373\n## X1 0.453 1.000 0.468\n## X2 0.373 0.468 1.000\n# Close enough!\n\nIn these regression diagrams, each variable is shown as a circle sized according to its variance, or:\n\\[\n\\operatorname{var} = \\sigma^2 = \\frac{\\sum (\\color{gray}{\\overbrace{\\color{black}{x_i}}^{\\substack{\\text{Single} \\\\ \\text{value}}}} - \\color{gray}{\\overbrace{\\color{black}{\\bar{x}}}^{\\substack{\\text{Mean of} \\\\ \\text{all values}}}\\color{black}{)^2}}}{\\color{gray}{\\underbrace{\\color{black}{n}}_{\\substack{\\text{Sample} \\\\ \\text{size}}}}-1}\n\\]\nAccording to Ip (2001), you can also size the circles based on just the numerator of that equation, or the sum-of-squares, which is the same value proportionally number (it’s just not divided by \\(n-1\\)). It’s also easier to work with when plotting, so that’s what we’ll use throughout this example.\n\\[\n\\color{gray}{\\overbrace{\\color{black}{{\\scriptstyle\\sum} (x_i - \\bar{x})^2}}^{\\text{Sum of squares}}}\n\\]\nWe can even make a little function to calculate the sum of squares for us:\n\n# Sum of squares, or the numerator of the variance equation\nss &lt;- function(x) {\n  sum((x - mean(x))^2)\n}\n\nEnough math, though—let’s make some plots! We’ll use the eulerr package for this, since (1) it calculates proportional overlaps between circles (that’s the key technical difference between Venn and Euler diagram—Venn diagrams are supposed to have equally sized overlapping areas), and (2) it uses grid graphics, so it fits nicely in the ggplot ecosystem.\nRelationship between two variables\nFirst let’s just look at the relationship between Y and X1. We’ll use the sum of squares to calculate the size for each circle:\n\nss_y &lt;- ss(df$Y)\nss_x1 &lt;- ss(df$X1)\n\nplot(euler(c(\"Y\" = ss_y,\n             \"X1\" = ss_x1)),\n     quantities = TRUE)\n\n\n\n\n\n\n\nJust looking at this plot, we can see that Y has more variation than X1. Neat.\nThese two variables are related to each other though and have some covariance. We can calculate the shared covariance using ANOVA. Ordinarily you need to feed R’s anova() function an lm() object to calculate different variance statistics (e.g. anova(lm(Y ~ X1, data = df))), but to save some typing, you can also use the aov() function to skip the intermediate lm() step. The nice thing about using the sum of squares values for these diagrams rather than variance values is that aov() reports its results as sums of squares, so we can use those results directly.\nLet’s see how much of the variation between Y and X1 is shared:\n\naov(Y ~ X1, data = df)\n## Call:\n##    aov(formula = Y ~ X1, data = df)\n## \n## Terms:\n##                  X1 Residuals\n## Sum of Squares   82       318\n## Deg. of Freedom   1        98\n## \n## Residual standard error: 1.8\n## Estimated effects may be unbalanced\n\nIf we look at the sum of squares row, we can see that 81.98 sum-of-squares units (whatever those mean) are shared between the two variables, with 317.87 not shared (or residual). To plot this overlap, we need to do a little bit of set theory math. We can’t just tell Y to be 399 units big—we need to subtract the shared space from both Y and X1. We’ll extract the sum of squares value from aov() (using broom::tidy() to make this easier):\n\nss_both_y_x1 &lt;- aov(Y ~ X1, data = df) %&gt;%\n  tidy() %&gt;%\n  filter(term == \"X1\") %&gt;%\n  pull(sumsq)\nss_both_y_x1\n## [1] 82\n\nplot(euler(c(\"Y\" = ss_y - ss_both_y_x1,\n             \"X1\" = ss_x1 - ss_both_y_x1,\n             \"Y&X1\" = ss_both_y_x1)),\n     quantities = TRUE)\n\n\n\n\n\n\n\nNow we can visualize the covariance between these two variables! Let’s get rid of the raw numbers and add some letters for the different segments:\n\nplot(euler(c(\"Y\" = ss_y - ss_both_y_x1,\n             \"X1\" = ss_x1 - ss_both_y_x1,\n             \"Y&X1\" = ss_both_y_x1)),\n     quantities = c(\"A\", \"B\", \"C\"))\n\n\n\n\n\n\n\nArea C here represents the amount of variation in Y explained by X1, while Area A represents the unexplained portion of Y. In regression language, A is basically the error term:\n\\[\n\\hat{Y} = \\beta_0 + \\beta_1 X_1 + \\color{gray}{\\overbrace{\\epsilon}^{\\color{black}{\\text{A}}}}\n\\]\nThe great thing about visualizing this is that C also represents the \\(R^2\\)! In general, \\(R^2\\) is the ratio between explained and total variance:\n\\[\nR^2 = \\frac{\\text{Explained variance in }Y}{\\text{Total variance in }Y}\n\\]\nBased on this diagram, we can write this as:\n\\[\nR^2 = \\frac{C}{A + C}\n\\]\nUsing actual numbers, we get\n\\[\nR^2 = \\frac{81.98}{317.87 + 81.98} = 0.205\n\\]\nWe can do this more precisely with code:\n\npart_a &lt;- ss_y - ss_both_y_x1\npart_c &lt;- ss_both_y_x1\n\npart_c / (part_a + part_c)\n## [1] 0.205\n\nAccording to this, X1 explains 20.5% of the variation in Y. That’s apparent visually—the overlapping space covers about 20% of the total Y circle.\nLet’s run a regression model and check the \\(R^2\\) value to see if it’s the same:\n\nlm(Y ~ X1, data = df) %&gt;%\n  glance() %&gt;%\n  pull(r.squared)\n## [1] 0.205\n\nIncredible!! It’s the same! That’s so cool.\nRelationship between three variables\nWhere this gets even more useful is when we look at the overlapping space between three different variables. It’s also a little more complicated, since we need to calculate a bunch of different shared variances and do some set theory calculations to find the exact size of these different slivers of the diagram. Here’s a general diagram of what we’ll be calculating (this is unrelated to the data we’ve been working with and doesn’t use any actual numbers—it’s just a reference so we can find which shared variances we need to calculate):\n\nplot(euler(c(\"Y\" = 4,\n             \"X1\" = 4,\n             \"X2\" = 4,\n             \"X1&Y\" = 2,\n             \"X2&Y\" = 2,\n             \"X1&X2\" = 2,\n             \"Y&X1&X2\" = 0.5)),\n     quantities = c(LETTERS[1:7]))\n\n\n\n\n\n\n\nThis looks complicated, but the same principles apply. The entire circles for Y, X1, and X2 represent each variable’s total variance. Overlapping areas represent shared variance. For instance, the combination of D and G here (or \\(A \\cap B\\)) is the covariance that we calculated previously. The \\(R^2\\) is still here too—the total explained variance in Y is the combination of D, E, and G, while A is the residual unexplained variance. That means we can calculate \\(R^2\\) like this:\n\\[\n\\frac{D + E + G}{A + D + E + G}\n\\]\nThis visualization also helps with the intuition of \\(R^2\\). Generally when you add additional variables to a regression model, the \\(R^2\\) increases. That’s because you’re adding another circle to the diagram and absorbing more of the variation in the outcome. For instance, even though the numbers in this diagram aren’t to scale at all, you can see that (D + G) (the \\(R^2\\) that we calculated in the two-variable diagram) is smaller than (D + E + G). There’s more explained variance here.\nTo calculate the actual values for each of these segments, we’ll use aov() again to find the shared variance. This will involve a lot of different calculations and some algebra to isolate each segment. This table will help us keep everything straight:\n\n\n\n\n\n\n\nSegment\nExplanation\nCode or algebra\n\n\n\nA + D + E + G\nTotal variation in Y\n\n\nss(df$Y) or aov(Y ~ 1)\n\n\n\nB + D + F + G\nTotal variation in X1\n\n\nss(df$X1) or aov(X1 ~ 1)\n\n\n\nC + E + F + G\nTotal variation in X2\n\n\nss(df$X2) or aov(X2 ~ 1)\n\n\n\nA\nUnexplained variation in Y after accounting for X1 and X2\n\nResiduals from aov(Y ~ X2 + X1)\n\n\n\nB\nUnexplained variation in X1 after accounting for Y and X2\n\nResiduals from aov(X1 ~ Y + X2)\n\n\n\nC\nUnexplained variation in X2 after accounting for Y and X1\n\nResiduals from aov(X2 ~ Y + X1)\n\n\n\nD + G\nVariance shared by Y and X1\n\n\nX1 in aov(Y ~ X1)\n\n\n\nE + G\nVariance shared by Y and X2\n\n\nX2 in aov(Y ~ X2)\n\n\n\nD + G\nVariance shared by X1 and X2\n\n\nX2 in aov(X1 ~ X2)\n\n\n\nD\nVariance only between Y and X1, without influence from X2\n\n(A + D + E + G) − A − (E + G)\n\n\nE\nVariance only between Y and X2, without influence from X1\n\n(C + E + F + G) − C − (F + G)\n\n\nF\nVariance only between X1 and X2, without influence from Y\n\n(B + D + F + G) − B − (D + G)\n\n\nG\nVariance shared by Y, X1, and X2\n\n(D + G) − D\n\n\n\nPHEW. That’s a lot. Here’s all the code for it:\n\ny_total &lt;- ss(df$Y)    # A + D + E + G\nx1_total &lt;- ss(df$X1)  # B + D + F + G\nx2_total &lt;- ss(df$X2)  # C + E + F + G\n\n# A\ny_alone &lt;- aov(Y ~ X2 + X1, data = df) %&gt;%\n  tidy() %&gt;%\n  filter(term == \"Residuals\") %&gt;%\n  pull(sumsq)\n\n# B\nx1_alone &lt;- aov(X1 ~ Y + X2, data = df) %&gt;%\n  tidy() %&gt;%\n  filter(term == \"Residuals\") %&gt;%\n  pull(sumsq)\n\n# C\nx2_alone &lt;- aov(X2 ~ Y + X1, data = df) %&gt;%\n  tidy() %&gt;%\n  filter(term == \"Residuals\") %&gt;%\n  pull(sumsq)\n\n# D + G\ny_plus_x1 &lt;- aov(Y ~ X1, data = df) %&gt;%\n  tidy() %&gt;%\n  filter(term == \"X1\") %&gt;%\n  pull(sumsq)\n\n# E + G\ny_plus_x2 &lt;- aov(Y ~ X2, data = df) %&gt;%\n  tidy() %&gt;%\n  filter(term == \"X2\") %&gt;%\n  pull(sumsq)\n\n# F + G\nx1_plus_x2 &lt;- aov(X1 ~ X2, data = df) %&gt;%\n  tidy() %&gt;%\n  filter(term == \"X2\") %&gt;%\n  pull(sumsq)\n\n# D = (A + D + E + G) − A − (E + G)\ny_x1_alone &lt;- y_total - y_alone - y_plus_x2\n\n# E = (A + D + E + G) − A − (D + G)\ny_x2_alone &lt;- y_total - y_alone - y_plus_x1\n\n# G = (D + G) − D\ny_x1_x2_alone &lt;- y_plus_x1 - y_x1_alone\n\n# F = (F + G) - G\nx1_x2_alone &lt;- x1_plus_x2 - y_x1_x2_alone\n\nAgain, that’s super complex, but it’s really just a ton of set theory algebra.\nNow that we have all these little pieces, let’s plot them!\n\nall_pieces &lt;- c(\"Y\" = y_alone,\n                \"X1\" = x1_alone,\n                \"X2\" = x2_alone,\n                \"X1&Y\" = y_x1_alone,\n                \"X2&Y\" = y_x2_alone,\n                \"X1&X2\" = x1_x2_alone,\n                \"Y&X1&X2\" = y_x1_x2_alone)\nall_pieces\n##       Y      X1      X2    X1&Y    X2&Y   X1&X2 Y&X1&X2 \n##   304.6   201.6   123.6    39.7    13.3    21.5    42.3\n\nplot(euler(all_pieces),\n     quantities = LETTERS[1:7])\n\n\n\n\n\n\n\nThese circles are now all proportional to the actual values in the data. Both X1 and X2 explain some of the variation in Y, but not a ton (and X1 explains more than X2). The explained variance is represented by the area D + E + G, which means we can now calculate the actual \\(R^2\\):\n\\[\nR^2 = \\frac{D + E + G}{A + D + E + G} = \\frac{39.69 + 13.27 + 42.29}{304.6 + 39.69 + 13.27 + 42.29} = 0.238\n\\]\nOr with code:\n\n(y_x1_alone + y_x2_alone + y_x1_x2_alone) /\n  (y_alone + y_x1_alone + y_x2_alone + y_x1_x2_alone)\n## [1] 0.238\n\nLet’s confirm it with a regression model:\n\nlm(Y ~ X1 + X2, data = df) %&gt;%\n  glance() %&gt;%\n  pull(r.squared)\n## [1] 0.238\n\nAMAZING. It’s the same. It worked!\nNicer plot\nWith everything nice and proportional, let’s make this plot a little fancier for teaching purposes. Since eulerr uses grid-based graphics, we can use it with patchwork to add annotations (or combine it with other ggplot objects if we really wanted to):\n\nlibrary(patchwork)  # For combining ggplot and grid elements\nlibrary(grid)       # For making custom grid grobs\nlibrary(latex2exp)  # For writing LaTeX-like text with grid plots\n\nnice_plot &lt;- plot(euler(all_pieces),\n                  quantities = list(labels = LETTERS[1:7],\n                                    fontfamily = \"Roboto Condensed Light\",\n                                    fontsize = 16),\n                  fills = list(fill = c(\"#7FDBFF\", \"grey30\", \"grey80\",\n                                        \"#FF851B\", \"#FF851B\", \"grey50\", \"#FF851B\"),\n                               alpha = c(1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.35)),\n                  labels = list(fontfamily = \"Roboto Condensed\",\n                                fontface = \"bold\", fontsize = 20))\n\nmath_part &lt;- textGrob(TeX(\"R^2 = \\\\frac{D + E + G}{A + D + E + G}\"),\n                      gp = gpar(fontfamily = \"Roboto Condensed\",\n                                col = \"grey50\", fontsize = 13))\n\n# Ordinarily patchwork works just fine with grid grob objects, like the results\n# from plot.euler(), but *not* when they're the first element in patchwork chain\n# of plots. To make grob objects work nicely with patchwork, we need to wrap\n# them in wrap_elements()\nwrap_elements(nice_plot) +\n  inset_element(math_part,\n                left = -0.04, bottom = 0.72, right = 0.3, top = 0.85) +\n  plot_annotation(\n    title = \"R² represented as an Euler diagram\",\n    subtitle = \"Orange area (D + E + G) shows the total variance in\\noutcome Y that is jointly explained by X1 and X2\",\n    caption = \"Circles sized according to each variable's sum of squares; size of overlapping areas\\nis not 100% correct due to limitations in available geometric space\",\n    theme = theme(plot.title = element_text(size = 20, family = \"Roboto Condensed\", face = \"bold\"),\n                  plot.subtitle = element_text(size = 15, family = \"Roboto Condensed\"),\n                  plot.caption = element_text(size = 10, family = \"Roboto Condensed Light\", hjust = 0))\n  )\n\n\n\n\n\n\n\nMulticollinearity\nAnother neat feature of this kind of diagram is that it helps visualize multicollinearity, or the issues that arise when you control for explanatory variables that explain the same kind of variation in the outcome. Multicollinearity leads to strange coefficient estimates and variance inflation because mathematically the regression model has no way of telling which of the highly correlated explanatory variables explain which parts of the outcome.\nIn the diagram, areas D and E are uniquely accounted for by X1 and X2 respectively, but G is is overlapped, making it impossible to know if X1 or X2 explains that portion of the variation in Y. Similarly, area F shows the variation shared by both X1 and X2, and again it’s impossible to know which parts are unique. As a result, the area (F + G) represents the total multicollinearity in the model:\n\nplot(euler(all_pieces),\n     quantities = LETTERS[1:7],\n     fills = c(rep(\"white\", 5), rep(\"#FF4136\", 2)))"
  },
  {
    "objectID": "blog/2021/08/21/r2-euler/index.html#why-this-matters",
    "href": "blog/2021/08/21/r2-euler/index.html#why-this-matters",
    "title": "Exploring R² and regression variance with Euler/Venn diagrams",
    "section": "Why this matters",
    "text": "Why this matters\nOne really neat thing about thinking about \\(R^2\\) this way is that it highlights a key difference in the purposes of regression: prediction and estimation.\nWith prediction, the goal of the regression model is to predict the outcome as accurately as possible. This is what Netflix does when guessing what show you might want to watch next, or what the US Holocaust Museum does to predict genocide and human rights abuses. You throw in as many control variables as you can in order to create a model that explains variation in Y as accurately as possible. As a result, these kinds of models will typically have a high \\(R^2\\).\n\nplot(euler(c(\"Next show\\non Netflix\" = 1,\n             \"X1\" = 4,\n             \"X2\" = 6,\n             \"X3\" = 4,\n             \"X4\" = 6,\n             \"X5\" = 6,\n             \"X1&Next show\\non Netflix\" = 2,\n             \"X2&Next show\\non Netflix\" = 4,\n             \"X3&Next show\\non Netflix\" = 4,\n             \"X4&Next show\\non Netflix\" = 2,\n             \"X5&Next show\\non Netflix\" = 4,\n             \"X1&X2\" = 2,\n             \"Next show\\non Netflix&X1&X2\" = 0.5)))\n\n\n\n\n\n\n\nLook at that squishy little area of Y that’s not explained by any of the other variables. Neato.\nIn the realm of causal inference in social science, or with estimation in general, the focus of the analysis is on a single X variable , like whether or not people in the sample participated in a social program. You might include a bunch of control variables if you identification strategy tells you do: an interaction term for time and group if you’re using difference-in-differences, an indicator showing if people are above/below a threshold if you’re using regression discontinuity, an instrument if you’re using instrumental variables, confounding control variables if you’re using inverse probability weighting and a DAG, or nothing(!) if you’re using a randomized controlled trial (like you can really just run lm(Y ~ treatment)!). Each of these types of models will inevitably return an \\(R^2\\) value, since that what regression does, but we care less about that when estimating causal effects. All we care about is the accuracy of one little sliver of the diagram—that single policy lever that we might have control over to influence the larger outcome.\nFor instance, let’s say you’re a new little nonprofit who created a cool new program to help reduce childhood poverty within your city. You want to know if that program does anything to poverty more generally. You have lots of donor money so you decide to run a carefully designed randomized trial. You run a regression model and get a tiny \\(R^2\\) value, like 0.015. You panic because that means your experiment only explains 1.5% of the variation in poverty, and that seems really bad and low!\nBut that’s actually totally fine. The goal with the experiment is not to explain all the factors that create poverty—there are far too many to include in a regression model. The goal here is to have an accurate estimate of how much your single program affects poverty. There’s no way your little program is going to explain 80% of the variation in poverty. If you had a high \\(R^2\\), I’d be incredibly worried! (Unless you really discovered a program that is a true silver bullet for poverty!)\nLooking at an Euler diagram demonstrates this more easily. All we care about in this experiment is that little sliver. Is it accurate? Does the treatment move the needle on poverty in any way? If so, great!\n\nplot(euler(c(\"Poverty\" = 100,\n             \"Program\" = 2,\n             \"Poverty&Program\" = 0.25)),\n     fills = c(\"white\", \"grey75\", \"#FF851B\"))\n\n\n\n\n\n\n\nSo don’t dismiss models immediately just because of a low \\(R^2\\)! The purpose of the models matters!"
  },
  {
    "objectID": "blog/2021/08/21/r2-euler/index.html#caveats",
    "href": "blog/2021/08/21/r2-euler/index.html#caveats",
    "title": "Exploring R² and regression variance with Euler/Venn diagrams",
    "section": "Caveats",
    "text": "Caveats\nThere are a few important caveats to keep in mind with these diagrams:\n1: This is inefficient\nCalculating each of these plot segments by hand is tedious and there will inevitably be typos and errors (there are probably errors in this very post!). Also, due to how ANOVA works, the order of the variables you specify matters a lot: aov(Y ~ X1 + X2) and aov(Y ~ X2 + X1) give completely different sum of squares for the joint variation of Y, X1, and X2. That in turn changes the sizes of the segments.\nWhen writing this, I spent way too much time playing with covariance matrices, partial correlation matrices, and variance-covariance matrices to see if I could calculate these areas more mathematically using the guts of regression and ANOVA, but it was too tricky. Smarter people than me will need to figure it out.\n2: This doesn’t work in all cases\nIt’s possible to have negative areas, depending on the data you have, and negative areas are unplottable (Ip (2001) shows an example of this at the end of his article). In order to use this kind of diagram when teaching, you have to use carefully constructed data—you can’t just throw any model into a chart like this. Also, when working with actual numbers, you’re generally limited to just one or two explanatory variables. Any more and the math gets too complex.\n3: This isn’t 100% accurate\nThe geometry behind creating these Euler diagrams is complex and doesn’t always work perfectly. Let’s plot the actual values of the sum of squares for each segment:\n\nplot(euler(all_pieces),\n     quantities = TRUE)\n\n\n\n\n\n\n\nThat central piece where Y, X1, and X2 is 42, but it’s smaller any of the other intersections, which all have smaller sum-of-squares values. Yikes!\nThe eulerr package provides a way of diagnosing how bad things are by either inspecting the results of euler() directly, or looking at the residuals and errors with error_plot():\n\neuler(all_pieces)\n##         original fitted residuals regionError\n## Y          304.6  303.9     0.742       0.001\n## X1         201.6  200.3     1.326       0.001\n## X2         123.6  121.3     2.319       0.002\n## Y&X1        39.7   47.6    -7.953       0.011\n## Y&X2        13.3   26.5   -13.236       0.018\n## X1&X2       21.5   33.0   -11.541       0.016\n## Y&X1&X2     42.3   10.7    31.558       0.042\n## \n## diagError: 0.042 \n## stress:    0.009\n\nerror_plot(euler(all_pieces),\n           quantities = TRUE)\n\n\n\n\n\n\n\nThat central piece is underrepresented, while the other intersections are overrepresented. Alas."
  },
  {
    "objectID": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html",
    "href": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html",
    "title": "How to automatically convert TikZ images to SVG (with fonts!) from knitr",
    "section": "",
    "text": "Update #1\n\n\n\nAn update to knitr has made it a ton easier to embed fonts in SVG files from R. Jump to the update to see how.\nknitr and R Markdown are essential parts of my academic writing workflow, and they make it really easy to write in one file format and then convert it to whatever output I need. When knitting to PDF, plots are all vector-based with selectable text, and math equations are all done in beautiful LaTeX. When knitting to HTML, plots are PNG files (since HTML struggles with cross-browser vector image support), and math equations are all done in beautiful MathJax. It’s a wonderful system.\nOne place where the system struggles, though, is with more complex types of content like TikZ figures. TikZ lets you make all sorts of fancy diagrams and equations, but it’s a tricky language and I barely understand it, lol. There are some helpful resources about it, though:\nThrough the magic of knitr and R Markdown, it’s actually possible to use TikZ chunks directly in an R Markdown document and have them knit to both PDF and vector-based SVG files in HTML. Magic!"
  },
  {
    "objectID": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#knitting-tikz-chunks-to-pdf",
    "href": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#knitting-tikz-chunks-to-pdf",
    "title": "How to automatically convert TikZ images to SVG (with fonts!) from knitr",
    "section": "Knitting TikZ chunks to PDF",
    "text": "Knitting TikZ chunks to PDF\nknitr has the ability to use custom engines for rendering other languages, including TikZ, by including a tikz chunk like this:\n```{tikz}\n\\begin{tikzpicture}\n% TikZ code here\n\\end{tikzpicture}\n```\nTo try this out, make a new blank .Rmd file and put this in it:\n---\ntitle: \"TikZ fun!\"\noutput:\n  pdf_document: default\n---\n\nHere's an empty DAG\n\n```{tikz, echo=FALSE, cache=TRUE, fig.cap=\"Empty DAG\", fig.align=\"center\"}\n\\usetikzlibrary{positioning}\n\\begin{tikzpicture}[every node/.append style={draw, minimum size=0.5cm}]\n\\node [circle] (X) at (0,0) {};\n\\node [circle] (Y) at (2,0) {};\n\\node [rectangle] (Z) at (1,1) {};\n\\path [-latex] (X) edge (Y);\n\\draw [-latex] (Z) edge (Y);\n\\draw [-latex] (Z) edge (X);\n\\end{tikzpicture}\n```\nKnit that to PDF and you’ll see a neat little diagram of a directed acyclic graph (or DAG) (see full PDF here):"
  },
  {
    "objectID": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#knitting-tikz-chunks-to-html-as-pngs",
    "href": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#knitting-tikz-chunks-to-html-as-pngs",
    "title": "How to automatically convert TikZ images to SVG (with fonts!) from knitr",
    "section": "Knitting TikZ chunks to HTML as PNGs",
    "text": "Knitting TikZ chunks to HTML as PNGs\nIf you knit that document to HTML, you’ll also see the same diagram, but it will be a low resolution PNG and will look huge and pixelated. Behind the scenes, knitr converts the TikZ chunk to PDF and then converts that PDF to PNG and includes it in the HTML file (see full HTML file here).\n\n\n\n\n\n\n\n\nThere’s theoretically a way to change the DPI of this PNG file using the engine.opts chunk option and passing additional arguments to ImageMagick, which handles the conversion, but it only causes errors for me (Error in magick_image_format(image, toupper(format), type, colorspace, : Invalid ImageType value: -density 300) and I can’t figure out why (see this StackOverflow post too) :\n```{tikz, engine.opts = list(convert.opts = '-density 300'), fig.ext=\"png\"}\n% Stuff here\n```"
  },
  {
    "objectID": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#knitting-tikz-chunks-to-html-as-svgs",
    "href": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#knitting-tikz-chunks-to-html-as-svgs",
    "title": "How to automatically convert TikZ images to SVG (with fonts!) from knitr",
    "section": "Knitting TikZ chunks to HTML as SVGs",
    "text": "Knitting TikZ chunks to HTML as SVGs\nBut even if I could get it to work, PNG files aren’t great for vector-like things like TikZ diagrams. It would be great if instead I could convert these diagrams to vector-based SVG files.\nFortunately knitr handles that too! Try this! (Note the self_contained: no in the YAML metadata—this will create a standalone DVI and SVG file in a folder named &lt;NAME_OF_RMD&gt;_files/html-figures/. You don’t technically need this—without it, R will embed the images as base64-encoded text—but to make it easier to see the files R makes, we’ll disable it)\n---\ntitle: \"TikZ fun!\"\noutput:\n  html_document: \n    self_contained: no\n---\n\nHere's an empty DAG:\n\n```{tikz empty-dag, echo=FALSE, fig.cap=\"Empty DAG\", fig.align=\"center\", fig.ext=\"svg\"}\n\\usetikzlibrary{positioning}\n\\begin{tikzpicture}[every node/.append style={draw, minimum size=0.5cm}]\n\\node [circle] (X) at (0,0) {};\n\\node [circle] (Y) at (2,0) {};\n\\node [rectangle] (Z) at (1,1) {};\n\\path [-latex] (X) edge (Y);\n\\draw [-latex] (Z) edge (Y);\n\\draw [-latex] (Z) edge (X);\n\\end{tikzpicture}\n```\nYou’ll most likely get an error about Ghostscript:\n#&gt; processing of PostScript specials is disabled (Ghostscript not found)\nConnecting dvisvgm to Ghostscript on macOS\nUnfortunately it takes a little bit of initial setup to get all the moving pieces working for it to happen. To go from TikZ to SVG, knitr first converts the tikz chunk to DVI and then uses the terminal program dvisvgm to convert the DVI to SVG. Getting dvisvgm working on macOS is tricky though (and presumably it’s tricky on Windows too, but I don’t have a Windows machine to test it on).\nThe most typical way of installing the LaTeX ecosystem on macOS is to install MacTeX, which comes with all the dozens of special utilities TeX uses for typesetting. MacTeX comes with both dvisvgm and a special program called Ghostscript that handles text in DVI files. HOWEVER, by default it does not provide a way to have dvisvgm and Ghostscript talk to each other (there’s a whole post about it here). Ghostscript really is installed, though—dvisvgm just can’t see it.\nTo fix it, you actually need to reinstall MacTex and choose a custom installation option. When you get to the “Installation Type” section in the package installation dialog box, click on “Customize” and then make sure the box is checked for “Ghostscript Dynamic Library”. By default this is not checked, and I have no clue why, since this is the bridge that you need to get dvisvgm and Ghostscript to talk to each other.\n\n\n\n\n\n\n\n\nInstall MacTeX with that option enabled and you’ll have a special new file nested deep within your filesystem at /usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53. This libgs.dylib.** file is the dynamic library that dvisvgm needs to know about to work. MacTex 2021 installs Ghostscript 9.53.3, so later versions of MacTex will place libgs.dylib somewhere else.\nTo tell dvisvgm where this dynamic library is, you then need to set an environment variable. Open ~/.bash_profile or ~/.zshrc or whatever you use for setting your PATH and other terminal settings and add this:\nexport LIBGS=/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\nOpen a new terminal window and type print $LIBGS to verify that the environment variable was actually set. Now you should be able to run dvisvgm -l and see an entry for ps in it (see this FAQ page for lots of other details and help for troubleshooting this):\n$ dvisvgm -l\n...\nps         dvips PostScript specials\n...\nThat means everything’s working and you should now be able to convert from DVI to SVG!\nConnecting dvisvgm to Ghostscript within R\nHowever, RStudio doesn’t pick up your shell’s environment variables when knitting files, so if you knit the document with fix.ext='svg' enabled, you’ll still get an error about Ghostscript not being found.\nTo get around this, you can actually set environment variables from within the R Markdown document itself. Add Sys.setenv() to an invisible chunk like this:\n---\ntitle: \"TikZ fun!\"\noutput:\n  html_document: \n    self_contained: no\n---\n\n```{r point-to-ghostscript, include=FALSE}\nSys.setenv(LIBGS = \"/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\")\n```\n\nHere's an empty DAG:\n\n```{tikz empty-dag, echo=FALSE, fig.cap=\"Empty DAG\", fig.align=\"center\", fig.ext=\"svg\"}\n\\usetikzlibrary{positioning}\n\\begin{tikzpicture}[every node/.append style={draw, minimum size=0.5cm}]\n\\node [circle] (X) at (0,0) {};\n\\node [circle] (Y) at (2,0) {};\n\\node [rectangle] (Z) at (1,1) {};\n\\path [-latex] (X) edge (Y);\n\\draw [-latex] (Z) edge (Y);\n\\draw [-latex] (Z) edge (X);\n\\end{tikzpicture}\n```\nAnd now everything should work! You’ll get an HTML file with a vector-based SVG version of the TikZ figure (see full HTML file here):\n\n\n\n\n\n\n\n\nChoosing the TikZ output format based on the knitted output\nWe can now knit this TikZ chunk to PDF if we use fig.ext=\"pdf\" and to SVG if we use fig.ext=\"svg\" in the chunk options. Remembering to switch these extensions manually, though, is tedious. To make life a little easier, we can create some conditional output types using knitr’s (newer?) opts-template ability, which lets you create chunk option templates that you can reuse throughout the document. Here, we’ll use fig.ext=\"pdf\" when knitting to PDF, and fig.ext=\"svg\" otherwise. If I could get the TikZ → PNG conversion working nicely, I’d make it use fig.ext=\"png\" when knitting to Word, too.\n---\ntitle: \"TikZ fun!\"\noutput:\n  html_document: \n    self_contained: no\n  pdf_document: default\n---\n\n```{r point-to-ghostscript, include=FALSE}\nSys.setenv(LIBGS = \"/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\")\n```\n\n```{r setup, include=FALSE}\n# Conditional tikz output types; use PDF if this is LaTeX, otherwise use SVG\nif (knitr::is_latex_output()) {\n  knitr::opts_template$set(\n    tikz_settings = list(fig.ext = \"pdf\", fig.align = \"center\")\n  )\n} else {\n  knitr::opts_template$set(\n    tikz_settings = list(fig.ext = \"svg\", fig.align = \"center\")\n  )\n}\n```\n\nHere's an empty DAG:\n\n```{tikz empty-dag, echo=FALSE, fig.cap=\"Empty DAG\", fig.align=\"center\", fig.ext=\"svg\"}\n\\usetikzlibrary{positioning}\n\\begin{tikzpicture}[every node/.append style={draw, minimum size=0.5cm}]\n\\node [circle] (X) at (0,0) {};\n\\node [circle] (Y) at (2,0) {};\n\\node [rectangle] (Z) at (1,1) {};\n\\path [-latex] (X) edge (Y);\n\\draw [-latex] (Z) edge (Y);\n\\draw [-latex] (Z) edge (X);\n\\end{tikzpicture}\n```\nNow if you knit that file ↑ to PDF, it will create a PDF TikZ image; if you knit to HTML, it will create an SVG image through dvisvgm. Magic!\nFont issues\nYou’ll notice that we’ve only been using text-less TikZ figures as an example so far. TikZ handles text just fine—it processes it through LaTeX and lets you do everything regular LaTeX typesetting does, so you can add fancy equations and Greek letters, etc. to diagrams. Let’s add some letters with subscripts:\n---\ntitle: \"TikZ fun!\"\noutput:\n  html_document: \n    self_contained: no\n---\n\n```{r point-to-ghostscript, include=FALSE}\nSys.setenv(LIBGS = \"/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\")\n```\n\n```{r setup, include=FALSE}\n# Conditional tikz output types; use PDF if this is LaTeX, otherwise use SVG\nif (knitr::is_latex_output()) {\n  knitr::opts_template$set(\n    tikz_settings = list(fig.ext = \"pdf\", fig.align = \"center\")\n  )\n} else {\n  knitr::opts_template$set(\n    tikz_settings = list(fig.ext = \"svg\", fig.align = \"center\")\n  )\n}\n```\n\nHere's a DAG with text:\n\n```{tikz dag-text, echo=FALSE, fig.cap=\"DAG with text\", opts.label=\"tikz_settings\"}\n\\usetikzlibrary{positioning}\n\\begin{tikzpicture}[every node/.append style={draw, minimum size=0.5cm}]\n\\node [draw=none] (X) at (0,0) {$X_{it}$};\n\\node [draw=none] (Y) at (2,0) {$Y_{it}$};\n\\node [rectangle] (Z) at (1,1) {$Z$};\n\\path [-latex] (X) edge (Y);\n\\draw [-latex] (Z) edge (Y);\n\\draw [-latex] (Z) edge (X);\n\\end{tikzpicture}\n```\nWhen knitting to PDF, this diagram looks great, typeset in Computer Modern like any standard LaTeX document(see full PDF here):\n\n\n\n\n\n\n\n\nWhen knitting to HTML, the SVG version of the diagram also works, except for the fonts, since dvisvgm doesn’t embed the fonts by default (see full HTML file here):\n\n\n\n\n\n\n\n\nIf you open the SVG file that R generates (find it at &lt;RMD_filename&gt;_files/figure-html/unnamed-chunk-1-1.svg) in something like Illustrator, it will complain about font references that are missing, like cmmi10 and cmmi7:\n\n\n\n\n\n\n\n\nAnd if you open the SVG file in a text editor you’ll notice references to font names, but those assume that you have fonts named “cmmi7” and “cmmi10” on your computer, which is doubtful:\n...\n&lt;font id='cmmi7' horiz-adv-x='0'&gt;\n&lt;font-face font-family='cmmi7' units-per-em='1000' ascent='750' descent='250'/&gt;\n...\nFont solutions\nFortunately there’s a solution! dvisvgm has an optional flag (--font-format=woff) that will embed the special LaTeX fonts using a base64-encoded web-optimized WOFF file, which means the SVG file should show the correct fonts in any modern browser or editor.\nHowever, there’s currently not a direct way to use that flag with knitr. The command for dvisvgm is hardcoded into knitr’s code and you can’t add any extra arguments (though I’ve opened an issue at GitHub about this).\nBut never fear! There’s a neat roundabout way around this!\nYou can use special chunk hooks in knitr to run functions before or after chunks are rendered. We can create a function that runs after a TikZ chunk is rendered that takes the temporary DVI file that knitr creates and then runs dvisvgm --font-format=woff on it, thus overwriting the non-embedded SVG file that knitr makes with its hardcoded dvisvgm command. Here’s one way to do it (which I adapted from the built-in hook_png() function):\nembed_svg_fonts &lt;- function(before, options, envir) {\n  # !before means after the chunk is run\n  if (!before) {\n    # Get a list of all the plot files this chunk makes\n    paths &lt;- knitr:::get_plot_files()\n\n    # Loop through all the plot files. When dealing with SVG, knitr actually\n    # creates two different files: an initial DVI and the converted SVG. We'll\n    # use the original DVI and pass it through dvisvgm again, this time with\n    # --font-format=woff enabled. This will overwrite the SVG that knitr makes\n    knitr:::in_base_dir(\n      lapply(paths, function(x) {\n        message(\"Embedding fonts in \", x)\n        path_svg &lt;- xfun::with_ext(x, \"svg\")\n        path_dvi &lt;- xfun::with_ext(x, \"dvi\")\n        \n        if (system2('dvisvgm', c('--font-format=woff', '-o', shQuote(path_svg), shQuote(path_dvi))) != 0)\n          stop('Failed to convert ', path_dvi, ' to ', path_svg)\n      })\n    )\n  }\n}\n\n# Register the function as a possible hook\n# Use this in a chunk by setting embed_svg_fonts=TRUE in the chunk options\nknitr::knit_hooks$set(embed_svg_fonts = embed_svg_fonts)\nStick that ↑ in an invisible chunk (probably in the same one with Sys.setenv(LIBGS = \"blah\")), and then add embed_svg_fonts=TRUE to the chunk options of the TikZ chunk, and it should work. When you knit, you’ll see two messages about converting DVI to SVG. We don’t care about the first—that’s knitr’s basic version without embedded fonts. The second message corresponds to the command we created with the hook, which includes --font-format=woff.\nHere’s what the SVG file looks like now with the fonts embedded in it (see full SVG file here):"
  },
  {
    "objectID": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#tldr-final-working-version",
    "href": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#tldr-final-working-version",
    "title": "How to automatically convert TikZ images to SVG (with fonts!) from knitr",
    "section": "tl;dr: Final working version",
    "text": "tl;dr: Final working version\nHere’s what your final R Markdown file should look like:\n---\ntitle: \"TikZ fun!\"\noutput:\n  html_document: \n    self_contained: no\n  pdf_document: default\n---\n\n```{r svg-settings, include=FALSE}\nSys.setenv(LIBGS = \"/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\")\nembed_svg_fonts &lt;- function(before, options, envir) {\n  # !before means after the chunk is run\n  if (!before) {\n    # Get a list of all the plot files this chunk makes\n    paths &lt;- knitr:::get_plot_files()\n    # Loop through all the plot files. When dealing with SVG, knitr actually\n    # creates two different files: an initial DVI and the converted SVG. We'll\n    # use the original DVI and pass it through dvisvgm again, this time with\n    # --font-format=woff enabled. This will overwrite the SVG that knitr makes\n    knitr:::in_base_dir(\n      lapply(paths, function(x) {\n        message(\"Embedding fonts in \", x)\n        path_svg &lt;- xfun::with_ext(x, \"svg\")\n        path_dvi &lt;- xfun::with_ext(x, \"dvi\")\n        \n        if (system2('dvisvgm', c('--font-format=woff', '-o', shQuote(path_svg), shQuote(path_dvi))) != 0)\n          stop('Failed to convert ', path_dvi, ' to ', path_svg)\n      })\n    )\n  }\n}\n# Register the function as a possible hook\n# Use this in a chunk by setting embed_svg_fonts=TRUE in the chunk options\nknitr::knit_hooks$set(embed_svg_fonts = embed_svg_fonts)\n```\n\n```{r setup, include=FALSE}\n# Conditional tikz output types; use PDF if this is LaTeX, otherwise use SVG\nif (knitr::is_latex_output()) {\n  knitr::opts_template$set(\n    tikz_settings = list(fig.ext = \"pdf\", fig.align = \"center\")\n  )\n} else {\n  knitr::opts_template$set(\n    tikz_settings = list(fig.ext = \"svg\", fig.align = \"center\", embed_svg_fonts = TRUE)\n  )\n}\n```\n\nHere's a DAG with text:\n\n```{tikz dag-text, echo=FALSE, fig.cap=\"DAG with text\", opts.label=\"tikz_settings\"}\n\\usetikzlibrary{positioning}\n\\begin{tikzpicture}[every node/.append style={draw, minimum size=0.5cm}]\n\\node [draw=none] (X) at (0,0) {$X_{it}$};\n\\node [draw=none] (Y) at (2,0) {$Y_{it}$};\n\\node [rectangle] (Z) at (1,1) {$Z$};\n\\path [-latex] (X) edge (Y);\n\\draw [-latex] (Z) edge (Y);\n\\draw [-latex] (Z) edge (X);\n\\end{tikzpicture}\n```\nWhen knitting to PDF, the TikZ figure will show up as a regular PDF image, as excepted. When knitting to HTML, though, knitr will create a DVI version of the image and then convert that to SVG (but without font embedding enabled in dvisvgm). Then finally, because of the post-rendering hook, knitr will run the embed_svg_fonts() function on the DVI and create a new SVG with fonts embedded that will overwrite the original SVG file.\nLook at that! So great! (see full HTML file here):"
  },
  {
    "objectID": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#update-easier-font-embedding",
    "href": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#update-easier-font-embedding",
    "title": "How to automatically convert TikZ images to SVG (with fonts!) from knitr",
    "section": "UPDATE: Easier font embedding!",
    "text": "UPDATE: Easier font embedding!\nI submitted a pull request to knitr that would allow users to specify additional arguments to dvisvgm from a chunk, and it was just merged in, so with the development version of knitr (and someday with the CRAN version, once a new version is eventually submitted), you can use the dvisvgm.opts option in the engine.opts chunk options like this:\n```{tikz, fig.ext=\"svg\", engine.opts=list(dvisvgm.opts = \"--font-format=woff\")}\n% Stuff here\n```\nSo now there’s no need for the embed_svg_fonts() hook function! This should work now for the complete DAG example:\n---\ntitle: \"TikZ fun!\"\noutput:\n  html_document: \n    self_contained: no\n  pdf_document: default\n---\n\n```{r point-to-ghostscript, include=FALSE}\nSys.setenv(LIBGS = \"/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\")\n```\n\n```{r setup, include=FALSE}\n# Conditional tikz output types; use PDF if this is LaTeX, otherwise use SVG with embedded fonts\nif (knitr::is_latex_output()) {\n  knitr::opts_template$set(\n    tikz_settings = list(fig.ext = \"pdf\", fig.align = \"center\")\n  )\n} else {\n  knitr::opts_template$set(\n    tikz_settings = list(fig.ext = \"svg\", fig.align = \"center\",\n                         engine.opts = list(dvisvgm.opts = \"--font-format=woff\"))\n  )\n}\n```\n\nHere's a DAG with text:\n\n```{tikz dag-text, echo=FALSE, fig.cap=\"DAG with text\", opts.label=\"tikz_settings\"}\n\\usetikzlibrary{positioning}\n\\begin{tikzpicture}[every node/.append style={draw, minimum size=0.5cm}]\n\\node [draw=none] (X) at (0,0) {$X_{it}$};\n\\node [draw=none] (Y) at (2,0) {$Y_{it}$};\n\\node [rectangle] (Z) at (1,1) {$Z$};\n\\path [-latex] (X) edge (Y);\n\\draw [-latex] (Z) edge (Y);\n\\draw [-latex] (Z) edge (X);\n\\end{tikzpicture}\n```"
  },
  {
    "objectID": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#another-update-changing-fonts",
    "href": "blog/2021/08/27/tikz-knitr-html-svg-fun/index.html#another-update-changing-fonts",
    "title": "How to automatically convert TikZ images to SVG (with fonts!) from knitr",
    "section": "ANOTHER UPDATE: Changing fonts!",
    "text": "ANOTHER UPDATE: Changing fonts!\nYou’re not limited to the default (and kinda ugly) Computer Modern font with TikZ chunks, but changing the font is a little tricky becuase each TikZ chunk in knitr is treated as a standalone LaTeX document, so you need to adjust the fonts in each chunk.\nKnitting only to PDF\nIf you’re only interested in knitting to PDF, you can use XeTeX to embed any font you want in the diagram. When using XeTeX, you’d typically add something like this to the preamble of your document to change the document font:\n\\usepackage{fontspec}\n\\setmainfont{Comic Sans MS}  % ew\nEach standalone TikZ knitr chunk gets passed to a special tikz2pdf.tex template. The content of the chunk is inserted in the %% TIKZ_CODE %% area, and there’s a space in the template for preamble things at %% EXTRA_TIKZ_PREAMBLE_CODE %%. To get content inserted there, you can use the extra.preamble option in engine.opts like so:\n```{r}\nfont_opts &lt;- list(extra.preamble=c(\"\\\\usepackage{fontspec}\", \n                                   \"\\\\setmainfont{Comic Sans MS}\"))\n```\n\n```{tikz, engine.opts=font_opts}\n% Stuff here\n```\nThat won’t work right away though. When you knit, knitr uses the default pdflatex LaTeX engine to render each TikZ chunk, and you’ll get an error because fontspec only works with XeTeX. You can tell knitr to use xelatex to render those chunks if you set the tinytex.engine option:\noptions(tinytex.engine = \"xelatex\")\nOnce you set xelatex as the default LaTeX engine for the document, all TikZ chunks will render correctly. Here’s a complete example (note that I moved the extra.preamble stuff to a separate variable so that it’s easier to reuse) (see full PDF here):\n---\ntitle: \"TikZ fun!\"\noutput:\n  pdf_document: \n    latex_engine: xelatex\n---\n\n```{r setup, include=FALSE}\noptions(tinytex.engine = \"xelatex\")\nfont_opts &lt;- list(extra.preamble = c(\"\\\\usepackage{fontspec}\", \n                                     \"\\\\setmainfont{Comic Sans MS}\",\n                                     \"\\\\usepackage{unicode-math}\",\n                                     \"\\\\usepackage{mathastext}\",\n                                     \"\\\\setmathfont{Comic Sans MS}\"))\n```\n\n```{tikz rectangle-text, echo=FALSE, fig.align=\"center\", engine.opts=font_opts}\n\\begin{tikzpicture}\n\\draw (0,0) rectangle (6, 2) node[midway, align=center] {I am a rectangle with math: \\\\ $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\text{Some text} + \\varepsilon$};\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n\nKnitting to PDF and HTML\nThis is a little trickier if you want to convert these TikZ chunks to SVG when knitting to HTML. knitr ignores the tinytex.engine option when you knit to HTML, so there’s no way to tell TikZ chunks to get processed through xelatex. Additionally, xelatex does not produce .dvi files, which dvisvgm uses to convert to SVG. xelatex instead creates its own special .xdv files. In theory, newer versions of dvisvgm should work with XDV files, but knitr is hard-coded to pass DVI files to dvisvgm, not XDV files. So even if we could force TikZ chunks to go through xelatex, they wouldn’t be converted to SVG. Alas.\nBut all is not lost! It’s possible to change fonts in non-XeTeX documents. Because you’re limited to whatever pdflatex can support, you can only use specific font packages instead of whatever font you want. For instance, to use TEX Gyre Pagella, include \\usepackage{tgpagella} in your preamble. See this for a basic list of possible font packages, or this for a pretty complete catalogue.\nFor instance, here’s how to use Linux Libertine for text and Libertinus Math for math with embedded SVG fonts when knitting to HTML (see full HTML file here):\n---\ntitle: \"TikZ fun!\"\noutput:\n  html_document: default\n---\n\n```{r point-to-ghostscript, include=FALSE}\nSys.setenv(LIBGS = \"/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\")\n```\n\n```{r setup, include=FALSE}\nfont_opts &lt;- list(extra.preamble = c(\"\\\\usepackage{libertine}\", \"\\\\usepackage{libertinust1math}\"),\n                  dvisvgm.opts = \"--font-format=woff\")\n```\n\n```{tikz rectangle-text, fig.ext=\"svg\", echo=FALSE, fig.align=\"center\", engine.opts=font_opts}\n\\begin{tikzpicture}\n\\draw (0,0) rectangle (6, 2) node[midway, align=center] {I am a rectangle with math: \\\\ $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\text{Some text} + \\varepsilon$};\n\\end{tikzpicture}\n```\n\n\n\n\n\n\n\n\nOr with GFS Artemisia with Euler for math (see full HTML file here):\n---\ntitle: \"TikZ fun!\"\noutput:\n  html_document: default\n---\n\n```{r point-to-ghostscript, include=FALSE}\nSys.setenv(LIBGS = \"/usr/local/share/ghostscript/9.53.3/lib/libgs.dylib.9.53\")\n```\n\n```{r setup, include=FALSE}\nfont_opts &lt;- list(extra.preamble = c(\"\\\\usepackage{gfsartemisia-euler}\"),\n                  dvisvgm.opts = \"--font-format=woff\")\n```\n\n```{tikz rectangle-text, fig.ext=\"svg\", echo=FALSE, fig.align=\"center\", engine.opts=font_opts}\n\\begin{tikzpicture}\n\\draw (0,0) rectangle (6, 2) node[midway, align=center] {I am a rectangle with math: \\\\ $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\text{Some text} + \\varepsilon$};\n\\end{tikzpicture}\n```"
  },
  {
    "objectID": "blog/2021/11/08/beta-regression-guide/index.html",
    "href": "blog/2021/11/08/beta-regression-guide/index.html",
    "title": "A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models",
    "section": "",
    "text": "In the data I work with, it’s really common to come across data that’s measured as proportions: the percent of women in the public sector workforce, the amount of foreign aid a country receives as a percent of its GDP, the percent of religious organizations in a state’s nonprofit sector, and so on.\nWhen working with this kind of data as an outcome variable (or dependent variable) in a model, analysis gets tricky if you use standard models like OLS regression. For instance, look at this imaginary simulated dataset of the relationship between some hypothetical x and y:\nY here is measured as a percentage and ranges between 0 and 1 (i.e. between the red lines), but the fitted line from a linear model creates predictions that exceed the 0–1 range (see the blue line in the top right corner). Since we’re thinking about proportions, we typically can’t exceed 0% and 100%, so it would be great if our modeling approach took those limits into account.\nThere are a bunch of different ways to model proportional data that vary substantially in complexity. In this post, I’ll explore four, but I’ll mostly focus on beta and zero-inflated beta regression:\nThroughout this example, we’ll use data from the Varieties of Democracy project (V-Dem) to answer a question common in comparative politics: do countries with parliamentary gender quotas (i.e. laws and regulations that require political parties to have a minimum proportion of women members of parliament (MPs)) have more women MPs in their respective parliaments? This question is inspired by existing research that looks at the effect of quotas on the proportion of women MPs, and a paper by Tripp and Kang (2008) was my first exposure to fractional logistic regression as a way to handle proportional data. The original data from that paper is available at Alice Kang’s website, but to simplify the post here, we won’t use it—we’ll just use a subset of equivalent data from V-Dem.\nThere’s a convenient R package for accessing V-Dem data, so we’ll use that to make a smaller panel of countries between 2010 and 2020. Let’s load all the libraries we need, clean up the data, and get started!\nlibrary(tidyverse)        # ggplot, dplyr, %&gt;%, and friends\nlibrary(brms)             # Bayesian modeling through Stan\nlibrary(tidybayes)        # Manipulate Stan objects in a tidy way\nlibrary(broom)            # Convert model objects to data frames\nlibrary(broom.mixed)      # Convert brms model objects to data frames\nlibrary(vdemdata)         # Use data from the Varieties of Democracy (V-Dem) project\nlibrary(betareg)          # Run beta regression models\nlibrary(extraDistr)       # Use extra distributions like dprop()\nlibrary(ggdist)           # Special geoms for posterior distributions\nlibrary(gghalves)         # Special half geoms\nlibrary(ggbeeswarm)       # Special distribution-shaped point jittering\nlibrary(ggrepel)          # Automatically position labels\nlibrary(patchwork)        # Combine ggplot objects\nlibrary(scales)           # Format numbers in nice ways\nlibrary(marginaleffects)  # Calculate marginal effects for regression models\nlibrary(modelsummary)     # Create side-by-side regression tables\n\nset.seed(1234)  # Make everything reproducible\n\n# Define the goodness-of-fit stats to include in modelsummary()\ngof_stuff &lt;- tribble(\n  ~raw, ~clean, ~fmt,\n  \"nobs\", \"N\", 0,\n  \"r.squared\", \"R²\", 3\n)\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Barlow+Semi+Condensed\ntheme_clean &lt;- function() {\n  theme_minimal(base_family = \"Barlow Semi Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(family = \"BarlowSemiCondensed-Bold\"),\n          axis.title = element_text(family = \"BarlowSemiCondensed-Medium\"),\n          strip.text = element_text(family = \"BarlowSemiCondensed-Bold\",\n                                    size = rel(1), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA))\n}\n\n# Make labels use Barlow by default\nupdate_geom_defaults(\"label_repel\", list(family = \"Barlow Semi Condensed\"))\n\n# Format things as percentage points\nlabel_pp &lt;- label_number(accuracy = 1, scale = 100, \n                         suffix = \" pp.\", style_negative = \"minus\")\nlabel_pp_tiny &lt;- label_number(accuracy = 0.01, scale = 100, \n                              suffix = \" pp.\", style_negative = \"minus\")\nV-Dem covers all countries since 1789 and includes hundreds of different variables. We’ll make a subset of some of the columns here and only look at years from 2010–2020. We’ll also make a subset of that and have a dataset for just 2015.\n# Make a subset of the full V-Dem data\nvdem_clean &lt;- vdem %&gt;% \n  select(country_name, country_text_id, year, region = e_regionpol_6C,\n         polyarchy = v2x_polyarchy, corruption = v2x_corr, \n         civil_liberties = v2x_civlib, prop_fem = v2lgfemleg, v2lgqugen) %&gt;% \n  filter(year &gt;= 2010, year &lt; 2020) %&gt;% \n  drop_na(v2lgqugen, prop_fem) %&gt;% \n  mutate(quota = v2lgqugen &gt; 0,\n         prop_fem = prop_fem / 100,\n         polyarchy = polyarchy * 100)\n\nvdem_2015 &lt;- vdem_clean %&gt;% \n  filter(year == 2015) %&gt;% \n  # Sweden and Denmark are tied for the highest polyarchy score (91.5), and R's\n  # max() doesn't deal with ties, so we cheat a little and add a tiny random\n  # amount of noise to each polyarchy score, mark the min and max of that\n  # perturbed score, and then remove that temporary column\n  mutate(polyarchy_noise = polyarchy + rnorm(n(), 0, sd = 0.01)) %&gt;% \n  mutate(highlight = polyarchy_noise == max(polyarchy_noise) | \n           polyarchy_noise == min(polyarchy_noise)) %&gt;% \n  select(-polyarchy_noise)"
  },
  {
    "objectID": "blog/2021/11/08/beta-regression-guide/index.html#explore-the-data",
    "href": "blog/2021/11/08/beta-regression-guide/index.html#explore-the-data",
    "title": "A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models",
    "section": "Explore the data",
    "text": "Explore the data\nBefore trying to model this data, we’ll look at it really quick first. Here’s the distribution of prop_fem across the two different values of quota. In general, it seems that countries without a gender-based quota have fewer women MPs, which isn’t all that surprising, since quotas were designed to boost the number of women MPs in the first place.\n\nquota_halves &lt;- ggplot(vdem_2015, aes(x = quota, y = prop_fem)) +\n  geom_half_point(aes(color = quota), \n                  transformation = position_quasirandom(width = 0.1),\n                  side = \"l\", size = 0.5, alpha = 0.5) +\n  geom_half_boxplot(aes(fill = quota), side = \"r\") + \n  scale_y_continuous(labels = label_percent()) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.8) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8) +\n  guides(color = \"none\", fill = \"none\") +\n  labs(x = \"Quota\", y = \"Proportion of women in parliament\") +\n  theme_clean()\n\nquota_densities &lt;- ggplot(vdem_2015, aes(x = prop_fem, fill = quota)) +\n  geom_density(alpha = 0.6) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.8) +\n  labs(x = \"Proportion of women in parliament\", y = \"Density\", fill = \"Quota\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\nquota_halves | quota_densities\n\n\n\n\n\n\n\nAnd here’s what the proportion of women MPs looks like across different levels of electoral democracy. V-Dem’s polyarchy index measures the extent of electoral democracy in different countries, incorporating measures of electoral, freedom of association, and universal suffrage, among other factors. It ranges from 0 to 1, but to make it easier to interpret in regression, we’ll multiply it by 100 so we can talk about unit changes in democracy. To help with the intuition of the index, we’ll highlight the countries with the minimum and maximum values of democracy. In general, the proportion of women MPs increases as democracy increases.\n\nggplot(vdem_2015, aes(x = polyarchy, y = prop_fem)) +\n  geom_point(aes(color = highlight), size = 1) +\n  geom_smooth(method = \"lm\") +\n  geom_label_repel(data = filter(vdem_2015, highlight == TRUE), \n                   aes(label = country_name),\n                   seed = 1234) +\n  scale_y_continuous(labels = label_percent()) +\n  scale_color_manual(values = c(\"grey30\", \"#FF4136\"), guide = \"none\") +\n  labs(x = \"Polyarchy (democracy)\", y = \"Proportion of women in parliament\") +\n  theme_clean()"
  },
  {
    "objectID": "blog/2021/11/08/beta-regression-guide/index.html#linear-probability-models",
    "href": "blog/2021/11/08/beta-regression-guide/index.html#linear-probability-models",
    "title": "A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models",
    "section": "1: Linear probability models",
    "text": "1: Linear probability models\nIn the world of econometrics, having the fitted regression line go outside the 0–1 range (like in the plot at the very beginning of this post) is totally fine and not a problem. Economists love to use a thing called a linear probability model (LPM) to model data like this, and for values that aren’t too extreme (generally if the outcome ranges between 0.2 and 0.8), the results from these weirdly fitting linear models are generally equivalent to fancier non-linear models. LPMs are even arguably best for experimental data. But it’s still a weird way to think about data and I don’t like them. As Alex Hayes says,\n\n[T]he thing [LPM] is generally inconsistent and aesthetically offensive, but whatever, it works on occasion.\n\nAn LPM is really just regular old OLS applied to a binary or proportional outcome, so we’ll use trusty old lm().\n\n# Linear probability models\nmodel_ols1 &lt;- lm(prop_fem ~ quota,\n                 data = vdem_2015)\n\nmodel_ols2 &lt;- lm(prop_fem ~ polyarchy,\n                 data = vdem_2015)\n\nmodel_ols3 &lt;- lm(prop_fem ~ quota + polyarchy,\n                 data = vdem_2015)\n\n\nmodelsummary(list(model_ols1, model_ols2, model_ols3),\n             gof_map = gof_stuff)\n\n\n\n\n (1)\n  (2)\n  (3)\n\n\n\n(Intercept)\n0.195\n0.154\n0.132\n\n\n\n(0.012)\n(0.020)\n(0.022)\n\n\nquotaTRUE\n0.047\n\n0.049\n\n\n\n(0.018)\n\n(0.018)\n\n\npolyarchy\n\n0.001\n0.001\n\n\n\n\n(0.000)\n(0.000)\n\n\nN\n172\n172\n172\n\n\nR²\n0.038\n0.060\n0.101\n\n\n\n\n\nBased on this, having a quota is associated with a 4.7 percentage point increase in the proportion of women MPs on average, and the coefficient is statistically significant. If we control for polyarchy, the effect changes to 4.9 percentage points. Great."
  },
  {
    "objectID": "blog/2021/11/08/beta-regression-guide/index.html#fractional-logistic-regression",
    "href": "blog/2021/11/08/beta-regression-guide/index.html#fractional-logistic-regression",
    "title": "A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models",
    "section": "2: Fractional logistic regression",
    "text": "2: Fractional logistic regression\nLogistic regression is normally used for binary outcomes, but surprisingly you can actually use it for proportional data too! This kind of model is called fractional logistic regression, and though it feels weird to use logistic regression with non-binary data, it’s legal! The Stata documentation actually recommends it, and there are tutorials (like this and this) papers about how to do it with Stata. This really in-depth post by Michael Clark shows several ways to run these models with R.\nBasically use regular logistic regression with glm(..., family = binomial(link = \"logit\")) with an outcome variable that ranges from 0 to 1, and you’re done. R will give you a warning when you use family = binomial() with a non-binary outcome variable, but it will still work. If you want to suppress that warning, use family = quasibinomial() instead.\n\nmodel_frac_logit1 &lt;- glm(prop_fem ~ quota, \n                         data = vdem_2015, \n                         family = quasibinomial())\n\nmodel_frac_logit2 &lt;- glm(prop_fem ~ polyarchy, \n                         data = vdem_2015, \n                         family = quasibinomial())\n\nmodel_frac_logit3 &lt;- glm(prop_fem ~ quota + polyarchy, \n                         data = vdem_2015, \n                         family = quasibinomial())\n\n\nmodelsummary(list(model_frac_logit1, model_frac_logit2, model_frac_logit3),\n             gof_map = gof_stuff)\n\n\n\n\n (1)\n  (2)\n  (3)\n\n\n\n(Intercept)\n−1.417\n−1.667\n−1.814\n\n\n\n(0.073)\n(0.129)\n(0.140)\n\n\nquotaTRUE\n0.276\n\n0.293\n\n\n\n(0.107)\n\n(0.106)\n\n\npolyarchy\n\n0.007\n0.007\n\n\n\n\n(0.002)\n(0.002)\n\n\nN\n172\n172\n172\n\n\n\n\n\nInterpreting this model is a little more involved than interpreting OLS. The coefficients in logistic regression are provided on different scale than the original data. With OLS (and the linear probability model), coefficients show the change in the probability (or proportion in this case, since we’re working with proportion data) that is associated with a one-unit change in an explanatory variable. With logistic regression, coefficients show the change in the natural logged odds of the outcome, also known as “logits.”\nThis log odds scale is weird and not very intuitive normally, so often people will convert these log odds into odds ratios by exponentiating them. I’ve done this my whole statistical-knowing-and-doing life. If a model provides a coefficient of 0.4, you can exponentiate that with \\(e^{0.4}\\), resulting in an odds ratio of 1.49. You’d interpret this by saying something like “a one unit change in x increases the likelihood of y by 49%.” That sounds neat and soundbite-y and you see stuff like this all the time in science reporting. But thinking about percent changes in probability is still a weird way of thinking (though admittedly less non-intuitive than thinking about logits/logged odds).\nFor instance, here the coefficient for quota in Model 1 is 0.276. If we exponentiate that (\\(e^{0.276}\\)) we get 1.32. If our outcome were binary, we could say that having a quota makes it 32% more likely that the outcome would occur. But our outcome isn’t binary—it’s a proportion—so we need to think about changes in probabilities or proportions instead, and odds ratios make that really really hard.\nInstead of odds ratios, we need to work directly with these logit-scale values. For a phenomenally excellent overview of binary logistic regression and how to interpret coefficients, see Steven Miller’s most excellent lab script here—because of that post I’m now far less afraid of dealing with logit-scale coefficients!\nWe can invert a logit value and convert it to a probability using the plogis() function in R. We’ll illustrate this a couple different ways, starting with a super simple intercept-only model:\n\nmodel_frac_logit0 &lt;- glm(prop_fem ~ 1, \n                         data = vdem_2015, \n                         family = quasibinomial())\ntidy(model_frac_logit0)\n## # A tibble: 1 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)    -1.29    0.0540     -24.0 1.24e-56\n\nlogit0_intercept &lt;- model_frac_logit0 %&gt;% \n  tidy() %&gt;% \n  filter(term == \"(Intercept)\") %&gt;% \n  pull(estimate)\n\n# The intercept in logits. Who even knows what this means.\nlogit0_intercept\n## [1] -1.29\n\nThe intercept of this intercept-only model is -1.295 in logit units, whatever that means. We can convert this to a probability though:\n\n# Convert logit to a probability (or proportion in this case)\nplogis(logit0_intercept)\n## [1] 0.215\n\nAccording to this probability/proportion, the average proportion of women MPs in the dataset is 0.215. Let’s see if that’s really the case:\n\nmean(vdem_2015$prop_fem)\n## [1] 0.215\n\nIt’s the same! That’s so neat!\nInverting logit coefficients to probabilities gets a little trickier when there are multiple moving parts, though. With OLS, each coefficient shows the marginal change in the outcome for each unit change in the explanatory variable. With logistic regression, that’s not the case—we have to incorporate information from the intercept in order to get marginal effects.\nFor example, in Model 1, the log odds coefficient for quota is 0.276. If we invert that with plogis(), we end up with a really big number:\n\nlogit1_intercept &lt;- model_frac_logit1 %&gt;% \n  tidy() %&gt;% \n  filter(term == \"(Intercept)\") %&gt;% \n  pull(estimate)\n\nlogit1_quota &lt;- model_frac_logit1 %&gt;% \n  tidy() %&gt;% \n  filter(term == \"quotaTRUE\") %&gt;% \n  pull(estimate)\n\n# Incorrect marginal effect of quota\nplogis(logit1_quota)\n## [1] 0.568\n\nIf that were true, we could say that having a quota increases the proportion of women MPs by nearly 60%! But that’s wrong. We have to incorporate information from the intercept, as well as any other coefficients in the model, like so:\n\nplogis(logit1_intercept + logit1_quota) - plogis(logit1_intercept)\n## [1] 0.0469\n\nThis is the effect of having a quota on the proportion of women MPs. Having a quota increases the proportion by 4.7 percentage points, on average.\nThe math of combining these different logit coefficients and converting them to probabilities gets really tricky once there are more than one covariate, like in Model 3 here where we include both quota and polyarchy. Rather than try to figure out, we can instead calculate the marginal effects of specific variables while holding others constant. In practice this entails plugging a custom hypothetical dataset into our model and estimating the predicted values of the outcome. For instance, we can create a small dataset with just two rows: in one row, quota is FALSE and in the other quota is TRUE, while we hold polyarchy at its mean value. The marginaleffects package makes this really easy to do:\n\n# Calculate the typical values for all variables in Model 3, except for quota,\n# which we'll force to be FALSE and TRUE\ndata_to_predict &lt;- datagrid(model = model_frac_logit3, \n                            quota = c(FALSE, TRUE))\ndata_to_predict\n##   prop_fem polyarchy quota\n## 1    0.215      53.2 FALSE\n## 2    0.215      53.2  TRUE\n\nNext we can use predictions() from the marginaleffects package to plug this data into Model 3 and figure out the fitted values for each of the rows:\n\n# Plug this new dataset into the model\nfrac_logit_pred &lt;- predictions(model_frac_logit3, newdata = data_to_predict)\nfrac_logit_pred\n## \n##  Estimate Pr(&gt;|z|) 2.5 % 97.5 % polyarchy quota\n##     0.192   &lt;0.001 0.171  0.215      53.2 FALSE\n##     0.242   &lt;0.001 0.215  0.271      53.2  TRUE\n## \n## Columns: rowid, estimate, p.value, conf.low, conf.high, prop_fem, polyarchy, quota\n\nThe estimate column here tells us the predicted or fitted value. When countries don’t have a quota, the average proportion of women MPs is 0.192; when they do have a quota, the average predicted proportion is 0.192. That’s a 4.96 percentage point difference, which is really similar to what we found from Model 1. We can plot these marginal effects too and see that this difference is statistically significant:\n\nggplot(frac_logit_pred, aes(x = quota, y = estimate)) +\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n  scale_y_continuous(labels = label_percent(accuracy = 1)) +\n  labs(x = \"Quota\", y = \"Predicted proportion of women in parliament\") +\n  theme_clean()\n\n\n\n\n\n\n\nWe can also compute marginal effects for polyarchy, holding quota at FALSE (its most common value) and ranging polyarchy from 0 to 100. And we technically don’t need to make a separate data frame first—we can use datagrid() inside predictions():\n\nfrac_logit_pred &lt;- predictions(model_frac_logit3, \n                               newdata = datagrid(polyarchy = seq(0, 100, by = 1)))\n\nggplot(frac_logit_pred, aes(x = polyarchy, y = estimate)) + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.4, fill = \"#480B6A\") +\n  geom_line(linewidth = 1, color = \"#480B6A\") +\n  scale_y_continuous(labels = label_percent()) +\n  labs(x = \"Polyarchy (democracy)\", \n       y = \"Predicted proportion of women in parliament\") +\n  theme_clean()\n\n\n\n\n\n\n\nThe predicted proportion of women MPs increases as democracy increases, which follows from the model results. But what’s the marginal effect of a one-unit increase in polyarchy? The coefficient from Model 3 was really small (0.007 logits). We can convert this to a probability while correctly accounting for the intercept and all other coefficients with the avg_slopes() function:\n\navg_slopes(model_frac_logit3, variables = \"polyarchy\")\n## \n##       Term Estimate Std. Error    z Pr(&gt;|z|)   2.5 %  97.5 %\n##  polyarchy  0.00119   0.000349 3.42   &lt;0.001 0.00051 0.00188\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nThe effect here is still small: a one-unit change in polyarchy (i.e. moving from a 56 to a 57) is associated with a 0.119 percentage point increase in the proportion of women MPs, on average. That’s not huge, but a one-unit change in polyarchy also isn’t huge. If a country moves 10 points from 56 to 66, for instance, there’d be a ≈10x increase in the effect (0.001 × 10, or 0.012). This is apparent in the predicted proportion when polyarchy is set to 56 and 66: 0.207 - 0.196 = 0.011\n\npredictions(model_frac_logit3,\n            newdata = datagrid(polyarchy = c(56, 66)))\n## \n##  Estimate Pr(&gt;|z|) 2.5 % 97.5 % quota polyarchy\n##     0.196   &lt;0.001 0.174  0.219 FALSE        56\n##     0.207   &lt;0.001 0.184  0.232 FALSE        66\n## \n## Columns: rowid, estimate, p.value, conf.low, conf.high, prop_fem, quota, polyarchy\n\nFinally, how do these fractional logistic regression coefficients compare to what we found with the linear probability model? We can look at them side-by-side:\n\nmodelsummary(list(\"OLS\" = model_ols3,\n                  \"Fractional logit\" = model_frac_logit3,\n                  \"Fractional logit&lt;br&gt;(marginal effects)\" = marginaleffects(model_frac_logit3)),\n             gof_map = gof_stuff,\n             fmt = 4,\n             escape = FALSE)\n\n\n\n\n\n\n\n\n\n\nOLS\nFractional logit\nFractional logit\n(marginal effects)\n\n\n\n(Intercept)\n0.1316\n−1.8136\n\n\n\n\n(0.0217)\n(0.1396)\n\n\n\nquotaTRUE\n0.0491\n0.2928\n\n\n\n\n(0.0176)\n(0.1057)\n\n\n\npolyarchy\n0.0012\n0.0071\n0.0012\n\n\n\n(0.0003)\n(0.0021)\n(0.0003)\n\n\nquota\n\n\n0.0496\n\n\n\n\n\n(0.0181)\n\n\nN\n172\n172\n172\n\n\nR²\n0.101\n\n\n\n\n\n\n\nThey’re basically the same! Having a quota is associated with a 4.96 percentage point increase in the proportion of women MPs, on average, in both OLS and the fractional logit model. That’s so neat!"
  },
  {
    "objectID": "blog/2021/11/08/beta-regression-guide/index.html#interlude-the-beta-distribution-and-distributional-regression",
    "href": "blog/2021/11/08/beta-regression-guide/index.html#interlude-the-beta-distribution-and-distributional-regression",
    "title": "A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models",
    "section": "Interlude: The beta distribution and distributional regression",
    "text": "Interlude: The beta distribution and distributional regression\nThe OLS-based linear probability model works (begrudgingly), and fractional logistic regression works (but it feels kinda weird to use logistic regression on proportions like that). The third method we’ll explore is beta regression, which is a little different from linear regression. With regular old linear regression, we’re essentially fitting a line to data. We find an intercept, we find a slope, and we draw a line. We can add some mathematical trickery to make sure the line stays within specific bounds (like using logits and logged odds), but fundamentally everything just has intercepts and slopes.\nAn alternative to this kind of mean-focused regression is to use something called distributional regression (Kneib, Silbersdorff, and Säfken 2021) to estimate not lines and averages, but the overall shapes of statistical distributions.\nBeta regression is one type of distributional regression, but before we explore how it works, we have to briefly review how distributions work.\nIn statistics, there are all sorts of probability distributions that can represent the general shape and properties of a variable (I have a whole guide for generating data using the most common distributions here). For instance, in a normal distribution, most of the values are clustered around a mean, and values are spread out based on some amount of variance, or standard deviation.\nHere are two normal distributions defined by different means and standard deviations. One is centered at 5 with a fairly wide standard deviation of 4 (so 95% of its values range from 5 ± (4 × 1.96), or from −3ish to 13ish), while the other is centered at 10 with a narrower standard deviation of 2 (so 95% of its values range from 10 ± (2 × 1.96), or from 6 to 14):\n\nggplot(data = tibble(x = seq(-10, 20)), aes(x = x)) +\n  geom_function(fun = dnorm, args = list(mean = 5, sd = 4),\n                aes(color = \"Normal(mean = 5, sd = 4)\"), linewidth = 1) +\n  geom_function(fun = dnorm, args = list(mean = 10, sd = 2),\n                aes(color = \"Normal(mean = 10, sd = 2)\"), linewidth = 1) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nMost other distributions are defined in similar ways: there’s a central tendency (often the mean), and there’s some spread or variation around that center.\nBeta distributions and shape parameters\nOne really common distribution that’s perfect for percentages and proportions is the beta distribution, which is naturally limited to numbers between 0 and 1 (but importantly doesn’t include 0 or 1). The beta distribution is an extremely flexible distribution and can take all sorts of different shapes and forms (stare at this amazing animated GIF for a while to see all the different shapes!)\nUnlike a normal distribution, where you use the mean and standard deviation as the distributional parameters, beta distributions take two non-intuitive parameters: (1) shape1 and (2) shape2, often abbreviated as \\(a\\) and \\(b\\). This answer at Cross Validated does an excellent job of explaining the intuition behind beta distributions and it’d be worth it to read it. Go do that—I’ll wait.\nBasically beta distributions are good at modeling the probabilities of things, and shape1 and shape2 represent specific parts of a formula for probabilities and proportions.\nLet’s say that there’s an exam with 10 points where most people score a 6/10. Another way to think about this is that an exam is a collection of correct answers and incorrect answers, and that the percent correct follows this equation:\n\\[\n\\frac{\\text{Number correct}}{\\text{Number correct} + \\text{Number incorrect}}\n\\]\nIf you scored a 6, you could write that as:\n\\[\n\\frac{6}{6+4} = \\frac{6}{10}\n\\]\nTo make this formula more general, we can use variable names: \\(a\\) for the number correct and \\(b\\) for the number incorrect, leaving us with this:\n\\[\n\\frac{a}{a + b}\n\\]\nIn a beta distribution, the \\(a\\) and the \\(b\\) in that equation correspond to the shape1 and shape2 parameters. If we want to look at the distribution of scores for this test where most people get 6/10, or 60%, we can use 6 and 4 as parameters. Most people score around 60%, and the distribution isn’t centered—it’s asymmetric. Neat!\n\nggplot() +\n  geom_function(fun = dbeta, args = list(shape1 = 6, shape2 = 4),\n                aes(color = \"Beta(shape1 = 6, shape2 = 4)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", name = \"\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nThe magic of—and most confusing part about—beta distributions is that you can get all sorts of curves by just changing the shape parameters. To make this easier to see, we can make a bunch of different beta distributions.\n\nggplot() +\n  geom_function(fun = dbeta, args = list(shape1 = 6, shape2 = 4),\n                aes(color = \"Beta(shape1 = 6, shape2 = 4)\"),\n                size = 1) +\n  geom_function(fun = dbeta, args = list(shape1 = 60, shape2 = 40),\n                aes(color = \"Beta(shape1 = 60, shape2 = 40)\"),\n                size = 1) +\n  geom_function(fun = dbeta, args = list(shape1 = 9, shape2 = 1),\n                aes(color = \"Beta(shape1 = 9, shape2 = 1)\"),\n                size = 1) +\n  geom_function(fun = dbeta, args = list(shape1 = 2, shape2 = 11),\n                aes(color = \"Beta(shape1 = 2, shape2 = 11)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\",\n                        guide = guide_legend(nrow = 2)) +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nTo figure out the center of each of these distributions, think of the \\(\\frac{a}{a+b}\\) formula. For the blue distribution on the far left, for instance, it’s \\(\\frac{2}{2+11}\\) or 0.154. The orange distribution on the far right is centered at \\(\\frac{9}{9+1}\\), or 0.9. The tall pink-ish distribution is centered at 0.6 (\\(\\frac{60}{60+40}\\)), just like the \\(\\frac{6}{6+4}\\) distribution, but it’s much narrower and less spread out. When working with these two shape parameters, you control the variance or spread of the distribution by scaling the values up or down.\nMean and precision instead of shapes\nBut thinking about these shapes and manually doing the \\(\\frac{a}{a+b}\\) calculation in your head is hard! It’s even harder to get a specific amount of spread. Most other distributions can be defined with a center and some amount of spread or variance, but with beta distributions you’re stuck with these weirdly interacting shape parameters.\nFortunately there’s an alternative way of parameterizing the beta distribution that uses a mean \\(\\mu\\) and precision \\(\\phi\\) (the same idea as variance) instead of these strange shapes.\nThese shapes and the \\(\\mu\\) and \\(\\phi\\) parameters are mathematically related and interchangeable. Formally, the two shapes can be defined using \\(\\mu\\) and \\(\\phi\\) like so:\n\\[\n\\begin{aligned}\n\\text{shape1 } (a) &= \\mu \\times \\phi \\\\\n\\text{shape2 } (b) &= (1 - \\mu) \\times \\phi\n\\end{aligned}\n\\]\nBased on this, we can do some algebra to figure out how to write \\(\\mu\\) and \\(\\phi\\) using \\(a\\) and \\(b\\):\n\\[\n\\begin{equation}\n\\begin{aligned}[t]\n&\\text{[get } \\phi \\text{ alone]} \\\\\n\\mu \\times \\phi &= a \\\\\n\\phi &= \\frac{a}{\\mu}\n\\end{aligned}\n\\qquad\n\\begin{aligned}[t]\n&\\text{[solve for } \\mu \\text{]} \\\\\nb &= (1 - \\mu) \\times \\phi \\\\\nb &= (1 - \\mu) \\times \\frac{a}{\\mu} \\\\\nb &= \\frac{a}{\\mu} - \\frac{a \\mu}{\\mu} \\\\\nb &= \\frac{a}{\\mu} - a \\\\\na + b &= \\frac{a}{\\mu} \\\\\n\\color{orange}{\\mu} &\\color{orange}{= \\frac{a}{a + b}}\n\\end{aligned}\n\\qquad\n\\begin{aligned}[t]\n&\\text{[solve for } \\phi \\text{]} \\\\\na &= \\mu \\times \\phi \\\\\na &= \\left( \\frac{a}{a + b} \\right) \\times \\phi \\\\\na &= \\frac{a \\phi}{a + b} \\\\\n(a + b) a &= a \\phi \\\\\na^2 + ba &= a \\phi \\\\\n\\color{orange}{\\phi} &\\color{orange}{= a + b}\n\\end{aligned}\n\\end{equation}\n\\]\nIt’s thus possible to translate between these two parameterizations:\n\\[\n\\begin{equation}\n\\begin{aligned}[t]\n\\text{Shape 1:} && a &= \\mu \\phi \\\\\n\\text{Shape 2:} && b &= (1 - \\mu) \\phi\n\\end{aligned}\n\\qquad\\qquad\\qquad\n\\begin{aligned}[t]\n\\text{Mean:} && \\mu &= \\frac{a}{a + b} \\\\\n\\text{Precision:} && \\phi &= a + b\n\\end{aligned}\n\\end{equation}\n\\]\nTo help with the intuition, we can make a couple little functions to switch between them.\n\nshapes_to_muphi &lt;- function(shape1, shape2) {\n  mu &lt;- shape1 / (shape1 + shape2)\n  phi &lt;- shape1 + shape2\n  return(list(mu = mu, phi = phi))\n}\n\nmuphi_to_shapes &lt;- function(mu, phi) {\n  shape1 &lt;- mu * phi\n  shape2 &lt;- (1 - mu) * phi\n  return(list(shape1 = shape1, shape2 = shape2))\n}\n\nRemember our initial distribution where shape1 was 6 and shape2 was 4? Here’s are the parameters for that using \\(\\mu\\) and \\(\\phi\\) instead:\n\nshapes_to_muphi(6, 4)\n## $mu\n## [1] 0.6\n## \n## $phi\n## [1] 10\n\nIt has a mean of 0.6 and a precision of 10. That more precise and taller distribution where shape1 was 60 and shape2 was 40?\n\nshapes_to_muphi(60, 40)\n## $mu\n## [1] 0.6\n## \n## $phi\n## [1] 100\n\nIt has the same mean of 0.6, but a much higher precision (100 now instead of 10).\nR has built-in support for the shape-based beta distribution with things like dbeta(), rbeta(), etc. We can work with this reparameterized \\(\\mu\\)- and \\(\\phi\\)-based beta distribution using the dprop() (and rprop(), etc.) from the extraDistr package. It takes two arguments: size for \\(\\phi\\) and mean for \\(\\mu\\).\n\nbeta_shapes &lt;- ggplot() +\n  geom_function(fun = dbeta, args = list(shape1 = 6, shape2 = 4),\n                aes(color = \"dbeta(shape1 = 6, shape2 = 4)\"),\n                size = 1) +\n  geom_function(fun = dbeta, args = list(shape1 = 60, shape2 = 40),\n                aes(color = \"dbeta(shape1 = 60, shape2 = 40)\"),\n                size = 1) +\n  geom_function(fun = dbeta, args = list(shape1 = 9, shape2 = 1),\n                aes(color = \"dbeta(shape1 = 9, shape2 = 1)\"),\n                size = 1) +\n  geom_function(fun = dbeta, args = list(shape1 = 2, shape2 = 11),\n                aes(color = \"dbeta(shape1 = 2, shape2 = 11)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\",\n                        guide = guide_legend(ncol = 1)) +\n  labs(title = \"Shape-based beta distributions\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\nbeta_mu_phi &lt;- ggplot() +\n  geom_function(fun = dprop, args = list(mean = 0.6, size = 10),\n                aes(color = \"dprop(mean = 0.6, size = 10)\"),\n                size = 1) +\n  geom_function(fun = dprop, args = list(mean = 0.6, size = 100),\n                aes(color = \"dprop(mean = 0.6, size = 100)\"),\n                size = 1) +\n  geom_function(fun = dprop, args = list(mean = 0.9, size = 10),\n                aes(color = \"dprop(mean = 0.9, size = 10)\"),\n                size = 1) +\n  geom_function(fun = dprop, args = list(mean = 0.154, size = 13),\n                aes(color = \"dprop(mean = 0.154, size = 13)\"),\n                size = 1) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8, name = \"\",\n                        guide = guide_legend(ncol = 1)) +\n  labs(title = \"Mean- and precision-based beta distributions\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\nbeta_shapes | beta_mu_phi\n\n\n\n\n\n\n\nPhew. That’s a lot of background to basically say that you don’t have to think about the shape parameters for beta distributions—you can think of mean (\\(\\mu\\)) and precision (\\(\\phi\\)) instead, just like you do with a normal distribution and its mean and standard deviation."
  },
  {
    "objectID": "blog/2021/11/08/beta-regression-guide/index.html#a-beta-regression",
    "href": "blog/2021/11/08/beta-regression-guide/index.html#a-beta-regression",
    "title": "A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models",
    "section": "3a: Beta regression",
    "text": "3a: Beta regression\nSo, with that quick background on how beta distributions work, we can now explore how beta regression lets us model outcomes that range between 0 and 1. Again, beta regression is a distributional regression, which means we’re ultimately modeling \\(\\mu\\) and \\(\\phi\\) and not just a slope and intercept. With this regression, we’ll be able to fit separate models for both parameters and see how the overall distribution shifts based on different covariates.\nThe betareg package provides the betareg() function for doing frequentist beta regression. For the sake of simplicity, we’ll just look at prop_fem ~ quota and ignore polyarchy for a bit.\nThe syntax is just like all other formula-based regression functions, but with an added bit: the first part of the equation (prop_fem ~ quota) models the mean, or \\(\\mu\\), while anything that comes after a | in the formula will explain variation in the precision, or \\(\\phi\\).\nLet’s try it out!\n\nmodel_beta &lt;- betareg(prop_fem ~ quota | quota, \n                      data = vdem_2015, \n                      link = \"logit\")\n## Error in betareg(prop_fem ~ quota | quota, data = vdem_2015, link = \"logit\"): invalid dependent variable, all observations must be in (0, 1)\n\nOh no! We have an error. Beta regression can only handle outcome values that range between 0 and 1—it cannot deal with values that are exactly 0 or exactly 1.\nWe’ll show how to deal with 0s and 1s later. For now we can do some trickery and add 0.001 to all the 0s so that they’re not actually 0. This is cheating, but it’s fine for now :)\n\nvdem_2015_fake0 &lt;- vdem_2015 %&gt;% \n  mutate(prop_fem = ifelse(prop_fem == 0, 0.001, prop_fem))\n\nmodel_beta &lt;- betareg(prop_fem ~ quota | quota, \n                      data = vdem_2015_fake0, \n                      link = \"logit\")\ntidy(model_beta)\n## # A tibble: 4 × 6\n##   component term        estimate std.error statistic  p.value\n##   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 mean      (Intercept)   -1.44     0.0861    -16.7  7.80e-63\n## 2 mean      quotaTRUE      0.296    0.115       2.58 9.79e- 3\n## 3 precision (Intercept)    2.04     0.140      14.6  2.42e-48\n## 4 precision quotaTRUE      0.440    0.214       2.06 3.93e- 2\n\nInterpreting coefficients\nWe now have two sets of coefficients, one set for each parameter (the mean and precision). The parameters for the mean are measured on the logit scale, just like with logistic regression previously, and we can calculate the marginal effect of having a quota by using plogis() and piecing together the coefficient and the intercept:\n\nbeta_mu_intercept &lt;- model_beta %&gt;% \n  tidy() %&gt;% \n  filter(component == \"mean\", term == \"(Intercept)\") %&gt;% \n  pull(estimate)\n\nbeta_mu_quota &lt;- model_beta %&gt;% \n  tidy() %&gt;% \n  filter(component == \"mean\", term == \"quotaTRUE\") %&gt;% \n  pull(estimate)\n\nplogis(beta_mu_intercept + beta_mu_quota) - plogis(beta_mu_intercept)\n## [1] 0.0501\n\nHaving a quota thus increases the average of the distribution of women MPs by 5.006 percentage points, on average. That’s the same value that we found with OLS and with fractional logistic regression!\nWorking with the precision parameter\nBut what about those precision parameters—what can we do with those? For mathy reasons, these are not measured on a logit scale. Instead, they’re logged values. We can invert them by exponentiating them with exp().\nThe phi intercept represents the precision of the distribution of the proportion of women MPs in countries without a quota, while the phi coefficient for quota represents the change in that precision when quota is TRUE. That means we have all the parameters to draw two different distributions of our outcome, split by whether countries have quotas. Let’s plot these two predicted distributions on top of the true underlying data and see how well they fit:\n\nbeta_phi_intercept &lt;- model_beta %&gt;% \n  tidy() %&gt;% \n  filter(component == \"precision\", term == \"(Intercept)\") %&gt;% \n  pull(estimate)\n\nbeta_phi_quota &lt;- model_beta %&gt;% \n  tidy() %&gt;% \n  filter(component == \"precision\", term == \"quotaTRUE\") %&gt;% \n  pull(estimate)\n\nno_quota_title &lt;- paste0(\"dprop(mean = plogis(\", round(beta_mu_intercept, 2),\n                         \"), size = exp(\", round(beta_phi_intercept, 2), \"))\")\n\nquota_title &lt;- paste0(\"dprop(mean = plogis(\", round(beta_mu_intercept, 2),\n                      \" + \", round(beta_mu_quota, 2), \n                      \"), size = exp(\", round(beta_phi_intercept, 2),\n                      \" + \", round(beta_phi_quota, 2), \"))\")\n\nggplot(data = tibble(x = 0:1), aes(x = x)) +\n  geom_density(data = vdem_2015_fake0, \n               aes(x = prop_fem, fill = quota), alpha = 0.5, color = NA) +\n  stat_function(fun = dprop, size = 1,\n                args = list(size = exp(beta_phi_intercept), \n                            mean = plogis(beta_mu_intercept)),\n                aes(color = no_quota_title)) +\n  stat_function(fun = dprop, size = 1,\n                args = list(size = exp(beta_phi_intercept + beta_phi_quota), \n                            mean = plogis(beta_mu_intercept + beta_mu_quota)),\n                aes(color = quota_title)) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.8, name = \"Quota\",\n                       guide = guide_legend(ncol = 1, order = 1)) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8, direction = -1, name = \"\",\n                        guide = guide_legend(reverse = TRUE, ncol = 1, order = 2)) +\n  labs(x = \"Proportion of women in parliament\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nNote how the whole distribution changes when there’s a quota. The mean doesn’t just shift rightward—the precision of the distribution changes too and the distribution where quota is TRUE has a whole new shape. The model actually captures it really well too! Look at how closely the predicted orange line aligns with the the underlying data. The blue predicted line is a little off, but that’s likely due to the 0s that we cheated with.\nYou don’t have to model the \\(\\phi\\) if you don’t want to. A model like this will work just fine too:\n\nmodel_beta_no_phi &lt;- betareg(prop_fem ~ quota, \n                             data = vdem_2015_fake0, \n                             link = \"logit\")\ntidy(model_beta_no_phi)\n## # A tibble: 3 × 6\n##   component term        estimate std.error statistic  p.value\n##   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 mean      (Intercept)   -1.48     0.0785    -18.8  3.70e-79\n## 2 mean      quotaTRUE      0.370    0.111       3.33 8.76e- 4\n## 3 precision (phi)          9.12     0.962       9.48 2.62e-21\n\nIn this case, you still get a precision component, but it’s universal across all the different coefficients—it doesn’t vary across quota (or any other variables, had we included any others in the model). Also, for whatever mathy reasons, when you don’t explicitly model the precision, the resulting coefficient in the table isn’t on the log scale—it’s a regular non-logged number, so there’s no need to exponentiate. We can plot the two distributions like before. The mean part is all the same! The only difference now is that phi is constant across the two groups\n\nbeta_phi &lt;- model_beta_no_phi %&gt;% \n  tidy() %&gt;% \n  filter(component == \"precision\") %&gt;% \n  pull(estimate)\n\nno_quota_title &lt;- paste0(\"dprop(mean = plogis(\", round(beta_mu_intercept, 2),\n                         \"), size = \", round(beta_phi, 2), \")\")\n\nquota_title &lt;- paste0(\"dprop(mean = plogis(\", round(beta_mu_intercept, 2),\n                      \" + \", round(beta_mu_quota, 2), \n                      \"), size = \", round(beta_phi, 2), \")\")\n\nggplot(data = tibble(x = 0:1), aes(x = x)) +\n  geom_density(data = vdem_2015_fake0, \n               aes(x = prop_fem, fill = quota), alpha = 0.5, color = NA) +\n  stat_function(fun = dprop, size = 1,\n                args = list(size = beta_phi, \n                            mean = plogis(beta_mu_intercept)),\n                aes(color = no_quota_title)) +\n  stat_function(fun = dprop, size = 1,\n                args = list(size = beta_phi, \n                            mean = plogis(beta_mu_intercept + beta_mu_quota)),\n                aes(color = quota_title)) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.8, name = \"Quota\",\n                       guide = guide_legend(ncol = 1, order = 1)) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8, direction = -1, name = \"\",\n                        guide = guide_legend(reverse = TRUE, ncol = 1, order = 2)) +\n  labs(x = \"Proportion of women in parliament\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAgain, the means are the same, so the average difference between having a quota and not having a quota is the same, but because the precision is constant, the distributions don’t fit the data as well.\nAverage marginal effects\nBecause beta regression coefficients are all in logit units (with the exception of the \\(\\phi\\) part), we can interpret their marginal effects just like we did with logistic regression. It’s tricky to piece together all the different parts and feed them through plogis(), so it’s best to calculate the slope with hypothetical data as before.\n\navg_slopes(model_beta,\n           newdata = datagrid(quota = c(FALSE, TRUE)))\n## \n##   Term     Contrast Estimate Std. Error   z Pr(&gt;|z|)  2.5 % 97.5 %\n##  quota TRUE - FALSE   0.0501     0.0192 2.6  0.00926 0.0124 0.0878\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nAfter incorporating all the moving parts of the model, we find that having a quota is associated with an increase of 5.006 percentage points of the proportion of women MPs, on average, which is what we found earlier by piecing together the intercept and coefficient. If we had more covariates, this avg_slopes() approach would be necessary, since it’s tricky to incorporate all the different model pieces by hand."
  },
  {
    "objectID": "blog/2021/11/08/beta-regression-guide/index.html#b-beta-regression-bayesian-style",
    "href": "blog/2021/11/08/beta-regression-guide/index.html#b-beta-regression-bayesian-style",
    "title": "A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models",
    "section": "3b: Beta regression, Bayesian style",
    "text": "3b: Beta regression, Bayesian style\nUncertainty is wonderful. We’re working with distributions already with distributional regression like beta regression—it would be great if we could quantify and model our uncertainty directly and treat all the parameters as distributions.\nFortunately Bayesian regression lets us do just that. We can generate thousands of plausible estimates based on the combination of our prior beliefs and the likelihood of the data, and then we can explore the uncertainty and the distributions of all those estimates.\nWe’ll also get to interpret the results Bayesianly. Goodbye confidence intervals; hello credible intervals.\nThe incredible brms package has built-in support for beta regression. I won’t go into details here about how to use brms—there are all sorts of tutorials and examples online for that (here’s a quick basic one).\nLet’s recreate the model we made with betareg() earlier, but now with a Bayesian flavor. The formula syntax is a little different with brms. Instead of using | to divide the \\(\\mu\\) part from the \\(\\phi\\) part, we specify two separate formulas: one for prop_fem for the \\(\\mu\\) part, and one for phi for the \\(\\phi\\) part. We’ll use the default priors here (but in real life, you’d want to set them to something sensible), and we’ll generate 2,000 Monte Carlo Markov Chain (MCMC) samples across 4 chains.\n\nmodel_beta_bayes &lt;- brm(\n  bf(prop_fem ~ quota,\n     phi ~ quota),\n  data = vdem_2015_fake0,\n  family = Beta(),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234, \n  # Use the cmdstanr backend for Stan because it's faster and more modern than\n  # the default rstan You need to install the cmdstanr package first\n  # (https://mc-stan.org/cmdstanr/) and then run cmdstanr::install_cmdstan() to\n  # install cmdstan on your computer.\n  backend = \"cmdstanr\",\n  file = \"model_beta_bayes\"  # Save this so it doesn't have to always rerun\n)\n\nCheck out the results—they’re basically the same that we found with betareg() in model_beta.\n\ntidy(model_beta_bayes, effects = \"fixed\")\n## # A tibble: 4 × 7\n##   effect component term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed  cond      (Intercept)       -1.44     0.0864  -1.61      -1.27 \n## 2 fixed  cond      phi_(Intercept)    2.02     0.140    1.74       2.29 \n## 3 fixed  cond      quotaTRUE          0.296    0.114    0.0713     0.518\n## 4 fixed  cond      phi_quotaTRUE      0.436    0.211    0.0159     0.837\n\nWorking with the posterior\nThe estimates are the same, but richer—we have a complete posterior distribution for each coefficient, which we can visualize in neat ways:\n\nposterior_beta &lt;- model_beta_bayes %&gt;% \n  gather_draws(`b_.*`, regex = TRUE) %&gt;% \n  mutate(component = ifelse(str_detect(.variable, \"phi_\"), \"Precision\", \"Mean\"),\n         intercept = str_detect(.variable, \"Intercept\"))\n\nggplot(posterior_beta, aes(x = .value, y = fct_rev(.variable), fill = component)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(aes(slab_alpha = intercept), \n               .width = c(0.8, 0.95), point_interval = \"median_hdi\") +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.6) +\n  scale_slab_alpha_discrete(range = c(1, 0.4)) +\n  guides(fill = \"none\", slab_alpha = \"none\") +\n  labs(x = \"Coefficient\", y = \"Variable\",\n       caption = \"80% and 95% credible intervals shown in black\") +\n  facet_wrap(vars(component), ncol = 1, scales = \"free_y\") +\n  theme_clean()\n\n\n\n\n\n\n\nThat shows the coefficients in their transformed scale (logits for the mean, logs for the precision), but we can transform them to the response scale too by feeding the \\(\\phi\\) parts to exp() and the \\(\\mu\\) parts to plogis():\n\nmodel_beta_bayes %&gt;% \n  spread_draws(`b_.*`, regex = TRUE) %&gt;% \n  mutate(across(starts_with(\"b_phi\"), ~exp(.))) %&gt;%\n  mutate(across((!starts_with(\".\") & !starts_with(\"b_phi\")), ~plogis(.))) %&gt;%\n  gather_variables() %&gt;% \n  median_hdi()\n## # A tibble: 4 × 7\n##   .variable       .value .lower .upper .width .point .interval\n##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 b_Intercept      0.192  0.167  0.219   0.95 median hdi      \n## 2 b_phi_Intercept  7.57   5.66   9.80    0.95 median hdi      \n## 3 b_phi_quotaTRUE  1.55   0.997  2.27    0.95 median hdi      \n## 4 b_quotaTRUE      0.573  0.522  0.630   0.95 median hdi\n\nBut beyond the intercepts here, this isn’t actually that helpful. The median back-transformed intercept for the \\(\\mu\\) part of the regression is 0.192, which means countries without a quota have an average proportion of women MPs of 19.2%. That’s all fine. The quota effect here, though, is 0.573. That does not mean that having a quota increases the proportion of women by 57 percentage points! In order to find the marginal effect of having a quota, we need to also incorporate the intercept. Remember when we ran plogis(intercept + coefficient) - plogis(intercept) earlier? We have to do that again.\nPosterior average marginal effects\nBinary predictor\nOnce again, the math for combining these coefficients can get hairy, especially when we’re working with more than one explanatory variable, so instead of figuring out that math, we’ll calculate the marginal effects across different values of quota.\nThe marginaleffects package doesn’t work with brms models (and there are no plans to add support for it), but the powerful emmeans package does, so we’ll use that. The syntax is a little different, but the documentation for emmeans is really complete and helpful. This website also has a bunch of worked out examples.\n\n\n\n\n\n\n2023-08-18 edit\n\n\n\nThe marginaleffects package does work with brms models now and I like its interface and its approach to estimating things a lot better than emmeans, so I’ve changed all the code here to use it instead. If you want to see the emmeans version of everything, look at this past version at GitHub.\nAlso, I’m pretty loosey-goosey about the term “marginal effect” throughout this post because this was early in my journey to actually understanding what all these different estimands are. See my “Marginalia” post, published a few months after this one, for a ton more about what “marginal effects” actually mean.\n\n\nWe can calculate the pairwise marginal effects across different values of quota, which conveniently shows us the difference, or the marginal effect of having a quota:\n\nmodel_beta_bayes %&gt;% \n  avg_comparisons(variables = \"quota\")\n## \n##   Term     Contrast Estimate  2.5 % 97.5 %\n##  quota TRUE - FALSE     0.05 0.0121 0.0875\n## \n## Columns: term, contrast, estimate, conf.low, conf.high\n\nThis summary information is helpful—we have our difference in predicted outcomes of 0.05, which is roughly what we found in earlier models. We also have a 95% highest posterior density interval for the difference.\nBut it would be great if we could visualize these predictions! What do the posteriors for quota and non-quota countries look like, and what does the distribution of differences look like? We can work with the original model and MCMC chains to visualize this, letting brms take care of the hard work of predicting the coefficients we need and back-transforming them into probabilities.\n\n\n\n\n\n\nmarginaleffects\n\n\n\nmarginaleffects can also handle all this with its comparisons() and hypotheses() functions—even with Bayesian models!\n\n\nWe’ll use epred_draws() from tidybayes to plug in a hypothetical dataset and generate predictions, as we did with previous models. Only this time, instead of getting a single predicted value for each level of quota, we’ll get 4,000. The epred_draws() (like all tidybayes functions) returns a long tidy data frame, so it’s really easy to plot with ggplot:\n\n# Plug a dataset where quota is FALSE and TRUE into the model\nbeta_bayes_pred &lt;- model_beta_bayes %&gt;% \n  epred_draws(newdata = tibble(quota = c(FALSE, TRUE)))\n\nggplot(beta_bayes_pred, aes(x = .epred, y = quota, fill = quota)) +\n  stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\") +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.8) +\n  guides(fill = \"none\") +\n  labs(x = \"Predicted proportion of women in parliament\", y = NULL,\n       caption = \"80% and 95% credible intervals shown in black\") +\n  theme_clean()\n\n\n\n\n\n\n\nThat’s super neat! We can easily see that countries with quotas have a higher predicted proportion of women in parliament, and we can see the uncertainty in those estimates. We can also work with these posterior predictions to calculate the difference in proportions, or the marginal effect of having a quota, which is the main thing we’re interested in.\nWe can use compare_levels() from tidybayes to calculate the difference between these two posteriors:\n\nbeta_bayes_pred_diff &lt;- beta_bayes_pred %&gt;% \n  compare_levels(variable = .epred, by = quota)\n\nbeta_bayes_pred_diff %&gt;% median_hdi()\n## # A tibble: 1 × 7\n##   quota        .epred .lower .upper .width .point .interval\n##   &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 TRUE - FALSE 0.0500 0.0136 0.0889   0.95 median hdi\n\nggplot(beta_bayes_pred_diff, aes(x = .epred)) +\n  stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\",\n               fill = \"#bc3032\") +\n  scale_x_continuous(labels = label_pp) +\n  labs(x = \"Average marginal effect of having a gender-based\\nquota on the proportion of women in parliament\", \n       y = NULL, caption = \"80% and 95% credible intervals shown in black\") +\n  theme_clean()\n\n\n\n\n\n\n\nPerfect! The median marginal effect of having a quota is 6 percentage points, but it can range from 2 to 10.\nContinuous predictor\nThe same approach works for continuous predictors too, just like with previous methods. Let’s build a model that explains both the mean and the precision with quota and polyarchy. We don’t necessarily have to model the precision with the same variables that we use for the mean—that can be a completely separate process—but we’ll do it just for fun.\n\n# We need to specify init here because the beta precision parameter can never\n# be negative, but Stan will generate random initial values from -2 to 2, even\n# for beta's precision, which leads to rejected chains and slower performance.\n# For even fancier init handling, see Solomon Kurz's post here:\n# https://solomonkurz.netlify.app/post/2021-06-05-don-t-forget-your-inits/\nmodel_beta_bayes_1 &lt;- brm(\n  bf(prop_fem ~ quota + polyarchy,\n     phi ~ quota + polyarchy),\n  data = vdem_2015_fake0,\n  family = Beta(),\n  init = 0,\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234, \n  backend = \"cmdstanr\",\n  file = \"model_beta_bayes_1\"\n)\n\nWe’ll forgo interpreting all these different coefficients, since we’d need to piece together the different parts and back-transform them with either plogis() (for the \\(\\mu\\) parts) or exp() (for the \\(\\phi\\) parts). Instead, we’ll skip to the marginal effect of polyarchy on the proportion of women MPs. First we can look at the posterior prediction of the outcome across the whole range of democracy scores:\n\n# Use a dataset where quota is FALSE and polyarchy is a range\nbeta_bayes_pred_1 &lt;- model_beta_bayes_1 %&gt;% \n  epred_draws(newdata = expand_grid(quota = FALSE,\n                                    polyarchy = seq(0, 100, by = 1)))\n\nggplot(beta_bayes_pred_1, aes(x = polyarchy, y = .epred)) +\n  stat_lineribbon() + \n  scale_y_continuous(labels = label_percent()) +\n  scale_fill_brewer(palette = \"Purples\") +\n  labs(x = \"Polyarchy (democracy)\", y = \"Predicted proportion of women MPs\",\n       fill = \"Credible interval\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nThe predicted proportion of women MPs increases as democracy increases, as we’ve seen before. Note how the slope here isn’t constant, though—the line is slightly curved. That’s because this is a nonlinear regression model, making it so the marginal effect of democracy is different depending on its level. We can see this if we look at the instantaneous slope (or first derivative) at different values of polyarchy. marginaleffects makes this really easy with the aavg_slopes() function. If we ask it for the slope of the line, it will provide it at the average value of polyarchy, or 53.5:\n\navg_slopes(model_beta_bayes_1, variables = \"polyarchy\")\n## \n##       Term Estimate    2.5 %  97.5 %\n##  polyarchy  0.00126 0.000502 0.00196\n## \n## Columns: term, estimate, conf.low, conf.high\n\nOur effect is thus 0.001, or 0.126 percentage points, but only for countries in the middle range of polyarchy. The slope is shallower and steeper depending on the level of democracy. We can check the slope (or effect) at different hypothetical levels:\n\nslopes(model_beta_bayes_1, \n       variables = \"polyarchy\",\n       newdata = datagrid(polyarchy = c(20, 50, 80)))\n## \n##       Term Estimate    2.5 %  97.5 % quota polyarchy\n##  polyarchy 0.000991 0.000438 0.00140 FALSE        20\n##  polyarchy 0.001153 0.000464 0.00178 FALSE        50\n##  polyarchy 0.001314 0.000489 0.00219 FALSE        80\n## \n## Columns: rowid, term, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, prop_fem, quota, polyarchy\n\nNeat! For observations with low democracy scores like 20, a one-unit increase in polyarchy is associated with a 0.099 percentage point increase in the proportion of women MPs, while countries with high scores like 80 see a 0.131 percentage point increase.\nWe can visualize the distribution of these marginal effects too by using slopes() to calculate the instantaneous slopes across all the MCMC chains, and then using posteriordraws() to extract the draws as tidy, plottable data.\n(Side note: You could technically do this without marginaleffects. The way marginaleffects calculates the instantaneous slope is by finding the predicted average for some value, finding the predicted average for that same value plus a tiny amount extra, subtracting the two predictions, and dividing by that tiny extra amount. If you really didn’t want to use marginaleffects, you could generate predictions for polyarchy = 20 and polyarchy = 20.001, subtract them by hand, and then divide by 0.001. That’s miserable though—don’t do it. Use marginaleffects instead.)\n\name_beta_bayes_1 &lt;- model_beta_bayes_1 %&gt;% \n  slopes(variables = \"polyarchy\",\n         newdata = datagrid(polyarchy = c(20, 50, 80))) %&gt;% \n  posterior_draws()\n\nggplot(ame_beta_bayes_1, aes(x = draw, fill = factor(polyarchy))) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\",\n               slab_alpha = 0.75) +\n  scale_x_continuous(labels = label_pp_tiny) +\n  scale_fill_viridis_d(option = \"viridis\", end = 0.6) +\n  labs(x = \"Average marginal effect of polyarchy (democracy)\", \n       y = \"Density\", fill = \"Polyarchy\",\n       caption = \"80% and 95% credible intervals shown in black\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nTo me this is absolutely incredible! Check out how the shapes of these distributions change as polyarchy increases! That’s because we modeled both the \\(\\mu\\) and the \\(\\phi\\) parameters of the beta distribution, so we get to see changes in both the mean and precision of the outcome. Had we left the phi part alone, doing something like phi ~ 1, the marginal effect would look the same and would just be shifted to the right."
  },
  {
    "objectID": "blog/2021/11/08/beta-regression-guide/index.html#zero-inflated-beta-regression-bayesian-style",
    "href": "blog/2021/11/08/beta-regression-guide/index.html#zero-inflated-beta-regression-bayesian-style",
    "title": "A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models",
    "section": "4: Zero-inflated beta regression, Bayesian style",
    "text": "4: Zero-inflated beta regression, Bayesian style\nBeta regression is really neat and modeling different components of the beta regression is a fascinating and rich way of thinking about the outcome variable. Plus the beta distribution naturally limits the outcome to the 0–1 range, so you don’t have to shoehorn the data into an OLS-based LPM or fractional logistic model.\nOne problem with beta regression, though, is that the beta distribution does not include 0 and 1. As we saw earlier, if your data has has 0s or 1s, the model breaks.\nA new parameter for modeling the zero process\nTo get around this, we can use a special zero-inflated beta regression. We’ll still model the \\(\\mu\\) and \\(\\phi\\) (or mean and precision) of the beta distribution, but we’ll also model one new special parameter \\(\\alpha\\). With zero-inflated regression, we’re actually modelling a mixture of data-generating processes:\n\nA logistic regression model that predicts if an outcome is 0 or not, defined by \\(\\alpha\\)\n\nA beta regression model that predicts if an outcome is between 0 and 1 if it’s not zero, defined by \\(\\mu\\) and \\(\\phi\\)\n\n\nLet’s see how many zeros we have in the data:\n\nvdem_2015 %&gt;% \n  count(prop_fem == 0) %&gt;% \n  mutate(prop = n / sum(n))\n##   prop_fem == 0   n   prop\n## 1         FALSE 169 0.9826\n## 2          TRUE   3 0.0174\n\nHahahaha only 3, or 1.7% of the data. That’s barely anything. Perhaps our approach of just adding a tiny amount to those three 0 observations is fine. For the sake of illustrating this approach, though, we’ll still model the zero-creating process.\nThe only difference between regular beta regression and zero-inflated regression is that we have to specify one more parameter: zi. This corresponds to the \\(\\alpha\\) parameter and determines the zero/not-zero process.\nTo help with the intuition, we’ll first run a model where we don’t actually define a model for zi—it’ll just return the intercept for the \\(\\alpha\\) parameter.\n\nmodel_beta_zi_int_only &lt;- brm(\n  bf(prop_fem ~ quota,\n     phi ~ quota,\n     zi ~ 1),\n  data = vdem_clean,\n  family = zero_inflated_beta(),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234,\n  backend = \"cmdstanr\",\n  file = \"model_beta_zi_int_only\"\n)\n\n\ntidy(model_beta_zi_int_only, effects = \"fixed\")\n## # A tibble: 5 × 7\n##   effect component term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed  cond      (Intercept)       -1.42     0.0229   -1.47     -1.38 \n## 2 fixed  cond      phi_(Intercept)    2.37     0.0437    2.29      2.46 \n## 3 fixed  zi        (Intercept)       -4.04     0.186    -4.43     -3.70 \n## 4 fixed  cond      quotaTRUE          0.303    0.0331    0.238     0.369\n## 5 fixed  cond      phi_quotaTRUE      0.237    0.0677    0.105     0.368\n\nAs before, the parameters for \\(\\mu\\) ((Intercept) and quotaTRUE) are on the logit scale, while the parameters for \\(\\phi\\) (phi_(Intercept) and phi_quotaTRUE) are on the log scale. The zero-inflated parameter (or \\(\\alpha\\)) here ((Intercept) for the zi component) is on the logit scale like the regular model coefficients. That means we can back-transform it with plogis():\n\nzi_intercept &lt;- tidy(model_beta_zi_int_only, effects = \"fixed\") %&gt;% \n  filter(component == \"zi\", term == \"(Intercept)\") %&gt;% \n  pull(estimate)\n\n# Logit scale intercept\nzi_intercept\n## b_zi_Intercept \n##          -4.04\n\n# Transformed to a probability/proportion\nplogis(zi_intercept)\n## b_zi_Intercept \n##         0.0172\n\nAfter transforming the intercept to a probability/proportion, we can see that it’s 1.72%—basically the same as the proportion of zeros in the data!\nRight now, all we’ve modeled is the overall proportion of 0s. We haven’t modeled what determines those zeros. We can see that if we plot the posterior predictions for prop_fem across quota, highlighting predicted values that are 0. In general, ≈2% of the predicted outcomes should be zero, and since we didn’t really model zi, the 0s should be equally spread across the different levels of quota. To visualize this we’ll use a histogram instead of a density plot so that we can better see the count of 0s. We’ll also cheat a little and make the 0s a negative number so that the histogram bin for the 0s appears outside of the main 0–1 range.\n\nbeta_zi_pred_int &lt;- model_beta_zi_int_only %&gt;% \n  predicted_draws(newdata = tibble(quota = c(FALSE, TRUE))) %&gt;% \n  mutate(is_zero = .prediction == 0,\n         .prediction = ifelse(is_zero, .prediction - 0.01, .prediction))\n\nggplot(beta_zi_pred_int, aes(x = .prediction)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 0.025, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.5,\n                       guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"Predicted proportion of women in parliament\", \n       y = \"Count\", fill = \"Is zero?\") +\n  facet_wrap(vars(quota), ncol = 2,\n             labeller = labeller(quota = c(`TRUE` = \"Quota\", \n                                           `FALSE` = \"No Quota\"))) + \n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nThe process for deciding 0/not 0 is probably determined by different political and social factors, though. It’s likely that having a quota, for instance, should increase the probability of having at least one woman. There are probably a host of other factors—if we were doing this in real life, we could fully specify a model that explains the no women vs. at-least-one-woman split. For now, we’ll just use quota, though (in part because there are actually only 3 zeros!). This model takes a little longer to run because of all the moving parts:\n\nmodel_beta_zi &lt;- brm(\n  bf(prop_fem ~ quota,\n     phi ~ quota,\n     zi ~ quota),\n  data = vdem_clean,\n  family = zero_inflated_beta(),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234,\n  backend = \"cmdstanr\",\n  file = \"model_beta_zi\"\n)\n\n\ntidy(model_beta_zi, effects = \"fixed\")\n## # A tibble: 6 × 7\n##   effect component term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed  cond      (Intercept)       -1.42     0.0236   -1.47     -1.38 \n## 2 fixed  cond      phi_(Intercept)    2.37     0.0433    2.29      2.45 \n## 3 fixed  zi        (Intercept)       -3.53     0.192    -3.93     -3.18 \n## 4 fixed  cond      quotaTRUE          0.304    0.0334    0.239     0.368\n## 5 fixed  cond      phi_quotaTRUE      0.236    0.0696    0.101     0.371\n## 6 fixed  zi        quotaTRUE         -5.57     2.64    -12.2      -2.21\n\nWe now have two estimates for the zi part: an intercept and a coefficient for quota. Since these are on the logit scale, we can combine them mathematically to figure out the marginal effect of quota on the probability of being 0:\n\nzi_intercept &lt;- tidy(model_beta_zi, effects = \"fixed\") %&gt;% \n  filter(component == \"zi\", term == \"(Intercept)\") %&gt;% \n  pull(estimate)\n\nzi_quota &lt;- tidy(model_beta_zi, effects = \"fixed\") %&gt;% \n  filter(component == \"zi\", term == \"quotaTRUE\") %&gt;% \n  pull(estimate)\n\nplogis(zi_intercept + zi_quota) - plogis(zi_intercept)\n## b_zi_Intercept \n##        -0.0283\n\nBased on this, having a quota reduces the proportion of 0s in the data by -2.83 percentage points. Unsurprisingly, we can also see this in the posterior predictions. We can look at the predicted proportion of 0s across the two levels of quota to find the marginal effect of having a quota on the 0/not 0 split. To do this with tidybayes, we can use the dpar argument in epred_draws() to also calculated the predicted zero-inflation part of the model (this is omitted by default). Here’s what that looks like:\n\npred_beta_zi &lt;- model_beta_zi %&gt;% \n  epred_draws(newdata = expand_grid(quota = c(FALSE, TRUE)),\n              dpar = \"zi\")\n\n# We now have columns for the overall prediction (.epred) and for the\n# zero-inflation probability (zi)\nhead(pred_beta_zi)\n## # A tibble: 6 × 7\n## # Groups:   quota, .row [1]\n##   quota  .row .chain .iteration .draw .epred     zi\n##   &lt;lgl&gt; &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 FALSE     1     NA         NA     1  0.186 0.0358\n## 2 FALSE     1     NA         NA     2  0.188 0.0247\n## 3 FALSE     1     NA         NA     3  0.187 0.0388\n## 4 FALSE     1     NA         NA     4  0.187 0.0388\n## 5 FALSE     1     NA         NA     5  0.191 0.0274\n## 6 FALSE     1     NA         NA     6  0.186 0.0162\n\n# Look at the average zero-inflation probability across quota\npred_beta_zi %&gt;%\n  group_by(quota) %&gt;% \n  median_hdci(zi)\n## # A tibble: 2 × 7\n##   quota       zi   .lower  .upper .width .point .interval\n##   &lt;lgl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 FALSE 0.0286   1.91e- 2 0.0396    0.95 median hdci     \n## 2 TRUE  0.000220 3.91e-11 0.00233   0.95 median hdci\n\npred_beta_zi %&gt;% \n  compare_levels(variable = zi, by = quota) %&gt;% \n  median_hdci()\n## # A tibble: 1 × 7\n##   quota             zi  .lower  .upper .width .point .interval\n##   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 TRUE - FALSE -0.0281 -0.0389 -0.0179   0.95 median hdci\n\nWithout even plotting this, we can see a neat trend. When there’s not a quota, ≈2–3% of the the predicted values are 0; when there is a quota, almost none of the predicted values are 0. Having a quota almost completely eliminates the possibility of having no women MPs. It’s the same marginal effect that we found with plogis()—the proportion of zeros drops by -2.8 percentage points when there’s a quota.\nWe can visualize this uncertainty by plotting the posterior predictions. We could either plot two distributions (for quota being TRUE or FALSE), or we can calculate the difference between quota/no quota to find the average marginal effect of having a quota on the proportion of 0s in the data:\n\nmfx_quota_zi &lt;- pred_beta_zi %&gt;% \n  compare_levels(variable = zi, by = quota)\n\nggplot(mfx_quota_zi, aes(x = zi)) +\n  stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\",\n               fill = \"#fb9e07\") +\n  scale_x_continuous(labels = label_pp) +\n  labs(x = \"Average marginal effect of having a gender-based quota\\non the proportion of countries with zero women in parliament\", \n       y = NULL, caption = \"80% and 95% credible intervals shown in black\") +\n  theme_clean()\n\n\n\n\n\n\n\nFinally, we can see how this shows up in the overall predictions from the model. Note how there are a bunch of 0s when quota is FALSE, and no 0s when it is TRUE, as expected:\n\nbeta_zi_pred &lt;- model_beta_zi %&gt;% \n  predicted_draws(newdata = tibble(quota = c(FALSE, TRUE))) %&gt;% \n  mutate(is_zero = .prediction == 0,\n         .prediction = ifelse(is_zero, .prediction - 0.01, .prediction))\n\nggplot(beta_zi_pred, aes(x = .prediction)) +\n  geom_histogram(aes(fill = is_zero), binwidth = 0.025, \n                 boundary = 0, color = \"white\") +\n  geom_vline(xintercept = 0) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.5,\n                       guide = guide_legend(reverse = TRUE)) +\n  labs(x = \"Predicted proportion of women in parliament\", \n       y = \"Count\", fill = \"Is zero?\") +\n  facet_wrap(vars(quota), ncol = 2,\n             labeller = labeller(quota = c(`TRUE` = \"Quota\", \n                                           `FALSE` = \"No Quota\"))) +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAgain, I think it’s so cool that we can explore all this uncertainty for specific parameters of this model, like the 0/not 0 process! You can’t do this with OLS!\nAverage marginal effects, incorporating the zero process\nThe main thing we’re interested in here, though, is the average marginal effect of having a quota on the proportion of women in parliament, not just the 0/not 0 process. Since we’ve incorporated quota in all the different parts of the model (the \\(\\mu\\), the \\(\\phi\\), and the zero-inflated \\(\\alpha\\)), we should definitely once again simulate this value using marginaleffects—trying to get all the coefficients put together manually is going to be too tricky.\n\name_beta_zi &lt;- model_beta_zi %&gt;%\n  avg_comparisons(variables = \"quota\") %&gt;% \n  posterior_draws()\n\name_beta_zi %&gt;% median_hdi(draw)\n## # A tibble: 1 × 6\n##     draw .lower .upper .width .point .interval\n##    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 0.0575 0.0466 0.0688   0.95 median hdi\n\nggplot(ame_beta_zi, aes(x = draw)) +\n  stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\",\n               fill = \"#bc3032\") +\n  scale_x_continuous(labels = label_pp) +\n  labs(x = \"Average marginal effect of having a gender-based\\nquota on the proportion of women in parliament\", y = NULL,\n       caption = \"80% and 95% credible intervals shown in black\") +\n  theme_clean()\n\n\n\n\n\n\n\nThe average marginal effect of having a quota on the proportion of women MPs in an average country, therefore, is 6.65 percentage points.\nSpecial case #1: Zero-one-inflated beta regression\nWith our data here, no countries have parliaments with 100% women (cue that RBG “when there are nine” quote). But what if you have both 0s and 1s in your outcome? Zero-inflated beta regression handles the zeros just fine (it’s in the name of the model!), but it can’t handle both 0s and 1s.\nFortunately there’s a variation for zero-one-inflated beta (ZOIB) regression. I won’t go into the details here—Matti Vuore has a phenomenal tutorial about how to run these models. In short, ZOIB regression makes you model a mixture of three things:\n\nA logistic regression model that predicts if an outcome is either 0 or 1 or not 0 or 1, defined by \\(\\alpha\\) (or alternatively, a model that predicts if outcomes are extreme (0 or 1) or not (between 0 and 1); thanks to Isabella Ghement for this way of thinking about it!)\nA logistic regression model that predicts if any of the 0 or 1 outcomes are actually 1s, defined by \\(\\gamma\\) (or alternatively, a model that predicts if the extreme values are 1)\nA beta regression model that predicts if an outcome is between 0 and 1 if it’s not zero or not one, defined by \\(\\mu\\) and \\(\\phi\\) (or alternatively, a model that predicts the non-extreme (0 or 1) values)\n\nWhen using brms, you get to model all these different parts:\n\nbrm(\n  bf(\n    outcome ~ covariates,  # The mean of the 0-1 values, or mu\n    phi ~ covariates,  # The precision of the 0-1 values, or phi\n    zoi ~ covariates,  # The zero-or-one-inflated part, or alpha\n    coi ~ covariates  # The one-inflated part, conditional on the 0s, or gamma\n  ),\n  data = whatever,\n  family = zero_one_inflated_beta(),\n  ...\n)\n\nPulling out all these different parameters, plotting their distributions, and estimating their average marginal effects looks exactly the same as what we did earlier with the zero-inflated model—all the brms, tidybayes, and marginaleffects functions we used for marginal effects and fancy plots work the same. All the coefficients are on the logit scale, except \\(\\phi\\), which is on the log scale.\nSpecial case #2: One-inflated beta regression\nWhat if your proportion-based outcome has a bunch of 1s and no 0s? It would be neat if there were a one_inflated_beta() family, but there’s not (also by design). But never fear! There are two ways to do one-inflated regression:\n\n\nReverse your outcome so that it’s 1 - outcome instead of outcome and then use zero-inflated regression. In the case of the proportion of women MPs, we would instead look at the proportion of not-women MPs:\n\nmutate(prop_not_fem = 1 - prop_fem)\n\n\nUse zero-one-inflated beta regression and force the conditional one parameter (coi, or the \\(\\gamma\\) in that model) to be one, meaning that 100% of the 0/not 0 splits would actually be 1: bf(..., coi = 1)\n\nEverything else that we’ve explored in this post—posterior predictions, average marginal effects, etc.—will all work as expected."
  },
  {
    "objectID": "blog/2021/11/08/beta-regression-guide/index.html#super-fancy-detailed-model-with-lots-of-moving-parts-just-for-fun",
    "href": "blog/2021/11/08/beta-regression-guide/index.html#super-fancy-detailed-model-with-lots-of-moving-parts-just-for-fun",
    "title": "A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models",
    "section": "Super fancy detailed model with lots of moving parts, just for fun",
    "text": "Super fancy detailed model with lots of moving parts, just for fun\nThroughout this post, for the sake of simplicity we’ve really only used one or two covariates at a time (quota and/or polyarchy). Real world models are more complex: we can use more covariates, multilevel hierarchical structures, or weighting, for instance. This all works just fine with zero|one|zero-one-inflated beta regression thanks to the power of brms, which handles all these extra features natively.\nJust for fun here at the end, let’s run a more fully specified model with more covariates and a multilevel structure that accounts for year and region. This will let us look at the larger V-Dem dataset from 2010–2020 instead of just 2015.\nSet better priors\nTo make sure this runs as efficiently as possible, we’ll set our own priors instead of relying on the default brms priors. This is especially important for beta models because of how parameters are used internally. Remember that most of our coefficients are on a logit scale, and those logits correspond to probabilities or proportions. While logits can theoretically range from −∞ to ∞, in practice they’re a lot more limited. For instance, we can convert some small percentages to the logit scale with qlogis():\n\nqlogis(c(0.01, 0.001, 0.0001))\n## [1] -4.60 -6.91 -9.21\nqlogis(c(0.99, 0.999, 0.9999))\n## [1] 4.60 6.91 9.21\n\nA 1%/99% value is 4.6 as a logit, while 0.01%/99.99% is 9. Most of the proportions we’ve seen in the models so far are much smaller than that—logits of like ≈0–2 at most. By default, brms puts a flat prior on coefficients in beta regression, so values like 10 or 15 could occur, which are excessively massive when converted to probabilities! Look at this plot to see the relationship between the logit scale and the probability scale—there are big changes in probability between −4ish and 4ish, but once you start getting into the 5s and beyond, the probability is all essentially the same.\n\ntibble(x = seq(-8, 8, by = 0.1)) %&gt;% \n  mutate(y = plogis(x)) %&gt;% \n  ggplot(aes(x = x, y = y)) +\n  geom_line(size = 1) +\n  labs(x = \"Logit scale\", y = \"Probability scale\") +\n  theme_clean()\n\n\n\n\n\n\n\nTo help speed up the model (and on philosophical grounds, since we know prior information about these model coefficients), we should tell brms that the logit-scale parameters in the model aren’t ever going to get huge.\nTo do this, we first need to define the formula for our fancy complex model. I added a bunch of plausible covariates to different portions of the model, along with random effects for region. In real life this should all be driven by theory.\n\nvdem_clean &lt;- vdem_clean %&gt;% \n  # Scale polyarchy back down to 0-1 values to help Stan with modeling\n  mutate(polyarchy = polyarchy / 100) %&gt;% \n  # Make region and year factors instead of numbers\n  mutate(region = factor(region),\n         year = factor(year))\n\n# Create the model formula\nfancy_formula &lt;- bf(\n  # mu (mean) part\n  prop_fem ~ quota + polyarchy + corruption + \n    civil_liberties + (1 | year) + (1 | region),\n  # phi (precision) part\n  phi ~ quota + (1 | year) + (1 | region),\n  # alpha (zero-inflation) part\n  zi ~ quota + polyarchy + (1 | year) + (1 | region)\n)\n\nWe can then feed that formula to get_prior() to see what the default priors are and how to change them:\n\nget_prior(\n  fancy_formula,\n  data = vdem_clean,\n  family = zero_inflated_beta()\n)\n##                 prior     class            coef  group resp dpar nlpar lb ub       source\n##                (flat)         b                                                   default\n##                (flat)         b civil_liberties                              (vectorized)\n##                (flat)         b      corruption                              (vectorized)\n##                (flat)         b       polyarchy                              (vectorized)\n##                (flat)         b       quotaTRUE                              (vectorized)\n##  student_t(3, 0, 2.5) Intercept                                                   default\n##  student_t(3, 0, 2.5)        sd                                         0         default\n##  student_t(3, 0, 2.5)        sd                 region                  0    (vectorized)\n##  student_t(3, 0, 2.5)        sd       Intercept region                  0    (vectorized)\n##  student_t(3, 0, 2.5)        sd                   year                  0    (vectorized)\n##  student_t(3, 0, 2.5)        sd       Intercept   year                  0    (vectorized)\n##                (flat)         b                              phi                  default\n##                (flat)         b       quotaTRUE              phi             (vectorized)\n##  student_t(3, 0, 2.5) Intercept                              phi                  default\n##  student_t(3, 0, 2.5)        sd                              phi        0         default\n##  student_t(3, 0, 2.5)        sd                 region       phi        0    (vectorized)\n##  student_t(3, 0, 2.5)        sd       Intercept region       phi        0    (vectorized)\n##  student_t(3, 0, 2.5)        sd                   year       phi        0    (vectorized)\n##  student_t(3, 0, 2.5)        sd       Intercept   year       phi        0    (vectorized)\n##                (flat)         b                               zi                  default\n##                (flat)         b       polyarchy               zi             (vectorized)\n##                (flat)         b       quotaTRUE               zi             (vectorized)\n##        logistic(0, 1) Intercept                               zi                  default\n##  student_t(3, 0, 2.5)        sd                               zi        0         default\n##  student_t(3, 0, 2.5)        sd                 region        zi        0    (vectorized)\n##  student_t(3, 0, 2.5)        sd       Intercept region        zi        0    (vectorized)\n##  student_t(3, 0, 2.5)        sd                   year        zi        0    (vectorized)\n##  student_t(3, 0, 2.5)        sd       Intercept   year        zi        0    (vectorized)\n\nThere are a ton of potential priors we can set here! If we really wanted, we could set the mean and standard deviation separately for each individual coefficient, but that’s probably excessive for this case.\nYou can see that everything with class b (for β coefficient in a regression model) has a flat prior. We need to narrow that down, perhaps with something like a normal distribution centered at 0 with a standard deviation of 1. Values here won’t get too big and will mostly be between −2 and 2:\n\nggplot(data = tibble(x = -4:4), aes(x = x)) +\n  geom_function(fun = dnorm, args = list(mean = 0, sd = 1), linewidth = 1) +\n  theme_clean()\n\n\n\n\n\n\n\nTo set the priors, we can create a list of priors as a separate object that we can then feed to the main brm() function. We’ll keep the default prior for the intercepts.\n\npriors &lt;- c(set_prior(\"student_t(3, 0, 2.5)\", class = \"Intercept\"),\n            set_prior(\"normal(0, 1)\", class = \"b\"))\n\nRun the model\nLet’s run the model! This will probably take a (long!) while to run. On my new 2021 MacBook Pro with a 10-core M1 Max CPU, running 4 chains with two CPU cores per chain (with cmdstanr’s support for within-chain threading) takes about 6 minutes. It probably takes a lot longer on a less powerful machine. To help Stan with estimation, I also set init to help with some of the parameters of the beta distribution, and increased adapt_delta and max_treedepth a little. I only knew to do this because Stan complained when I used the defaults and told me to increase them :)\n\nfancy_model &lt;- brm(\n  fancy_formula,\n  data = vdem_clean,\n  family = zero_inflated_beta(),\n  prior = priors,\n  init = 0,\n  control = list(adapt_delta = 0.97,\n                 max_treedepth = 12),\n  chains = 4, iter = 2000, warmup = 1000,\n  cores = 4, seed = 1234, \n  threads = threading(2),\n  backend = \"cmdstanr\",\n  file = \"fancy_model\"\n)\n\nThere are a handful of divergent chains but since this isn’t an actual model that I’d use in a publication, I’m not too worried. In real life, I’d increase the adapt_delta or max_treedepth parameters even more or do some other fine tuning, but we’ll just live with the warnings for now.\nAnalyze and plot the results\nWe now have a fully specified multilevel model with all sorts of rich moving parts: we account for the mean and precision of the beta distribution and we account for the zero-inflation process, all with different covariates with region-specific offsets. Here are the full results, with medians and 95% credible intervals for all the different coefficients:\n\ntidy(fancy_model)\n## # A tibble: 16 × 8\n##    effect   component group  term                estimate std.error  conf.low conf.high\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n##  1 fixed    cond      &lt;NA&gt;   (Intercept)         -0.654      0.239   -1.12      -0.196 \n##  2 fixed    cond      &lt;NA&gt;   phi_(Intercept)      2.83       0.274    2.26       3.34  \n##  3 fixed    zi        &lt;NA&gt;   (Intercept)         -0.888      3.56    -6.55       6.92  \n##  4 fixed    cond      &lt;NA&gt;   quotaTRUE            0.367      0.0294   0.310      0.424 \n##  5 fixed    cond      &lt;NA&gt;   polyarchy           -0.104      0.152   -0.409      0.192 \n##  6 fixed    cond      &lt;NA&gt;   corruption          -0.738      0.0721  -0.885     -0.599 \n##  7 fixed    cond      &lt;NA&gt;   civil_liberties     -0.589      0.132   -0.843     -0.328 \n##  8 fixed    cond      &lt;NA&gt;   phi_quotaTRUE       -0.0729     0.0763  -0.223      0.0770\n##  9 fixed    zi        &lt;NA&gt;   quotaTRUE           -6.81       4.60   -18.9       -2.11  \n## 10 fixed    zi        &lt;NA&gt;   polyarchy            0.00477    0.949   -1.88       1.85  \n## 11 ran_pars cond      region sd__(Intercept)      0.483      0.227    0.232      1.05  \n## 12 ran_pars cond      year   sd__(Intercept)      0.0817     0.0312   0.0382     0.153 \n## 13 ran_pars cond      region sd__phi_(Intercept)  0.610      0.288    0.293      1.36  \n## 14 ran_pars cond      year   sd__phi_(Intercept)  0.0367     0.0312   0.00121    0.117 \n## 15 ran_pars zi        region sd__(Intercept)      8.47       6.11     1.91      24.9   \n## 16 ran_pars zi        year   sd__(Intercept)      0.204      0.171    0.00770    0.653\n\nWe can’t really interpret any of these coefficients directly, since (1) they’re on different scales (the phi parts are all logged; all the other parts are logits), and (2) we need to combine coefficients with intercepts in order to back-transform them into percentage point values, and doing that mathematically is tricky.\nWe’ll focus on finding the marginal effect of quota, since that’s the main question we’ve been exploring throughout this post (and it was the subject of the original Tripp and Kang (2008) paper). For fun, we’ll also look at polyarchy, since it’s a continuous variable.\nAs before, marginaleffects makes it really easy to get posterior predictions of the difference between quota and no quota with avg_comparisons()\n\n\n\n\n\n\nMore about these epreds and random effects\n\n\n\nA year after writing this post, I wrote a couple other guides about the differences between posterior predictions, linear predictions, and epreds + different ways of handling random effects in predictions. Check those out for more details.\n\n\n\name_fancy_zi_quota &lt;- fancy_model %&gt;%\n  avg_comparisons(variables = \"quota\") %&gt;% \n  posterior_draws()\n\name_fancy_zi_quota %&gt;% median_hdi(draw)\n## # A tibble: 1 × 6\n##     draw .lower .upper .width .point .interval\n##    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 0.0653 0.0556 0.0749   0.95 median hdi\n\nggplot(ame_fancy_zi_quota, aes(x = draw)) +\n  stat_halfeye(.width = c(0.8, 0.95), point_interval = \"median_hdi\",\n               fill = \"#bc3032\") +\n  scale_x_continuous(labels = label_pp) +\n  labs(x = \"Average marginal effect of having a gender-based\\nquota on the proportion of women in parliament\", y = NULL,\n       caption = \"80% and 95% credible intervals shown in black\") +\n  theme_clean()\n\n\n\n\n\n\n\n\nr_fancy &lt;- ame_fancy_zi_quota %&gt;% median_hdi(draw)\n\nAfter accounting for democracy, corruption, respect for civil liberties, year, and region, the proportion of women MPs in countries with a parliamentary gender quota is 6.5 percentage points higher than in countries without a quota, on average, with a 95% credible interval ranging from 5.6 to 7.5 percentage points.\nFor fun, we can look at how the predicted outcome changes across both polyarchy and quota simultaneously (and for fun we’ll do it with marginaleffects, though it’s also doable with tidybayes::epred_draws() %&gt;% compare_levels()). Because we didn’t use any interaction terms, the slope will be the same across both levels of quota, but the plot still looks neat. Weirdly, due to the constellation of controls we included in the model, the predicted proportion of women MPs decreases with more democracy. But that’s not super important—if we were really only interested in the quota effect and we had included a sufficient set of variables to close backdoor paths, the coefficients for all other variables shouldn’t be interpreted (Keele, Stevenson, and Elwert 2020). We’ll pretend that’s the case here—this code mostly just shows how you can do all sorts of neat stuff with marginaleffects.\n\name_fancy_zi_polyarchy_quota &lt;- fancy_model %&gt;% \n  predictions(newdata = datagrid(quota = unique,\n                                 polyarchy = seq(0, 1, by = 0.1))) %&gt;% \n  posterior_draws() %&gt;% \n  # Scale polyarchy back up for plotting\n  mutate(polyarchy = polyarchy * 100)\n\n# With tidybayes instead\n# ame_fancy_zi_polyarchy_quota &lt;- fancy_model %&gt;%\n#   epred_draws(newdata = datagrid(model = fancy_model, \n#                                  quota = unique, \n#                                  polyarchy = seq(0, 1, by = 0.1)))\n\nggplot(ame_fancy_zi_polyarchy_quota,\n       aes(x = polyarchy, y = draw, color = quota, fill = quota)) +\n  stat_lineribbon(aes(fill_ramp = stat(level))) +\n  scale_y_continuous(labels = label_percent()) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.8) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8) +\n  scale_fill_ramp_discrete(range = c(0.2, 0.7)) +\n  facet_wrap(vars(quota), ncol = 2,\n             labeller = labeller(quota = c(`TRUE` = \"Quota\",\n                                           `FALSE` = \"No Quota\"))) +\n  labs(x = \"Polyarchy (democracy)\",\n       y = \"Predicted proportion of women MPs\",\n       fill = \"Quota\", color = \"Quota\",\n       fill_ramp = \"Credible interval\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html",
    "href": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html",
    "title": "A guide to working with country-year panel data and Bayesian multilevel models",
    "section": "",
    "text": "In most of my research, I work with country-level panel data where each row is a country in a specific year (Afghanistan in 2010, Afghanistan in 2011, and so on), also known as time-series cross-sectional (TSCS) data. Building statistical models for TSCS panel data is tricky, and most introductory statistics classes don’t typically cover how to do it. Countries are repeated longitudinally over time, which means that time itself influences changes in the outcome variable. Countries also have internal trends and characteristics that influence the outcome—some might have a legacy of post-colonial civil war, for example. And to make it all more complex, countries are nested in continents and regions, and there might be regional trends that influence the outcome. Oh, and time-based trends can be different across countries and continents too. Working with all these different moving parts gets really difficult.\nI have a few different past blog posts where I show some of the complexities of working with this kind of data, like calculating average marginal effects, generating inverse probability weights for panel data, or running marginal structural models on panel data, and even one where I play with economists’ preferred approach to panel data: two-way fixed effects (TWFE). But I keep forgetting the basics of how to correctly structure multilevel models for country-year data, and all the examples I find online and in textbooks are about contexts I don’t work with (participants in an experimental sleep study!). When there are examples involving countries and years, they typically refer to individual people nested in countries nested in continents (like survey respondents) and not to whole countries, which trips me up when adapting their approaches.\nSo, as a service to future-me, here’s a basic guide to dealing with country-year panel data using Bayesian multilevel modeling!\nThis guide is hardly comprehensive or rigorous. I don’t use the intimidating math notation for nested random effects that everyone else uses. I don’t set any priors for the Bayesian models and instead just stick with the defaults, which are often inefficient. I don’t check any of the Bayesian model diagnostics (like pp_check(). In real life I do do all that, but this is just an overview of the practical mechanics of multilevel models, so I’m keeping it as basic and simple as possible.\nThere are a ton of other really helpful resources about multilevel models. I had all these open in tabs while building this guide:"
  },
  {
    "objectID": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#who-this-guide-is-for",
    "href": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#who-this-guide-is-for",
    "title": "A guide to working with country-year panel data and Bayesian multilevel models",
    "section": "Who this guide is for",
    "text": "Who this guide is for\nHere’s what I assume you know:\n\nYou’re familiar with R and the tidyverse (particularly dplyr and ggplot2).\nYou’re familiar with brms for running Bayesian regression models. See the vignettes here, examples like this, or resources like these for an introduction.\n\nYou’re somewhat familiar with multilevel models. Confusingly, these are also called mixed effect models, random effect models, and hierarchical models, among others! They’re all the same thing! (image below by Chelsea Parlett-Pelleriti)\n\n\n\n\n\n\n\n\nSee examples like this or this or this or this. Basically Google “lme4 example” (lme4 is what you use for frequentist, non-Bayesian multilevel models with R) or “brms multilevel example” and you’ll find a bunch. For a more formal treatment, see chapter 12 in Richard McElreath’s Statistcal Rethinking book (or this R translation of it by Solomon Kurz).\n\n\nLet’s get started by loading all the libraries we’ll need (and creating a couple helper functions):\n\nlibrary(tidyverse)    # ggplot, dplyr, %&gt;%, and friends\nlibrary(gapminder)    # Country-year panel data from the Gapminder Project\nlibrary(brms)         # Bayesian modeling through Stan\nlibrary(tidybayes)    # Manipulate Stan objects in a tidy way\nlibrary(broom)        # Convert model objects to data frames\nlibrary(broom.mixed)  # Convert brms model objects to data frames\nlibrary(emmeans)      # Calculate marginal effects in even fancier ways\nlibrary(ggh4x)        # For nested facets in ggplot\nlibrary(ggrepel)      # For nice non-overlapping labels in ggplot\nlibrary(ggdist)       # For distribution-related ggplot geoms\nlibrary(scales)       # For formatting numbers with comma(), dollar(), etc.\nlibrary(patchwork)    # For combining plots\nlibrary(ggokabeito)   # Colorblind-friendly color palette\n\n# Make all the random draws reproducible\nset.seed(1234)\n\n# Bayes stuff\n# Use the cmdstanr backend for Stan because it's faster and more modern than\n# the default rstan. You need to install the cmdstanr package first\n# (https://mc-stan.org/cmdstanr/) and then run cmdstanr::install_cmdstan() to\n# install cmdstan on your computer.\noptions(mc.cores = 4,  # Use 4 cores\n        brms.backend = \"cmdstanr\")\nbayes_seed &lt;- 1234\n\n# Custom ggplot theme to make pretty plots\n# Get Barlow Semi Condensed at https://fonts.google.com/specimen/Barlow+Semi+Condensed\ntheme_clean &lt;- function() {\n  theme_minimal(base_family = \"Barlow Semi Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\", size = rel(0.8), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}\n\n# Make labels use Barlow by default\nupdate_geom_defaults(\"label_repel\", \n                     list(family = \"Barlow Semi Condensed\",\n                          fontface = \"bold\"))\nupdate_geom_defaults(\"label\", \n                     list(family = \"Barlow Semi Condensed\",\n                          fontface = \"bold\"))\n\n# The ggh4x paackage includes a `facet_nested()` function for nesting facets\n# (like countries in continents). Throughout this post, I want the\n# continent-level facets to use bolder text and a lighter gray strip. I don't\n# want to keep repeating all these settings, though, so I create a list of the\n# settings here with `strip_nested()` and feed it to `facet_nested_wrap()` later\nnested_settings &lt;- strip_nested(\n  text_x = list(element_text(family = \"Barlow Semi Condensed Black\", \n                             face = \"plain\"), NULL),\n  background_x = list(element_rect(fill = \"grey92\"), NULL),\n  by_layer_x = TRUE)"
  },
  {
    "objectID": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#example-data-health-and-wealth-over-time",
    "href": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#example-data-health-and-wealth-over-time",
    "title": "A guide to working with country-year panel data and Bayesian multilevel models",
    "section": "Example data: health and wealth over time",
    "text": "Example data: health and wealth over time\nThroughout this example, we’re going to use data from Hans Rosling’s Gapminder project to explore the relationship between wealth (measured as GDP per capita, or average income per person) and health (measured as life expectancy). You may have seen Hans Rosling’s delightful TED talk showing how global health and wealth have been increasing since 1800—he became internet famous because of this work. Sadly, Hans died in February 2017.\nBefore we start, watch this short 4-minute version of his famous health/wealth presentation. It’s the best overview of the complexities of the panel data we’re going to be working with:\n\n\n\n\n\n\n\nThe original health/wealth data is all available at the Gapminder Project, but Jenny Bryan has conveniently created an R package called gapminder with the data already cleaned and nicely structured, so we’ll use that. The data from gapminder includes observations from 142 different countries across 5 continents, and it ranges from 1952–2007 (skipping every five years, so there are observations for 1952, 1957, 1962, and so on). To simplify things, we’ll drop the Oceania continent (which includes only two countries: Australia and New Zealand), and we’ll choose two representative-ish countries in each of the remaining continents for the different country-specific trends we’ll be looking at throughout this guide.\n\n# Little dataset of 8 countries (2 for each of the 4 continents in the data)\n# that are good examples of different trends and intercepts\ncountries &lt;- tribble(\n  ~country,       ~continent,\n  \"Egypt\",        \"Africa\",\n  \"Sierra Leone\", \"Africa\",\n  \"Pakistan\",     \"Asia\",\n  \"Yemen, Rep.\",  \"Asia\",\n  \"Bolivia\",      \"Americas\",\n  \"Canada\",       \"Americas\",\n  \"Italy\",        \"Europe\",\n  \"Portugal\",     \"Europe\"\n)\n\n# Clean up the gapminder data a little\ngapminder &lt;- gapminder::gapminder %&gt;%\n  # Remove Oceania since there are only two countries there and we want bigger\n  # continent clusters\n  filter(continent != \"Oceania\") %&gt;%\n  # Scale down GDP per capita so it's more interpretable (\"a $1,000 increase in\n  # GDP\" vs. \"a $1 increase in GDP\")\n  # Also log it\n  mutate(gdpPercap_1000 = gdpPercap / 1000,\n         gdpPercap_log = log(gdpPercap)) %&gt;% \n  mutate(across(starts_with(\"gdp\"), list(\"z\" = ~scale(.)))) %&gt;% \n  # Make year centered on 1952 (so we're counting the years since 1952). This\n  # (1) helps with interpretability, since the intercept will show the average\n  # at 1952 instead of the average at 0 CE, and (2) helps with estimation speed\n  # since brms/Stan likes to work with small numbers\n  mutate(year_orig = year,\n         year = year - 1952) %&gt;% \n  # Indicator for the 8 countries we're focusing on\n  mutate(highlight = country %in% countries$country)\n\n# Extract rows for the example countries\noriginal_points &lt;- gapminder %&gt;% \n  filter(country %in% countries$country) %&gt;% \n  # Use real years\n  mutate(year = year_orig)\n\nHere’s what this cleaned up data looks like. Notice how the scaled and centered columns (the ones with the _z suffix) show up a little differently here (as one-column matrices)—we’ll see why that is later.\n\nglimpse(gapminder)\n## Rows: 1,680\n## Columns: 13\n## $ country          &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Af…\n## $ continent        &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi…\n## $ year             &lt;dbl&gt; 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 0, 5, 10, 15, 20,…\n## $ lifeExp          &lt;dbl&gt; 28.8, 30.3, 32.0, 34.0, 36.1, 38.4, 39.9, 40.8, 41.7, 41.8, 42.…\n## $ pop              &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12881…\n## $ gdpPercap        &lt;dbl&gt; 779, 821, 853, 836, 740, 786, 978, 852, 649, 635, 727, 975, 160…\n## $ gdpPercap_1000   &lt;dbl&gt; 0.779, 0.821, 0.853, 0.836, 0.740, 0.786, 0.978, 0.852, 0.649, …\n## $ gdpPercap_log    &lt;dbl&gt; 6.66, 6.71, 6.75, 6.73, 6.61, 6.67, 6.89, 6.75, 6.48, 6.45, 6.5…\n## $ gdpPercap_z      &lt;dbl[,1]&gt; &lt;matrix[30 x 1]&gt;\n## $ gdpPercap_1000_z &lt;dbl[,1]&gt; &lt;matrix[30 x 1]&gt;\n## $ gdpPercap_log_z  &lt;dbl[,1]&gt; &lt;matrix[30 x 1]&gt;\n## $ year_orig        &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,…\n## $ highlight        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…"
  },
  {
    "objectID": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#the-effect-of-continent-country-and-time-on-life-expectancy",
    "href": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#the-effect-of-continent-country-and-time-on-life-expectancy",
    "title": "A guide to working with country-year panel data and Bayesian multilevel models",
    "section": "The effect of continent, country, and time on life expectancy",
    "text": "The effect of continent, country, and time on life expectancy\nBefore we look at the relationship between wealth and health over time, it’s important to understand how multilevel modeling works, so we’ll simplify things and only look at the the relationship between life expectancy and time and how that relationship differs across continent and country.\nAs we saw in the video, pretty much every country’s life expectancy has increased over time. Countries all have different starting points or intercepts in 1952, and continent-specific differences influence those intercepts (e.g., Western Europe has much higher life expectancy than sub-Saharan Africa), and countries each have their own trends over time. Compare Egypt, which starts off fairly low and then rockets up fairly quickly from the 1970s to the 1990s, with Canada, which starts fairly high and ends really high. Some countries see negative trends over time, like Rwanda and Sierra Leone.\n\nggplot(gapminder, aes(x = year_orig, y = lifeExp, \n                      group = country, color = continent)) +\n  geom_line(aes(size = highlight)) +\n  geom_smooth(method = \"lm\", aes(color = NULL, group = NULL), \n              color = \"grey60\", size = 1, linetype = \"21\",\n              se = FALSE, show.legend = FALSE) +\n  geom_label_repel(data = filter(gapminder, year == 0, highlight == TRUE), \n                   aes(label = country), direction = \"y\", size = 3, seed = 1234, \n                   show.legend = FALSE) +\n  annotate(geom = \"label\", label = \"Global trend\", x = 1952, y = 50,\n           size = 3, color = \"grey60\") +\n  scale_size_manual(values = c(0.075, 1), guide = \"none\") +\n  scale_color_okabe_ito(order = c(2, 3, 6, 1)) +\n  labs(x = NULL, y = \"Life expectancy\", color = \"Continent\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nRegular regression\nTo start, we’ll make a simple boring model that completely ignores continent- and country-level differences and only looks at how much life expectancy increases over time. This is the dotted gray line in the plot above.\n\nmodel_boring &lt;- brm(\n  bf(lifeExp ~ year),\n  data = gapminder,\n  chains = 4, seed = bayes_seed\n)\n## Start sampling\n\n\ntidy(model_boring)\n## # A tibble: 3 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       50.3      0.530    49.2      51.3  \n## 2 fixed    cond      &lt;NA&gt;     year               0.327    0.0166    0.295     0.360\n## 3 ran_pars cond      Residual sd__Observation   11.6      0.197    11.2      12.0\n\nWhen year is zero, or in 1952, the average life expectancy is 50.26 years, and it increases by 0.33 years each year after that.\nThat’s cool, I guess, but we’re ignoring the structure of the data. We’re not accounting for any of the country-specific or region-specific trends that might also influence life expectancy. This is apparent when we plot predictions from this model—each country has the same predicted trend and the same starting point. None of these lines fit any of the example countries at all.\n\npred_model_boring &lt;- model_boring %&gt;%\n  epred_draws(newdata = expand_grid(country = countries$country,\n                                    year = unique(gapminder$year))) %&gt;% \n  mutate(year = year + 1952) %&gt;% \n  left_join(countries, by = \"country\")\n\nggplot(pred_model_boring, aes(x = year, y = .epred)) +\n  geom_point(data = original_points, aes(y = lifeExp), \n             color = \"grey50\", size = 3, alpha = 0.5) +\n  stat_lineribbon(alpha = 0.5, size = 0.5) +\n  scale_fill_brewer(palette = \"Reds\") +\n  labs(title = \"Global year trend with no country-based variation\",\n       subtitle = \"lifeExp ~ year\",\n       x = NULL, y = \"Predicted life expectancy\") +\n  guides(fill = \"none\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings) +\n  theme_clean() +\n  theme(legend.position = \"bottom\",\n        plot.subtitle = element_text(family = \"Consolas\"),\n        axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\n\n\n\n\n\n\nIntroduction to random effects: Intercepts for each continent\nFor now, we’re interested in the effect of time on life expectancy: how much does life expectancy increase on average as time passes (or as year increases)? We already estimated this basic model:\n\\[\n\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Year} + \\epsilon\n\\]\nThe year effect here is the \\(\\beta_1\\) coefficient, but this global estimate doesn’t account for continent- or country-specific differences, and that kind of variation is important—we don’t necessarily want to combine the trends in life expectancy across Western Europe and Latin America since they’re so different.\nThat \\(\\epsilon\\) error term is doing a lot of work. It represents all the variation in life expectancy that’s not explained by (1) the baseline global life expectancy when year is 0 (or 1952), and by (2) increases in year. All sorts of unmeasured things are hidden in that \\(\\epsilon\\), like continent-level differences that might explain some additional variation in life expectancy. We can pull that continent-specific variation out of the error term to show that it exists:\n\\[\n\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Year} + (b_{\\text{Continent}} + \\epsilon)\n\\]\nWe use the Latin letter \\(b\\) instead of the Greek \\(\\beta\\) to signal that it’s a different kind of parameter. The \\(\\beta\\) parameters are “fixed effects” (this gets so confusing since in the world of econometrics and political science, “fixed effects” typically refer to indicator variables, like when you control for state or country. That’s not the case here! Fixed effects here refer to population-level parameters that apply to all the observations in the data. Random effects refer to variations or deviations within subpopulations in the data (country, year, region, etc.)). Random effects, or \\(b\\), show the offset from specific fixed parameters.\nRight now, this continent-level offset is hidden in the unestimated error term \\(\\epsilon\\), but we can move it around and pair it with one of the fixed terms in the model: either the intercept or year. For now we’ll lump it in with the intercept and say that this continent random effect shifts the intercept up and down based on continent-level differences (we’ll lump it in with the year effect a little later). We can rewrite our model like this now:\n\\[\n\\text{Life expectancy} = (\\beta_0 + b_{0, \\text{Continent}}) + \\beta_1 \\text{Year} + \\epsilon\n\\]\nAll we’ve really done is take \\(b_{\\text{Continent}}\\), move it over next to the intercept term \\(\\beta_0\\), and add a 0 subscript to show that it represents the offset or deviation from \\(\\beta_0\\) for each continent.\nIf you’ve ever worked with regular old OLS regression with lm(), you might be thinking that this is basically just the same as including an indicator variable for continent. For instance, if you run a model like this…\n\nmodel_super_boring &lt;- lm(lifeExp ~ year + continent, data = gapminder)\ntidy(model_super_boring)\n## # A tibble: 5 × 5\n##   term              estimate std.error statistic   p.value\n##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)         39.9      0.411       97.0 0        \n## 2 year                 0.328    0.0104      31.5 2.97e-171\n## 3 continentAmericas   15.8      0.517       30.5 3.47e-163\n## 4 continentAsia       11.2      0.473       23.7 3.69e-107\n## 5 continentEurope     23.0      0.487       47.3 9.85e-311\n\n…you get coefficients for each of the continents (except Africa, which is the base case). You can then add these continent-specific coefficients to the intercept to get continent-specific intercepts. For example, the average life expectancy in Africa in 1952 (i.e. when year is 0, or the intercept) is 39.86 years. Europe’s average life expectancy in 1952 is 39.86 + 23.04, or 62.9 years, and so on.\nSo why use these random effects instead of using indicator variables like this?\nOne reason is that thinking about these group-specific offsets as random effects allows us to model them more richly. We can define the whole range of group offsets using a distribution. For example, we could say that the intercept offsets are normally distributed with a mean of 0 and a standard deviation of \\(\\tau\\) (note that we’re using Greek again—this \\(\\tau\\) variance is a population-level parameter and doesn’t vary by group). Some continents will have a positive offset; some will have a negative offset; in general the average of the offsets should be 0; and the variation in those offsets can be measured with \\(\\tau\\).\n\\[\nb_{0, \\text{Continent}} \\sim \\mathcal{N}(0, \\tau)\n\\]\nNotice how we’re now working with a regression model with two parts: one part at a continent level where we estimate continent-specific offsets in the intercept, and one part at the observation level (which in this case is countries). That’s why this approach is often called “multilevel modeling”—we have multiple levels in our model:\n\\[\n\\begin{aligned}\n\\text{Life expectancy} &= (\\beta_0 + b_{0, \\text{Continent}}) + \\beta_1 \\text{Year} + \\epsilon \\\\\nb_{0, \\text{Continent}} &\\sim \\mathcal{N}(0, \\tau)\n\\end{aligned}\n\\]\nFixed (population-level) effects\nHere’s what this looks like in practice. We’ll take our boring basic model and add a (1 | continent) term, which means that we’re letting the intercept (or 1) vary by continent. This is the code version of lumping the random continent effect with the intercept, or creating \\((\\beta_0 + b_{0, \\text{Continent}})\\).\n\n# Each continent gets its own intercept\nmodel_continent_only &lt;- brm(\n  bf(lifeExp ~ year + (1 | continent)),\n  data = gapminder,\n  control = list(adapt_delta = 0.95),\n  chains = 4, seed = bayes_seed\n)\n## Start sampling\n## Warning: 2 of 4000 (0.0%) transitions ended with a divergence.\n## See https://mc-stan.org/misc/warnings for details.\n\nIf we look at the results we find basically the same coefficients we found in the original boring model: average global life expectancy in 1952 is 52.4 years, and it increases by 0.33 each year after that, on average. We also have errors and credible intervals, like any regular brms-based regression model.\n\ntidy(model_continent_only)\n## # A tibble: 4 × 8\n##   effect   component group     term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;      (Intercept)       52.4      5.83     40.8      64.3  \n## 2 fixed    cond      &lt;NA&gt;      year               0.328    0.0105    0.306     0.348\n## 3 ran_pars cond      continent sd__(Intercept)   12.4      5.52      5.67     27.3  \n## 4 ran_pars cond      Residual  sd__Observation    7.37     0.126     7.13      7.62\n\nThese \\(\\beta_0\\) and \\(\\beta_1\\) coefficients for the intercept and year are considered “fixed effects” (again, this is totally unrelated to the idea of indicator variables from econometrics, political science, and other social sciences!). These are population-level parameters that transcend continent-specific differences. The default results from summary() make this distinction a little more clear and calls them “Population-Level Effects”:\n\nsummary(model_continent_only)\n...\n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    52.40      5.83    40.78    64.29 1.01      865     1119\n## year          0.33      0.01     0.31     0.35 1.00     2351     2081\n...\n\nWe can clean up the results from tidy() and show only these fixed effects with the effects = \"fixed\" argument:\n\ntidy(model_continent_only, effects = \"fixed\")\n## # A tibble: 2 × 7\n##   effect component term        estimate std.error conf.low conf.high\n##   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed  cond      (Intercept)   52.4      5.83     40.8      64.3  \n## 2 fixed  cond      year           0.328    0.0105    0.306     0.348\n\nContinent-level variance\nThe whole reason we’re using a multilevel model here instead of just using indicator variables is that we can use information about the variation in continents to improve our coefficient estimates and model accuracy. Notice that there were extra new parameters in the output from tidy(), which we can also see if we use the effects = \"ran_pars\" argument (these also show up in the output from summary() as “Group-Level Effects”):\n\ntidy(model_continent_only, effects = \"ran_pars\")\n## # A tibble: 2 × 8\n##   effect   component group     term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 ran_pars cond      continent sd__(Intercept)    12.4      5.52      5.67     27.3 \n## 2 ran_pars cond      Residual  sd__Observation     7.37     0.126     7.13      7.62\n\nWe have two extra random parameters here:\n\nThe estimated variance/standard deviation of the continent effect, or \\(\\tau\\) from our \\(b_{0, \\text{Continent}} \\sim \\mathcal{N}(0, \\tau)\\) model level from before, represented with the sd__(Intercept) term in the continent group: 12.4\n\nThe total residual variation (i.e. unexplained variation in life expectancy), represented with the sd__Observation term in the Residual group: 7.37\n\n\nTo borrow heavily from Michael Clark’s excellent explanation of these variance components, this \\(\\tau\\) parameter tells how much life expectancy bounces around as we move from continent to continent. We can predict life expectancy based on the year trend, but each continent has its own unique life expectancy offset, and this \\(\\tau\\) is the average deviation across all the continents. Practically speaking, there’s a substantial amount of cross-continent variation here: 12.4 years!\nWe can also think about this continent-level variance as a percentage of the total residual variance in the model. Continent-level variation contributes 63% (12.4 / (12.4 + 7.37)) of the total variance in life expectancy.\nContinent-level random effects\nWe can see these actual offsets with the ranef() function. By default this returns an unwieldy list with multi-dimensional arrays nested in it, so we’ll convert it to a nice data frame here so we can work with it later.\n\ncontinent_offsets &lt;- ranef(model_continent_only)$continent %&gt;% \n  as_tibble(rownames = \"continent\")\ncontinent_offsets\n## # A tibble: 4 × 5\n##   continent Estimate.Intercept Est.Error.Intercept Q2.5.Intercept Q97.5.Intercept\n##   &lt;chr&gt;                  &lt;dbl&gt;               &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n## 1 Africa                -12.5                 5.81         -24.3           -0.830\n## 2 Americas                3.24                5.81          -8.52          15.1  \n## 3 Asia                   -1.34                5.81         -13.1           10.5  \n## 4 Europe                 10.5                 5.82          -1.37          22.3\n\nEach of these estimates represent the continent-specific offsets in the intercept, or \\(b_{0, \\text{Continent}}\\). To help with the intuition, we can add these offsets to the global population-level intercept to see the actual intercept for each continent. Let’s plug these values into our model equation, with year set to 0 (or 1952):\n\\[\n\\begin{aligned}\n\\text{Life expectancy}_\\text{general} &= (\\beta_0 + b_{0, \\text{Continent}}) + \\beta_1 \\text{Year} + \\epsilon \\\\\n\\text{Life expectancy}_\\text{Africa} &= (52.4 + -12.52) + (0.33 \\times 0) = 39.87 \\\\\n\\text{Life expectancy}_\\text{Americas} &= (52.4 + 3.24) + (0.33 \\times 0) = 55.64 \\\\\n\\text{Life expectancy}_\\text{Asia} &= (52.4 + -1.34) + (0.33 \\times 0) = 51.06 \\\\\n\\text{Life expectancy}_\\text{Europe} &= (52.4 + 10.48) + (0.33 \\times 0) = 62.88\n\\end{aligned}\n\\]\nWe don’t need to do this all by hand though. The coef() function will return these group-specific intercepts with the offsets already incorporated. Notice how the results are the same—each continent has its own intercept, while the \\(\\beta_1\\) coefficient for year is the same across continents:\n\ncoef(model_continent_only)$continent %&gt;% \n  as_tibble(rownames = \"continent\") %&gt;% \n  select(continent, starts_with(\"Estimate\"))\n## # A tibble: 4 × 3\n##   continent Estimate.Intercept Estimate.year\n##   &lt;chr&gt;                  &lt;dbl&gt;         &lt;dbl&gt;\n## 1 Africa                  39.9         0.328\n## 2 Americas                55.6         0.328\n## 3 Asia                    51.1         0.328\n## 4 Europe                  62.9         0.328\n\nAlternatively, we can also use the powerful emmeans package to calculate predicted values at different combinations of our model’s variables. In this case our model uses a Gaussian distribution so it works like regular OLS regression (which means we can just add coefficients together), but other models like logistic regression or beta regression require a lot more work to combine the coefficients correctly. The emmeans() function will calculate predicted values of life expectancy, while the emtrends() function will calculate coefficients, incorporating group-level effects as needed (that’s what the re_formula = NULL argument is doing here; see this post for more details about that). These values should be the same that we found both manually with math and with coef():\n\nmodel_continent_only %&gt;% \n  emmeans(~ continent + year,\n          at = list(year = 0),  # Look at predicted values for 1952\n          epred = TRUE,  # Use expected predictions from the posterior\n          re_formula = NULL)  # Incorporate random effects\n## Loading required namespace: rstanarm\n##  continent year emmean lower.HPD upper.HPD\n##  Africa       0   39.9      39.1      40.7\n##  Americas     0   55.6      54.7      56.7\n##  Asia         0   51.1      50.1      51.9\n##  Europe       0   62.9      61.9      63.8\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nVisualize continent-specific trends\nFinally, let’s visualize this continent-specific year trend. Each of these lines has the same slope, but the intercepts now shift up and down based on continent. The lines are substantially higher in Europe and substantially lower in Africa:\n\nnewdata_country_continent &lt;- expand_grid(country = countries$country,\n                                         year = unique(gapminder$year)) %&gt;% \n  left_join(countries, by = \"country\")\n\npred_model_continent_only &lt;- model_continent_only %&gt;%\n  epred_draws(newdata_country_continent, re_formula = NULL) %&gt;% \n  mutate(year = year + 1952)\n\nggplot(pred_model_continent_only, aes(x = year, y = .epred)) +\n  geom_point(data = original_points, aes(y = lifeExp),\n             color = \"grey50\", size = 3, alpha = 0.5) +\n  stat_lineribbon(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Reds\") +\n  labs(title = \"Intercepts for year trend vary by continent\",\n       subtitle = \"lifeExp ~ year + (1 | continent)\",\n       x = NULL, y = \"Predicted life expectancy\") +\n  guides(fill = \"none\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings) +\n  theme_clean() +\n  theme(legend.position = \"bottom\",\n        plot.subtitle = element_text(family = \"Consolas\"),\n        axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\n\n\n\n\n\n\nIntercepts for each country\nContinent-level effects are neat, but the predicted trends in the plot above still don’t really fit the data that well (except maybe in Europe). Instead of including random effects for continents, we can include country effects so that each country gets its own offset and intercept. Let’s make this model:\n\\[\n\\begin{aligned}\n\\text{Life expectancy} &= (\\beta_0 + b_{0, \\text{Country}}) + \\beta_1 \\text{Year} + \\epsilon \\\\\nb_{0, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau)\n\\end{aligned}\n\\]\n\n# Each country gets its own intercept\nmodel_country_only &lt;- brm(\n  bf(lifeExp ~ year + (1 | country)),\n  data = gapminder,\n  chains = 4, seed = bayes_seed,\n  iter = 4000  # Double the number of iterations to help with convergence\n)\n## Start sampling\n\n\ntidy(model_country_only)\n## # A tibble: 4 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       50.3     0.909     48.5      52.0  \n## 2 fixed    cond      &lt;NA&gt;     year               0.328   0.00516    0.317     0.338\n## 3 ran_pars cond      country  sd__(Intercept)   11.1     0.655      9.93     12.5  \n## 4 ran_pars cond      Residual sd__Observation    3.60    0.0657     3.48      3.73\n\nFixed (population-level) effects\nThe global population-level coefficients (or “fixed effects”, but again these are not the same as indicator variables!) are roughly the same that we saw in both the original boring model and the model with continent effects: in 1952 (when year is 0) the average life expectancy is 50.3 years, and it increases by 0.33 years annually after that.\nCountry-level variation\nBecause we’re using random effects, we have information about the \\(\\tau\\) parameter for the variance in country offsets, which shows us how much life expectancy bounces around from country to country. As with continents, we have a ton of cross-country variation: 11.07 years! Country-specific offsets are centered around 0 ± a huge standard deviation of 11.07. Accounting for country differences is going to be crucial for this model.\nIf we look at country-level variance as a percentage of the total residual variance, we can see that country differences are really important. Country-level variation contributes 75% (11.07 / (11.07 + 3.6)) of the total variance in life expectancy.\nCountry-level random effects\nWe can look at country-level offsets for our eight example countries with ranef(). Canada, Italy, and Portugal have offsets that are substantially above the global average, while Yemen and Sierra Leone are substantially below it.\n\ncountry_offsets &lt;- ranef(model_country_only)$country %&gt;%\n  as_tibble(rownames = \"country\") %&gt;% \n  filter(country %in% countries$country) %&gt;% \n  select(country, starts_with(\"Estimate\"))\ncountry_offsets\n## # A tibble: 8 × 2\n##   country      Estimate.Intercept\n##   &lt;chr&gt;                     &lt;dbl&gt;\n## 1 Bolivia                   -6.76\n## 2 Canada                    15.5 \n## 3 Egypt                     -3.02\n## 4 Italy                     14.6 \n## 5 Pakistan                  -4.39\n## 6 Portugal                  11.0 \n## 7 Sierra Leone             -22.3 \n## 8 Yemen, Rep.              -12.4\n\nWe can see the actual country-specific intercepts too. Each of these intercepts represents the life expectancy in these countries in 1952. Note that the coefficient for year is the same in each—that’s because it’s a global population-level effect and isn’t country-specific.\n\ncoef(model_country_only)$country %&gt;%\n  as_tibble(rownames = \"country\") %&gt;% \n  filter(country %in% countries$country) %&gt;% \n  select(country, starts_with(\"Estimate\"))\n## # A tibble: 8 × 3\n##   country      Estimate.Intercept Estimate.year\n##   &lt;chr&gt;                     &lt;dbl&gt;         &lt;dbl&gt;\n## 1 Bolivia                    43.5         0.328\n## 2 Canada                     65.8         0.328\n## 3 Egypt                      47.3         0.328\n## 4 Italy                      64.9         0.328\n## 5 Pakistan                   45.9         0.328\n## 6 Portugal                   61.3         0.328\n## 7 Sierra Leone               28.0         0.328\n## 8 Yemen, Rep.                37.9         0.328\n\nVisualize country-specific trends\nWith these country-specific intercepts, we can now see that the model fits a lot better across countries. Each predicted year trend still has the same slope, but the line is shifted up and down based on the country-specific offsets. Great!\n\npred_model_country_only &lt;- model_country_only %&gt;%\n  epred_draws(newdata = expand_grid(country = countries$country,\n                                    year = unique(gapminder$year)),\n              re_formula = NULL) %&gt;% \n  mutate(year = year + 1952) %&gt;% \n  left_join(countries, by = \"country\")\n\nggplot(pred_model_country_only, aes(x = year, y = .epred)) +\n  geom_point(data = original_points, aes(y = lifeExp), \n             color = \"grey50\", size = 3, alpha = 0.5) +\n  stat_lineribbon(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Reds\") +\n  labs(title = \"Intercepts for year trend vary by country\",\n       subtitle = \"lifeExp ~ year + (1 | country)\",\n       x = NULL, y = \"Predicted life expectancy\") +\n  guides(fill = \"none\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings) +\n  theme_clean() +\n  theme(legend.position = \"bottom\",\n        plot.subtitle = element_text(family = \"Consolas\"),\n        axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\n\n\n\n\n\n\n⭐ Intercepts and slopes for each country ⭐\nThese country-level effects are cool and fit the data fairly well, but there are still some substantial errors. The predicted lines fit Italy and Portugal pretty well, but the year trend in Yemen, Egypt, and Bolivia is steeper than predicted, while the year trend in Sierra Leone is shallower. It would be neat if we could incorporate country-specific offsets into both the intercept and the year trend. Fortunately, multilevel modeling lets us do this!\nWe’d previously excised random country effects \\(b_{\\text{Country}}\\) from the error term \\(\\epsilon\\) and lumped them in with the fixed intercept term, creating country-specific offsets \\(b_{0, \\text{Country}}\\). We can do a similar thing and add country-specific offsets to the fixed year term too. We’ll call this \\(b_{1, \\text{Country}}\\) (with a 1 subscript to show that it goes with \\(\\beta_1\\)). This random term also gets its own distribution of errors with its own variation. We’ll add subscripts to these \\(\\tau\\) terms so we can keep track of the intercept variance and the year trend variance.\n\\[\n\\begin{aligned}\n\\text{Life expectancy} &= (\\beta_0 + b_{0, \\text{Country}}) + (\\beta_1 + b_{1, \\text{Country}}) \\text{Year} + \\epsilon \\\\\nb_{0, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_0) \\\\\nb_{1, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_1)\n\\end{aligned}\n\\]\nThe R syntax for this kind of random effects term is (1 + year | country), which means that we’re letting both the intercept (1) and year vary by country.\n\n# Each country gets its own slope and intercept for the year trend\nmodel_country_year &lt;- brm(\n  bf(lifeExp ~ year + (1 + year | country)),\n  data = gapminder,\n  chains = 4, seed = bayes_seed,\n  iter = 4000  # Double the number of iterations to help with convergence\n)\n## Start sampling\n\n\ntidy(model_country_year)\n## # A tibble: 6 × 8\n##   effect   component group    term                  estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)             50.3      1.08     48.1      52.3  \n## 2 fixed    cond      &lt;NA&gt;     year                     0.328    0.0139    0.301     0.355\n## 3 ran_pars cond      country  sd__(Intercept)         12.3      0.747    10.9      13.8  \n## 4 ran_pars cond      country  sd__year                 0.161    0.0103    0.142     0.182\n## 5 ran_pars cond      country  cor__(Intercept).year   -0.422    0.0710   -0.551    -0.274\n## 6 ran_pars cond      Residual sd__Observation          2.20     0.0414    2.12      2.28\n\nFixed (population-level) effects\nThe global population-level coefficients are once again basically the same that we’ve seen before: in 1952 the average life expectancy is 50.25 years, and it increases by 0.33 years annually after that.\nCountry-level variation\nWe have a couple new rows in the random parameters part of our results, though. The variance for the intercept (sd__(Intercept)) corresponds to \\(\\tau_0\\) and still shows us how much life expectancy bounces around across countries. We have a new term for country-specific variation in the year effect (sd__year), which corresponds to \\(\\tau_1\\). This is considerably smaller than the intercept variance, but that’s because the year trend is a slope that measures the increase or decrease in life expectancy as year increases. Recall that our global year trend is 0.33. Country-specific differences imply that our trend is something like 0.33 ± 0.16 standard deviations across countries.\nSince we’re assuming a normal distribution of random offsets, the distribution of all country-specific year trends looks something like this. The year trend in most countries is positive, sometimes going as high as 0.8 years of life expectancy per year:\n\nggplot(data = tibble(x = seq(-0.3, 1, by = 0.1)), aes(x = x)) +\n  stat_function(geom = \"area\",\n                fun = dnorm, args = list(mean = 0.327, sd = 0.161),\n                fill = palette_okabe_ito(order = 6)) +\n  geom_vline(xintercept = 0) +\n  labs(title = \"Year trend across countries\",\n       x = \"Annual increase in life expectancy\") +\n  theme_clean()\n\n\n\n\n\n\n\nThe second new parameter we have is cor__(Intercept).year, which shows us the correlation of the country-specific intercepts and slopes. This is a correlation, so it ranges from −1 to 1, with 0 indicating no correlation. In this case, we see a correlation of -0.42, which is fairly strong. It indicates that countries with lower intercepts are associated with larger year trends. This makes sense—every country in the dataset sees increased life expectancy over time, and countries with a lower starting point have more room to grow and grow more quickly. Canada, for instance, starts off with high life expectancy and grows a little between 1952 and 2007, while Egypt starts off with low life expectancy and grows rapidly through 2007.\nCountry-level random effects\nWe can look at the country-level offsets for both the intercept and the year slope now. As before, Canada, Italy, and Portugal have intercept offsets that are substantially above the global average, while Bolivia, Yemen, and Sierra Leone are substantially below it. We can also now look at country-specific slope offsets: Egypt and Yemen have much steeper slopes than the global average, while Canada’s is shallower than the global effect.\n\ncountry_year_offsets &lt;- ranef(model_country_year)$country %&gt;%\n  as_tibble(rownames = \"country\") %&gt;% \n  filter(country %in% countries$country) %&gt;% \n  select(country, starts_with(\"Estimate\"))\ncountry_year_offsets\n## # A tibble: 8 × 3\n##   country      Estimate.Intercept Estimate.year\n##   &lt;chr&gt;                     &lt;dbl&gt;         &lt;dbl&gt;\n## 1 Bolivia                  -11.2        0.163  \n## 2 Canada                    18.5       -0.104  \n## 3 Egypt                     -8.96       0.216  \n## 4 Italy                     16.2       -0.0562 \n## 5 Pakistan                  -6.42       0.0746 \n## 6 Portugal                  10.9        0.00814\n## 7 Sierra Leone             -19.5       -0.107  \n## 8 Yemen, Rep.              -19.7        0.265\n\nWe can also look at the already-combined fixed effect + random offset for country-specific intercepts and slopes. Notice how both the intercept and the slope is different in each country. That’s magical!\n\ncoef(model_country_year)$country %&gt;%\n  as_tibble(rownames = \"country\") %&gt;% \n  filter(country %in% countries$country) %&gt;% \n  select(country, starts_with(\"Estimate\"))\n## # A tibble: 8 × 3\n##   country      Estimate.Intercept Estimate.year\n##   &lt;chr&gt;                     &lt;dbl&gt;         &lt;dbl&gt;\n## 1 Bolivia                    39.0         0.491\n## 2 Canada                     68.7         0.224\n## 3 Egypt                      41.3         0.544\n## 4 Italy                      66.5         0.272\n## 5 Pakistan                   43.8         0.402\n## 6 Portugal                   61.1         0.336\n## 7 Sierra Leone               30.8         0.221\n## 8 Yemen, Rep.                30.5         0.592\n\nVisualize country-specific intercepts and slopes\nWith country-specific intercepts and slopes, predicted life expectancy fits really well in each country over time. Each country has a different starting point and grows at different rates.\n\npred_model_country_year &lt;- model_country_year %&gt;%\n  epred_draws(newdata = expand_grid(country = countries$country,\n                                    year = unique(gapminder$year)),\n              re_formula = NULL) %&gt;% \n  mutate(year = year + 1952) %&gt;% \n  left_join(countries, by = \"country\")\n\nggplot(pred_model_country_year, aes(x = year, y = .epred)) +\n  geom_point(data = original_points, aes(y = lifeExp), \n             color = \"grey50\", size = 3, alpha = 0.5) +\n  stat_lineribbon(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Reds\") +\n  labs(title = \"Intercepts and slopes for year trend vary by country\",\n       subtitle = \"lifeExp ~ year + (1 + year | country)\",\n       x = NULL, y = \"Predicted life expectancy\") +\n  guides(fill = \"none\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings) +\n  theme_clean() +\n  theme(legend.position = \"bottom\",\n        plot.subtitle = element_text(family = \"Consolas\"),\n        axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\n\n\n\n\n\n\nIntercepts and slopes for each country + account for year-specific differences\nOrdinarily, the year + (1 + year | country) approach is sufficient for working with panel data, since the population-level year effects gets their own country-specific intercepts and slopes. However, we can do a couple extra bonus things to make our model even richer.\nFirst, we’ll add extra year-specific random effects. Theoretically we’d want to do this if something happens to observations at a year-level that is completely unrelated to country-specific trends. One way to think about this is as year-based shocks to life expectancy. At around 1:50 in the video at the beginning of this post, Hans Rosling slows down his animation to show the impact of World War I and the Spanish Flu epidemic on global life expectancy—tons of the country circles in his animation dropped dramatically. We might see something similar in the future with 2020–21 life expectancy levels too, given the COVID-19 pandemic. That sudden drop in life expectancy across all countries is an excellent example of a year-specific change that is arguably unrelated to trends in specific continents (kind of; despite its name, not all countries fought in WWI, and Spanish Flu didn’t have the same effects in all countries, but whatever; just go with it).\nTo do this, we’ll add independent year-based offsets to the intercept term, or \\(b_{0, \\text{Year}}\\):\n\\[\n\\begin{aligned}\n\\text{Life expectancy} &= (\\beta_0 + b_{0, \\text{Country}} + b_{0, \\text{Year}}) + (\\beta_1 + b_{1, \\text{Country}}) \\text{Year} + \\epsilon \\\\\nb_{0, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_{0, \\text{Country}}) \\\\\nb_{1, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_{1, \\text{Country}}) \\\\\nb_{0, \\text{Year}} &\\sim \\mathcal{N}(0, \\tau_{0, \\text{Year}})\n\\end{aligned}\n\\]\nThe code version of this random effects structure is (1 | year) + (1 + year | country), which means that we’re letting the intercept (1) vary by year and letting both the intercept (1) and year vary by country. Let’s see how this model looks!\n\n# Each country gets its own slope and intercept for the year trend\nmodel_country_year_year &lt;- brm(\n  bf(lifeExp ~ year + (1 | year) + (1 + year | country)),\n  data = gapminder,\n  chains = 4, seed = bayes_seed,\n  iter = 4000  # Double the number of iterations to help with convergence\n)\n## Start sampling\n\n\ntidy(model_country_year_year)\n## # A tibble: 7 × 8\n##   effect   component group    term                  estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)             50.2     1.24      47.8      52.6  \n## 2 fixed    cond      &lt;NA&gt;     year                     0.328   0.0246     0.278     0.377\n## 3 ran_pars cond      country  sd__(Intercept)         12.3     0.738     11.0      13.8  \n## 4 ran_pars cond      country  sd__year                 0.161   0.00988    0.143     0.182\n## 5 ran_pars cond      year     sd__(Intercept)          1.21    0.333      0.749     2.00 \n## 6 ran_pars cond      country  cor__(Intercept).year   -0.421   0.0704    -0.550    -0.272\n## 7 ran_pars cond      Residual sd__Observation          1.93    0.0362     1.86      2.00\n\nFixed (population-level) effects\nAs always, the global population-level coefficients are the same that we’ve seen before: in 1952 the average life expectancy is 50.19 years, and it increases by 0.33 years annually after that.\nCountry-level variation\nThe random effects section of our model results is getting longer and longer! We still have the variation in our country-based intercepts and year slopes, and we have the correlation between these country-based offsets. We also have a new term: sd__(Intercept) for the year group, which corresponds to \\(\\tau_{0, \\text{Year}}\\) in the year-specific level of the model. This shows us how much life expectancy bounces around from year to year.\nYear-level random effects\nWe can look at these new year-level offsets for the intercept using ranef() (but this time extracting data from the $year slot of the results). For whatever reason, average life expectancy in 1952 is nearly 1.5 years lower than the population-level average, but more than 1 year higher in the 1980s. If the Gapminder data went back further in time to the 1910s, we’d probably see a huge negative offset due to World War I and the Spanish Flu.\n\nyear_offsets &lt;- ranef(model_country_year_year)$year %&gt;%\n  as_tibble(rownames = \"year\") %&gt;% \n  mutate(year = as.numeric(year) + 1952) %&gt;% \n  select(year, starts_with(\"Estimate\"))\nyear_offsets\n## # A tibble: 12 × 2\n##     year Estimate.Intercept\n##    &lt;dbl&gt;              &lt;dbl&gt;\n##  1  1952             -1.44 \n##  2  1957             -0.623\n##  3  1962             -0.159\n##  4  1967              0.290\n##  5  1972              0.632\n##  6  1977              0.922\n##  7  1982              1.25 \n##  8  1987              1.29 \n##  9  1992              0.610\n## 10  1997             -0.168\n## 11  2002             -1.12 \n## 12  2007             -1.43\n\nWhere this gets really interesting is when we incorporate these different year offsets into the overall model estimate of the year trend. We can use emtrends() to calculate the predicted year slope for each of our example countries in both 1952 and 1957 (year = 0 and year = 5):\n\nmodel_country_year_year %&gt;% \n  emtrends(~ year + country,\n           var = \"year\",\n           at = list(year = c(0, 5), country = countries$country),\n           epred = TRUE, re_formula = NULL, allow_new_levels = TRUE)\n##  year country      year.trend lower.HPD upper.HPD\n##     0 Egypt              28.0     -14.7      67.1\n##     5 Egypt              13.9     -24.4      52.2\n##     0 Sierra Leone       27.7     -14.9      66.8\n##     5 Sierra Leone       13.6     -24.7      51.8\n##     0 Pakistan           27.8     -14.8      67.0\n##     5 Pakistan           13.7     -24.5      52.1\n##     0 Yemen, Rep.        28.1     -14.5      67.2\n##     5 Yemen, Rep.        13.9     -24.3      52.3\n##     0 Bolivia            27.9     -14.7      67.1\n##     5 Bolivia            13.8     -24.4      52.1\n##     0 Canada             27.7     -14.9      66.8\n##     5 Canada             13.6     -24.7      51.9\n##     0 Italy              27.7     -14.8      66.9\n##     5 Italy              13.6     -24.7      52.0\n##     0 Portugal           27.8     -14.8      67.0\n##     5 Portugal           13.7     -24.5      52.0\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nNotice how the slope of year now changes based on country and on year—it’s different (and sizably smaller) in 1957.\nVisualize country-specific intercepts and slopes and year-specific intercepts\nThese year shocks are apparent when we plot predicted life expectancy. Each country gets its own slope and intercept like before, but now the intercept is also shifted up and down in specific years. Note how all these lines start getting a little shallower in the 1990s—something happened worldwide to shift average life expectancy outside of continent-level effects.\n\npred_model_country_year_year &lt;- model_country_year_year %&gt;%\n  epred_draws(newdata = expand_grid(country = countries$country,\n                                    year = unique(gapminder$year)),\n              re_formula = NULL) %&gt;% \n  mutate(year = year + 1952) %&gt;% \n  left_join(countries, by = \"country\")\n\nggplot(pred_model_country_year_year, aes(x = year, y = .epred)) +\n  geom_point(data = original_points, aes(y = lifeExp), \n             color = \"grey50\", size = 3, alpha = 0.5) +\n  stat_lineribbon(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Reds\") +\n  labs(title = \"Intercepts and slopes for year trend vary by country and intercepts vary by year\",\n       subtitle = \"lifeExp ~ year + (1 | year) + (1 + year | country)\",\n       x = NULL, y = \"Predicted life expectancy\") +\n  guides(fill = \"none\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings) +\n  theme_clean() +\n  theme(legend.position = \"bottom\",\n        plot.subtitle = element_text(family = \"Consolas\"),\n        axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5))\n\n\n\n\n\n\n\nIntercepts and slopes for each country and continent\nFor our second bonus extension, we can incorporate random continent effects, since countries are nested in continents. This is probably overkill for this example, but it would be helpful in other situations with nested data structures.\nIn this case, we’re adding continent-specific effects and country-specific effects that incorporate information about parent continents. We end up with four different \\(b\\) terms:\n\n\n\\(b_{0, \\text{Continent}}\\): Continent-based shifts in the intercept\n\n\\(b_{0, \\text{Continent, Country}}\\): Country-within-continent-based shifts in the intercept\n\n\\(b_{1, \\text{Continent}}\\): Continent-based shifts in the year slope\n\n\\(b_{1, \\text{Continent, Country}}\\): Country-within-continent-based shifts in the year slope\n\nEach of these group terms also has its own variance term (\\(\\tau\\)) like the other models—I’m not including them below, but pretend they exist.\n\\[\n\\begin{aligned}\n\\text{Life expectancy} =&\\ (\\beta_0 + b_{0, \\text{Continent}} + b_{0, \\text{Continent, Country}}) + \\\\\n&\\ (\\beta_1 + b_{1, \\text{Continent}} + b_{1, \\text{Continent, Country}}) \\text{Year} + \\epsilon\n\\end{aligned}\n\\]\nThe code version of this nested random effects structure is (1 + year | continent / country).\nHowever, we won’t actually fit this model here, since it takes 10+ minutes to run (likely because I’m using all the default priors). If we were fitting the model, the code would look like this:\n\n# This takes a while! It also has a bunch of divergent transitions, and pretty\n# much all the chains hit the maximum treedepth limit, but this is just a toy\n# example, so whatever\nmodel_country_continent_year &lt;- brm(\n  bf(lifeExp ~ year + (1 + year | continent / country)),\n  data = gapminder,\n  chains = 4, seed = bayes_seed,\n  iter = 4000  # Double the number of iterations to help with convergence\n)\n\n\n# You'd use this to calculate the continent/country effects\nmodel_country_continent_year %&gt;%\n  emmeans(~ year + continent:country,\n          at = list(year = c(0), country = countries$country),\n          nesting = \"country %in% continent\",\n          epred = TRUE, re_formula = NULL, allow_new_levels = TRUE)\n\nThe model results include a bunch of new group-level terms. We get estimates for all the group-specific variances (the \\(\\tau\\)s), listed as sd__(Intercept) and sd__year for both continent and country-in-continent. We also get the correlation between continent-level offsets and year and country-in-continent-level offsets and year.\nAgain, this kind of complexity is probably overkill for this situation, since country-specific offsets seem to work just fine on their own without needing broader continent-level trends. But it’s a neat approach to thinking about hierarchy in the model."
  },
  {
    "objectID": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#quick-digression-on-logging-scaling-and-centering",
    "href": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#quick-digression-on-logging-scaling-and-centering",
    "title": "A guide to working with country-year panel data and Bayesian multilevel models",
    "section": "Quick digression on logging, scaling, and centering",
    "text": "Quick digression on logging, scaling, and centering\nPart of the reason the countries-nested-in-continents model takes so long to fit and runs into so many warnings and divergences is because the columns we used have values that are too large and that are distributed non-normally.\nBayesian sampling in Stan (through brms) works best and fastest when the ranges of covariates are small and centered around zero. We’ve been using year centered at 1952 (so that year 0 is 1952), and the max year in the data is 2007 (or 55), and even with that, we’re probably pushing the envelope of what Stan likes to work with.\nFor our main question, we want to know the relationship between GDP per capita and life expectancy. GDP per capita has a huge range with huge values. Some countries have a GDP per capita of \\$1,000; some have \\$40,000 or \\$50,000. To make these models run quickly (and actually fit and converge), we need to shrink those values down.\nWe can do this in a few different ways, each with their own benefits and quirks of interpretation, which we’ll explore really quick.\n\n\nDivide by 1,000 (or any amount, really)\nScale and center\nLog\nLog + scale and center\n\nHere’s what the distributions of these different scaling approaches look like. Notice how all the red GDP per capita distributions are identical. The per-1000 version is shrunken down so that most values are between 0 and 30 instead of 3 and 30,000, while the centered and scaled version ranges from ≈−0.5 and 8, with 0 in the middle of the distribution. The two logged versions are also identical—the original logged GDP per capita ranges from ≈5.5–11.5 (or exp(5.5) ($148) to exp(11.5) ($98,716)), while the centered and scaled version ranges from −2 to 3, centered at 0.\n\ndifferent_gdps &lt;- gapminder %&gt;% \n  select(gdpPercap, gdpPercap_1000, gdpPercap_z, gdpPercap_log, gdpPercap_log_z) %&gt;% \n  pivot_longer(everything()) %&gt;% \n  mutate(name_nice = recode(\n    name, \"gdpPercap\" = \"GDP per capita\",\n    \"gdpPercap_1000\" = \"GDP per capita ($1,000)\",\n    \"gdpPercap_z\" = \"GDP per capita (centered & scaled by one standard deviation)\",\n    \"gdpPercap_log\" = \"GDP per capita (logged)\",\n    \"gdpPercap_log_z\" = \"GDP per capita (logged; centered & scaled by one standard deviation)\")) %&gt;% \n  mutate(name_nice = fct_inorder(name_nice)) %&gt;% \n  mutate(type = ifelse(str_detect(name, \"log\"), \"Logged\", \"Original scale\")) %&gt;% \n  mutate(vline = ifelse(name == \"gdpPercap_log\", NA, 0))\n\nggplot(different_gdps, aes(x = value, fill = type)) +\n  geom_density(color = NA) +\n  geom_vline(data = different_gdps %&gt;% drop_na(vline) %&gt;% \n               group_by(name_nice) %&gt;% slice(1),\n             aes(xintercept = vline)) +\n  scale_fill_viridis_d(option = \"rocket\", begin = 0.3, end = 0.6) +\n  guides(fill = \"none\") +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(vars(name_nice), nrow = 3, scales = \"free\", dir = \"v\") +\n  theme_clean()\n\n\n\n\n\n\n\nScaling and centering\n1: Divide by 1,000\nOne quick and easy way to shrink the GDP values down is to divide them all by some arbitrary number like 1,000 so that we work with GDP per capita in thousands of dollars. A country with a GDP per capita of \\$2,000 would thus have a value of 2, and so on. Often this kind of downscaling is enough to make Stan happy, and it makes for easily interpretable results. We can interpret coefficients by saying “A \\$1,000 increase in GDP per capita is associated with a \\(\\beta\\) increase in life expectancy,” and if we really wanted, we could divide the coefficient by 1,000 to shrink it back down to the original scale and talk about \\$1 increases in GDP per capita.\n2: Scale and center\nA better alternative that requires a little bit more post-estimation finagling is to rescale the column by subtracting the mean so that it is centered at 0, and then dividing by the standard deviation. R’s built-in scale() function does this for us. There’s no built-in unscale() function to back-transform the values to their original numbers, but we can use a little algebra to see how to unscale the values: multiply by the standard deviation and add the mean. As long as we have a mean and standard deviation, we can flip between scaled and original values:\n\\[\n\\begin{aligned}\n\\text{Scaled value}\\ &=\\ \\frac{\\text{Original value} - \\text{Mean}}{\\text{Standard deviation}} \\\\\\\\\n\\text{Original value}\\ &=\\ (\\text{Scaled value } \\times \\text{ Standard deviation})\\ + \\text{Mean}\n\\end{aligned}\n\\]\nThe scale() function in R doesn’t return a regular set of values. For convenience, it stores the mean and standard deviation of the values as a special kind of metadata called “attributes” in slots named scaled:center (for the mean) and scaled:scale (for the standard deviation):\n\nsome_numbers &lt;- 1:5\nscaled_numbers &lt;- scale(some_numbers)\nscaled_numbers\n##        [,1]\n## [1,] -1.265\n## [2,] -0.632\n## [3,]  0.000\n## [4,]  0.632\n## [5,]  1.265\n## attr(,\"scaled:center\")\n## [1] 3\n## attr(,\"scaled:scale\")\n## [1] 1.58\n\nWe can access and extract those attributes with the attributes function, which converts them to a more easily accessible list. Note that when we reference scaled:center or scaled:scale we have to use backticks around the names—that’s because the : character isn’t normally allowed as an object name:\n\nscaled_numbers_mean_sd &lt;- attributes(scaled_numbers)\nscaled_numbers_mean_sd\n## $dim\n## [1] 5 1\n## \n## $`scaled:center`\n## [1] 3\n## \n## $`scaled:scale`\n## [1] 1.58\nscaled_numbers_mean_sd$`scaled:center`\n## [1] 3\nscaled_numbers_mean_sd$`scaled:scale`\n## [1] 1.58\n\nTo unscale some_numbers, we can multiply by the standard deviation and add the mean:\n\n(scaled_numbers * scaled_numbers_mean_sd$`scaled:scale`) +\n  scaled_numbers_mean_sd$`scaled:center`\n##      [,1]\n## [1,]    1\n## [2,]    2\n## [3,]    3\n## [4,]    4\n## [5,]    5\n## attr(,\"scaled:center\")\n## [1] 3\n## attr(,\"scaled:scale\")\n## [1] 1.58\n\nThe same thing works with the actual data. When we initially loaded and cleaned the data, we used scale() to scale down all the different GDP columns to versions with a _z suffix, and we can see that they’re all a little different from the other columns:\n\nglimpse(gapminder)\n## Rows: 1,680\n## Columns: 13\n## $ country          &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Af…\n## $ continent        &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asi…\n## $ year             &lt;dbl&gt; 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 0, 5, 10, 15, 20,…\n## $ lifeExp          &lt;dbl&gt; 28.8, 30.3, 32.0, 34.0, 36.1, 38.4, 39.9, 40.8, 41.7, 41.8, 42.…\n## $ pop              &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12881…\n## $ gdpPercap        &lt;dbl&gt; 779, 821, 853, 836, 740, 786, 978, 852, 649, 635, 727, 975, 160…\n## $ gdpPercap_1000   &lt;dbl&gt; 0.779, 0.821, 0.853, 0.836, 0.740, 0.786, 0.978, 0.852, 0.649, …\n## $ gdpPercap_log    &lt;dbl&gt; 6.66, 6.71, 6.75, 6.73, 6.61, 6.67, 6.89, 6.75, 6.48, 6.45, 6.5…\n## $ gdpPercap_z      &lt;dbl[,1]&gt; &lt;matrix[30 x 1]&gt;\n## $ gdpPercap_1000_z &lt;dbl[,1]&gt; &lt;matrix[30 x 1]&gt;\n## $ gdpPercap_log_z  &lt;dbl[,1]&gt; &lt;matrix[30 x 1]&gt;\n## $ year_orig        &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,…\n## $ highlight        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n\ngapminder %&gt;% \n  select(country, continent, year_orig, gdpPercap, gdpPercap_z) %&gt;% \n  head()\n## # A tibble: 6 × 5\n##   country     continent year_orig gdpPercap gdpPercap_z[,1]\n##   &lt;fct&gt;       &lt;fct&gt;         &lt;int&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n## 1 Afghanistan Asia           1952      779.          -0.640\n## 2 Afghanistan Asia           1957      821.          -0.636\n## 3 Afghanistan Asia           1962      853.          -0.632\n## 4 Afghanistan Asia           1967      836.          -0.634\n## 5 Afghanistan Asia           1972      740.          -0.644\n## 6 Afghanistan Asia           1977      786.          -0.639\n\nThat’s because (1) scale() returns values as a 1-column matrix for whatever reason, and (2) the special mean and standard deviation metadata attributes are stored in that matrix. We can access those attributes like normal:\n\ngdp_mean_sd &lt;- attributes(gapminder$gdpPercap_z)\ngdp_mean_sd\n## $dim\n## [1] 1680    1\n## \n## $`scaled:center`\n## [1] 7052\n## \n## $`scaled:scale`\n## [1] 9804\n\nJust for fun, we can verify that these values are the same as the average and standard deviation of GDP per capita:\n\ngapminder %&gt;% \n  summarize(avg = mean(gdpPercap),\n            sd = sd(gdpPercap))\n## # A tibble: 1 × 2\n##     avg    sd\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1 7052. 9804.\n\nThey’re the same!\nTo make sure that we really can scale and unscale this stuff, let’s look at the first few rows of the data and multiply the scaled gdpPercap_z column by the attributes we extracted:\n\ngapminder %&gt;% \n  slice(1:5) %&gt;% \n  select(gdpPercap, gdpPercap_z) %&gt;% \n  mutate(unscaled = gdpPercap_z * gdp_mean_sd$`scaled:scale` + gdp_mean_sd$`scaled:center`)\n## # A tibble: 5 × 3\n##   gdpPercap gdpPercap_z[,1] unscaled[,1]\n##       &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n## 1      779.          -0.640         779.\n## 2      821.          -0.636         821.\n## 3      853.          -0.632         853.\n## 4      836.          -0.634         836.\n## 5      740.          -0.644         740.\n\nThey’re the same!\nUnscaling regression coefficients is a little different. We don’t need to add the mean, since we’re working with marginal effects (i.e. the effect of adding one additional dollar of GDP per capita), and instead of multiplying by the standard deviation, we divide by it. Once again, just to make sure we can unscale correctly, we can run a couple models: one with GDP per capita in the thousands, and one with the scaled and centered GDP per capita. In theory, if we divide the coefficient from the second model by the standard deviation and multiply it by 1,000 (to scaled it up so it shows GDP per capita in the thousands), we’ll get the same value:\n\nmodel_super_simple &lt;- lm(lifeExp ~ gdpPercap_1000, data = gapminder)\ntidy(model_super_simple)  # 0.755\n## # A tibble: 2 × 5\n##   term           estimate std.error statistic   p.value\n##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      53.9      0.317      170.  0        \n## 2 gdpPercap_1000    0.755    0.0262      28.8 1.53e-148\n\nmodel_super_simple_z &lt;- lm(lifeExp ~ gdpPercap_z, data = gapminder)\ntidy(model_super_simple_z)  # 7.41\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    59.3      0.257     231.  0        \n## 2 gdpPercap_z     7.41     0.257      28.8 1.53e-148\n\n# Can we get the 7.41 coefficient from the centered/scaled model to turn into\n# 0.755 from the original model?\ntidy(model_super_simple_z) %&gt;% \n  filter(term == \"gdpPercap_z\") %&gt;% \n  mutate(estimate_unscaled_1000 = estimate / gdp_mean_sd$`scaled:scale` * 1000)\n## # A tibble: 1 × 6\n##   term        estimate std.error statistic   p.value estimate_unscaled_1000\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;                  &lt;dbl&gt;\n## 1 gdpPercap_z     7.41     0.257      28.8 1.53e-148                  0.755\n\nThey’re the same!\nLogging\n3: Log\nStan also seems to like to work with more normally distributed variables. With GDP per capita, lots of countries have a low value, and a few have really high values, resulting in a skewed distribution that can sometimes cause issues when sampling. To address this, we can take the log of GDP per capita and think about changes in orders of magnitude (i.e. moving from \\$300 to \\$3,000 to \\$30,000, and so on) rather than linear changes. Back-transforming logged values is easy—exponentiate the value (i.e. \\(e^\\text{logged value}\\)) if you’re using a natural log, which is what R does by default when you use log():\n\nlog(5000)\n## [1] 8.52\nexp(log(5000))\n## [1] 5000\n\nWe can verify this works with our data:\n\ngapminder %&gt;% \n  slice(1:5) %&gt;% \n  select(gdpPercap, gdpPercap_log) %&gt;% \n  mutate(unlogged = exp(gdpPercap_log))\n## # A tibble: 5 × 3\n##   gdpPercap gdpPercap_log unlogged\n##       &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n## 1      779.          6.66     779.\n## 2      821.          6.71     821.\n## 3      853.          6.75     853.\n## 4      836.          6.73     836.\n## 5      740.          6.61     740.\n\nWorking with logged variables in regression, though, is a little trickier since there’s no consistent way to back-transform the logged effects to their original scale—with logs, we’re working with orders of magnitude, so the coefficient represents a different kind of change in the outcome. But it’s okay! We can interpret things using percent changes rather than actual values. This response at Cross Validated is my favorite resource for remembering how to interpret regression results with logs in them (this resource is helpful too).\nWhen we have a regression model with a logged predictor, we talk about percent increases in the predictor variables. In a model like this, for instance…\n\\[\n\\text{Outcome} = \\beta_0 + \\beta_1 \\log(\\text{Predictor}) + \\epsilon\n\\]\n…we’d say “A 1% increase in the predictor is associated with a (\\(\\beta_1\\) / 100) unit increase in the outcome.” If we want to think about larger increases in the predictor, like 10%, we can divide the \\(\\beta_1\\) coefficient by 10 instead.\nHere’s what this looks like with logged GDP per capita:\n\nmodel_super_simple_log &lt;- lm(lifeExp ~ gdpPercap_log, data = gapminder)\ntidy(model_super_simple_log)  # 8.38\n## # A tibble: 2 × 5\n##   term          estimate std.error statistic  p.value\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)      -8.94     1.25      -7.16 1.17e-12\n## 2 gdpPercap_log     8.38     0.152     55.3  0\n\nIn this model, a one-unit increase in logged GDP per capita is associated with a 8.38 year increase in life expectancy. But thinking about increases in logged values is weird—we can think about percent changes instead.\nThe 8.38 coefficient by itself shows the effect of a 100% increase in GDP per capita, which is huge, so we can scale it down to see the effect of a 1% increase (if we divide by 100) or a 10% increase (if we divide by 10). A 10% increase in income for country with a GDP per capita of \\$1,000 would boost its income to $1,100 (1000 * 1.1), which is a \\$100 change. For a country with a GDP per capita of \\$10,000, a 10% increase would result in $11,000 (10000 * 1.1), or a \\$1,000 boost. According to our model, both of those changes (\\$100 for a poor country, \\$1,000 for a wealthier country) should result in the same change in life expectancy.\nWe’ll consider 10% increases since those are more sizable (e.g., a 1% increase in a country with a GDP per capita of \\$1,000 would lead to $1,010—that \\$10 increase is hardly going to move the needle in life expectancy).\nThus, according to this not-very-great-at-all model, a 10% increase in GDP per capita is associated with a 0.838 (8.38 / 10) year increase in life expectancy, on average.\n4: Log + scale and center\nAnd finally, since Stan likes working with centered variables, we can also scale and center logged values. If we want to interpret them we have to back-transform them by rescaling them up with the standard deviation. When we created the scaled version of logged GDP per capita at the beginning of this post, R stored the mean and standard deviation as metadata attributes. We can extract those and scale things back up as needed:\n\nattributes(gapminder$gdpPercap_log_z)\n## $dim\n## [1] 1680    1\n## \n## $`scaled:center`\n## [1] 8.14\n## \n## $`scaled:scale`\n## [1] 1.23"
  },
  {
    "objectID": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#the-effect-of-wealth-on-health-accounting-for-country-and-time",
    "href": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#the-effect-of-wealth-on-health-accounting-for-country-and-time",
    "title": "A guide to working with country-year panel data and Bayesian multilevel models",
    "section": "The effect of wealth on health, accounting for country and time",
    "text": "The effect of wealth on health, accounting for country and time\nOkay, phew. So far we’ve looked at how to model time trends across countries and continents with multilevel models, and we’ve seen how to scale and unscale variables to work better with Stan and brms. We’re really actually interested in Hans Rosling’s and the Gapminder Project’s original question—what’s the relationship between wealth and health, or GDP per capita and life expectancy? This fancy multilevel structure so far lets us model the time relationships, but we can use that year trend to account for time in more complex models.\nFor instance, we know from the Gapminder video that life expectancy increases as wealth increases, but that relationship differs by country. Visualizing the across-time trajectories of each country in the data helps with the intuition here. Countries like Canada, Italy, and Portugal have fairly steady trajectories over time, while places like Bolivia, Egypt, and Yemen see really steep increases in life expectancy. Sierra Leone tragically backtracks in both income and life expectancy in the 1990s.\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent)) + \n  geom_point(size = 0.5, alpha = 0.25) +\n  geom_smooth(method = \"lm\", aes(color = NULL), color = \"grey60\", size = 0.5, \n              se = FALSE, show.legend = FALSE) +\n  annotate(geom = \"label\", label = \"Global trend\", x = 64000, y = 84, \n           size = 3, color = \"grey60\") +\n  geom_path(aes(group = country, size = highlight),\n            arrow = arrow(type = \"open\", angle = 30, length = unit(0.75, \"lines\")),\n            show.legend = FALSE) +\n  geom_label_repel(data = filter(gapminder, year == 15, highlight == TRUE), \n                   aes(label = country), size = 3, seed = 1234, \n                   show.legend = FALSE,\n                   family = \"Barlow Semi Condensed\", fontface = \"bold\") +\n  scale_size_manual(values = c(0.075, 1), guide = \"none\") +\n  scale_color_okabe_ito(order = c(2, 3, 6, 1),\n                        guide = guide_legend(override.aes = list(size = 3, alpha = 1))) +\n  scale_x_log10(labels = dollar_format(accuracy = 1), breaks = 125 * (2^(1:10))) +\n  labs(x = \"GDP per capita (log)\", y = \"Life expectancy\", color = \"Continent\") +\n  theme_clean() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe’re interested in the effect of GDP per capita on life expectancy, but getting a single value for this is a little tricky. The grey line here shows the global trend—on average, health and wealth clearly move together. But that’s not a universal effect. Look at Italy, for instance—its slope is relatively constant over time. Egypt’s wealth slope gets steeper and shallower over time, while Sierra Leone reverses at some points. For the rest of this post, we’ll explore how to calculate and visualize these different country-specific and global effects.\nIn the first part of this guide, we cared about average life expectancy over time, so we plotted predicted life expectancy. Now, though, we care about the GDP effect, or the GDP coefficient in a model like lifeExp ~ gdpPercap, so we’ll work with and visualize that throughout the rest of this guide. We’ll look at the posterior distribution of this coefficient to see how much this wealth effect varies. We’ll use the emtrends() function from emmeans throughout, since it’s designed to calculate instantaneous average marginal effects (see this for more on average marginal effects).\nWe’ll use the centered version of GDP per capita to speed up model fitting, which means we’ll need to unscale coefficients to interpret them. We’ll also interpret coefficients using \\$1,000 increases in GDP per capita (since a \\$1 increase in wealth won’t lead to much of a difference in life expectancy). As these models get more complex, we’ll switch from regular GDP per capita to logged GDP per capita for the sake of computation.\nLet’s extract the mean and standard deviation from both centered GDP per capita and centered logged GDP per capita so we can use them below:\n\ngdp_mean_sd &lt;- attributes(gapminder$gdpPercap_z)\ngdp_mean &lt;- gdp_mean_sd$`scaled:center`\ngdp_sd &lt;- gdp_mean_sd$`scaled:scale`\n\ngdp_log_mean_sd &lt;- attributes(gapminder$gdpPercap_log_z)\ngdp_log_mean &lt;- gdp_log_mean_sd$`scaled:center`\ngdp_log_sd &lt;- gdp_log_mean_sd$`scaled:scale`\n\nRegular regression\nLike we did when we looked at just year trends, we’ll start with a regular old non-multilevel regression model to help get our bearings. We’ll actually run two different models: one with scaled and centered GDP per capita and one with scaled and centered logged GDP per capita, mostly to get used to scaling the coefficients up. These simple models fit just fine in under a second, but as we add more complexity, the non-logged GDP per capita gets really unstable and unwieldy, so we’ll shift to the logged version.\n\nmodel_gdp_boring &lt;- brm(\n  bf(lifeExp ~ gdpPercap_z + year),\n  data = gapminder,\n  chains = 4, seed = bayes_seed\n)\n## Start sampling\n\nmodel_gdp_boring_log &lt;- brm(\n  bf(lifeExp ~ gdpPercap_log_z + year),\n  data = gapminder,\n  chains = 4, seed = bayes_seed\n)\n## Start sampling\n\n\ntidy(model_gdp_boring)\n## # A tibble: 4 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       52.6      0.453    51.7      53.4  \n## 2 fixed    cond      &lt;NA&gt;     gdpPercap_z        6.46     0.246     6.00      6.95 \n## 3 fixed    cond      &lt;NA&gt;     year               0.244    0.0138    0.217     0.271\n## 4 ran_pars cond      Residual sd__Observation    9.71     0.169     9.40     10.0\ntidy(model_gdp_boring_log)\n## # A tibble: 4 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       53.8      0.323    53.2      54.5  \n## 2 fixed    cond      &lt;NA&gt;     gdpPercap_log_z    9.54     0.174     9.20      9.88 \n## 3 fixed    cond      &lt;NA&gt;     year               0.198    0.0101    0.178     0.218\n## 4 ran_pars cond      Residual sd__Observation    6.92     0.118     6.69      7.15\n\nWe can look at the results, but the coefficients for the scaled variables are a little tricky to interpret here. Technically they represent the increase in life expectancy that follows a 1 standard deviation increase in GDP per capita (not logged and logged), but I don’t think in standard deviations and I prefer to shift these coefficients back to their original scales. To do that we just need to divide by the standard deviation of the original columns. We’ll also multiply the GDP per capita coefficient by 1,000 so we can talk about changes in life expectancy that follow a \\$1,000 increase in wealth, and we’ll divide the logged GDP per capita coefficient by 10 so we can talk about changes in life expectancy that follow a 10% increase in wealth. This neat combination of across() and ifelse() inside mutate() lets us rescale multiple columns for just a single row, which is cool.\n\ntidy(model_gdp_boring) %&gt;% \n  mutate(across(c(estimate, std.error, conf.low, conf.high),\n                ~ifelse(term == \"gdpPercap_z\", (. / gdp_sd) * 1000, .)))\n## # A tibble: 4 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       52.6      0.453    51.7      53.4  \n## 2 fixed    cond      &lt;NA&gt;     gdpPercap_z        0.659    0.0251    0.612     0.709\n## 3 fixed    cond      &lt;NA&gt;     year               0.244    0.0138    0.217     0.271\n## 4 ran_pars cond      Residual sd__Observation    9.71     0.169     9.40     10.0\n\ntidy(model_gdp_boring_log) %&gt;% \n  mutate(across(c(estimate, std.error, conf.low, conf.high),\n                ~ifelse(term == \"gdpPercap_log_z\", (. / gdp_log_sd), .)))\n## # A tibble: 4 × 8\n##   effect   component group    term            estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)       53.8      0.323    53.2      54.5  \n## 2 fixed    cond      &lt;NA&gt;     gdpPercap_log_z    7.73     0.141     7.46      8.01 \n## 3 fixed    cond      &lt;NA&gt;     year               0.198    0.0101    0.178     0.218\n## 4 ran_pars cond      Residual sd__Observation    6.92     0.118     6.69      7.15\n\nBased on this model, when year is 0 (or in 1952) and when a country’s GDP per capita is \\$0, the average life expectancy is 52.55 years on average. It increases by 0.24 years annually after that, holding income constant, and it increases by 0.66 years for every \\$1,000 increase in wealth, holding time constant.\nIf we look at logged GDP per capita, we see a similar story: a 10% increase in GDP per capita is associated with a 0.773 (7.73 / 10) year increase in life expectancy, holding time constant.\n\name_model_gdp_boring &lt;- model_gdp_boring %&gt;% \n  emtrends(~ 1,\n           var = \"gdpPercap_z\",\n           at = list(year = 0),\n           epred = TRUE, re_formula = NULL)\n\npred_ame_model_gdp_boring &lt;- ame_model_gdp_boring %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = .value / gdp_sd * 1000) %&gt;% \n  mutate(fake_facet_title = \"GDP per capita\")\n\nplot_ame_gdp &lt;- ggplot(pred_ame_model_gdp_boring, aes(x = .value)) +\n  stat_halfeye(fill = palette_okabe_ito(5),\n               .width = c(0.8, 0.95)) +\n  labs(x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a $1,000 increase in GDP per capita\"), \n       y = \"Density\") +\n  facet_wrap(vars(fake_facet_title)) +\n  theme_clean() +\n  theme(strip.text = element_text(size = rel(1.1)))\n\n\name_model_gdp_boring_log &lt;- model_gdp_boring_log %&gt;% \n  emtrends(~ 1,\n           var = \"gdpPercap_log_z\",\n           at = list(year = 0),\n           epred = TRUE, re_formula = NULL)\n\npred_ame_model_gdp_boring_log &lt;- ame_model_gdp_boring_log %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = .value / gdp_log_sd / 10) %&gt;% \n  mutate(fake_facet_title = \"Logged GDP per capita\")\n\nplot_ame_gdp_log &lt;- ggplot(pred_ame_model_gdp_boring_log, aes(x = .value)) +\n  stat_halfeye(fill = palette_okabe_ito(5),\n               .width = c(0.8, 0.95)) +\n  labs(x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a 10% increase in GDP per capita\"), \n       y = \"Density\") +\n  facet_wrap(vars(fake_facet_title)) +\n  theme_clean() +\n  theme(strip.text = element_text(size = rel(1.1)))\n\n(plot_ame_gdp + plot_ame_gdp_log) +\n  plot_annotation(title = paste0(\"Average marginal effect of GDP per capita\", \"\\n\",\n                                 \"(includes year trend with no country-based variation)\"), \n                  subtitle = \"lifeExp ~ gdpPercap_log_z + year\",\n                  caption = \"80% and 95% credible intervals shown with error bar\",\n                  theme = theme_clean() + theme(plot.subtitle = element_text(family = \"Consolas\")))\n\n\n\n\n\n\n\nEach country gets its own intercept and GDP slope\nAs we saw in the plot with the different across-time trajectories, each country has a slightly different relationship between wealth and life expectancy. Using multilevel modeling, we can incorporate country-specific offsets into both the intercept (\\(b_{0, \\text{Country}}\\)) and the fixed GDP per capita effect (\\(b_{1, \\text{Country}}\\)). As always, each of these country-specific offsets has its own variation that we call \\(\\tau\\).\n\\[\n\\begin{aligned}\n\\text{Life expectancy} &= (\\beta_0 + b_{0, \\text{Country}}) + (\\beta_1 + b_{1, \\text{Country}}) \\text{GDP} + \\beta_2 \\text{Year} + \\epsilon \\\\\nb_{0, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_0) \\\\\nb_{1, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_1)\n\\end{aligned}\n\\]\nThe syntax for this random effects term is (1 + gdpPercap_z | country)—we’re letting both the intercept (1) and GDP per capita vary by country. We’ll use the scaled versions of both regular GDP per capita and logged GDP per capita.\nNote the addition of the decomp = \"QR\" argument here—that tells brms to use QR decomposition for decorrelating highly correlated covariates (like year and GDP across country), and it can speed up computation (see here for more about that).\n\nmodel_gdp_country_only &lt;- brm(\n  bf(lifeExp ~ gdpPercap_z + year + (1 + gdpPercap_z | country),\n     decomp = \"QR\"),\n  data = gapminder,\n  chains = 4, seed = bayes_seed,\n  threads = threading(2)  # Two CPUs per chain to speed things up\n)\n## Start sampling\n\nmodel_gdp_country_only_log &lt;- brm(\n  bf(lifeExp ~ gdpPercap_log_z + year + (1 + gdpPercap_log_z | country),\n     decomp = \"QR\"),\n  data = gapminder,\n  chains = 4, seed = bayes_seed,\n  threads = threading(2)  # Two CPUs per chain to speed things up\n)\n## Start sampling\n\nAdding this country-level complexity starts slowing down the regular GDP version substantially—it takes twice as long to fit the model with gdpPercap_z than it does gdpPercap_log_z, and there are issues with divergent chains and model fit. This is a great example of Andrew Gelman’s folk theorem of statistical computing: “when you have computational problems, often there’s a problem with your model.” We could fix this by using more specific priors, using more iterations in the chains, or adjusting some of Stan’s parameters like adapt_delta, but for the sake of this example, we won’t.\n\n# Model with regular GDP per capita\nrstan::get_elapsed_time(model_gdp_country_only$fit) %&gt;% \n  as_tibble(rownames = \"chain\") %&gt;% mutate(total_seconds = warmup + sample)\n## # A tibble: 4 × 4\n##   chain   warmup sample total_seconds\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n## 1 chain:1   27.6   30.3          57.8\n## 2 chain:2   31.9   27.9          59.9\n## 3 chain:3   28.3   28.5          56.9\n## 4 chain:4   29.4   28.1          57.5\n\n# Model with logged GDP per capita \nrstan::get_elapsed_time(model_gdp_country_only_log$fit) %&gt;% \n  as_tibble(rownames = \"chain\") %&gt;% mutate(total_seconds = warmup + sample)\n## # A tibble: 4 × 4\n##   chain   warmup sample total_seconds\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n## 1 chain:1   14.4   13.8          28.2\n## 2 chain:2   15.0   13.3          28.3\n## 3 chain:3   14.7   13.7          28.4\n## 4 chain:4   14.4   13.9          28.2\n\nFixed (population-level) effects\nThe global population-level coefficients are similar to what we saw in the basic model without random country effects: holding year trends constant, life expectancy increases by 1.17 years for every \\$1,000 increase in wealth and 0.34 years for every 10% increase in wealth, on average.\n\n# Unscale both the GDP coefficient and the GDP random variance coefficient\ntidy(model_gdp_country_only) %&gt;% \n  mutate(across(c(estimate, std.error, conf.low, conf.high),\n                ~ifelse(term %in% c(\"gdpPercap_z\", \"sd__gdpPercap_z\"), \n                        (. / gdp_sd) * 1000, .)))\n## # A tibble: 7 × 8\n##   effect   component group    term                   estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)              57.1     1.22      54.8      59.5  \n## 2 fixed    cond      &lt;NA&gt;     gdpPercap_z               1.17    0.208      0.793     1.59 \n## 3 fixed    cond      &lt;NA&gt;     year                      0.310   0.00618    0.298     0.322\n## 4 ran_pars cond      country  sd__(Intercept)           9.94    0.918      8.35     11.9  \n## 5 ran_pars cond      country  sd__gdpPercap_z           1.73    0.210      1.33      2.14 \n## 6 ran_pars cond      country  cor__(Intercept).gdpP…    0.201   0.154     -0.118     0.477\n## 7 ran_pars cond      Residual sd__Observation           2.92    0.0584     2.81      3.04\n\ntidy(model_gdp_country_only_log) %&gt;% \n  mutate(across(c(estimate, std.error, conf.low, conf.high),\n                ~ifelse(term %in% c(\"gdpPercap_log_z\", \"sd__gdpPercap_log_z\"), \n                        (. / gdp_log_sd), .)))\n## # A tibble: 7 × 8\n##   effect   component group    term                   estimate std.error conf.low conf.high\n##   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 fixed    cond      &lt;NA&gt;     (Intercept)              53.7     0.976     51.8     55.6   \n## 2 fixed    cond      &lt;NA&gt;     gdpPercap_log_z           3.40    0.576      2.27     4.52  \n## 3 fixed    cond      &lt;NA&gt;     year                      0.283   0.00628    0.270    0.295 \n## 4 ran_pars cond      country  sd__(Intercept)          11.2     0.759      9.80    12.8   \n## 5 ran_pars cond      country  sd__gdpPercap_log_z       5.69    0.443      4.88     6.62  \n## 6 ran_pars cond      country  cor__(Intercept).gdpP…   -0.277   0.101     -0.462   -0.0677\n## 7 ran_pars cond      Residual sd__Observation           2.76    0.0532     2.66     2.87\n\nCountry-level variation\nWe also have a few random effects terms that we can interpret. The variance for the intercept (sd__(Intercept)) corresponds to \\(\\tau_0\\) and shows us how much life expectancy bounces around across countries. We can also see what proportion this country-level variation contributes to the the total residual variation of the model. For the GDP per capita model, the country structure contributes 77% (9.94 / (9.94 + 2.92)) of the total variance in life expectancy, while it contributes 80% (11.16 / (11.16 + 2.76)) in the logged GDP per capita model.\nThe sd__gdpPercap_z term is \\(\\tau_1\\) and shows the country-specific variation in the GDP effect. This is smaller than the intercept variance, but that’s because the GDP effect is a slope that measures the marginal change in life expectancy as wealth increases by one unit (or $1,000 here), not life expectancy itself. (We unscaled the values for sd__gdpPercap_z and sd__gdpPercap_log_z here too, like we did their actual fixed coefficients.)\nFinally, we have a term that shows the correlation of the country-specific intercepts and slopes. In the regular GDP per capita model, we see a correlation of 0.2, which means that countries with lower intercepts are associated with lower GDP effects—countries that start out poor in 1952 grow slower. This correlation reverses in the logged GDP per capita model, where we have a correlation of -0.28, which implies that countries with lower intercepts are associated with bigger GDP effects—countries that start out poor in 1952 grow faster. Why the reversal? Haha I don’t know. It could be because the two measures of GDP capture different things: the logged version shows changes in orders of magnitude, while the regular version shows changes in dollar amounts. Somehow that difference makes the correlation flip. Weird.\nCountry-level random effects\nFor fun, we can look at the country-level offsets for the intercept and the GDP slope.\n\nranef(model_gdp_country_only)$country %&gt;%\n  as_tibble(rownames = \"country\") %&gt;% \n  filter(country %in% countries$country) %&gt;% \n  select(country, starts_with(\"Estimate\")) %&gt;% \n  # Unscale the GDP offsets\n  mutate(Estimate.gdpPercap_z = Estimate.gdpPercap_z / gdp_sd * 1000)\n## # A tibble: 8 × 3\n##   country      Estimate.Intercept Estimate.gdpPercap_z\n##   &lt;chr&gt;                     &lt;dbl&gt;                &lt;dbl&gt;\n## 1 Bolivia                  -0.587                1.90 \n## 2 Canada                   11.7                 -1.34 \n## 3 Egypt                     1.85                 1.66 \n## 4 Italy                     8.92                -1.24 \n## 5 Pakistan                  0.895                0.912\n## 6 Portugal                  4.37                -1.09 \n## 7 Sierra Leone             -9.42                 2.07 \n## 8 Yemen, Rep.               9.64                 4.04\n\nWe can also look at the already-combined fixed effect + random offset for country-specific intercepts and slopes. The intercept and the slope are different in each country, while the year coefficient is the same, as expected:\n\ncoef(model_gdp_country_only)$country %&gt;%\n  as_tibble(rownames = \"country\") %&gt;% \n  filter(country %in% countries$country) %&gt;% \n  select(country, starts_with(\"Estimate\")) %&gt;% \n  # Unscale the GDP offsets\n  mutate(Estimate.gdpPercap_z = Estimate.gdpPercap_z / gdp_sd * 1000)\n## # A tibble: 8 × 4\n##   country      Estimate.Intercept Estimate.gdpPercap_z Estimate.year\n##   &lt;chr&gt;                     &lt;dbl&gt;                &lt;dbl&gt;         &lt;dbl&gt;\n## 1 Bolivia                    56.5               3.07           0.310\n## 2 Canada                     68.8              -0.167          0.310\n## 3 Egypt                      59.0               2.83           0.310\n## 4 Italy                      66.1              -0.0712         0.310\n## 5 Pakistan                   58.0               2.08           0.310\n## 6 Portugal                   61.5               0.0801         0.310\n## 7 Sierra Leone               47.7               3.24           0.310\n## 8 Yemen, Rep.                66.8               5.21           0.310\n\nVisualize results\nWe can see the full posterior distributions for these country-specific GDP effects to get a sense of their variation. Looking at regular GDP shows some interesting trends—and reveals some potential issues with the model. In all countries except Canada, Italy, and Portugal, a \\$1,000 increase in GDP per capita—holding all other factors constant—is associated with a 2–5 year increase in life expectancy. This difference is “statistically significant” if we want to use that kind of frequentist language—the credible intervals for all these effects are fairly far from 0. Canada, Italy, and Portugal, however, see no practical GDP effect. Their slopes are essentially zero, and they’re precisely estimated around zero (hence those really tall peaks). This could be an artifact of the model—those three countries had already-high levels of income and Stan struggles with extremes like that.\n\name_model_gdp_country_only &lt;- model_gdp_country_only %&gt;% \n  emtrends(~ country,\n           var = \"gdpPercap_z\",\n           at = list(year = 0, country = countries$country),\n           epred = TRUE, re_formula = NULL)\n\npred_ame_model_gdp_country_only &lt;- ame_model_gdp_country_only %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = .value / gdp_sd * 1000) %&gt;% \n  left_join(countries, by = \"country\")\n\nggplot(pred_ame_model_gdp_country_only, aes(x = .value)) +\n  stat_halfeye(aes(fill = continent)) +\n  geom_vline(xintercept = 0) +\n  scale_fill_okabe_ito(order = c(2, 3, 6, 1), guide = \"none\") +\n  labs(title = paste0(\"Average marginal effect of GDP per capita\", \"\\n\",\n                      \"(intercepts and slope of GDP per capita by country)\"), \n       subtitle = \"lifeExp ~ gdpPercap_z + year + (1 + gdpPercap_z | country)\",\n       x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a $1,000 increase in GDP per capita\"), \n       y = \"Density\",\n       caption = \"80% and 95% credible intervals shown with error bar\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings) +\n  theme_clean() +\n  theme(plot.subtitle = element_text(family = \"Consolas\"))\n\n\n\n\n\n\n\nWe get some more normal looking distributions when looking at the effect of logged GDP per capita instead. Canada, Italy, and Portugal are all still apparent null effects, since their credible intervals include zero, while all other countries have positive effects. A 10% increase in GDP per capita is associated with a 0.5ish to 1.2ish year increase in life expectancy, on average.\n\name_model_gdp_country_only_log &lt;- model_gdp_country_only_log %&gt;% \n  emtrends(~ country,\n           var = \"gdpPercap_log_z\",\n           at = list(year = 0, country = countries$country),\n           epred = TRUE, re_formula = NULL)\name_model_gdp_country_only_log\n##  country      gdpPercap_log_z.trend lower.HPD upper.HPD\n##  Egypt                        11.37      7.41     15.22\n##  Sierra Leone                  6.42      0.88     12.51\n##  Pakistan                      5.91      1.67      9.82\n##  Yemen, Rep.                  14.33     10.14     18.37\n##  Bolivia                      13.59      5.33     22.11\n##  Canada                       -1.90     -6.58      2.82\n##  Italy                         0.14     -3.38      3.43\n##  Portugal                      2.32     -0.67      5.33\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\npred_ame_model_gdp_country_only_log &lt;- ame_model_gdp_country_only_log %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = .value / gdp_log_sd / 10) %&gt;% \n  left_join(countries, by = \"country\")\n\nggplot(pred_ame_model_gdp_country_only_log, aes(x = .value)) +\n  stat_halfeye(aes(fill = continent)) +\n  geom_vline(xintercept = 0) +\n  scale_fill_okabe_ito(order = c(2, 3, 6, 1), guide = \"none\") +\n  labs(title = paste0(\"Average marginal effect of GDP per capita\", \"\\n\",\n                      \"(intercepts and slope of GDP per capita by country)\"), \n       subtitle = \"lifeExp ~ gdpPercap_z + year + (1 + gdpPercap_z | country)\",\n       x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a 10% increase in GDP per capita\"), \n       y = \"Density\",\n       caption = \"80% and 95% credible intervals shown with error bar\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings) +\n  theme_clean() +\n  theme(plot.subtitle = element_text(family = \"Consolas\"))\n\n\n\n\n\n\n\n⭐ Each country gets its own intercept and GDP and year slopes ⭐\nWhile it’s cool that GDP now varies by country, we saw earlier that year-based trends in life expectancy also vary across country. We should probably incorporate country-specific differences for both GDP and year. Once again, multilevel modeling makes this easy—we’ll allow year to vary by country too by adding country-specific offsets to the year slope with a \\(b_{2, \\text{Country}}\\) term:\n\\[\n\\begin{aligned}\n\\text{Life expectancy} &= (\\beta_0 + b_{0, \\text{Country}}) + (\\beta_1 + b_{1, \\text{Country}}) \\text{GDP} + (\\beta_2 + b_{2, \\text{Country}}) \\text{Year} + \\epsilon \\\\\nb_{0, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_0) \\\\\nb_{1, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_1) \\\\\nb_{2, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_2)\n\\end{aligned}\n\\]\nThe syntax for this random effects term is (1 + gdpPercap_z + year | country), which lets the intercept (1), GDP per capita, and year vary by country. We’ll use the scaled versions of both regular GDP per capita and logged GDP per capita again, but we’ll see that we run into some serious computational issues now.\n\nmodel_gdp_country_year &lt;- brm(\n  bf(lifeExp ~ gdpPercap_z + year + (1 + gdpPercap_z + year | country),\n     decomp = \"QR\"),\n  data = gapminder,\n  chains = 4, seed = bayes_seed,\n  threads = threading(2)  # Two CPUs per chain to speed things up\n)\n## Start sampling\n## Warning: 47 of 4000 (1.0%) transitions ended with a divergence.\n## See https://mc-stan.org/misc/warnings for details.\n## Warning: 2 of 4000 (0.0%) transitions hit the maximum treedepth limit of 10.\n## See https://mc-stan.org/misc/warnings for details.\n## Warning: 1 of 4 chains had an E-BFMI less than 0.2.\n## See https://mc-stan.org/misc/warnings for details.\n\nmodel_gdp_country_year_log &lt;- brm(\n  bf(lifeExp ~ gdpPercap_log_z + year + (1 + gdpPercap_log_z + year | country),\n     decomp = \"QR\"),\n  data = gapminder,\n  chains = 4, seed = bayes_seed,\n  threads = threading(2)  # Two CPUs per chain to speed things up\n)\n## Start sampling\n\nIt takes nearly 2.5 times as long to fit the model with gdpPercap_z than it does gdpPercap_log_z, and there are some serious-ish issues with model convergence and effective sample size. Normally, the R-hat values for each parameter are supposed to be as close to 1 as possible, and values above 1.05 are a bad sign that the model didn’t converge. Again, we could try to fix this by using specific priors, using more iterations in the chains, or adjusting some of Stan’s parameters like adapt_delta, but for the sake of this example, we won’t.\n\n# Model with regular GDP per capita\nrstan::get_elapsed_time(model_gdp_country_year$fit) %&gt;% \n  as_tibble(rownames = \"chain\") %&gt;% mutate(total_seconds = warmup + sample)\n## # A tibble: 4 × 4\n##   chain   warmup sample total_seconds\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n## 1 chain:1   38.0   16.6          54.6\n## 2 chain:2   39.0   30.8          69.8\n## 3 chain:3   40.1   36.9          76.9\n## 4 chain:4   40.0   32.8          72.8\n\n# Model with logged GDP per capita \nrstan::get_elapsed_time(model_gdp_country_year_log$fit) %&gt;% \n  as_tibble(rownames = \"chain\") %&gt;% mutate(total_seconds = warmup + sample)\n## # A tibble: 4 × 4\n##   chain   warmup sample total_seconds\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n## 1 chain:1   23.8   17.2          40.9\n## 2 chain:2   23.7   17.1          40.8\n## 3 chain:3   23.5   16.8          40.4\n## 4 chain:4   24.4   16.8          41.2\n\n\n# Bad R-hats and low effective sample sizes\nbayestestR::diagnostic_posterior(model_gdp_country_year)\n##       Parameter Rhat   ESS     MCSE\n## 1 b_gdpPercap_z 1.02  60.6 0.113683\n## 2   b_Intercept 1.00 346.3 0.053887\n## 3        b_year 1.01 730.0 0.000593\n\n# Okay R-hats and okay-ish effective sample sizes\nbayestestR::diagnostic_posterior(model_gdp_country_year_log)\n##           Parameter Rhat  ESS     MCSE\n## 1 b_gdpPercap_log_z 1.00 1695 0.011915\n## 2       b_Intercept 1.01  310 0.052171\n## 3            b_year 1.00  632 0.000726\n\nFixed (population-level) effects\nThe global population-level coefficients are again similar to what we saw in the previous models: holding year trends constant, life expectancy increases by 0.39 years for every \\$1,000 increase in wealth and 0.424 years for every 10% increase in wealth, on average.\n\n# Unscale both the GDP coefficient and the GDP random variance coefficient\ntidy(model_gdp_country_year) %&gt;% \n  mutate(across(c(estimate, std.error, conf.low, conf.high),\n                ~ifelse(term %in% c(\"gdpPercap_z\", \"sd__gdpPercap_z\"), \n                        (. / gdp_sd) * 1000, .)))\n## # A tibble: 10 × 8\n##    effect   component group    term                  estimate std.error conf.low conf.high\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 fixed    cond      &lt;NA&gt;     (Intercept)             52.2      1.00     50.3      54.2  \n##  2 fixed    cond      &lt;NA&gt;     gdpPercap_z              0.391    0.0906    0.264     0.620\n##  3 fixed    cond      &lt;NA&gt;     year                     0.316    0.0160    0.286     0.348\n##  4 ran_pars cond      country  sd__(Intercept)         11.0      0.736     9.75     12.6  \n##  5 ran_pars cond      country  sd__gdpPercap_z          0.497    0.212     0.278     0.954\n##  6 ran_pars cond      country  sd__year                 0.172    0.0127    0.150     0.199\n##  7 ran_pars cond      country  cor__(Intercept).gdp…   -0.366    0.241    -0.677     0.114\n##  8 ran_pars cond      country  cor__(Intercept).year   -0.548    0.0816   -0.703    -0.382\n##  9 ran_pars cond      country  cor__gdpPercap_z.year   -0.430    0.0984   -0.618    -0.234\n## 10 ran_pars cond      Residual sd__Observation          2.12     0.0617    2.00      2.23\n\ntidy(model_gdp_country_year_log) %&gt;% \n  mutate(across(c(estimate, std.error, conf.low, conf.high),\n                ~ifelse(term %in% c(\"gdpPercap_log_z\", \"sd__gdpPercap_log_z\"), \n                        (. / gdp_log_sd), .)))\n## # A tibble: 10 × 8\n##    effect   component group    term                  estimate std.error conf.low conf.high\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 fixed    cond      &lt;NA&gt;     (Intercept)             52.0      0.919    50.2      53.7  \n##  2 fixed    cond      &lt;NA&gt;     gdpPercap_log_z          4.24     0.397     3.44      5.03 \n##  3 fixed    cond      &lt;NA&gt;     year                     0.259    0.0182    0.224     0.295\n##  4 ran_pars cond      country  sd__(Intercept)         10.4      0.700     9.19     11.9  \n##  5 ran_pars cond      country  sd__gdpPercap_log_z      3.26     0.379     2.55      4.05 \n##  6 ran_pars cond      country  sd__year                 0.202    0.0145    0.175     0.232\n##  7 ran_pars cond      country  cor__(Intercept).gdp…    0.383    0.112     0.150     0.584\n##  8 ran_pars cond      country  cor__(Intercept).year   -0.711    0.0485   -0.796    -0.605\n##  9 ran_pars cond      country  cor__gdpPercap_log_z…   -0.697    0.0660   -0.810    -0.555\n## 10 ran_pars cond      Residual sd__Observation          1.97     0.0391    1.90      2.05\n\nCountry-level variation and random effects\nWe’ll forgo detailed interpretations of all our new random effects values and instead just point out that we have estimates for each of the \\(\\tau\\)s: country-based variation in the intercept, the GDP slope, and the year slope. We also have three different correlations now: the correlation between the intercept and the GDP slope, the intercept and the year slope, and the GDP and year slopes. Neat.\nIf we look at the country-level coefficients, we can see that the intercepts, GDP slopes, and year slopes all vary by different amounts across countries, as intended:\n\ncoef(model_gdp_country_year)$country %&gt;%\n  as_tibble(rownames = \"country\") %&gt;% \n  filter(country %in% countries$country) %&gt;% \n  select(country, starts_with(\"Estimate\")) %&gt;% \n  # Unscale the GDP offsets\n  mutate(Estimate.gdpPercap_z = Estimate.gdpPercap_z / gdp_sd * 1000)\n## # A tibble: 8 × 4\n##   country      Estimate.Intercept Estimate.gdpPercap_z Estimate.year\n##   &lt;chr&gt;                     &lt;dbl&gt;                &lt;dbl&gt;         &lt;dbl&gt;\n## 1 Bolivia                    40.7               0.375          0.485\n## 2 Canada                     68.5               0.0646         0.193\n## 3 Egypt                      42.9               0.272          0.525\n## 4 Italy                      66.6               0.0311         0.255\n## 5 Pakistan                   46.7               0.441          0.386\n## 6 Portugal                   61.3               0.0353         0.324\n## 7 Sierra Leone               38.0               1.23           0.224\n## 8 Yemen, Rep.                33.6               0.489          0.577\n\nVisualize results\nAs usual, we can visualize the distribution of these country-specific GDP effects. If we look at regular GDP, we can immediately see some weirdness—a few of these distributions like Sierra Leone and Bolivia aren’t very smooth, likely because we didn’t specify good priors and because the model didn’t fully converge. These things look goofy and we shouldn’t put too much stock in them. Canada, Italy, and Portugal all seem to have null effects, along with Egypt now for whatever reason. But again, these are strange and the model isn’t that great, so we can stop looking at this. Scroll down.\n\name_model_gdp_country_year &lt;- model_gdp_country_year %&gt;% \n  emtrends(~ year + country,\n           var = \"gdpPercap_z\",\n           at = list(year = 0, country = countries$country),\n           epred = TRUE, re_formula = NULL)\n\npred_ame_model_gdp_country_year &lt;- ame_model_gdp_country_year %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = .value / gdp_sd * 1000) %&gt;% \n  left_join(countries, by = \"country\")\n\nggplot(pred_ame_model_gdp_country_year, aes(x = .value)) +\n  stat_halfeye(aes(fill = continent)) +\n  geom_vline(xintercept = 0) +\n  scale_fill_okabe_ito(order = c(2, 3, 6, 1), guide = \"none\") +\n  labs(title = paste0(\"Average marginal effect of GDP per capita\", \"\\n\",\n                      \"(intercepts and slopes of both GDP per capita and year vary by country)\"), \n       subtitle = \"lifeExp ~ gdpPercap_log_z + year + (1 + gdpPercap_z + year | country)\",\n       x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a $1,000 increase in GDP per capita\"), \n       y = \"Density\",\n       caption = \"80% and 95% credible intervals shown with error bar\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings) +\n  theme_clean() +\n  theme(plot.subtitle = element_text(family = \"Consolas\"))\n\n\n\n\n\n\n\nThe average marginal effects for logged GDP per capita look a lot smoother (and the model actually converged), so I trust these results more. After accounting for country- and year-specific differences, Canada, Italy, and Portugal all have positive GDP per capita slopes: a 10% increase in GDP per capita is associated with a 0.5ish year increase in life expectancy. We see a slightly smaller effect in Sierra Leone, and even smaller (and less “significant”) effects everywhere else.\n\name_model_gdp_country_year_log &lt;- model_gdp_country_year_log %&gt;% \n  emtrends(~ year + country,\n           var = \"gdpPercap_log_z\",\n           at = list(year = 0, country = countries$country),\n           epred = TRUE, re_formula = NULL)\n\npred_ame_model_gdp_country_year_log &lt;- ame_model_gdp_country_year_log %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = .value / gdp_log_sd / 10) %&gt;% \n  left_join(countries, by = \"country\")\n\nggplot(pred_ame_model_gdp_country_year_log, aes(x = .value)) +\n  stat_halfeye(aes(fill = continent)) +\n  geom_vline(xintercept = 0) +\n  scale_fill_okabe_ito(order = c(2, 3, 6, 1), guide = \"none\") +\n  labs(title = paste0(\"Average marginal effect of GDP per capita\", \"\\n\",\n                      \"(intercepts and slopes of both GDP per capita and year vary by country)\"), \n       subtitle = \"lifeExp ~ gdpPercap_log_z + year + (1 + gdpPercap_log_z + year | country)\",\n       x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a 10% increase in GDP per capita\"), \n       y = \"Density\",\n       caption = \"80% and 95% credible intervals shown with error bar\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings) +\n  theme_clean() +\n  theme(plot.subtitle = element_text(family = \"Consolas\"))\n\n\n\n\n\n\n\nVisualize more generic average marginal effects\nIt’s really really neat that we can look at the specific effect of GDP on life expectancy in every country individually, but often we just want report a single number—what is the average effect of health on wealth? We don’t want to report 140 separate GDP effects! We just want one!\nWe can consolidate these country-specific effects into a single value a couple different ways:\n\nReport a global grand mean, or the average predicted outcome ignoring country-specific deviations\nReport the average predicted outcome for a hypothetical new country\n\nThere are a ton of details about these different approaches for finding average marginal effects at this post here—go there to learn all about the nuances of doing this.\nHere we’ll do it both ways: calculate a global grand mean and simulate a fake new country (we’ll call it Atlantis).\n\n# Calculate the overall global effect\n# `re_formula = NA` means that no random effects will be incorporated\name_global_gdp_country_year &lt;- model_gdp_country_year_log %&gt;% \n  emtrends(~ 1,\n           var = \"gdpPercap_log_z\",\n           epred = TRUE, re_formula = NA)\name_global_gdp_country_year\n##  1       gdpPercap_log_z.trend lower.HPD upper.HPD\n##  overall                  5.25      4.28      6.23\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\n# Get the posterior distribution of this global effect and make a plot\npred_global_gdp_country_year &lt;- ame_global_gdp_country_year %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = (.value / gdp_log_sd) / 10) %&gt;% \n  mutate(fake_facet_title = \"Global grand mean\")\n\nplot_global_gdp_country_year &lt;- ggplot(pred_global_gdp_country_year, aes(x = .value)) +\n  stat_halfeye(fill = palette_okabe_ito(5)) +\n  geom_vline(xintercept = 0) +\n  labs(x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a 10% increase in GDP per capita\"), \n       y = \"Density\") +\n  facet_wrap(vars(fake_facet_title)) +\n  theme_clean() +\n  theme(strip.text = element_text(size = rel(1.1)))\n\n# Calculate the effect for Atlantis\n# `re_formula = NULL` means the full random effects structure will be\n# incorporated. `allow_new_levels` lets R deal with a new country, and\n# `sample_new_levels = \"gaussian\"` means that the characteristics for this new\n# country will be based on random draws from the model\name_hypo_gdp_country_year &lt;- model_gdp_country_year_log %&gt;% \n  emtrends(~ 1 + country,\n           var = \"gdpPercap_log_z\",\n           at = list(country = \"Atlantis\"),\n           epred = TRUE, re_formula = NULL, \n           allow_new_levels = TRUE, sample_new_levels = \"gaussian\")\name_hypo_gdp_country_year\n##  country  gdpPercap_log_z.trend lower.HPD upper.HPD\n##  Atlantis                  5.19     -2.77      13.1\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\n# Get the posterior distribution of this Atlantis effect and make a plot\npred_hypo_gdp_country_year &lt;- ame_hypo_gdp_country_year %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = (.value / gdp_log_sd) / 10) %&gt;% \n  mutate(fake_facet_title = \"AME for hypothetical Atlantis\")\n\nplot_hypo_gdp_country_year &lt;- ggplot(pred_hypo_gdp_country_year, aes(x = .value)) +\n  stat_halfeye(fill = palette_okabe_ito(7)) +\n  geom_vline(xintercept = 0) +\n  labs(x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a 10% increase in GDP per capita\"), \n       y = \"Density\") +\n  facet_wrap(vars(fake_facet_title)) +\n  theme_clean() +\n  theme(strip.text = element_text(size = rel(1.1)))\n\n# Show the two AME distributions side-by-side\n(plot_global_gdp_country_year | plot_hypo_gdp_country_year) +\n  plot_annotation(title = paste0(\"Average marginal effect of GDP per capita\", \"\\n\",\n                                 \"(intercepts and slopes of both GDP per capita and year vary by country)\"), \n                  subtitle = \"lifeExp ~ gdpPercap_log_z + year + (1 + gdpPercap_log_z + year | country)\",\n                  caption = \"80% and 95% credible intervals shown with error bar\",\n                  theme = theme_clean() + theme(plot.subtitle = element_text(family = \"Consolas\")))\n\n\n\n\n\n\n\nEach country gets its own intercept and GDP and year slopes and year-specific offsets in the GDP slope\nThis lifeExp ~ gdpPercap_z + year + (1 + gdpPercap_z + year | country) approach is typically sufficient for models based on panel data, but like we did earlier, we can be extra fancy and include random effects for year-specific differences in the GDP per capita effect that are independent of country-specific differences. These typically represent time-specific global shocks, like recessions or pandemics.\nTo do this, we’ll add independent year-based offsets to the intercept term (\\(b_{0, \\text{Year}}\\)) and to the GDP effect:\n\\[\n\\begin{aligned}\n\\text{Life expectancy} &= (\\beta_0 + b_{0, \\text{Country}} + b_{0, \\text{Year}}) + (\\beta_1 + b_{1, \\text{Country}} + b_{1, \\text{Year}}) \\text{GDP} + \\\\\n&= (\\beta_2 + b_{2, \\text{Country}}) \\text{Year} + \\epsilon \\\\\nb_{0, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_{0, \\text{Country}}) \\\\\nb_{1, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_{1, \\text{Country}}) \\\\\nb_{2, \\text{Country}} &\\sim \\mathcal{N}(0, \\tau_{2, \\text{Country}}) \\\\\nb_{0, \\text{Year}} &\\sim \\mathcal{N}(0, \\tau_{0, \\text{Year}}) \\\\\nb_{1, \\text{Year}} &\\sim \\mathcal{N}(0, \\tau_{1, \\text{Country}})\n\\end{aligned}\n\\]\nThere are so many moving parts here!\nIn code, we add two random effects terms: (1 + gdpPercap_log_z | year) + (1 + gdpPercap_log_z + year | country). The intercept and the GDP effect get their own year-specific offsets, and the intercept, GDP effect, and year effect get their own country-specific offsets.\nWe won’t even try to use the regular GDP per capita measure—Stan will choke unless we do some fine tuning. We’ll just look at logged GDP per capita.\n\nmodel_gdp_country_year_year &lt;- brm(\n  bf(lifeExp ~ gdpPercap_log_z + year + (1 + gdpPercap_log_z | year) + \n       (1 + gdpPercap_log_z + year | country),\n     decomp = \"QR\"),\n  data = gapminder,\n  cores = 4, chains = 4, seed = bayes_seed,\n  threads = threading(2)  # Two CPUs per chain to speed things up\n)\n## Start sampling\n## Warning: 994 of 4000 (25.0%) transitions hit the maximum treedepth limit of 10.\n## See https://mc-stan.org/misc/warnings for details.\n\n\nrstan::get_elapsed_time(model_gdp_country_year_year$fit) %&gt;% \n  as_tibble(rownames = \"chain\") %&gt;% mutate(total_seconds = warmup + sample)\n## # A tibble: 4 × 4\n##   chain   warmup sample total_seconds\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n## 1 chain:1   37.7   22.0          59.7\n## 2 chain:2   37.3   23.7          61.0\n## 3 chain:3   95.5  144.          239. \n## 4 chain:4   38.7   22.1          60.8\n\nFixed (population-level) effects\nThe global population-level coefficients are again similar to what we saw in the previous models: holding year trends constant, life expectancy increases by 0.333 years for every 10% increase in wealth, on average.\n\n# Unscale both the GDP coefficient and the GDP random variance coefficient\ntidy(model_gdp_country_year_year) %&gt;% \n  mutate(across(c(estimate, std.error, conf.low, conf.high),\n                ~ifelse(term %in% c(\"gdpPercap_log_z\", \"sd__gdpPercap_log_z\"), \n                        (. / gdp_log_sd), .)))\n## # A tibble: 13 × 8\n##    effect   component group    term                  estimate std.error conf.low conf.high\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 fixed    cond      &lt;NA&gt;     (Intercept)             52.5      1.11    50.3       54.7  \n##  2 fixed    cond      &lt;NA&gt;     gdpPercap_log_z          3.33     0.416    2.49       4.15 \n##  3 fixed    cond      &lt;NA&gt;     year                     0.258    0.0250   0.210      0.306\n##  4 ran_pars cond      country  sd__(Intercept)         10.6      0.729    9.29      12.1  \n##  5 ran_pars cond      country  sd__gdpPercap_log_z      2.81     0.354    2.16       3.54 \n##  6 ran_pars cond      country  sd__year                 0.192    0.0135   0.168      0.220\n##  7 ran_pars cond      year     sd__(Intercept)          0.947    0.254    0.597      1.59 \n##  8 ran_pars cond      year     sd__gdpPercap_log_z      0.263    0.0976   0.120      0.503\n##  9 ran_pars cond      country  cor__(Intercept).gdp…    0.283    0.130    0.0158     0.521\n## 10 ran_pars cond      country  cor__(Intercept).year   -0.647    0.0569  -0.748     -0.528\n## 11 ran_pars cond      country  cor__gdpPercap_log_z…   -0.681    0.0713  -0.804     -0.528\n## 12 ran_pars cond      year     cor__(Intercept).gdp…   -0.819    0.177   -0.993     -0.338\n## 13 ran_pars cond      Residual sd__Observation          1.80     0.0371   1.72       1.87\n\nCountry-level variation and random effects\nWe’ll again forgo detailed interpretations of all our new random effects values. Just note that there are five different \\(\\tau\\) values, as expected, and four different correlations.\nIf we look at the country-level coefficients, we can see that the intercepts, GDP slopes, and year slopes all vary by different amounts across countries:\n\ncoef(model_gdp_country_year)$country %&gt;%\n  as_tibble(rownames = \"country\") %&gt;% \n  filter(country %in% countries$country) %&gt;% \n  select(country, starts_with(\"Estimate\")) %&gt;% \n  # Unscale the GDP offsets\n  mutate(Estimate.gdpPercap_z = Estimate.gdpPercap_z / gdp_sd * 1000)\n## # A tibble: 8 × 4\n##   country      Estimate.Intercept Estimate.gdpPercap_z Estimate.year\n##   &lt;chr&gt;                     &lt;dbl&gt;                &lt;dbl&gt;         &lt;dbl&gt;\n## 1 Bolivia                    40.7               0.375          0.485\n## 2 Canada                     68.5               0.0646         0.193\n## 3 Egypt                      42.9               0.272          0.525\n## 4 Italy                      66.6               0.0311         0.255\n## 5 Pakistan                   46.7               0.441          0.386\n## 6 Portugal                   61.3               0.0353         0.324\n## 7 Sierra Leone               38.0               1.23           0.224\n## 8 Yemen, Rep.                33.6               0.489          0.577\n\nEven better, though, is that the GDP slope changes each year too! That’s what we get for including the (1 + gdpPercap_log_z | year) term. The country-specific GDP slopes shift over time now:\n\n# Different slopes in each year!\nmodel_gdp_country_year_year %&gt;% \n  emtrends(~ year + country,\n           var = \"gdpPercap_log_z\",\n           at = list(year = c(0, 5), country = countries$country),\n           epred = TRUE, re_formula = NULL)\n##  year country      gdpPercap_log_z.trend lower.HPD upper.HPD\n##     0 Egypt                         1.09     -5.11      6.61\n##     5 Egypt                         0.93     -5.44      6.27\n##     0 Sierra Leone                  4.60      0.99      7.77\n##     5 Sierra Leone                  4.46      0.92      7.70\n##     0 Pakistan                      3.84     -1.34      9.62\n##     5 Pakistan                      3.70     -1.65      9.36\n##     0 Yemen, Rep.                   2.35     -2.17      7.07\n##     5 Yemen, Rep.                   2.20     -2.28      7.00\n##     0 Bolivia                       1.95     -3.06      6.40\n##     5 Bolivia                       1.82     -3.02      6.45\n##     0 Canada                        5.77      0.28     11.70\n##     5 Canada                        5.63      0.30     11.69\n##     0 Italy                         4.82     -1.08     10.20\n##     5 Italy                         4.69     -1.20     10.12\n##     0 Portugal                      5.26     -0.04     10.60\n##     5 Portugal                      5.15     -0.42     10.23\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95\n\nVisualize results\nWe can see this time-based change by making a slightly fancier version of our country-specific average marginal effect plot. We’ll show the posterior distribution for each year within each country using ridgeplots. We’ll also shade the credible intervals here using the neat fill_ramp aesthetic instead of showing a point and line, since it would get too visually busy otherwise.\nThis looks so cool! In some countries like Egypt and Canada, the GDP effect is fairly constant over time. In others, like Sierra Leone, you can see the GDP effect shrink slightly in the 1980s and 1990s, and then increase again after that.\n\nCodeame_model_gdp_country_year_year &lt;- model_gdp_country_year_year %&gt;% \n  emtrends(~ year + country,\n           var = \"gdpPercap_log_z\",\n           at = list(year = seq(0, 55, by = 5), country = countries$country),\n           epred = TRUE, re_formula = NULL)\n\npred_model_gdp_country_year_year &lt;- ame_model_gdp_country_year_year %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = (.value / gdp_log_sd) / 10) %&gt;% \n  mutate(year = year + 1952,\n         year = fct_inorder(factor(year))) %&gt;%\n  left_join(countries, by = \"country\")\n\nggplot(pred_model_gdp_country_year_year, aes(x = .value, fill = year)) +\n  stat_slab(aes(y = fct_rev(year), fill = year,\n                fill_ramp = after_stat(cut_cdf_qi(cdf, .width = c(0.02, 0.8, 0.95, 1)))),\n            height = 2, color = \"white\", slab_size = 0.5) +\n  scale_fill_viridis_d(option = \"rocket\", guide = \"none\", end = 0.9) +\n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") +\n  geom_vline(xintercept = 0) +\n  labs(title = paste0(\"Average marginal effect of GDP per capita\", \"\\n\",\n                      \"(intercepts and slopes of both GDP per capita and year \",\n                      \"vary by country + intercepts vary by year)\"), \n       subtitle = paste0(\"lifeExp ~ gdpPercap_log_z + year + \", \n                         \"(1 + gdpPercap_log_z | year) + \", \n                         \"(1 + gdpPercap_log_z + year | country)\"),\n       x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a 10% increase in GDP per capita\"), \n       y = \"Year\",\n       caption = \"80% and 95% credible intervals shown with shading\") +\n  facet_nested_wrap(vars(continent, country), nrow = 2, strip = nested_settings,\n                    scales = \"free_x\") +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank(),\n        plot.subtitle = element_text(family = \"Consolas\"))\n\n\n\n\n\n\n\nVisualize more generic average marginal effects\nFinally we can calculate some aggregate average marginal effects to talk about the general effect of wealth on health. We’ll (1) find the global grand mean, (2) calculate the average effect in our fake Atlantis country in a fake 2020 year, and (2) calculate the average effect in Atlantis over time.\n\nCode# Calculate the overall global effect\n# `re_formula = NA` means that no random effects will be incorporated\name_global_gdp_country_year_year &lt;- model_gdp_country_year_year %&gt;% \n  emtrends(~ 1,\n           var = \"gdpPercap_log_z\",\n           epred = TRUE, re_formula = NA)\n\n# Get the posterior distribution of this global effect and make a plot\npred_global_gdp_country_year_year &lt;- ame_global_gdp_country_year_year %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = (.value / gdp_log_sd) / 10) %&gt;% \n  mutate(fake_facet_title = \"Global grand mean\")\n\nplot_global_gdp_country_year_year &lt;- ggplot(pred_global_gdp_country_year_year, aes(x = .value)) +\n  stat_slab(aes(fill_ramp = after_stat(cut_cdf_qi(cdf, .width = c(0.02, 0.8, 0.95, 1)))),\n            fill = palette_okabe_ito(5)) +\n  scale_fill_viridis_d(option = \"plasma\", guide = \"none\", begin = 0.5) +\n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") +\n  geom_vline(xintercept = 0) +\n  labs(x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a 10% increase in GDP per capita\"), \n       y = \"Density\") +\n  facet_wrap(vars(fake_facet_title)) +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank(),\n        strip.text = element_text(size = rel(1.1)))\n\n# Calculate the effect for Atlantis in 2020\n# `re_formula = NULL` means the full random effects structure will be\n# incorporated. `allow_new_levels` lets R deal with a new country, and\n# `sample_new_levels = \"gaussian\"` means that the characteristics for this new\n# country will be based on random draws from the model\name_hypo1_gdp_country_year_year &lt;- model_gdp_country_year_year %&gt;% \n  emtrends(~ country + year,\n           var = \"gdpPercap_log_z\",\n           at = list(year = (2020 - 1952), country = \"Atlantis\"),\n           epred = TRUE, re_formula = NULL, \n           allow_new_levels = TRUE, sample_new_levels = \"gaussian\")\n\n# Atlantis median 2020 effect\name_hypo1_gdp_country_year_year %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = (.value / gdp_log_sd) / 10) %&gt;% \n  median_hdci()\n## # A tibble: 1 × 8\n##   country   year .value .lower .upper .width .point .interval\n##   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Atlantis    68  0.330 -0.231  0.905   0.95 median hdci\n\n# Get the posterior distribution of this Atlantis effect and make a plot\npred_hypo1_gdp_country_year_year &lt;- ame_hypo1_gdp_country_year_year %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = (.value / gdp_log_sd) / 10) %&gt;% \n  mutate(fake_facet_title = \"AME for hypothetical Atlantis in 2020\")\n\nplot_hypo1_gdp_country_year_year &lt;- ggplot(pred_hypo1_gdp_country_year_year, aes(x = .value)) +\n  stat_slab(aes(fill_ramp = after_stat(cut_cdf_qi(cdf, .width = c(0.02, 0.8, 0.95, 1)))),\n            fill = palette_okabe_ito(7)) +\n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") +\n  geom_vline(xintercept = 0) +\n  labs(x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a 10% increase in GDP per capita\"), \n       y = \"Density\") +\n  facet_wrap(vars(fake_facet_title)) +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank(),\n        strip.text = element_text(size = rel(1.1)))\n\n# Calculate the effect for Atlantis across all existing years\name_hypo_gdp_country_year_year &lt;- model_gdp_country_year_year %&gt;% \n  emtrends(~ country + year,\n           var = \"gdpPercap_log_z\",\n           at = list(year = seq(0, 55, by = 5), country = \"Atlantis\"),\n           epred = TRUE, re_formula = NULL, \n           allow_new_levels = TRUE, sample_new_levels = \"gaussian\")\n\n# Get the posterior distribution of this Atlantis effect and make a plot\npred_hypo_gdp_country_year_year &lt;- ame_hypo_gdp_country_year_year %&gt;% \n  gather_emmeans_draws() %&gt;% \n  mutate(.value = (.value / gdp_log_sd) / 10) %&gt;% \n  mutate(year = year + 1952,\n         year = fct_inorder(factor(year))) %&gt;%\n  mutate(fake_facet_title = \"AME for hypothetical Atlantis across time\")\n\nplot_hypo_gdp_country_year_year &lt;- ggplot(pred_hypo_gdp_country_year_year, aes(x = .value)) +\n  stat_slab(aes(y = fct_rev(year), fill = year,\n                fill_ramp = after_stat(cut_cdf_qi(cdf, .width = c(0.02, 0.8, 0.95, 1)))),\n            height = 2, color = \"white\") +\n  scale_fill_viridis_d(option = \"rocket\", guide = \"none\", end = 0.9) +\n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") +\n  geom_vline(xintercept = 0) +\n  labs(x = paste0(\"Average marginal effect on life expectancy\", \"\\n\", \n                  \"of a 10% increase in GDP per capita\"), \n       y = \"Year\") +\n  facet_wrap(vars(fake_facet_title)) +\n  theme_clean() +\n  theme(panel.grid.major.y = element_blank(),\n        strip.text = element_text(size = rel(1.1)))\n\n# Show all the AME distributions in a mega plot\n((plot_global_gdp_country_year_year / plot_hypo1_gdp_country_year_year) | \n    plot_hypo_gdp_country_year_year) +\n  plot_annotation(title = paste0(\"Average marginal effect of GDP per capita\", \"\\n\",\n                                 \"(intercepts and slopes of both GDP per capita and year \",\n                                 \"vary by country + intercepts and GDP slopes vary by year)\"), \n                  subtitle = paste0(\"lifeExp ~ gdpPercap_log_z + year + \", \n                         \"(1 + gdpPercap_log_z | year) + \", \n                         \"(1 + gdpPercap_log_z + year | country)\"),\n                  caption = \"80% and 95% credible intervals shown with shading\",\n                  theme = theme_clean() + theme(plot.subtitle = element_text(family = \"Consolas\")))\n\n\n\n\n\n\n\nEach continent and nested country gets its own intercept and GDP and year slopes\nFinally, for super bonus fun and games, we could incorporate an extra continent-level hierarchy into the model, since countries are nested in continents and regions might see specific trends in the GDP/life expectancy relationship. I’m not actually going to run this model here—it takes a while even on a fast computer, and you have to specify informative priors and let it run for more iterations, etc. If we did run this, we’d get 7 different \\(\\tau\\) parameters and 6 different correlation parameters and we could do all sorts of fun stuff with the posterior. But we won’t run this for now—you can though!\n\nmodel_gdp_continent_country_year &lt;- brm(\n  bf(lifeExp ~ gdpPercap_log_z + year + \n       (1 + gdpPercap_log_z + year | continent / country),\n     decomp = \"QR\"),\n  data = gapminder,\n  cores = 4, chains = 4, seed = bayes_seed,\n  threads = threading(2)  # Two CPUs per chain to speed things up\n)\n\n\n# You'd use this to calculate the continent/country specific effects\nmodel_gdp_continent_country_year %&gt;%\n  emtrends(~ year + continent:country,\n           var = \"gdpPercap_log_z\",\n           at = list(year = c(0), country = countries$country),\n           nesting = \"country %in% continent\",\n           epred = TRUE, re_formula = NULL, allow_new_levels = TRUE)"
  },
  {
    "objectID": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#other-ways-of-dealing-with-time",
    "href": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#other-ways-of-dealing-with-time",
    "title": "A guide to working with country-year panel data and Bayesian multilevel models",
    "section": "Other ways of dealing with time",
    "text": "Other ways of dealing with time\nThroughout this guide, we’ve treated the time effect as linear, and that’s probably okay for this toy Gapminder example. In reality, though you can model time in all sorts of fancier ways, and there are whole textbooks and courses about how to do that (like Solomon Kurz’s excellent brms and tidyverse translation of Singer and Willett’s Applied Longitudinal Data Analysis here). For example, Solomon Kurz here uses R’s built-in ChickWeight dataset to model the effect of different diets across time on chick weights:\n\n\n\n\n\n\n\n\nThe model he uses looks like this—he uses time, time², the interaction of time and diet, the interaction of time² and diet, and adds chick-specific offsets to both time and time²! That lets the model pick up all sorts of linear and quadratic trends in time and their interactions with different diets, all across individual chicks in the dataset. That’s super neat.\n\nfit &lt;- lme4::lmer(\n  weight ~ 1 + Time + I(Time^2) + Diet + Time:Diet + I(Time^2:Diet) + \n    (1 + Time + I(Time^2) | Chick),\n  data = ChickWeight\n)\n\nYou could also add splines, higher-order polynomials (time⁴!), and other complexity to model time, but that goes way beyond the scope of this guide. Refer to Solomon’s guide for more about all that."
  },
  {
    "objectID": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#tldr-holy-crap-this-was-so-long",
    "href": "blog/2021/12/01/multilevel-models-panel-data-guide/index.html#tldr-holy-crap-this-was-so-long",
    "title": "A guide to working with country-year panel data and Bayesian multilevel models",
    "section": "tl;dr holy crap this was so long",
    "text": "tl;dr holy crap this was so long\nThis guide was long and detailed, but again, the audience for this post is future-me. You’re all just along for the ride :)\nMultilevel models let us deal with both time and country-level differences in panel data, and the ways we structure the random effects terms determine what we’re actually estimating.\nHere’s a super quick review of the different models we ran and what they do:\n\n\nCountry-specific intercepts + global time trend:\n\nbf(outcome ~ x + year + (1 | country))\n\n\n\nCountry-specific intercepts and country-specific slopes for year + global time trend:\n\nbf(outcome ~ x + year + (1 + year | country))\n\n\n\nCountry-specific intercepts and country-specific slopes for year and x + global time trend (this seems to be the most common and sensible approach):\n\nbf(outcome ~ x + year + (1 + x + year | country))\n\n\n\nCountry-specific intercepts and country-specific slopes for year and x and year-specific intercepts + global time trend:\n\nbf(outcome ~ x + year + (1 | year) + (1 + x + year | country))\n\n\n\nCountry-inside-continent-specific intercepts and country-inside-continent specific slopes for year and x + global time trend\n\nbf(outcome ~ x + year + (1 + x + year | continent / country))\n\n\n\nSuper wild party time: Country-inside-continent-specific intercepts and country-inside-continent specific slopes for year and x and year-specific intercepts and slopes for x + global time trend\n\nbf(outcome ~ x + year + (1 + x | year) + (1 + x + year | continent / country))"
  },
  {
    "objectID": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html",
    "href": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html",
    "title": "How to create a(n almost) fully Bayesian outcome model with inverse probability weights",
    "section": "",
    "text": "In my previous post about how to create Bayesian propensity scores and how to legally use them in a second stage outcome model, I ended up using frequentist models for the outcome stage. I did this for the sake of computational efficiency—running 2,000 models with lm() is way faster than running 2,000 individual Bayesian models with brm() from brms (like, creating 2,000 Bayesian models could take hours or days or weeks!)\nI concluded with this paragraph:\nJordan Nafa took on this challenge, though, and figured out a way to do exactly this with Stan!! He has a super well-documented example at GitHub, and I’ve adapted and borrowed liberally from his code for this post. Check out his example for additional updates and even more fantastic explanation!"
  },
  {
    "objectID": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#the-intuition",
    "href": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#the-intuition",
    "title": "How to create a(n almost) fully Bayesian outcome model with inverse probability weights",
    "section": "The intuition",
    "text": "The intuition\nAfter creating a treatment model in the design stage (i.e. predicting net usage based on the confounders), we generate a matrix of propensity scores or inverse probability weights (\\(\\nu\\) in Liao and Zigler (2020)’s approach). Each column of this matrix contains one posterior draw of propensity scores or weights.\nIn my original post, we generated 2,000 sets of propensity scores and then ran 2,000 outcome models with lm() to calculate the average treatment effect (ATE).\nbrms allows you to use weights with a slightly different syntax from lm():\nbrm(bf(outcome | weights(iptw) ~ treatment), data = whatever)\nThe argument we feed to weights() needs to be a column in the data that we’re working with (like iptw here for the inverse probability weights).\nSince it’s not really feasible to run thousands of separate Bayesian models, we can instead run the outcome model once and feed one set of weights for each MCMC iteration. Conceptually, we want to do something like this:\n\nOutcome model posterior draw 1: bf(outcome | weights(iptw[,1]) ~ treatment)\n\nOutcome model posterior draw 2: bf(outcome | weights(iptw[,2]) ~ treatment)\n\nOutcome model posterior draw 3: bf(outcome | weights(iptw[,3]) ~ treatment)\n\n(and so on)\n\nEach posterior draw thus gets its own set of weights, and all the draws are combined in the end as a single posterior distribution of the average treatment effect."
  },
  {
    "objectID": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#using-a-matrix-of-weights-with-raw-stan-code",
    "href": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#using-a-matrix-of-weights-with-raw-stan-code",
    "title": "How to create a(n almost) fully Bayesian outcome model with inverse probability weights",
    "section": "Using a matrix of weights with raw Stan code",
    "text": "Using a matrix of weights with raw Stan code\nUnfortunately brms can’t handle a matrix of weights. The weights() term in the model formula can only take a single column—it can’t iterate through a bunch of columns in a matrix. This is sad because it means we have to leave the comfort and convenience of brms and work with Stan code directly.\nFortunately, brms makes it easy to work with raw Stan and does most of the hard work for us. The make_stancode() function will convert a brms-based model into Stan code that we can then edit and tinker with.\nHere’s how we can do it (again with heavily annotated code from Jordan Nafa’s phenomenal example of how this all works):\n\nlibrary(tidyverse)    # ggplot2, dplyr, %&gt;%, and friends\nlibrary(brms)         # Nice interface for Bayesian models through Stan\nlibrary(rstan)        # For running Stan through R\nlibrary(tidybayes)    # For dealing with MCMC draws in a tidy way\nlibrary(ggdist)       # For distribution-related geoms\nlibrary(broom.mixed)  # For converting Stan-based model results to data frames\nlibrary(here)         # Convenient way for locating files\n\nset.seed(6348)  # From random.org\n\n# Use the Egypt palette from the MetBrewer package\negypt &lt;- MetBrewer::met.brewer(\"Egypt\")\n\n# Custom ggplot theme to make pretty plots\n# Get Archivo Narrow at https://fonts.google.com/specimen/Archivo+Narrow\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Archivo Narrow\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\", size = rel(0.8), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}\n\n# Use this theme on all plots\ntheme_set(\n  theme_nice()\n)\n\n# Make all labels use Archivo by default\nupdate_geom_defaults(\"label\",\n                     list(family = \"Archivo Narrow\",\n                          fontface = \"bold\"))\n\nFirst we’ll load the data (see the previous post or my class example on inverse probability weighting for more details about what these columns are). Our main question here is the effect of mosquito net usage on malaria risk.\n\nnets &lt;- read_csv(\"https://evalf21.classes.andrewheiss.com/data/mosquito_nets.csv\")\n\nWe’ll then create a model that predicts whether people self-select into using a mosquito net, based on our confounders of income, health status, and nighttime temperatures.\n\n# First stage model predicting net usage with the confounders\nmodel_treatment &lt;- brm(\n  bf(net ~ income + temperature + health,\n     decomp = \"QR\"),  # QR decomposition handles scaling and unscaling for us\n  family = bernoulli(),  # Logistic regression\n  data = nets,\n  chains = 8, cores = 8, iter = 2000,\n  seed = 1234, backend = \"cmdstanr\"\n)\n## Start sampling\n\nWe’ll the extract the predicted probabilities / propensity scores from the model and calculate inverse probability weights in each posterior draw for each person in the dataset:\n\n# Extract posterior predicted propensity scores\npred_probs_chains &lt;- posterior_epred(model_treatment)\n\n# Rows are posterior draws, columns are original rows in dataset\ndim(pred_probs_chains)\n## [1] 8000 1752\n\n# Create a matrix of weights where each column is a posterior draw\n# Transpose the matrix so that columns are posterior draws\nipw_matrix &lt;- t(pred_probs_chains) %&gt;%\n  as_tibble(.name_repair = \"universal\") %&gt;% \n  # Add treatment column so we can calculate weights\n  mutate(net_num = nets$net_num) %&gt;% \n  # Calculate weights\n  mutate(across(starts_with(\"...\"),\n                ~ (net_num / .x) + ((1 - net_num) / (1 - .x))\n  )) %&gt;% \n  # Get rid of treatment column\n  select(-net_num)\n\n# Rows are original rows in data and columns are posterior draws\ndim(ipw_matrix)\n## [1] 1752 8000\n\nhead(ipw_matrix, c(5, 10))\n## # A tibble: 5 × 10\n##    ...1  ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 ...10\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  2.49  3.01  2.50  2.62  2.66  2.78  2.73  2.74  2.60  2.70\n## 2  1.70  1.57  1.70  1.61  1.62  1.65  1.64  1.63  1.69  1.63\n## 3  1.22  1.16  1.22  1.17  1.21  1.17  1.14  1.14  1.22  1.20\n## 4  3.64  4.02  3.63  4.03  3.68  3.92  4.16  4.17  3.26  3.64\n## 5  1.44  1.44  1.45  1.41  1.45  1.44  1.43  1.43  1.58  1.47\n\nExport starter Stan code\nNext we need to use these weights in an outcome model using the neat trick of using one column of weights for each iteration of the outcome model, which requires actual Stan code (which I haven’t really written since this blog post or this research project (see here)).\nTo simplify life and make it so we don’t have to really write any raw Stan code, we’ll create the skeleton of our Bayesian outcome model with brms, but we don’t actually run it—instead we’ll pass the formula and priors to make_stancode() and save the underlying Stan code as a new file.\n\n# Outcome model\n# We can just use a placeholder weights(1) since the nets data doesn't have any\n# actual weights in it. It's okay bc we'll never actually run this model.\noutcome_formula &lt;- bf(malaria_risk | weights(1) ~ net,\n                      family = gaussian)\n\noutcome_priors &lt;- c(prior(\"student_t(3, 0, 2.5)\", class = \"Intercept\"),\n                    prior(\"normal(0, 2.5)\", class = \"b\"))\n\n# Generate Stan code for the outcome model\nmake_stancode(\n  formula = outcome_formula,\n  data = nets,\n  prior = outcome_priors,\n  save_model = \"original_stan_code.stan\"\n)\n\nAdjust the Stan code slightly\nNext we need to make some minor adjustments to the Stan code. You can also download a complete version of all these changes in this .zip file:\n\n modified-stan-stuff.zip\n\nHere’s what needs to change:\nChange 1: Declare two functions in the functions block. The actual code for these two functions will live in a separate C++ file that we’ll look at and make in just a minute. These will let us keep track of the current iteration number of the MCMC chains. By design, Stan doesn’t let you see the current iteration number (Stan code is designed to be stateless and not dependent on specific iteration numbers) but these two functions allow us to keep track of this ourselves. (These functions were originally written by Louis at the Stan forum; this StackOverflow answer shows another example of them working in the wild)\n\n\n\n\n\n\nTip\n\n\n\nIf you’re editing this file in RStudio, you’ll likely see a syntax error after this block, with the complaint “Function declared, but not defined.” That’s okay. We’ll officially define these functions in an external file and inject it into this Stan code when compiling. You can ignore the error.\n\n\n\n\n// Original block\n\nfunctions {\n}\n\n// Modified block\n// ADD 2 NEW LINES\nfunctions {\n  void add_iter();  // ~*~THIS IS NEW~*~\n  int get_iter();  // ~*~THIS IS NEW~*~\n}\n\n\nChange 2: Remove the declaration for the weights variable from the data block and add new declarations for two new variables: L for keeping track of the number of columns in the weights matrix, and IPW for the weights matrix:\n\n\n// Original block\n\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  vector&lt;lower=0&gt;[N] weights;  // model weights\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\n\n// Modified block\n// REMOVE 1 LINE; ADD 2 NEW LINES\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  //vector&lt;lower=0&gt;[N] weights;  // model weights -- ~*~REMOVE THIS~*~\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n  int L;  // number of columns in the weights matrix -- ~*~THIS IS NEW~*~\n  matrix[N, L] IPW;  // weights matrix -- ~*~THIS IS NEW~*~\n}\n\n\nChange 3: In the model block, add the ability to keep track of the current iteration with get_iter(). And—most importantly—modify the actual model so that it uses a column from the weights matrix IPW[n, M] rather than the single weights[n] vector.\n\n\n// Original block\n\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = Intercept + Xc * b;\n    for (n in 1:N) {\n      target += weights[n] * (normal_lpdf(Y[n] | mu[n], sigma));\n    }\n  }\n  // priors including constants\n  target += normal_lpdf(b | 0, 2.5);\n  target += student_t_lpdf(Intercept | 3, 0, 2.5);\n  target += student_t_lpdf(sigma | 3, 0, 14.8)\n    - 1 * student_t_lccdf(0 | 3, 0, 14.8);\n}\n\n// Modified block\n// ADD 2 LINES\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = Intercept + Xc * b;\n    \n    int M = get_iter();  // get the current iteration -- ~*~THIS IS NEW~*~\n    vector[N] weights = IPW[, M];  // get the weights for this iteration -- ~*~THIS IS NEW~*~\n    \n    for (n in 1:N) {\n      target += weights[n] * (normal_lpdf(Y[n] | mu[n], sigma));\n    }\n  }\n  // priors including constants\n  target += normal_lpdf(b | 0, 2.5);\n  target += student_t_lpdf(Intercept | 3, 0, 2.5);\n  target += student_t_lpdf(sigma | 3, 0, 14.8)\n    - 1 * student_t_lccdf(0 | 3, 0, 14.8);\n}\n\n\nChange 4: And finally in the generated quantities block, add the add_iter() function so that the iteration counter increases by one at the end of the draw.\n\n\n// Original block\n\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n}\n\n// Modified block\n// ADD 1 LINE\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n  \n  add_iter();  // update the counter each iteration --  ~*~THIS IS NEW~*~\n}\n\n\nDefine C++ functions for the counter\nBefore compiling this and running our shiny new model, we need to officially define our two new functions for keeping track of the MCMC iteration: add_iter() and get_iter(). We need to do this with C++ (But don’t worry if you’ve never touched C++ before! This is my first time using it too!). Create a file named iterfuns.hpp (or whatever you want to call it) with this in it:\n\n\niterfuns.hpp\n\n// Declare an integer to keep track of the iteration count\nstatic int itct = 1;\n\n// Increment the counter\ninline void add_iter(std::ostream* pstream__) {\n  itct += 1;\n}\n\n// Retrieve the current count\ninline int get_iter(std::ostream* pstream__) {\n  return itct;\n}\n\nWhen we compile the Stan code, we’ll inject this C++ code into it and our add_iter() and get_iter() functions will work.\nCompile the Stan code\nBefore running the model and creating a bunch of MCMC chains, we need to compile this Stan code into a binary program that we can then use for MCMC. This is the same thing that happens when you use brms() and wait for a few seconds while it says Compiling Stan program....\nTechnically we can do this by passing the file name of our modified Stan code to stan_model(), but rstan and/or Stan does stuff behind the scenes (storing compiled programs in /tmp or as hidden files somewhere on the computer) for caching purposes, and it can store older (and incorrect) versions of compiled models that can be hard to track down and get rid of. According to the documentation of stan_model(), we can force Stan to recompile the model every time and avoid this caching headache (it’s seriously a headache!) by first explicitly converting the Stan code to C++ with stanc(), and then passing that code to stan_model().\nSo to avoid caching issues, we’ll do this in two steps. First we’ll convert our modified Stan code to C++. The allow_undefined = TRUE option here is necessary because add_iter() and get_iter() are defined in an external file and Stan will complain because they’re not formally defined here (similar to why RStudio complains).\nThen we’ll feed this converted code to stan_model() and inject the C++ file with the counter functions in it. To reference an external file in C++, we have to include a line that looks like this:\n#include \"/full/path/to/iterfuns.hpp\"\nInstead of hand-typing that, we’ll use here() from the here package to automatically generate the absolute path, along with some extra newlines (\\n).\n\n# Convert the modified Stan code to C++\noutcome_c &lt;- stanc(\"modified_stan_code.stan\",\n                   allow_undefined = TRUE)\n\n# Compile C++ified Stan to a binary model object\noutcome_model &lt;- stan_model(\n  stanc_ret = outcome_c, \n  includes = paste0('\\n#include \"', here('iterfuns.hpp'), '\"\\n')\n)\n\nAfter a few seconds, we should have a new outcome_model object. This doesn’t contain any results or chains or anything. This is essentially a mini standalone program that’s designed to take our nets data, our weights, and the priors that we specified.\n\n\n\n\n\n\nFun fact\n\n\n\nThis is essentially the difference between brms and rstanarm. rstanarm comes with a bunch of pre-compiled model programs like stan_glm(), so you don’t have to compile anything yourself ever—you don’t need to wait for the Compiling Stan program... message to finish like you do with brms. But that extra pre-compiled speed comes at the cost of flexibility—you can’t run as many models or do as many neat extra things with rstanarm as you can with brms.\n\n\nRun the model\nFinally, we can feed our data into the outcome_model object and run the MCMC chains. Unlike brms, we can’t just feed it a formula and a data frame. Instead, we need to feed this model a list with each of the variables we declared in the data block of our code. The syntax is a little wonky, and the best way to get a feel for how this list needs to be structured is to use make_standata() and follow the same structure.\nImportantly, we need to use at least the same number of posterior chains as we have weights for (i.e. we can’t have 2,000 sets of weights and use 8,000 chains in the outcome model since we’ll run out of weights). Here we’ll use the same amount (2,000 per chain, only 1,000 of which are kept due to the warmup phase).\n\n# Make a dataset of all the covariates and an intercept column\noutcome_covariates &lt;- model.matrix(~ net_num, data = nets)\nhead(outcome_covariates)\n##   (Intercept) net_num\n## 1           1       1\n## 2           1       0\n## 3           1       0\n## 4           1       1\n## 5           1       0\n## 6           1       0\n\n# Make a list of all the required pieces for the data block in the Stan model\noutcome_data &lt;- list(\n  N = nrow(nets),\n  Y = nets$malaria_risk,\n  K = ncol(outcome_covariates),\n  X = outcome_covariates,\n  L = ncol(ipw_matrix),\n  IPW = as.matrix(ipw_matrix),\n  prior_only = 0\n)\nstr(outcome_data)\n## List of 7\n##  $ N         : int 1752\n##  $ Y         : num [1:1752] 33 42 80 34 44 25 19 35 32 40 ...\n##  $ K         : int 2\n##  $ X         : num [1:1752, 1:2] 1 1 1 1 1 1 1 1 1 1 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:1752] \"1\" \"2\" \"3\" \"4\" ...\n##   .. ..$ : chr [1:2] \"(Intercept)\" \"net_num\"\n##   ..- attr(*, \"assign\")= int [1:2] 0 1\n##  $ L         : int 8000\n##  $ IPW       : num [1:1752, 1:8000] 2.49 1.7 1.22 3.64 1.44 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr [1:8000] \"...1\" \"...2\" \"...3\" \"...4\" ...\n##  $ prior_only: num 0\n\n\n# FINALLY run the model!\noutcome_samples &lt;- sampling(\n  outcome_model, \n  data = outcome_data, \n  chains = 8, \n  iter = 2000,\n  cores = 8,\n  seed = 1234\n)"
  },
  {
    "objectID": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#analyze-the-results",
    "href": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#analyze-the-results",
    "title": "How to create a(n almost) fully Bayesian outcome model with inverse probability weights",
    "section": "Analyze the results",
    "text": "Analyze the results\nThe results from rstan::sampling() behave pretty similarly to the output from rstanarm or brms (which isn’t surprising, since both of those packages are just fancier frontends for Stan). We can print the results, for instance:\n\nprint(outcome_samples)\n## Inference for Stan model: modified_stan_code.\n## 8 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=8000.\n## \n##                 mean se_mean     sd     2.5%      25%      50%       75%     97.5% n_eff Rhat\n## b[1]            -9.8    0.01   1.06    -12.0    -10.4     -9.8     -9.16     -7.66  8120    1\n## Intercept       35.7    0.00   0.25     35.2     35.5     35.7     35.87     36.19  3020    1\n## sigma           13.8    0.00   0.22     13.4     13.7     13.8     13.92     14.22  3991    1\n## b_Intercept     39.5    0.01   0.47     38.6     39.2     39.5     39.81     40.46  5295    1\n## lp__        -14315.3    2.01 183.11 -14727.6 -14418.5 -14298.7 -14185.22 -14009.48  8325    1\n## \n## Samples were drawn using NUTS(diag_e) at Mon Nov 28 15:17:32 2022.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\n\nOr we can use tidy() from broom.mixed:\n\ntidy(outcome_samples, conf.int = TRUE)\n## # A tibble: 4 × 5\n##   term        estimate std.error conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 b[1]           -9.80     1.06     -12.0     -7.66\n## 2 Intercept      35.7      0.253     35.2     36.2 \n## 3 sigma          13.8      0.216     13.4     14.2 \n## 4 b_Intercept    39.5      0.470     38.6     40.5\n\nThe coefficients aren’t nicely named and separated into fixed and random effects like they are in brms output, but we can still get the results out (and we could rename them in the model if we wanted, or we can do that on our own as we work with the data). The main coefficient we care about here is the one for net, or b[1]. Based on this Bayesian outcome model, after incorporating the uncertainty from the posterior distribution of inverse probability weights, the ATE of using a net is -9.8, with an error of 1.06.\nThis is so cool! At the end of the previous post, after running 2,000 frequentist models and combining the ATEs and standard errors, we ended up with these results:\n\n# ----------------------------------------------------------------------------------------------\n# Excerpt from https://www.andrewheiss.com/blog/2021/12/18/bayesian-propensity-scores-weights/\n# ----------------------------------------------------------------------------------------------\n# Combined average treatment effect\nmean(outcome_models$ate)\n## [1] -10.1\n\n# Combined standard errors with Rubin's rules (this is correct)\nrubin_se(outcome_models$ate, outcome_models$ate_se)\n## [1] 1.01\n\nWe get comparable results with this Bayesian outcome model, only now we get to do fun Bayesian inference with the results!\nInstead of confidence intervals, we have credible intervals: given the data we have, there’s a 95% chance that the ATE is between -12.01 and -7.66. We can see this in the posterior distribution of the ATE:\n\n# Make a long, tidy data frame of the posterior draws\noutcome_tidy &lt;- outcome_samples %&gt;% \n  tidy_draws() %&gt;% \n  rename(ate = `b[1]`)\n\n\nggplot(outcome_tidy, aes(x = ate)) +\n  stat_slab(aes(fill_ramp = after_stat(cut_cdf_qi(cdf, .width = c(0.02, 0.8, 0.95, 1)))),\n            fill = egypt[2]) +\n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") +\n  labs(x = \"Average treatment effect of using a mosquito net\", y = NULL,\n       caption = \"Median shown with vertical line; 80% and 95% credible intervals shown with shading\")\n\n\n\n\n\n\n\nWe can calculate the proportion of this posterior distribution that is less than zero to find the probability of direction, or the probability that the ATE is negative:\n\n# Find the proportion of posterior draws that are less than 0\noutcome_tidy %&gt;% \n  summarize(prop_lessthan_0 = sum(ate &lt; 0) / n())\n## # A tibble: 1 × 1\n##   prop_lessthan_0\n##             &lt;dbl&gt;\n## 1               1\n\nNot surprisingly, given the distribution we saw above, there’s a 100% chance that the ATE is negative.\nWe can also calculate the proportion of the posterior distribution that falls within a region of practical equivalence (or ROPE). We can think of this as a “dead zone” of sorts. If the ATE is 0, we know for sure that there’s no effect. If the ATE is something small like -0.51 or 0.73, we probably don’t actually care—that’s not a huge effect and could just be because of measurement error. If the ATE is sizable and far away from this “dead zone” / ROPE, we can be pretty confident of the substantiality of the effect.\nThere are a lot of ways to determine the size of the ROPE. You can base it on experience with the phenomenon (e.g. you’re an expert in public health and know that a 1-2 point change in malaria risk scores is small, while a 10+ change is big), or you can base it on the data you have (like 0.1 * sd(outcome), which is a common approach).\nFor this example, we’ll pretend that any effect between −7 and 7 doesn’t matter—for an expert, values like 2.4 or −6 would be negligible. This is a huge ROPE, by the way! If we follow the 0.1 × the standard deviation of the outcome rule, the ROPE should only be ±0.1 * sd(nets$malaria_risk), or ±1.546. I’m using ±7 here just for the sake of illustration—I want to see the ROPE on the plot :)\n\n# Find the proportion of posterior draws that are less than 0\nprop_outside &lt;- outcome_tidy %&gt;% \n  summarize(prop_outside_rope = 1 - sum(ate &gt;= -7 & ate &lt;= 7) / n())\nprop_outside\n## # A tibble: 1 × 1\n##   prop_outside_rope\n##               &lt;dbl&gt;\n## 1             0.992\n\n\nggplot(outcome_tidy, aes(x = ate)) +\n  stat_halfeye(aes(fill_ramp = after_stat(x &gt;= 7 | x &lt;= -7)), \n               fill = egypt[2], .width = c(0.95, 0.8)) +\n  scale_fill_ramp_discrete(from = egypt[1], guide = \"none\") +\n  annotate(geom = \"rect\", xmin = -7, xmax = 0, ymin = -Inf, ymax = Inf, \n           fill = egypt[1], alpha = 0.3) +\n  annotate(geom = \"label\", x = -3.5, y = 0.75, label = \"ROPE\\n(dead zone)\\n0 ± 7\") +\n  labs(x = \"Average treatment effect of using a mosquito net\", y = NULL,\n       caption = \"Median shown with point; 80% and 95% credible intervals shown with black bars\")\n\n\n\n\n\n\n\nUsing a gigantic ROPE of 0±7 malaria risk points, we can see that 99.2% of the posterior distribution lies outside the region of practical equivalence, which provides pretty strong evidence of a nice big ATE. This program definitely has an effect! (It’s fake data! I made sure it had an effect!)"
  },
  {
    "objectID": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#using-the-results-with-brms",
    "href": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#using-the-results-with-brms",
    "title": "How to create a(n almost) fully Bayesian outcome model with inverse probability weights",
    "section": "Using the results with brms\n",
    "text": "Using the results with brms\n\nOne issue with using rstan::sampling() instead of brms is that we can’t do nice things like automatic extraction of posterior expectations or predictions, since the MCMC samples we have are a stanfit object and not a stanreg object (which is what both rstanarm and brms produce):\n\nposterior_epred(outcome_samples)\n## Error in UseMethod(\"posterior_epred\"): no applicable method for 'posterior_epred' applied to an object of class \"stanfit\"\n\nHowever, it is possible to create an empty brms object and stick the MCMC samples we made with rstan::sampling() into it, which then allows us to do anything a regular brms model can do! This only works when making minimal changes to the Stan code—we can’t modify the likelihood (or the target part of the model block of the Stan code). We didn’t touch this part, though, so we can safely stick these samples into a brms object.\nTo do this, we first have to create an empty model by using the empty = TRUE argument. We also have to create a placeholder weights column, even though we’re not actually fitting the model—brms will complain otherwise.\nWe can then assign the outcome_samples object to the $fit slot of the empty model. Finally, we have to change the coefficient names to be nicer and compatible with brms. Note how earlier the coefficient for net_num was named b[1], which was inconvenient. By using brms::rename_pars(), we can change those subscripted/indexed names into nice names again.\n\n# Placeholder outcome model\noutcome_brms &lt;- brm(bf(malaria_risk | weights(iptw) ~ net_num),\n    # brms needs a column for the weights, even though we're not\n    # fitting the model, so create a placeholder column of 1s\n    data = nets %&gt;% mutate(iptw = 1),\n    empty = TRUE\n)\n\n# Add the samples from rstan::sampling() into the empty brms object and fix the\n# coefficient names (i.e. b[1] → b_net_num)\noutcome_brms$fit &lt;- outcome_samples\noutcome_brms &lt;- rename_pars(outcome_brms)\n\nThe outcome_brms object is now just like any regular brms model, and everything works with it:\n\n# It works!\nsummary(outcome_brms)\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: malaria_risk | weights(iptw) ~ net_num \n##    Data: nets %&gt;% mutate(iptw = 1) (Number of observations: 1752) \n##   Draws: 8 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 8000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept    39.51      0.47    38.59    40.46 1.00     5341     6208\n## net_num      -9.80      1.06   -12.01    -7.66 1.00     8048     6964\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma    13.79      0.22    13.37    14.22 1.00     3986     4864\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Predicted values work\nepreds &lt;- posterior_epred(outcome_brms)\nhead(epreds, c(5, 10))\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n## [1,] 30.3 39.1 39.1 30.3 39.1 39.1 30.3 39.1 39.1  39.1\n## [2,] 30.1 39.1 39.1 30.1 39.1 39.1 30.1 39.1 39.1  39.1\n## [3,] 30.1 39.2 39.2 30.1 39.2 39.2 30.1 39.2 39.2  39.2\n## [4,] 29.5 39.8 39.8 29.5 39.8 39.8 29.5 39.8 39.8  39.8\n## [5,] 30.0 39.4 39.4 30.0 39.4 39.4 30.0 39.4 39.4  39.4\n\n\n# posterior predictive checks work!\npp_check(outcome_brms)\n## Using 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\n# emmeans works too!\nlibrary(emmeans)\noutcome_brms %&gt;%\n    emtrends(~net_num, var = \"net_num\")\n##  net_num net_num.trend lower.HPD upper.HPD\n##        0          -9.8       -12     -7.65\n##        1          -9.8       -12     -7.65\n## \n## Point estimate displayed: median \n## HPD interval probability: 0.95"
  },
  {
    "objectID": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#only-tiny-downside",
    "href": "blog/2021/12/20/fully-bayesian-ate-iptw/index.html#only-tiny-downside",
    "title": "How to create a(n almost) fully Bayesian outcome model with inverse probability weights",
    "section": "Only tiny downside",
    "text": "Only tiny downside\nThere’s just one minor issue with this approach: all the sampling has to be done with rstan. For whatever reason, cmdstanr doesn’t like the iteration tracking functions and it crashes. This might be because cmdstanr is pickier about C++ namespaces? I’m not sure. If it did work, the code would look something like this:\n\nlibrary(cmdstanr)\n\noutcome_model_cmdstanr &lt;- cmdstan_model(\n  stan_file = \"modified_stan_code.stan\",\n  cpp_options = list(USER_HEADER = here('iterfuns.hpp')),\n  stanc_options = list(\"allow-undefined\")\n)\n\noutcome_samples_cmdstanr &lt;- outcome_model_cmdstanr$sample(data = outcome_data)\n\nIn spite of this, being able to use Bayesian models in both the treatment/design stage and the outcome/analysis stage is incredibly powerful!"
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html",
    "href": "blog/2022/05/20/marginalia/index.html",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "",
    "text": "I’m a huge fan of doing research and analysis in public. I try to make my research public and freely accessible, but ever since watching David Robinson’s “The unreasonable effectiveness of public work” keynote from rstudio::conf 2019, I’ve tried to make my research process open and accessible too.\nAccording to David, researchers typically view their work like this:\nPeople work towards a final published product, which is the most valuable output of the whole process. The intermediate steps like the code, data, preliminary results, and so on, are less valuable and often hidden from the public. People only see the final published thing.\nDavid argues that we should instead see our work like this:\nIn this paradigm, anything on your computer and only accessible by you isn’t that valuable. Anything you make accessible to the public online—including all the intermediate stuff like code, data, and preliminary results, in addition to the final product—is incredibly valuable. The world can benefit from neat code tricks you stumble on while making graphs; the world can benefit from new data sources you find or your way of processing data; the world can benefit from a toy example of a new method you read about in some paper, even if the actual code you write to play around with the method never makes it into any published paper. It’s all useful to the broader community of researchers.\nPublic work also builds community norms—if more people share their behind-the-scenes work, it encourages others to do the same and engage with it and improve it (see this super detailed and helpful comment with corrections to my previous post, for example!).\nPublic work is also valuable for another more selfish reason. Building an online presence with a wide readership is hard, and my little blog post contributions aren’t famous or anything—they’re just sitting out here in a tiny corner of the internet. But these guides have been indispensable for me. They’ve allowed me to work through and understand tricky statistical and programming concepts, and then have allowed me to come back to them months later and remember how they work. This whole blog is primarily a resource for future me.\nSo here’s yet another blog post that is hopefully potentially useful for the general public, but that is definitely useful for future me.\nIn a few of my ongoing research projects, I’m working with non-linear regression models, and I’ve been struggling to interpret their results. In my past few posts (like this one on hurdle models, or this one on multilevel panel data, or this one on beta and zero-inflated models), I’ve explored a bunch of different ways to work with and interpret these more complex models and calculate their marginal effects. I even wrote a guide to calculating average marginal effects for multilevel models. TURNS OUT™, though, that I’ve actually been a bit wrong about my terminology for all the marginal effects I’ve talked about in those posts.\nPart of the reason for this wrongness is because there are so many quasi-synonyms for the idea of “marginal effects” and people seem to be pretty loosey goosey about what exactly they’re referring to. There are statistical effects, marginal effects, marginal means, marginal slopes, conditional effects, conditional marginal effects, marginal effects at the mean, and many other similarly-named ideas. There are also regression coefficients and estimates, which have marginal effects vibes, but may or may not actually be marginal effects depending on the complexity of the model.\nThe question of what the heck “marginal effects” are has plagued me for a while. In October 2021 I publicly announced that I would finally buckle down and figure out their definitions and nuances:\nAnd then I didn’t.\nSo here I am, 7 months later, publicly figuring out the differences between regression coefficients, regression predictions, marginaleffects, emmeans, marginal slopes, average marginal effects, marginal effects at the mean, and all these other “marginal” things that researchers and data scientists use.\nThis guide is highly didactic and slowly builds up the concept of marginal effects as slopes and partial derivatives. The tl;dr section at the end has a useful summary of everything here, with a table showing all the different approaches to marginal effects with corresponding marginaleffects and emmeans code, as well as some diagrams outlining the two packages’ different approaches to averaging. Hopefully it’s useful—it is for me!\nLet’s get started by looking at some lines and slopes (after loading a bunch of packages and creating some useful little functions).\n# Load packages\n# ---------------\nlibrary(tidyverse)        # dplyr, ggplot2, and friends\nlibrary(broom)            # Convert models to data frames\nlibrary(marginaleffects)  # Marginal effects stuff\nlibrary(emmeans)          # Marginal effects stuff\n\n# Visualization-related packages\nlibrary(ggtext)           # Add markdown/HTML support to text in plots\nlibrary(glue)             # Python-esque string interpolation\nlibrary(scales)           # Functions to format numbers nicely\nlibrary(gganimate)        # Make animated plots\nlibrary(patchwork)        # Combine ggplots\nlibrary(ggrepel)          # Make labels that don't overlap\nlibrary(MetBrewer)        # Artsy color palettes\n\n# Data-related packages\nlibrary(palmerpenguins)   # Penguin data\nlibrary(WDI)              # Get data from the World Bank's API\nlibrary(countrycode)      # Map country codes to different systems\nlibrary(vdemdata)         # Use data from the Varieties of Democracy (V-Dem) project\n# Install vdemdata from GitHub, not CRAN\n# devtools::install_github(\"vdeminstitute/vdemdata\")\n\n\n# Helpful functions\n# -------------------\n# Format numbers in pretty ways\nnice_number &lt;- label_number(style_negative = \"minus\", accuracy = 0.01)\nnice_p &lt;- label_pvalue(prefix = c(\"p &lt; \", \"p = \", \"p &gt; \"))\n\n# Point-slope formula: (y - y1) = m(x - x1)\nfind_intercept &lt;- function(x1, y1, slope) {\n  intercept &lt;- slope * (-x1) + y1\n  return(intercept)\n}\n\n# Visualization settings\n# ------------------------\n\n# Custom ggplot theme to make pretty plots\n# Get IBM Plex Sans Condensed at https://fonts.google.com/specimen/IBM+Plex+Sans+Condensed\ntheme_mfx &lt;- function() {\n  theme_minimal(base_family = \"IBM Plex Sans Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          axis.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\"),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          legend.title = element_text(face = \"bold\"))\n}\n\n# Make labels use IBM Plex Sans by default\nupdate_geom_defaults(\"label\", \n                     list(family = \"IBM Plex Sans Condensed\"))\nupdate_geom_defaults(ggtext::GeomRichText, \n                     list(family = \"IBM Plex Sans Condensed\"))\nupdate_geom_defaults(\"label_repel\", \n                     list(family = \"IBM Plex Sans Condensed\"))\n\n# Use the Johnson color palette\nclrs &lt;- met.brewer(\"Johnson\")"
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#what-does-marginal-even-mean-in-the-first-place",
    "href": "blog/2022/05/20/marginalia/index.html#what-does-marginal-even-mean-in-the-first-place",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "What does “marginal” even mean in the first place?",
    "text": "What does “marginal” even mean in the first place?\nPut as simply as possible, in the world of statistics, “marginal” means “additional,” or what happens to outcome variable \\(y\\) when explanatory variable \\(x\\) changes a little.\nTo find out precisely how much things change, we need to use calculus.\nOh no.\nSuper quick crash course in differential calculus (it’s not scary, I promise!)\nI haven’t taken a formal calculus class since my senior year of high school in 2002. I enjoyed it a ton and got the highest score on the AP Calculus BC test, which gave me enough college credits to not need it as an undergraduate, given that I majored in Middle East Studies, Arabic, and Italian. I figured I’d never need to think about calculus every again. lol.\nIn my first PhD-level stats class in 2012, the professor cancelled class for the first month and assigned us all to go relearn calculus with Khan Academy, since I wasn’t alone in my unlearning of calculus. Even after that crash course refresher, I don’t really ever use it in my own research. When I do, I only use it to think about derivatives and slopes, since those are central to statistics.\nCalculus can be boiled down to two forms: (1) differential calculus is all about finding rates of changes by calculating derivatives, or slopes, while (2) integral calculus is all about finding total amounts, or areas, by adding infinitesimally small things together. According to the fundamental theorem of calculus, these two types are actually the inverse of each other—you can find the total area under a curve based on its slope, for instance. Super neat stuff. If you want a cool accessible refresher / history of all this, check out Steven Strogatz’s Infinite Powers: How Calculus Reveals the Secrets of the Universe—it’s great.\nIn the world of statistics and marginal effects all we care about are slopes, which are solely a differential calculus idea.\nLet’s pretend we have a line that shows the relationship between \\(x\\) and \\(y\\) that’s defined with an equation using the form \\(y = mx + b\\), where \\(m\\) is the slope and \\(b\\) is the y-intercept. We can plot it with ggplot using the helpful geom_function() function:\n\\[\ny = 2x - 1\n\\]\n\n# y = 2x - 1\na_line &lt;- function(x) (2 * x) - 1\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = a_line, linewidth = 1, color = clrs[2]) +\n  scale_x_continuous(breaks = -2:5, limits = c(-1, 3)) +\n  scale_y_continuous(breaks = -3:9) +\n  annotate(geom = \"segment\", x = 1, y = 1.3, xend = 1, yend = 3, color = clrs[4], linewidth = 0.5) +\n  annotate(geom = \"segment\", x = 1, y = 3, xend = 1.8, yend = 3, color = clrs[4], linewidth = 0.5) +\n  annotate(geom = \"richtext\", x = 1.4, y = 3.1, label = \"Slope: **2**\", vjust = 0) +\n  labs(x = \"x\", y = \"y\") +\n  coord_equal() +\n  theme_mfx()\n\n\n\n\n\n\n\nThe line crosses the y-axis at -1, and its slope, or its \\(\\frac{\\text{rise}}{\\text{run}}\\) is 2, or \\(\\frac{2}{1}\\), meaning that we go up two units and to the right one unit.\nImportantly, the slope shows the relationship between \\(x\\) and \\(y\\). If \\(x\\) increases by 1 unit, \\(y\\) increases by 2: when \\(x\\) is 1, \\(y\\) is 1; when \\(x\\) is 2, \\(y\\) is 3, and so on. We can call this the marginal effect, or the change in \\(y\\) that results from one additional \\(x\\).\nWe can think about this slope using calculus language too. In differential calculus, slopes are called derivatives and they represent the change in \\(y\\) that results from changes in \\(x\\), or \\(\\frac{dy}{dx}\\). The \\(d\\) here refers to an infinitesimal change in the values of \\(x\\) and \\(y\\), rather than a one-unit change like we think of when looking at the slope as \\(\\frac{\\text{rise}}{\\text{run}}\\). Even more technically, the \\(d\\) indicates that we’re working with the total derivative, since there’s only one variable (\\(x\\)) to consider. If we had more variables (like \\(y = 2x + 3z -1\\)), we would need to find the partial derivative for \\(x\\), holding \\(z\\) constant, and we’d write the derivative with a \\(\\partial\\) symbol instead: \\(\\frac{\\partial y}{\\partial x}\\). More on that in a bit.\nBy plotting this line, we can figure out \\(\\frac{dy}{dx}\\) visually—the slope is 2. But we can figure it out mathematically too. Differential calculus is full of fancy tricks and rules of thumb for figuring out derivatives, like the power rule, the chain rule, and so on. The easiest one for me to remember is the power rule, which says you can find the slope of a variable like \\(x\\) by decreasing its exponent by 1 and multiplying that exponent by the variable’s coefficient. All constants (terms without \\(x\\)) disappear.\n\\[\n\\begin{aligned}\ny &= 2x - 1 \\\\\n&= 2x^1 - 1 \\\\\n\\frac{dy}{dx}&= (1 \\times 2) x^0 \\\\\n&=  2\n\\end{aligned}\n\\]\n(My secret is that I only know the power rule and so I avoid calculus at all costs and either use R or use Wolfram Alpha—go to Wolfram Alpha, type in derivative y = 2x - 1 and you’ll see some magic.)\nWe thus know that the derivative of \\(y = 2x - 1\\) is \\(\\frac{dy}{dx} = 2\\). At every point on this line, the slope is 2—it never changes.\n\nslope_annotations &lt;- tibble(x = c(-0.25, 1.2, 2.4)) |&gt; \n  mutate(y = a_line(x)) |&gt; \n  mutate(nice_y = y + 1) |&gt; \n  mutate(nice_label = glue(\"x: {x}; y: {y}&lt;br&gt;\",\n                           \"Slope (dy/dx): **{2}**\"))\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = a_line, linewidth = 1, color = clrs[2]) +\n  geom_point(data = slope_annotations, aes(x = x, y = y)) +\n  geom_richtext(data = slope_annotations, \n                aes(x = x, y = y, label = nice_label),\n                nudge_y = 0.5) +\n  scale_x_continuous(breaks = -2:5, limits = c(-1, 3)) +\n  scale_y_continuous(breaks = -3:9) +\n  labs(x = \"x\", y = \"y\") +\n  coord_equal() +\n  theme_mfx()\n\n\n\n\n\n\n\nThe power rule seems super basic for equations with non-exponentiated \\(x\\)s, but it’s really helpful with more complex equations, like this parabola \\(y = -0.5x^2 + 5x + 5\\):\n\n# y = -0.5x^2 + 5x + 5\na_parabola &lt;- function(x) (-0.5 * x^2) + (5 * x) + 5\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = a_parabola, linewidth = 1, color = clrs[2]) +\n  xlim(-5, 15) +\n  labs(x = \"x\", y = \"y\") +\n  coord_cartesian(ylim = c(-5, 20)) +\n  theme_mfx()\n\n\n\n\n\n\n\nWhat’s interesting here is that there’s no longer a single slope for the whole function. The steepness of the slope across a range of \\(x\\)s depends on whatever \\(x\\) currently is. The curve is steeper at really low and really high values of \\(x\\) and it is shallower around 5 (and it is completely flat when \\(x\\) is 5).\nIf we apply the power rule to the parabola formula we can find the exact slope:\n\\[\n\\begin{aligned}\ny &= -0.5x^2 + 5x^1 + 5 \\\\\n\\frac{dy}{dx} &= (2 \\times -0.5) x + (1 \\times 5) x^0 \\\\\n&= -x + 5\n\\end{aligned}\n\\]\nWhen \\(x\\) is 0, the slope is 5 (\\(-0 + 5\\)); when \\(x\\) is 8, the slope is −3 (\\(-8 + 5\\)), and so on. We can visualize this if we draw some lines tangent to some different points on the equation. The slope of each of these tangent lines represents the instantaneous slope of the parabola at each \\(x\\) value.\n\n# dy/dx = -x + 5\nparabola_slope &lt;- function(x) (-x) + 5\n\nslope_annotations &lt;- tibble(\n  x = c(0, 3, 8)\n) |&gt; \n  mutate(y = a_parabola(x),\n         slope = parabola_slope(x),\n         intercept = find_intercept(x, y, slope),\n         nice_slope = glue(\"Slope (dy/dx)&lt;br&gt;&lt;span style='font-size:12pt;color:{clrs[4]}'&gt;**{slope}**&lt;/span&gt;\"))\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = a_parabola, linewidth = 1, color = clrs[2]) +\n  geom_abline(data = slope_annotations,\n              aes(slope = slope, intercept = intercept),\n              linewidth = 0.5, color = clrs[4], linetype = \"21\") +\n  geom_point(data = slope_annotations, aes(x = x, y = y),\n             size = 3, color = clrs[4]) +\n  geom_richtext(data = slope_annotations, aes(x = x, y = y, label = nice_slope),\n                nudge_y = 2) +\n  xlim(-5, 15) +\n  labs(x = \"x\", y = \"y\") +\n  coord_cartesian(ylim = c(-5, 20)) +\n  theme_mfx()\n\n\n\n\n\n\n\nAnd here’s an animation of what the slope looks like across a whole range of \\(x\\)s. Neat!\n\n\n\n\n\n\nNote\n\n\n\nFor the sake of space, I didn’t include the code for this here, but you can see how I made this animation with gganimate at the R Markdown file for this post at GitHub.\n\n\nMarginal things in economics\nIn the calculus world, the term “marginal” isn’t used all that often. Instead they talk about derivatives. But in the end, all these marginal/derivative things are just slopes.\nBefore looking at how this applies to the world of statistics, let’s look at a quick example from economics, since economists also use the word “marginal” to refer to slopes. My first exposure to the word “marginal” meaning “changes in things” wasn’t actually in the world of statistics, but in economics. I took my first microeconomics class as a first-year MPA student in 2010 (and hated it; ironically I teach it now 🤷).\nOne common question in microeconomics relates to how people maximize their happiness, or utility, under budget constraints (see here for an R-based example). Economists imagine that people have utility functions in their heads that take inputs and convert them to utility (or happiness points). For instance, let’s pretend that the happiness/utility (\\(u\\)) you get from the number of cookies you eat (\\(x\\)) is defined like this:\n\\[\nu = -0.5x^2 + 5x\n\\]\nHere’s what that looks like:\n\n# u = -0.5x^2 + 5x\nu_cookies &lt;- function(x) (-0.5 * x^2) + (5 * x)\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_function(fun = u_cookies, linewidth = 1, color = clrs[2]) +\n  scale_x_continuous(breaks = seq(0, 12, 2), limits = c(0, 12)) +\n  labs(x = \"Cookies\", y = \"Utility (happiness points)\") +\n  theme_mfx()\n\n\n\n\n\n\n\nThis parabola represents your total utility from cookies. Eat 1 cookie, get 4.5 happiness points; eat 3 cookies, get 10.5 points; eat 6, get 12 points; and so on.\nThe marginal utility, on the other hand, tells how how much more happiness you’d get from eating one more cookie. If you’re currently eating 1, how many more happiness points would you get by moving to 2? If if you’re eating 7, what would happen to your happiness if you moved to 8? We can figure this out by looking at the slope of the parabola, which will show us the instantaneous rate of change, or marginal utility, for any number of cookies.\nPower rule time! (or type derivative -0.5x^2 + 5x at Wolfram Alpha)\n\\[\n\\begin{aligned}\nu &= -0.5x^2 + 5x \\\\\n\\frac{du}{dx} &= (2 \\times -0.5) x^1 + 5x^0 \\\\\n&= -x + 5\n\\end{aligned}\n\\]\nLet’s plot this really quick too:\n\n# du/dx = -x + 5\nmu_cookies &lt;- function(x) -x + 5\n\nggplot() +\n  geom_vline(xintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_hline(yintercept = 0, linewidth = 0.5, color = \"grey50\") +\n  geom_vline(xintercept = 5, linewidth = 0.5, \n             linetype = \"21\", color = clrs[3]) +\n  geom_function(fun = mu_cookies, linewidth = 1, color = clrs[5]) +\n  scale_x_continuous(breaks = seq(0, 12, 2), limits = c(0, 12)) +\n  labs(x = \"Cookies\", y = \"Marginal utility (additional happiness points)\") +\n  theme_mfx()\n\n\n\n\n\n\n\nIf you’re currently eating 1 cookie and you grab another one, you’ll gain 4 extra or marginal happiness points. If you’re eating 6 and you grab another one, you’ll actually lose some happiness—the marginal utility at 6 is -1. If you’re an economist who wants to maximize your happiness, you should eat the number of cookies where the extra happiness you’d get is 0, or where marginal utility is 0:\n\\[\n\\begin{aligned}\n\\frac{du}{dx} &= -x + 5 \\\\\n0 &= -x + 5 \\\\\nx &= 5\n\\end{aligned}\n\\]\nEat 5 cookies, maximize your happiness. Eat any more and you’ll start getting disutility (like a stomachache). This is apparent in the marginal utility plot too. All the values of marginal utility to the left of 5 are positive; all the values to the right of 5 are negative. Economists call this decreasing marginal utility.\nThis relationship between total utility and marginal utility is even more apparent if we look at both simultaneously (for fun I included the second derivative (\\(\\frac{d^2u}{dx^2}\\)), or the slope of the first derivative, in the marginal utility panel):\n\n\n\n\n\n\nNote\n\n\n\nAgain, I omitted the code for this here, but you can see it at GitHub."
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#what-about-marginal-things-in-statistics",
    "href": "blog/2022/05/20/marginalia/index.html#what-about-marginal-things-in-statistics",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "What about marginal things in statistics?",
    "text": "What about marginal things in statistics?\nMarginal utility, marginal revenue, marginal costs, and all those other marginal things are great for economists, but how does this “marginal” concept relate to statistics? Is it the same?\nYep! Basically!\nAt its core, regression modeling in statistics is all about fancy ways of finding averages and fancy ways of drawing lines. Even if you’re doing non-regression things like t-tests, those are technically still just regression behind the scenes.\nStatistics is all about lines, and lines have slopes, or derivatives. These slopes represent the marginal changes in an outcome. As you move an independent/explanatory variable \\(x\\), what happens to the dependent/outcome variable \\(y\\)?\nRegression, sliders, switches, and mixing boards\nBefore getting into the mechanics of statistical marginal effects, it’s helpful to review what exactly regression coefficients are doing in statistical models, especially when dealing with both continuous and categorical explanatory variables.\nWhen I teach statistics to my students, my favorite analogy for regression is to think of sliders and switches. Sliders represent continuous variables: as you move them up and down, something gradual happens to the resulting light. Switches represent categorical variables: as you turn them on and off, there are larger overall changes to the resulting light.\n\n\n\n\n\n\n\n\nLet’s look at some super tiny quick models to illustrate this, using data from palmerpenguins:\n\npenguins &lt;- penguins |&gt; drop_na()\n\nmodel_slider &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\ntidy(model_slider)\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic   p.value\n##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)        -5872.     310.       -18.9 1.18e- 54\n## 2 flipper_length_mm     50.2      1.54      32.6 3.13e-105\n\nmodel_switch &lt;- lm(body_mass_g ~ species, data = penguins)\ntidy(model_switch)\n## # A tibble: 3 × 5\n##   term             estimate std.error statistic   p.value\n##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)        3706.       38.1    97.2   6.88e-245\n## 2 speciesChinstrap     26.9      67.7     0.398 6.91e-  1\n## 3 speciesGentoo      1386.       56.9    24.4   1.01e- 75\n\nDisregard the intercept for now and just look at the coefficients for flipper_length_mm and species*. Flipper length is a continuous variable, so it’s a slider—as flipper length increases by 1 mm, penguin body mass increases by 50 grams. Slide it up more and you’ll see a bigger increase: if flipper length increases by 10 mm, body mass should increase by 500 grams. Slide it down for fun too! If flipper length decreases by 1 mm, body mass decreases by 50 grams. Imagine it like a sliding light switch.\nSpecies, on the other hand, is a switch. There are three possible values here: Adelie, Chinstrap, and Gentoo. The base case in the results here is Adelie since it comes fist alphabetically. The coefficients for speciesChinstrap and speciesGentoo aren’t sliders—you can’t talk about one-unit increases in Gentoo-ness or Chinstrap-ness. Instead, the values show what happens in relation to the average weight of Adelie penguins if you flip the Chinstrap or Gentoo switch. Chinstrap penguins are 29 grams heavier than Adelie penguins on average, while the chonky Gentoo penguins are 1.4 kg heavier than Adellie penguins. With these categorical coefficients, we’re flipping a switch on and off: Adelie vs. Chinstrap and Adelie vs. Gentoo.\nThis slider and switch analogy holds when thinking about multiple regression too, though we need to think of lots of sliders and switches, like in an audio mixer board:\n\n\n\n\n\n\n\n\nWith a mixer board, we can move many different sliders up and down and use different combinations of switches, all of which ultimately influence the audio output.\nLet’s make a more complex mixer-board-esque regression model with multiple continuous (slider) and categorical (switch) explanatory variables:\n\nmodel_mixer &lt;- lm(body_mass_g ~ flipper_length_mm + bill_depth_mm + species + sex,\n                  data = penguins)\ntidy(model_mixer)\n## # A tibble: 6 × 5\n##   term              estimate std.error statistic  p.value\n##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)        -1212.     568.       -2.13 3.36e- 2\n## 2 flipper_length_mm     17.5      2.87      6.12 2.66e- 9\n## 3 bill_depth_mm         74.4     19.7       3.77 1.91e- 4\n## 4 speciesChinstrap     -78.9     45.5      -1.73 8.38e- 2\n## 5 speciesGentoo       1154.     119.        9.73 8.02e-20\n## 6 sexmale              435.      44.8       9.72 8.79e-20\n\nInterpreting these coefficients is a little different now, since we’re working with multiple moving parts. In regular stats class, you’ve probably learned to say something like “Holding all other variables constant, a 1 mm increase in flipper length is associated with a 17.5 gram increase in body mass, on average” (slider) or “Holding all other variables constant, Chinstrap penguins are 79 grams lighter than Adelie penguins, on average” (switch).\nThis idea of “holding everything constant” though can be tricky to wrap your head around. Imagining this model like a mixer board can help, though. Pretend that you set the bill depth slider to some value (0, the average, whatever), you flip the Chinstrap and Gentoo switches off, you flip the male switch off, and then you slide only the flipper length switch up and down. You’d be looking at the marginal effect of flipper length for female Adelie penguins with an average (or 0 or whatever) length of bill depth. Stop moving the flipper length slider and start moving the bill depth slider and you’ll see the marginal effect of bill depth for female Adelie penguins. Flip on the male switch and you’ll see the marginal effect of bill depth for male Adelie penguins. Flip on the Gentoo switch and you’ll see the marginal effect of bill depth for male Gentoo penguins. And so on.\nIn calculus, if you have a model like model_slider with just one continuous variable, the slope or derivative of that variable is the total derivative, or \\(\\frac{dy}{dx}\\). If you have a model like model_mixer with lots of other variables, the slope or derivative of any of the individual explanatory variables is the partial derivative, or \\(\\frac{\\partial y}{\\partial x}\\), where all other variables are held constant."
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#what-are-marginal-effects",
    "href": "blog/2022/05/20/marginalia/index.html#what-are-marginal-effects",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "What are marginal effects?",
    "text": "What are marginal effects?\nOops. When talking about these penguin regression results up there ↑ I used the term “marginal effect,” but we haven’t officially defined it in the statistics world yet. It’s tricky to do that, though, because there are so many synonyms and near synonyms for the idea of a statistical effect, like marginal effect, marginal mean, marginal slope, conditional effect, conditional marginal effect, and so on.\nFormally defined, a marginal effect is a partial derivative from a regression equation. It’s the instantaneous slope of one of the explanatory variables in a model, with all the other variables held constant. If we continue with the mixing board analogy, it represents what would happen to the resulting audio levels if we set all sliders and switches to some stationary level and we moved just one slider up a tiny amount.\nHowever, in practice, people use the term “marginal effect” to mean a lot more than just a partial derivative. For instance, in a randomized controlled trial, the difference in group means between the treatment and control groups is often called a marginal effect (and sometimes called a conditional effect, or even a conditional marginal effect). The term is also often used to talk about other group differences, like differences in penguin weights across species.\nIn my mind, all these quasi-synonymous terms represent the same idea of a statistical effect, or what would happen to an outcome \\(y\\) if one of the explanatory variables \\(x\\) (be it continuous, categorical, or whatever) were different. The more precise terms like marginal effect, conditional effect, marginal mean, and so on, are variations on this theme. This is similar to how a square is a rectangle, but a rectangle is not a square—they’re all super similar, but with minor subtle differences depending on the type of \\(x\\) we’re working with:\n\n\nMarginal effect: the statistical effect for continuous explanatory variables; the partial derivative of a variable in a regression model; the effect of a single slider\n\nConditional effect or group contrast: the statistical effect for categorical explanatory variables; the difference in means when a condition is on vs. when it is off; the effect of a single switch"
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#slopes-and-marginal-effects",
    "href": "blog/2022/05/20/marginalia/index.html#slopes-and-marginal-effects",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "Slopes and marginal effects",
    "text": "Slopes and marginal effects\nLet’s look at true marginal effects, or the partial derivatives of continuous variables in a model (or sliders, in our slider/switch analogy). For the rest of this post, we’ll move away from penguins and instead look at some cross-national data about the relationship between public sector corruption, the legal requirement to disclose donations to political campaigns, and respect for human rights, since that’s all more related to what I do in my own research (I know nothing about penguins). We’ll explore two different political science/policy questions:\n\nWhat is the relationship between a country’s respect for civil liberties and its level of public sector corruption? Do countries that respect individual human rights tend to have less corruption too?\nDoes a country’s level of public sector corruption influence whether it has laws that require campaign finance disclosure? How does corruption influence a country’s choice to be electorally transparent?\n\nWe’ll use data from the World Bank and from the Varieties of Democracy project and just look at one year of data (2020) so we don’t have to worry about panel data. There’s a great R package for accessing V-Dem data without needing to download it manually from their website, but it’s not on CRAN—it has to be installed from GitHub.\nV-Dem and the World Bank have hundreds of different variables, but we only need a few, and we’ll make a few adjustments to the ones we do need. Here’s what we’ll do:\n\n\nMain continuous outcome and continuous explanatory variable: Public sector corruption index (v2x_pubcorr in V-Dem). This is a 0–1 scale that measures…\n\nTo what extent do public sector employees grant favors in exchange for bribes, kickbacks, or other material inducements, and how often do they steal, embezzle, or misappropriate public funds or other state resources for personal or family use?\n\nHigher values represent worse corruption.\n\n\nMain binary outcome: Disclosure of campaign donations (v2eldonate_ord in V-Dem). This is an ordinal variable with these possible values:\n\n\n0: No. There are no disclosure requirements.\n1: Not really. There are some, possibly partial, disclosure requirements in place but they are not observed or enforced most of the time.\n2: Ambiguous. There are disclosure requirements in place, but it is unclear to what extent they are observed or enforced.\n3: Mostly. The disclosure requirements may not be fully comprehensive (some donations not covered), but most existing arrangements are observed and enforced.\n4: Yes. There are comprehensive requirements and they are observed and enforced almost all the time.\n\n\nFor the sake of simplicity, we’ll collapse this into a binary variable. Countries have disclosure laws if they score a 3 or a 4; they don’t if they score a 0, 1, or 2.\n\n\nOther continuous explanatory variables:\n\nElectoral democracy index, or polyarchy (v2x_polyarchy in V-Dem): a continuous variable measured from 0–1 with higher values representing greater achievement of democratic ideals\nCivil liberties index (v2x_civlib in V-Dem): a continuous variable measured from 0–1 with higher values representing better respect for human rights and civil liberties\nLog GDP per capita (NY.GDP.PCAP.KD at the World Bank): GDP per capita in constant 2015 USD\n\n\n\nRegion: V-Dem provides multiple regional variables with varying specificity (19 different regions, 10 different regions, and 6 different regions). We’ll use the 6-region version (e_regionpol_6C) for simplicity here:\n\n\n1: Eastern Europe and Central Asia (including Mongolia)\n2: Latin America and the Caribbean\n3: The Middle East and North Africa (including Israel and Turkey, excluding Cyprus)\n4: Sub-Saharan Africa\n5: Western Europe and North America (including Cyprus, Australia and New Zealand)\n6: Asia and Pacific (excluding Australia and New Zealand)\n\n\n\n\n\n# Get data from the World Bank's API\nwdi_raw &lt;- WDI(country = \"all\", \n               indicator = c(population = \"SP.POP.TOTL\",\n                             gdp_percapita = \"NY.GDP.PCAP.KD\"), \n               start = 2000, end = 2020, extra = TRUE)\n\n# Clean up the World Bank data\nwdi_2020 &lt;- wdi_raw |&gt; \n  filter(region != \"Aggregates\") |&gt; \n  filter(year == 2020) |&gt; \n  mutate(log_gdp_percapita = log(gdp_percapita)) |&gt; \n  select(-region, -status, -year, -country, -lastupdated, -lending)\n\n# Get data from V-Dem and clean it up\nvdem_2020 &lt;- vdem %&gt;% \n  select(country_name, country_text_id, year, region = e_regionpol_6C,\n         disclose_donations_ord = v2eldonate_ord, \n         public_sector_corruption = v2x_pubcorr,\n         polyarchy = v2x_polyarchy, civil_liberties = v2x_civlib) %&gt;% \n  filter(year == 2020) %&gt;% \n  mutate(disclose_donations = disclose_donations_ord &gt;= 3,\n         disclose_donations = ifelse(is.na(disclose_donations), FALSE, disclose_donations)) %&gt;% \n  # Scale these up so it's easier to talk about 1-unit changes\n  mutate(across(c(public_sector_corruption, polyarchy, civil_liberties), ~ . * 100)) |&gt; \n  mutate(region = factor(region, \n                         labels = c(\"Eastern Europe and Central Asia\",\n                                    \"Latin America and the Caribbean\",\n                                    \"Middle East and North Africa\",\n                                    \"Sub-Saharan Africa\",\n                                    \"Western Europe and North America\",\n                                    \"Asia and Pacific\")))\n\n# Combine World Bank and V-Dem data into a single dataset\ncorruption &lt;- vdem_2020 |&gt; \n  left_join(wdi_2020, by = c(\"country_text_id\" = \"iso3c\")) |&gt; \n  drop_na(gdp_percapita)\n\nglimpse(corruption)\n\n\n\n\n\n\n\nStatic data!\n\n\n\nWhen I wrote this in May 2022, I based it on the data that both the World Bank and V-Dem had available at that time. In the months since then, they’ve revised their data. If you run all this code now (I’m writing this in February 2024), you’ll get slightly different results because of these revisions. For instance, when this ran in 2022, the World Bank reported that Mexico’s log GDP per capita in 2020 was 9.10; when running this in 2024, it reports that it was 9.13. This is a minor difference, but all these minor differences change the results slightly. The code in this chunk above still runs just fine, you’ll just get slightly different data, and thus slightly different results.\nSo, in the interest of reproducibility, you should download and load this .RData file that contains wdi_raw, wdi_2020, vdem_2020, and corruption from when I wrote this in 2022.\n\ndata_2022.RData\n\n\n\n\nload(\"data_2022.RData\")\nglimpse(corruption)\n## Rows: 168\n## Columns: 17\n## $ country_name             &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\",…\n## $ country_text_id          &lt;chr&gt; \"MEX\", \"SUR\", \"SWE\", \"CHE\", \"GHA\", \"ZAF\", \"JPN\", \"MMR\",…\n## $ year                     &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2…\n## $ region                   &lt;fct&gt; Latin America and the Caribbean, Latin America and the …\n## $ disclose_donations_ord   &lt;dbl&gt; 3, 1, 2, 0, 2, 1, 3, 2, 3, 2, 2, 0, 3, 3, 4, 3, 4, 2, 1…\n## $ public_sector_corruption &lt;dbl&gt; 48.8, 24.8, 1.3, 1.4, 65.2, 57.1, 3.7, 36.8, 70.6, 71.2…\n## $ polyarchy                &lt;dbl&gt; 64.7, 76.1, 90.8, 89.4, 72.0, 70.3, 83.2, 43.6, 26.2, 4…\n## $ civil_liberties          &lt;dbl&gt; 71.2, 87.7, 96.9, 94.8, 90.4, 82.2, 92.8, 56.9, 43.0, 8…\n## $ disclose_donations       &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, T…\n## $ iso2c                    &lt;chr&gt; \"MX\", \"SR\", \"SE\", \"CH\", \"GH\", \"ZA\", \"JP\", \"MM\", \"RU\", \"…\n## $ population               &lt;dbl&gt; 1.29e+08, 5.87e+05, 1.04e+07, 8.64e+06, 3.11e+07, 5.93e…\n## $ gdp_percapita            &lt;dbl&gt; 8923, 7530, 51542, 85685, 2021, 5659, 34556, 1587, 9711…\n## $ capital                  &lt;chr&gt; \"Mexico City\", \"Paramaribo\", \"Stockholm\", \"Bern\", \"Accr…\n## $ longitude                &lt;chr&gt; \"-99.1276\", \"-55.1679\", \"18.0645\", \"7.44821\", \"-0.20795…\n## $ latitude                 &lt;chr&gt; \"19.427\", \"5.8232\", \"59.3327\", \"46.948\", \"5.57045\", \"-2…\n## $ income                   &lt;chr&gt; \"Upper middle income\", \"Upper middle income\", \"High inc…\n## $ log_gdp_percapita        &lt;dbl&gt; 9.10, 8.93, 10.85, 11.36, 7.61, 8.64, 10.45, 7.37, 9.18…\n\nLet’s start off by looking at the effect of civil liberties on public sector corruption by using a really simple model with one explanatory variable:\n\nplot_corruption &lt;- corruption |&gt; \n  mutate(highlight = civil_liberties == min(civil_liberties) | \n           civil_liberties == max(civil_liberties))\n\nggplot(plot_corruption, aes(x = civil_liberties, y = public_sector_corruption)) +\n  geom_point(aes(color = highlight)) +\n  stat_smooth(method = \"lm\", formula = y ~ x, linewidth = 1, color = clrs[1]) +\n  geom_label_repel(data = filter(plot_corruption, highlight == TRUE), \n                   aes(label = country_name), seed = 1234) +\n  scale_color_manual(values = c(\"grey30\", clrs[3]), guide = \"none\") +\n  labs(x = \"Civil liberties index\", y = \"Public sector corruption index\") +\n  theme_mfx()\n\n\n\n\n\n\n\n\nmodel_simple &lt;- lm(public_sector_corruption ~ civil_liberties,\n                   data = corruption)\ntidy(model_simple)\n## # A tibble: 2 × 5\n##   term            estimate std.error statistic  p.value\n##   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)      102.       5.72        17.8 2.21e-40\n## 2 civil_liberties   -0.805    0.0779     -10.3 1.27e-19\n\nWe have a nice fitted OLS line here with uncertainty around it. What’s the marginal effect of civil liberties on public sector corruption? What kind of calculus and math do we need to do to find it? Not much, happily!\nIn general, we have a regression formula here that looks a lot like the \\(y = mx + b\\) stuff we were using before, only now the intercept \\(b\\) is \\(\\beta_0\\) and the slope \\(m\\) is \\(\\beta_1\\). If we use the power rule to find the first derivative of this equation, we’ll see that the slope of the entire line is \\(\\beta_1\\):\n\\[\n\\begin{aligned}\n\\operatorname{E}[y \\mid x] &= \\beta_0 + \\beta_1 x \\\\[4pt]\n\\frac{\\partial \\operatorname{E}[y \\mid x]}{\\partial x} &= \\beta_1\n\\end{aligned}\n\\]\nIf we add actual coefficients from the model into the formula we can see that the \\(\\beta_1\\) coefficient for civil_liberties (−0.80) is indeed the marginal effect:\n\\[\n\\begin{aligned}\n\\operatorname{E}[\\text{Corruption} \\mid \\text{Civil liberties}] &= 101.89 + (−0.80 \\times \\text{Civil liberties}) \\\\[6pt]\n\\frac{\\partial \\operatorname{E}[\\text{Corruption} \\mid \\text{Civil liberties}]}{\\partial\\ \\text{Civil liberties}} &= −0.80\n\\end{aligned}\n\\]\nThe \\(\\beta_1\\) coefficient by itself is thus enough to tell us what the effect of moving civil liberties around is—it is the marginal effect of civil liberties on public sector corruption. Slide the civil liberties index up by 1 point and public sector corruption will be −0.80 points lower, on average.\nImportantly, this is only the case because we’re using simple linear regression without any curvy parts. If your model is completely linear without any polynomials or logs or interaction terms or doesn’t use curvy regression families like logistic or beta regression, you can use individual coefficients as marginal effects.\nLet’s see what happens when we add curves. We’ll add a polynomial term, including both civil_liberties and civil_liberties^2 so that we can capture the parabolic shape of the relationship between civil liberties and corruption:\n\nggplot(plot_corruption, aes(x = civil_liberties, y = public_sector_corruption)) +\n  geom_point(aes(color = highlight)) +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2), linewidth = 1, color = clrs[2]) +\n  geom_label_repel(data = filter(plot_corruption, highlight == TRUE), \n                   aes(label = country_name), seed = 1234) +\n  scale_color_manual(values = c(\"grey30\", clrs[3]), guide = \"none\") +\n  labs(x = \"Civil liberties index\", y = \"Public sector corruption index\") +\n  theme_mfx()\n\n\n\n\n\n\n\nThis is most likely not a great model fit in real life, but using the quadratic term here makes a neat curved line, so we’ll go with it for the sake of the example. But don’t, like, make any policy decisions based on this line.\nWhen working with polynomials in regression, the coefficients appear and work a little differently:\n\nmodel_sq &lt;- lm(public_sector_corruption ~ civil_liberties + I(civil_liberties^2),\n               data = corruption)\ntidy(model_sq)\n## # A tibble: 3 × 5\n##   term                 estimate std.error statistic      p.value\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n## 1 (Intercept)           41.9     11.6          3.60 0.000427    \n## 2 civil_liberties        1.58     0.419        3.77 0.000230    \n## 3 I(civil_liberties^2)  -0.0197   0.00341     -5.77 0.0000000382\n\nWe now have two coefficients for civil liberties: \\(\\beta_1\\) and \\(\\beta_2\\). Importantly, we cannot use just one of these to talk about the marginal effect of changing civil liberties. A one-point increase in the civil liberties index is not associated with a 1.58 increase or a −0.02 decrease in corruption. The slope of the fitted line now comprises multiple moving parts: (1) the coefficient for the non-squared term, (2) the coefficient for the squared term, and (3) some value of civil liberties, since the slope isn’t the same across the whole line. The math shows us why and how.\nWe have terms for both \\(x\\) and \\(x^2\\) in our model. To find the derivative, we can use the power rule to get rid of the \\(x\\) term (\\(\\beta x^1 \\rightarrow (1 \\times \\beta x^0) \\rightarrow \\beta\\)), but the \\(x\\) in the \\(x^2\\) term doesn’t disappear (\\(\\beta x^2 \\rightarrow (2 \\times \\beta \\times x^1) \\rightarrow 2 \\beta x\\)). The slope of the line thus depends on both the βs and the \\(x\\):\n\\[\n\\begin{aligned}\n\\operatorname{E}[y \\mid x] &= \\beta_0 + \\beta_1 x + \\beta_2 x^2 \\\\[4pt]\n\\frac{\\partial \\operatorname{E}[y \\mid x]}{\\partial x} &= \\beta_1 + 2 \\beta_2 x\n\\end{aligned}\n\\]\nHere’s what that looks like with the results of our civil liberties and corruption model:\n\\[\n\\begin{aligned}\n\\operatorname{E}[\\text{Corruption} \\mid \\text{Civil liberties}] &= 41.86 + (1.58 \\times \\text{Civil liberties}) + (−0.02 \\times \\text{Civil liberties}^2) \\\\[6pt]\n\\frac{\\partial \\operatorname{E}[\\text{Corruption} \\mid \\text{Civil liberties}]}{\\partial\\ \\text{Civil liberties}} &= 1.58 + (2\\times −0.02 \\times \\text{Civil liberties})\n\\end{aligned}\n\\]\nBecause the actual slope depends on the value of civil liberties, we need to plug in different values to get the instantaneous slopes at each value. Let’s plug in 25, 55, and 80, for fun:\n\n# Extract the two civil_liberties coefficients\nciv_lib1 &lt;- tidy(model_sq) |&gt; filter(term == \"civil_liberties\") |&gt; pull(estimate)\nciv_lib2 &lt;- tidy(model_sq) |&gt; filter(term == \"I(civil_liberties^2)\") |&gt; pull(estimate)\n\n# Make a little function to do the math\nciv_lib_slope &lt;- function(x) civ_lib1 + (2 * civ_lib2 * x)\n\nciv_lib_slope(c(25, 55, 80))\n## [1]  0.594 -0.587 -1.572\n\nWe have three different slopes now: 0.59, −0.59, and −1.57 for civil liberties of 25, 55, and 80, respectively. We can plot these as tangent lines:\n\ntangents &lt;- model_sq |&gt; \n  augment(newdata = tibble(civil_liberties = c(25, 55, 80))) |&gt; \n  mutate(slope = civ_lib_slope(civil_liberties),\n         intercept = find_intercept(civil_liberties, .fitted, slope)) |&gt; \n  mutate(nice_label = glue(\"Civil liberties: {civil_liberties}&lt;br&gt;\",\n                           \"Fitted corruption: {nice_number(.fitted)}&lt;br&gt;\",\n                           \"Slope: **{nice_number(slope)}**\"))\n\nggplot(corruption, aes(x = civil_liberties, y = public_sector_corruption)) +\n  geom_point(color = \"grey30\") +\n  stat_smooth(method = \"lm\", formula = y ~ x + I(x^2), linewidth = 1, se = FALSE, color = clrs[4]) +\n  geom_abline(data = tangents, aes(slope = slope, intercept = intercept), \n              linewidth = 0.5, color = clrs[2], linetype = \"21\") +\n  geom_point(data = tangents, aes(x = civil_liberties, y = .fitted), size = 4, shape = 18, color = clrs[2]) +\n  geom_richtext(data = tangents, aes(x = civil_liberties, y = .fitted, label = nice_label), nudge_y = -7) +\n  labs(x = \"Civil liberties index\", y = \"Public sector corruption index\") +\n  theme_mfx()\n\n\n\n\n\n\n\nDoing the calculus by hand here is tedious though, especially once we start working with even more covariates in a model. Plus we don’t have any information about uncertainty, like standard errors and confidence intervals. There are official mathy ways to figure those out by hand, but who even wants to do that. Fortunately there are two different packages that let us find marginal slopes automatically, with important differences in their procedures, which we’ll explore in detail below. But before looking at their differences, let’s first see how they work.\nFirst, we can use the slopes() function from marginaleffects to see the slope (the estimate column here) at various levels of civil liberties. We’ll look at the mechanics of this function in more detail in the next section—for now we’ll just plug in our three values of civil liberties and see what happens. We’ll also set the eps argument: behind the scenes, slopes() doesn’t actually do the by-hand calculus of piecing together first derivatives—instead, it calculates the fitted value of corruption when civil liberties is a value, calculates the fitted value of corruption when civil liberties is that same value plus a tiny bit more, and then subtracts them. The eps value controls that tiny amount. In this case, it’ll calculate the predictions for civil_liberties = 25 and civil_liberties = 25.001 and then find the slope of the tiny tangent line between those two points. It’s a neat little mathy trick to avoid calculus.\n\nmodel_sq |&gt; \n  slopes(newdata = datagrid(civil_liberties = c(25, 55, 80)),\n         eps = 0.001)\n## \n##             Term Estimate Std. Error      z Pr(&gt;|z|)   2.5 % 97.5 % civil_liberties\n##  civil_liberties    0.594     0.2528   2.35   0.0187  0.0989  1.090              25\n##  civil_liberties   -0.587     0.0806  -7.28   &lt;0.001 -0.7452 -0.429              55\n##  civil_liberties   -1.572     0.1509 -10.42   &lt;0.001 -1.8676 -1.276              80\n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, public_sector_corruption, civil_liberties\n\nSecond, we can use the emtrends() function from emmeans to also see the slope (the civil_liberties.trend column here) at various levels of civil liberties. The syntax is different (note the delta.var argument instead of eps), but the results are essentially the same:\n\nmodel_sq |&gt; \n  emtrends(~ civil_liberties, var = \"civil_liberties\",\n           at = list(civil_liberties = c(25, 55, 80)),\n           delta.var = 0.001)\n##  civil_liberties civil_liberties.trend     SE  df lower.CL upper.CL\n##               25                 0.594 0.2527 165    0.095    1.093\n##               55                -0.587 0.0806 165   -0.746   -0.428\n##               80                -1.572 0.1509 165   -1.870   -1.274\n## \n## Confidence level used: 0.95\n\nBoth slopes() and emtrends() also helpfully provide uncertainty, with standard errors and confidence intervals, with a lot of super fancy math behind the scenes to make it all work. slopes() provides p-values automatically; if you want p-values from emtrends() you need to wrap it in test():\n\nmodel_sq |&gt; \n  emtrends(~ civil_liberties, var = \"civil_liberties\",\n           at = list(civil_liberties = c(25, 55, 80)),\n           delta.var = 0.001) |&gt; \n  test()\n##  civil_liberties civil_liberties.trend     SE  df t.ratio p.value\n##               25                 0.594 0.2527 165   2.350  0.0198\n##               55                -0.587 0.0806 165  -7.280  &lt;.0001\n##               80                -1.572 0.1509 165 -10.420  &lt;.0001\n\nAnother neat thing about these more automatic functions is that we can use them to create a marginal effects plot, placing the value of the slope on the y-axis rather than the fitted value of public corruption. marginaleffects helpfully has plot_slopes() that will plot the values of estimate across the whole range of civil liberties automatically. Alternatively, if we want full control over the plot, we can use either slopes() or emtrends() to create a data frame that we can plot ourselves with ggplot:\n\n# Automatic plot from marginaleffects::plot_slopes()\nmfx_marginaleffects_auto &lt;- plot_slopes(model_sq, \n                                        variables = \"civil_liberties\", \n                                        condition = \"civil_liberties\") +\n  labs(x = \"Civil liberties\", y = \"Marginal effect of civil liberties on public sector corruption\",\n       subtitle = \"Created automatically with marginaleffects::plot_slopes()\") +\n  theme_mfx()\n\n# Piece all the geoms together manually with results from marginaleffects::slopes()\nmfx_marginaleffects &lt;- model_sq |&gt; \n  slopes(newdata = datagrid(civil_liberties = \n                              seq(min(corruption$civil_liberties), \n                                 max(corruption$civil_liberties), 0.1)),\n                  eps = 0.001) |&gt; \n  ggplot(aes(x = civil_liberties, y = estimate)) +\n  geom_vline(xintercept = 42, color = clrs[3], linewidth = 0.5, linetype = \"24\") +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.1, fill = clrs[1]) +\n  geom_line(linewidth = 1, color = clrs[1]) +\n  labs(x = \"Civil liberties\", y = \"Marginal effect of civil liberties on public sector corruption\",\n       subtitle = \"Calculated with slopes()\") +\n  theme_mfx()\n\n# Piece all the geoms together manually with results from emmeans::emtrends()\nmfx_emtrends &lt;- model_sq |&gt; \n  emtrends(~ civil_liberties, var = \"civil_liberties\",\n           at = list(civil_liberties = \n                       seq(min(corruption$civil_liberties), \n                           max(corruption$civil_liberties), 0.1)),\n           delta.var = 0.001) |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = civil_liberties, y = civil_liberties.trend)) +\n  geom_vline(xintercept = 42, color = clrs[3], linewidth = 0.5, linetype = \"24\") +\n  geom_ribbon(aes(ymin = lower.CL, ymax = upper.CL), alpha = 0.1, fill = clrs[1]) +\n  geom_line(linewidth = 1, color = clrs[1]) +\n  labs(x = \"Civil liberties\", y = \"Marginal effect of civil liberties on public sector corruption\",\n       subtitle = \"Calculated with emtrends()\") +\n  theme_mfx()\n\nmfx_marginaleffects_auto | mfx_marginaleffects | mfx_emtrends\n\n\n\n\n\n\n\nThis kind of plot is useful since it shows precisely how the effect changes across civil liberties. The slope is 0 at around 42, positive before that, and negative after that, which—assuming this is a good model and who even knows if that’s true—implies that countries with low levels of respect for civil liberties will see an increase in corruption as civil liberties increases, while countries with high respect for civil liberties will see a decrease in corruption as they improve their respect for human rights."
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#marginaleffectss-and-emmeanss-philosophies-of-averaging",
    "href": "blog/2022/05/20/marginalia/index.html#marginaleffectss-and-emmeanss-philosophies-of-averaging",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "\nmarginaleffects’s and emmeans’s philosophies of averaging",
    "text": "marginaleffects’s and emmeans’s philosophies of averaging\nFinding marginal effects for lines like \\(y = 2x - 1\\) and \\(y = -0.5x^2 + 5x + 5\\) with calculus is fairly easy since there’s no uncertainty involved. Finding marginal effects for fitted lines from a regression model, on the other hand, is more complicated because uncertainty abounds. The estimated partial slopes all have standard errors and measures of statistical significance attached to them. The slope of civil liberties at 55 is −0.59, but it could be higher and it could be lower. Could it even possibly be zero? Maybe! (But most likely not; the p-value that we saw above is less than 0.001, so there’s only a sliver of a chance of seeing a slope like −0.59 in a world where it is actually 0ish).\nWe deal with the uncertainty of these marginal effects by taking averages, which is why we talk about “average marginal effects” when interpreting these effects. So far, marginaleffects::slopes() and emmeans::emtrends() have given identical results. But behind the scenes, these packages take two different approaches to calculating these marginal averages. The difference is very subtle, but incredibly important.\nLet’s look at how these two packages calculate their marginal effects by default.\nAverage marginal effects (the default in marginaleffects)\nBy default, marginaleffects calculates the average marginal effect (AME) for its partial slopes/coefficients. To do this, it follows a specific process of averaging:\n\n\n\n\n\n\n\n\nIt first plugs each row of the original dataset into the model and generates predictions for each row. It then uses fancy math (i.e. adding 0.001) to calculate the instantaneous slope for each row and stores each individual slope in the estimate column here:\n\nmfx_sq &lt;- slopes(model_sq)\nhead(mfx_sq)\n## \n##             Term Estimate Std. Error      z Pr(&gt;|z|) 2.5 % 97.5 %\n##  civil_liberties    -1.23      0.102 -12.02   &lt;0.001 -1.43  -1.03\n##  civil_liberties    -1.88      0.199  -9.43   &lt;0.001 -2.26  -1.49\n##  civil_liberties    -2.24      0.258  -8.66   &lt;0.001 -2.74  -1.73\n##  civil_liberties    -2.16      0.245  -8.81   &lt;0.001 -2.63  -1.68\n##  civil_liberties    -1.98      0.216  -9.17   &lt;0.001 -2.41  -1.56\n##  civil_liberties    -1.66      0.164 -10.10   &lt;0.001 -1.98  -1.34\n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, public_sector_corruption, civil_liberties\n\nIt finally calculates the average of the estimate column. We can do that ourselves:\n\nmfx_sq |&gt; \n  group_by(term) |&gt; \n  summarize(avg_slope = mean(estimate))\n## # A tibble: 1 × 2\n##   term            avg_slope\n##   &lt;chr&gt;               &lt;dbl&gt;\n## 1 civil_liberties     -1.17\n\nOr we can feed a marginaleffects object to summary(), which will calculate the correct uncertainty statistics, like the standard errors:\n\nsummary(mfx_sq)\n## \n##             Term    Contrast Estimate Std. Error     z Pr(&gt;|z|) 2.5 % 97.5 %\n##  civil_liberties mean(dY/dX)    -1.17     0.0948 -12.3   &lt;0.001 -1.35  -0.98\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nAlternatively, we can use avg_slopes() instead of slopes(...) |&gt; summary() to do the averaging automatically:\n\navg_slopes(model_sq)\n## \n##             Term Estimate Std. Error     z Pr(&gt;|z|) 2.5 % 97.5 %\n##  civil_liberties    -1.17     0.0948 -12.3   &lt;0.001 -1.35  -0.98\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nNote that the average marginal effect here isn’t the same as what we saw before when we set civil liberties to different values. In this case, the effect is averaged across the whole range of civil liberties—one single grand average mean. It shows that in general, the overall average slope of the fitted line is −1.17.\nDon’t worry about the number too much here—we’re just exploring the underlying process of calculating this average marginal effect. In general, as the image shows above, for average marginal effects, we take the full original data, feed it to the model, generate fitted values for each original row, and then collapse the results into a single value.\nThe main advantage of doing this is that each estimate prediction uses values that exist in the actual data. The first estimate slope estimate is for Mexico in 2020 and is based on Mexico’s actual value of civil_liberties (and any other covariates if we had included any others in the model). It’s thus more reflective of reality.\nMarginal effects at the mean (the default in emmeans)\nA different approach for this averaging is to calculate the marginal effect at the mean, or MEM. This is what the emmeans package does by default. (The emmeans package actually calculates two average things: “marginal effects at the means” (MEM), or average slopes using emtrends(), and “estimated marginal means” (EMM), or average predictions using emmeans(). It’s named after the second of these, hence the name emmeans).\nTo do this, we follow a slightly different process of averaging:\n\n\n\n\n\n\n\n\nFirst, we calculate the average value of each of the covariates in the model (in this case, just civil_liberties):\n\navg_civ_lib &lt;- mean(corruption$civil_liberties)\navg_civ_lib\n## [1] 69.7\n\nWe then plug that average (and that average plus 0.001) into the model and generate fitted values:\n\nciv_lib_fitted &lt;- model_sq |&gt; \n  augment(newdata = tibble(civil_liberties = c(avg_civ_lib, avg_civ_lib + 0.001)))\nciv_lib_fitted\n## # A tibble: 2 × 2\n##   civil_liberties .fitted\n##             &lt;dbl&gt;   &lt;dbl&gt;\n## 1            69.7    56.3\n## 2            69.7    56.3\n\nBecause of rounding (and because the values are so tiny), this looks like the two rows are identical, but they’re not—the second one really is 0.001 more than 69.682.\nWe then subtract the two and divide by 0.001 to get the final marginal effect at the mean:\n\n(civ_lib_fitted[2,2] - civ_lib_fitted[1,2]) / 0.001\n##   .fitted\n## 1   -1.17\n\nThat doesn’t give us any standard errors or uncertainty or anything, so it’s better to use emtrends() or slopes(). emtrends() calculates this MEM automatically:\n\nmodel_sq |&gt; \n  emtrends(~ civil_liberties, var = \"civil_liberties\", delta.var = 0.001)\n##  civil_liberties civil_liberties.trend     SE  df lower.CL upper.CL\n##             69.7                 -1.17 0.0948 165    -1.35   -0.978\n## \n## Confidence level used: 0.95\n\nWe can also calculate the MEM with avg_slopes() if we include the newdata = \"mean\" argument, which will automatically shrink the original data down into average or typical values:\n\nmodel_sq |&gt; \n  avg_slopes(newdata = \"mean\")\n## \n##             Term Estimate Std. Error     z Pr(&gt;|z|) 2.5 % 97.5 %\n##  civil_liberties    -1.17     0.0948 -12.3   &lt;0.001 -1.35  -0.98\n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nThe disadvantage of this approach is that no actual country has a civil_liberties score of exactly 69.682. If we had other covariates in the model, no country would have exactly the average of every variable. The marginal effect is thus calculated based on a hypothetical country that might not possibly exist in real life."
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#where-this-subtle-difference-really-matters",
    "href": "blog/2022/05/20/marginalia/index.html#where-this-subtle-difference-really-matters",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "Where this subtle difference really matters",
    "text": "Where this subtle difference really matters\nSo far, comparing average marginal effects (AME) with marginal effects at the mean (MEM) hasn’t been that useful, since both slopes() and emtrends() provided nearly identical results with our simple model with civil liberties squared. That’s because nothing that strange is going on in the model—there are no additional explanatory variables, no interactions or logs, and we’re using OLS and not anything fancy like logistic regression or beta regression.\nThings change once we leave the land of OLS.\nLet’s make a new model that predicts if a country has campaign finance disclosure laws based on public sector corruption. Disclosure laws is a binary outcome, so we’ll use logistic regression to constrain the fitted values and predictions to between 0 and 1.\n\nplot_corruption_logit &lt;- corruption |&gt; \n  mutate(highlight = public_sector_corruption == min(public_sector_corruption) | \n           public_sector_corruption == max(public_sector_corruption))\n  \nggplot(plot_corruption_logit, \n       aes(x = public_sector_corruption, y = as.numeric(disclose_donations))) +\n  geom_point(aes(color = highlight)) +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial(link = \"logit\")),\n              color = clrs[2]) +\n  geom_label(data = slice(filter(plot_corruption_logit, highlight == TRUE), 1), \n             aes(label = country_name), nudge_y = 0.06, hjust = 1) +\n  geom_label(data = slice(filter(plot_corruption_logit, highlight == TRUE), 2), \n             aes(label = country_name), nudge_y = -0.06, hjust = 0) +\n  scale_color_manual(values = c(\"grey30\", clrs[3]), guide = \"none\") +\n  labs(x = \"Public sector corruption\", \n       y = \"Presence or absence of\\ncampaign finance disclosure laws\\n(Line shows predicted probability)\") +\n  theme_mfx()\n\n\n\n\n\n\n\nEven without any squared terms, we’re already in non-linear land. We can build a model and explore this relationship:\n\nmodel_logit &lt;- glm(\n  disclose_donations ~ public_sector_corruption,\n  family = binomial(link = \"logit\"),\n  data = corruption\n)\n\ntidy(model_logit)\n## # A tibble: 2 × 5\n##   term                     estimate std.error statistic  p.value\n##   &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)                1.98     0.388        5.09 3.51e- 7\n## 2 public_sector_corruption  -0.0678   0.00991     -6.84 7.85e-12\n\nThe coefficients here are on a different scale and are measured in log odds units (or logits), not probabilities or percentage points. That means we can’t use those coefficients directly. We can’t say things like “a one-unit increase in public sector corruption is associated with a −0.068 percentage point decrease in the probability of having a disclosure law.” That’s wrong! We have to convert those logit scale coefficients to a probability scale instead. We can do this mathematically by combining both the intercept and the coefficient using plogis(intercept + coefficient) - plogis(intercept), but that’s generally not recommended, especially when there are other coefficients (see this section on logistic regression for more details). Additionally, manually combining intercepts and coefficients won’t give us standard errors or any other kind of uncertainty.\nInstead, we can calculate the average slope of the logistic regression fit using either slopes() or emtrends().\nFirst we’ll use avg_slopes(). Remember that it calculates the average marginal effect (AME) by plugging each row of the original data into the model, generating predictions and instantaneous slopes for each row, and then averaging the estimate column. Each row contains actual observed data, so the predictions arguably reflect variation in reality. avg_slopes() helpfully converts the AME into percentage points, so we can interpret the value directly.\n\nmodel_logit |&gt; \n  avg_slopes()\n## \n##                      Term Estimate Std. Error     z Pr(&gt;|z|)    2.5 %   97.5 %\n##  public_sector_corruption -0.00846   0.000261 -32.4   &lt;0.001 -0.00897 -0.00795\n## \n## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nThe average marginal effect for public sector corruption is −0.0085, which means that on average, a one-point increase in the public sector corruption index (i.e. as corruption gets worse) is associated with a −0.85 percentage point decrease in the probability of a country having a disclosure law.\nNext we’ll use emtrends(), which calculates the marginal effect at the mean (MEM) by averaging all the model covariates first, plugging those averages into the model, and generating a single instantaneous slope. The values that get plugged into the model won’t necessarily reflect reality—especially once more covariates are involved, which we’ll see later. By default emtrends() returns the results on the logit scale, but we can convert them to the response/percentage point scale by adding the regrid = \"response\" argument:\n\nmodel_logit |&gt; \n  emtrends(~ public_sector_corruption, \n           var = \"public_sector_corruption\", \n           regrid = \"response\")\n##  public_sector_corruption public_sector_corruption.trend     SE  df asymp.LCL asymp.UCL\n##                      45.8                        -0.0125 0.0017 Inf   -0.0158  -0.00916\n## \n## Confidence level used: 0.95\n\n# avg_slopes() will show the same MEM result with `newdata = \"mean\"`\n# avg_slopes(model_logit, newdata = \"mean\")\n\nWhen we plug the average public sector corruption (45.82) into the model, we get an MEM of −0.0125, which means that on average, a one-point increase in the public sector corruption index is associated with a −1.25 percentage point decrease in the probability of a country having a disclosure law. That’s different (and bigger!) than the AME we found with slopes()!\nLet’s plot these marginal effects and their uncertainty to see how much they differ:\n\n# Get tidied results from slopes()\nplot_ame &lt;- model_logit |&gt; \n  avg_slopes()\n\n# Get tidied results from emtrends()\nplot_mem &lt;- model_logit |&gt; \n  emtrends(~ public_sector_corruption, \n           var = \"public_sector_corruption\", \n           regrid = \"response\") |&gt; \n  tidy(conf.int = TRUE) |&gt; \n  rename(estimate = public_sector_corruption.trend)\n\n# Combine the two tidy data frames for plotting\nplot_effects &lt;- bind_rows(\"AME\" = plot_ame, \"MEM\" = plot_mem, .id = \"type\") |&gt; \n  mutate(nice_slope = nice_number(estimate * 100))\n\nggplot(plot_effects, aes(x = estimate * 100, y = fct_rev(type), color = type)) +\n  geom_vline(xintercept = 0, linewidth = 0.5, linetype = \"24\", color = clrs[1]) +\n  geom_pointrange(aes(xmin = conf.low * 100, xmax = conf.high * 100)) +\n  geom_label(aes(label = nice_slope), nudge_y = 0.3) +\n  labs(x = \"Marginal effect (percentage points)\", y = NULL) +\n  scale_color_manual(values = c(clrs[2], clrs[5]), guide = \"none\") +\n  theme_mfx()\n\n\n\n\n\n\n\nThat’s fascinating! The confidence interval around the AME is really small compared to the MEM, likely because the AME estimate comes from the average of 168 values, while the MEM is the prediction of a single value. Additionally, while both estimates hover around a 1 percentage point decrease, the AME is larger than −1 while the MEM is smaller.\nFor fun, let’s make a super fancy logistic regression model with a quadratic term and an interaction. We’ll compare the AME and MEM for public sector corruption again. This is where either slopes() or emtrends() is incredibly helpful—correctly combining all the necessary coefficients, given that corruption is both squared and interacted, and given that there are other variables to worry about, would be really hard.\n\nmodel_logit_fancy &lt;- glm(\n  disclose_donations ~ public_sector_corruption + I(public_sector_corruption^2) + \n    polyarchy + log_gdp_percapita + public_sector_corruption * region,\n  family = binomial(link = \"logit\"),\n  data = corruption\n)\n\nHere are the average marginal effects (AME) (again, each original row is plugged into the model, a slope is calculated for each, and then they’re all averaged together):\n\nmodel_logit_fancy |&gt; \n  avg_slopes()\n## \n##                      Term                                                           Contrast Estimate Std. Error      z Pr(&gt;|z|)   2.5 %   97.5 %\n##  log_gdp_percapita        dY/dX                                                               0.00960    0.03784  0.254  0.79977 -0.0646  0.08377\n##  polyarchy                dY/dX                                                               0.00226    0.00172  1.318  0.18757 -0.0011  0.00563\n##  public_sector_corruption dY/dX                                                              -0.00653    0.00198 -3.303  &lt; 0.001 -0.0104 -0.00266\n##  region                   Asia and Pacific - Eastern Europe and Central Asia                 -0.20418    0.09156 -2.230  0.02575 -0.3836 -0.02473\n##  region                   Latin America and the Caribbean - Eastern Europe and Central Asia  -0.26174    0.10447 -2.505  0.01223 -0.4665 -0.05699\n##  region                   Middle East and North Africa - Eastern Europe and Central Asia     -0.20647    0.10545 -1.958  0.05023 -0.4132  0.00021\n##  region                   Sub-Saharan Africa - Eastern Europe and Central Asia               -0.24986    0.11238 -2.223  0.02619 -0.4701 -0.02960\n##  region                   Western Europe and North America - Eastern Europe and Central Asia -0.29109    0.09378 -3.104  0.00191 -0.4749 -0.10728\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nAnd here are the marginal effects at the mean (MEM) (again, the average values for each covariate are plugged into the model). Using emtrends() results in a note about interactions, so we’ll use avg_slopes(..., newdata = \"mean\") instead:\n\nmodel_logit_fancy |&gt; \n  emtrends(~ public_sector_corruption, \n           var = \"public_sector_corruption\", \n           regrid = \"response\")\n## NOTE: Results may be misleading due to involvement in interactions\n##  public_sector_corruption public_sector_corruption.trend      SE  df asymp.LCL asymp.UCL\n##                      45.8                       -0.00955 0.00301 Inf   -0.0155  -0.00366\n## \n## Results are averaged over the levels of: region \n## Confidence level used: 0.95\n\n# This uses avg_slopes() to find the MEM instead\nmodel_logit_fancy |&gt; \n  avg_slopes(newdata = \"mean\")\n## \n##                      Term                                                           Contrast Estimate Std. Error      z Pr(&gt;|z|)    2.5 %   97.5 %\n##  log_gdp_percapita        dY/dX                                                               0.00919    0.03944  0.233  0.81579 -0.06811  0.08649\n##  polyarchy                dY/dX                                                               0.00217    0.00222  0.976  0.32915 -0.00219  0.00652\n##  public_sector_corruption dY/dX                                                              -0.01004    0.00607 -1.654  0.09816 -0.02194  0.00186\n##  region                   Asia and Pacific - Eastern Europe and Central Asia                 -0.53122    0.19300 -2.752  0.00592 -0.90950 -0.15294\n##  region                   Latin America and the Caribbean - Eastern Europe and Central Asia  -0.37448    0.16569 -2.260  0.02381 -0.69921 -0.04974\n##  region                   Middle East and North Africa - Eastern Europe and Central Asia     -0.57907    0.20653 -2.804  0.00505 -0.98387 -0.17427\n##  region                   Sub-Saharan Africa - Eastern Europe and Central Asia               -0.56591    0.16579 -3.413  &lt; 0.001 -0.89086 -0.24097\n##  region                   Western Europe and North America - Eastern Europe and Central Asia -0.65700    0.16341 -4.021  &lt; 0.001 -0.97728 -0.33672\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nNow that we’re working with multiple covariates, we have instantaneous marginal effects for each regression term, which is neat. We only care about corruption here, so let’s extract the slopes and plot them:\n\nplot_ame_fancy &lt;- model_logit_fancy |&gt; \n  avg_slopes()\n\nplot_mem_fancy &lt;- model_logit_fancy |&gt; \n  avg_slopes(newdata = \"mean\")\n\n# Combine the two tidy data frames for plotting\nplot_effects &lt;- bind_rows(\"AME\" = plot_ame_fancy, \"MEM\" = plot_mem_fancy, .id = \"type\") |&gt; \n  filter(term == \"public_sector_corruption\") |&gt; \n  mutate(nice_slope = nice_number(estimate * 100))\n\nggplot(plot_effects, aes(x = estimate * 100, y = fct_rev(type), color = type)) +\n  geom_vline(xintercept = 0, linewidth = 0.5, linetype = \"24\", color = clrs[1]) +\n  geom_pointrange(aes(xmin = conf.low * 100, xmax = conf.high * 100)) +\n  geom_label(aes(label = nice_slope), nudge_y = 0.3) +\n  labs(x = \"Marginal effect (percentage points)\", y = NULL) +\n  scale_color_manual(values = c(clrs[2], clrs[5]), guide = \"none\") +\n  theme_mfx()\n\n\n\n\n\n\n\nYikes! The AME is statistically significant (p &lt; 0.001) with a narrower confidence interval, but the MEM includes zero in its confidence interval and isn’t significant (p = 0.098).\nThe choice of marginal effect averaging thus matters a lot!"
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#other-marginal-slope-things",
    "href": "blog/2022/05/20/marginalia/index.html#other-marginal-slope-things",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "Other marginal slope things",
    "text": "Other marginal slope things\nTo make life even more exciting, we’re not limited to just average marginal effects (AMEs) or marginal effects at the mean (MEMs). Additionally, if we think back to the slider/switch/mixing board analogy, all we’ve really done so far with our logistic regression model is move one slider (public_sector_corruption) up and down. What happens if we move other switches and sliders at the same time? (i.e. the marginal effect of corruption at specific values of corruption, or across different regions, or at different levels of GDP per capita and polyarchy)\nWe can use both slopes() and emtrends()/emmeans() to play with our model’s full mixing board. We’ll continue to use the logistic regression model as an example since it’s sensitive to the order of averaging.\nGroup average marginal effects\nIf we have categorical covariates in our model like region, we can find the average marginal effect (AME) of continuous predictors across those different groups. This is fairly straightforward when working with slopes() because of its approach to averaging. Remember that with the AME, each original row gets its own fitted value and each individual slope, which we can then average and collapse into a single row. Group characteristics like region are maintained after calculating predictions, so we can calculate group averages of the individual slopes. This outlines the process:\n\n\n\n\n\n\n\n\nBecause we’re working with the AME, we have an estimate column with instantaneous slopes for each row in the original data:\n\n# We'll specify variables = \"public_sector_corruption\" here to filter the\n# marginal effects results. If we don't we'll get dozens of separate marginal\n# effects later when using summary() for each of the coefficients, interactions,\n# and cross-region contrasts\nmfx_logit_fancy &lt;- model_logit_fancy |&gt; \n  slopes(variables = \"public_sector_corruption\")\n\n# Original data frame + estimated slope for each row\nhead(mfx_logit_fancy)\n## \n##                      Term  Estimate Std. Error      z Pr(&gt;|z|)    2.5 %  97.5 %\n##  public_sector_corruption -0.008825    0.00653 -1.352    0.176 -0.02162 0.00397\n##  public_sector_corruption -0.000897    0.00788 -0.114    0.909 -0.01634 0.01454\n##  public_sector_corruption -0.006759    0.00511 -1.324    0.186 -0.01677 0.00325\n##  public_sector_corruption -0.006727    0.00546 -1.231    0.218 -0.01744 0.00398\n##  public_sector_corruption -0.002460    0.00307 -0.802    0.423 -0.00847 0.00355\n##  public_sector_corruption -0.005864    0.00557 -1.052    0.293 -0.01679 0.00506\n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, disclose_donations, public_sector_corruption, polyarchy, log_gdp_percapita, region\n\nAll the original columns are still there, which means we can collapse the results however we want. For instance, here’s the average marginal effect across each region:\n\nmfx_logit_fancy |&gt; \n  group_by(region) |&gt; \n  summarize(region_ame = mean(estimate))\n## # A tibble: 6 × 2\n##   region                           region_ame\n##   &lt;fct&gt;                                 &lt;dbl&gt;\n## 1 Eastern Europe and Central Asia    -0.00751\n## 2 Latin America and the Caribbean    -0.00326\n## 3 Middle East and North Africa       -0.00629\n## 4 Sub-Saharan Africa                 -0.00435\n## 5 Western Europe and North America   -0.0112 \n## 6 Asia and Pacific                   -0.00830\n\nWe can also use summarizing methods built in to marginaleffects by using the by argument in slopes(). This is the better option, since it does some tricky standard error calculations behind the scenes:\n\nmodel_logit_fancy |&gt; \n  slopes(variables = \"public_sector_corruption\",\n         by = \"region\")\n## \n##                      Term    Contrast                           region Estimate Std. Error      z Pr(&gt;|z|)   2.5 %   97.5 %\n##  public_sector_corruption mean(dY/dX) Eastern Europe and Central Asia  -0.00751    0.00222 -3.376  &lt; 0.001 -0.0119 -0.00315\n##  public_sector_corruption mean(dY/dX) Latin America and the Caribbean  -0.00326    0.00395 -0.825  0.40957 -0.0110  0.00448\n##  public_sector_corruption mean(dY/dX) Middle East and North Africa     -0.00629    0.00193 -3.264  0.00110 -0.0101 -0.00251\n##  public_sector_corruption mean(dY/dX) Sub-Saharan Africa               -0.00435    0.00156 -2.788  0.00530 -0.0074 -0.00129\n##  public_sector_corruption mean(dY/dX) Western Europe and North America -0.01123    0.00911 -1.233  0.21749 -0.0291  0.00662\n##  public_sector_corruption mean(dY/dX) Asia and Pacific                 -0.00830    0.00285 -2.916  0.00354 -0.0139 -0.00272\n## \n## Columns: term, contrast, region, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nThese are on the percentage point scale, not the logit scale, so we can interpret them directly. In Western Europe, the AME of corruption is −0.0033, so a one-point increase in the public sector corruption index there is associated with a −0.33 percentage point decrease in the probability of having a campaign finance disclosure law, on average (though it’s not actually significant (p = 0.410)). In the Middle East, on the other hand, corruption seems to matter less for disclosure laws—an increase in the corruption index there is associated with a −0.83 percentage point decrease in the probability of having a laws, on average (and that is significant (p = 0.004)).\nWe can use emtrends() to get region-specific slopes, but we’ll get different results because of the order of averaging. emmeans creates averages and then plugs them in; marginaleffects plugs all the values in and then creates averages:\n\nmodel_logit_fancy |&gt; \n  emtrends(~ public_sector_corruption + region,\n           var = \"public_sector_corruption\", regrid = \"response\")\n##  public_sector_corruption region                           public_sector_corruption.trend      SE  df asymp.LCL asymp.UCL\n##                      45.8 Eastern Europe and Central Asia                        -0.01119 0.00554 Inf   -0.0221  -0.00033\n##                      45.8 Latin America and the Caribbean                        -0.00734 0.00611 Inf   -0.0193   0.00463\n##                      45.8 Middle East and North Africa                           -0.01172 0.01105 Inf   -0.0334   0.00993\n##                      45.8 Sub-Saharan Africa                                     -0.01001 0.00607 Inf   -0.0219   0.00188\n##                      45.8 Western Europe and North America                       -0.00335 0.00769 Inf   -0.0184   0.01172\n##                      45.8 Asia and Pacific                                       -0.01371 0.00667 Inf   -0.0268  -0.00063\n## \n## Confidence level used: 0.95\n\nWe can replicate the results from emtrends() with avg_slopes() if we plug in average or representative values (more on that in the next section), since that follows the same averaging order as emmeans (i.e. plugging averages into the model)\n\nmodel_logit_fancy |&gt; \n  avg_slopes(variables = \"public_sector_corruption\",\n             newdata = datagrid(region = levels(corruption$region)),\n             by = \"region\")\n## \n##                      Term    Contrast                           region Estimate Std. Error      z Pr(&gt;|z|)   2.5 %    97.5 %\n##  public_sector_corruption mean(dY/dX) Eastern Europe and Central Asia  -0.01117    0.00554 -2.016   0.0437 -0.0220 -0.000313\n##  public_sector_corruption mean(dY/dX) Latin America and the Caribbean  -0.00733    0.00611 -1.200   0.2301 -0.0193  0.004641\n##  public_sector_corruption mean(dY/dX) Middle East and North Africa     -0.01177    0.01106 -1.064   0.2873 -0.0334  0.009907\n##  public_sector_corruption mean(dY/dX) Sub-Saharan Africa               -0.01004    0.00607 -1.654   0.0982 -0.0219  0.001859\n##  public_sector_corruption mean(dY/dX) Western Europe and North America -0.00336    0.00771 -0.436   0.6627 -0.0185  0.011756\n##  public_sector_corruption mean(dY/dX) Asia and Pacific                 -0.01375    0.00667 -2.059   0.0395 -0.0268 -0.000664\n## \n## Columns: rowid, term, contrast, region, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nMarginal effects at user-specified or representative values\nIf we want to unlock the full potential of our regression mixing board, we can feed the model any values we want. In general, we’ll (1) make a little dataset with covariate values set to either specific values that we care about, or typical or average values, (2) plug that little dataset into the the model and get fitted values, and (3) work with the results. There are a bunch of different names for this little fake dataset like “data grid” and “reference grid”, but they’re all the same idea. Here’s an overview of the approach:\n\n\n\n\n\n\n\n\nCreating representative values\nBefore plugging anything in, it’s helpful to look at different ways of creating data grids with R. For all these examples, we’ll make a dataset with public sector corruption set to 20 and 80 across Western Europe, Latin America, and the Middle East, with all other variables in the model set to their means. We’ll make a little list of these regions to save typing time:\n\nregions_to_use &lt;- c(\"Western Europe and North America\", \n                    \"Latin America and the Caribbean\",\n                    \"Middle East and North Africa\")\n\nFirst, we can do it all manually with the expand_grid() function from tidyr (or expand.grid() from base R). This creates a data frame from all combinations of the vectors and single values we feed it.\n\nexpand_grid(public_sector_corruption = c(20, 80),\n            region = regions_to_use,\n            polyarchy = mean(corruption$polyarchy),\n            log_gdp_percapita = mean(corruption$log_gdp_percapita))\n## # A tibble: 6 × 4\n##   public_sector_corruption region                           polyarchy log_gdp_percapita\n##                      &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;             &lt;dbl&gt;\n## 1                       20 Western Europe and North America      52.8              8.57\n## 2                       20 Latin America and the Caribbean       52.8              8.57\n## 3                       20 Middle East and North Africa          52.8              8.57\n## 4                       80 Western Europe and North America      52.8              8.57\n## 5                       80 Latin America and the Caribbean       52.8              8.57\n## 6                       80 Middle East and North Africa          52.8              8.57\n\nA disadvantage of using expand_grid() like this is that the averages we calculated aren’t necessarily the same averages of the data that gets used in the model. If any rows are dropped in the model because of missing values, that won’t be reflected here. We could get around that by doing model.frame(model_logit_fancy)$polyarchy, but that’s starting to get unwieldy. Instead, we can use a function that takes information about the model into account.\nSecond, we can use data_grid() from modelr, which is part of the really neat tidymodels ecosystem. An advantage of doing this is that it will handle the typical value part automatically—it will calculate the mean for continuous predictors and the mode for categorical predictors.\n\nmodelr::data_grid(corruption,\n                  public_sector_corruption = c(20, 80),\n                  region = regions_to_use,\n                  .model = model_logit_fancy)\n## # A tibble: 6 × 4\n##   public_sector_corruption region                           polyarchy log_gdp_percapita\n##                      &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;             &lt;dbl&gt;\n## 1                       20 Latin America and the Caribbean       54.2              8.52\n## 2                       20 Middle East and North Africa          54.2              8.52\n## 3                       20 Western Europe and North America      54.2              8.52\n## 4                       80 Latin America and the Caribbean       54.2              8.52\n## 5                       80 Middle East and North Africa          54.2              8.52\n## 6                       80 Western Europe and North America      54.2              8.52\n\nThird, we can use marginaleffects’s datagrid(), which will also calculate typical values for any covariates we don’t specify:\n\ndatagrid(model = model_logit_fancy,\n         public_sector_corruption = c(20, 80),\n         region = regions_to_use)\n##   disclose_donations polyarchy log_gdp_percapita public_sector_corruption                           region\n## 1              FALSE      52.8              8.57                       20 Western Europe and North America\n## 2              FALSE      52.8              8.57                       20  Latin America and the Caribbean\n## 3              FALSE      52.8              8.57                       20     Middle East and North Africa\n## 4              FALSE      52.8              8.57                       80 Western Europe and North America\n## 5              FALSE      52.8              8.57                       80  Latin America and the Caribbean\n## 6              FALSE      52.8              8.57                       80     Middle East and North Africa\n\nAnd finally, we can use emmeans’s ref_grid(), which will also automatically create typical values. This doesn’t return a data frame—it’s some sort of special ref_grid object, but all the important information is still there:\n\nref_grid(model_logit_fancy,\n         at = list(public_sector_corruption = c(20, 80),\n                   region = regions_to_use))\n## 'emmGrid' object with variables:\n##     public_sector_corruption = 20, 80\n##     polyarchy = 52.79\n##     log_gdp_percapita = 8.5674\n##     region = Western Europe and North America, Latin America and the Caribbean, Middle East and North Africa\n## Transformation: \"logit\"\n\nWorking with representative values\nNow that we have a hypothetical data grid of sliders and switches set to specific values, we can plug it into the model and generate fitted values. Importantly, doing this provides us with results that are analogous to the marginal effects at the mean (MEM) that we found earlier, and not the average marginal effect (AME), since we’re not feeding the entire original dataset to the model. None of these hypothetical rows exist in real life—there is no country with any of these exact combinations of corruption, polyarchy/democracy, GDP per capita, or region.\n\nmodel_logit_fancy |&gt; \n  slopes(variables = \"public_sector_corruption\",\n         newdata = datagrid(public_sector_corruption = c(20, 80),\n                            region = regions_to_use))\n## \n##                      Term  Estimate Std. Error       z Pr(&gt;|z|)     2.5 %   97.5 % polyarchy log_gdp_percapita public_sector_corruption                           region\n##  public_sector_corruption -2.49e-02   1.56e-02 -1.5982    0.110 -0.055495 0.005643      52.8              8.57                       20 Western Europe and North America\n##  public_sector_corruption  8.27e-04   8.59e-03  0.0962    0.923 -0.016018 0.017672      52.8              8.57                       20 Latin America and the Caribbean \n##  public_sector_corruption -2.04e-02   1.43e-02 -1.4240    0.154 -0.048490 0.007680      52.8              8.57                       20 Middle East and North Africa    \n##  public_sector_corruption -1.49e-05   8.59e-05 -0.1734    0.862 -0.000183 0.000153      52.8              8.57                       80 Western Europe and North America\n##  public_sector_corruption -4.36e-03   3.53e-03 -1.2356    0.217 -0.011289 0.002559      52.8              8.57                       80 Latin America and the Caribbean \n##  public_sector_corruption -1.06e-04   3.83e-04 -0.2762    0.782 -0.000857 0.000646      52.8              8.57                       80 Middle East and North Africa    \n## \n## Columns: rowid, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, disclose_donations, polyarchy, log_gdp_percapita, public_sector_corruption, region\n\n\nmodel_logit_fancy |&gt; \n  emtrends(~ public_sector_corruption + region, var = \"public_sector_corruption\",\n           at = list(public_sector_corruption = c(20, 80),\n                     region = regions_to_use),\n           regrid = \"response\", delta.var = 0.001) \n##  public_sector_corruption region                           public_sector_corruption.trend      SE  df asymp.LCL asymp.UCL\n##                        20 Western Europe and North America                       -0.02493 0.01560 Inf   -0.0555   0.00565\n##                        80 Western Europe and North America                       -0.00001 0.00009 Inf   -0.0002   0.00015\n##                        20 Latin America and the Caribbean                         0.00083 0.00860 Inf   -0.0160   0.01768\n##                        80 Latin America and the Caribbean                        -0.00437 0.00353 Inf   -0.0113   0.00256\n##                        20 Middle East and North Africa                           -0.02040 0.01433 Inf   -0.0485   0.00768\n##                        80 Middle East and North Africa                           -0.00011 0.00038 Inf   -0.0009   0.00065\n## \n## Confidence level used: 0.95\n\nWe have a ton of marginal effects here, but this is all starting to get really complicated. These are slopes, but slopes for which lines? What do these marginal effects actually look like?\nPlotting these regression lines is tricky because we’re no longer working with a single variable on the x-axis. Instead, we need to generate predicted values of the regression outcome across a range of one \\(x\\) while holding all the other variables constant. This is exactly what we’ve been doing to get marginal effects, only now instead of getting slopes as the output, we want fitted values. Both marginaleffects and emmeans make this easy.\nIn the marginaleffects world, we can use predictions(). The estimate column now shows the fitted value instead of the slope:\n\nmodel_logit_fancy |&gt; \n  predictions(newdata = datagrid(public_sector_corruption = c(20, 80),\n                                 region = regions_to_use))\n## \n##  Estimate Pr(&gt;|z|)    2.5 % 97.5 % polyarchy log_gdp_percapita public_sector_corruption                           region\n##  3.81e-01   0.7001 4.93e-02  0.879      52.8              8.57                       20 Western Europe and North America\n##  3.97e-01   0.5855 1.29e-01  0.746      52.8              8.57                       20 Latin America and the Caribbean \n##  6.58e-01   0.5566 1.78e-01  0.945      52.8              8.57                       20 Middle East and North Africa    \n##  7.69e-05   0.1363 2.98e-10  0.952      52.8              8.57                       80 Western Europe and North America\n##  5.45e-02   0.0579 3.01e-03  0.524      52.8              8.57                       80 Latin America and the Caribbean \n##  5.93e-04   0.0708 1.87e-07  0.653      52.8              8.57                       80 Middle East and North Africa    \n## \n## Columns: rowid, estimate, p.value, conf.low, conf.high, disclose_donations, polyarchy, log_gdp_percapita, public_sector_corruption, region\n\nIn the emmeans world, we can use emmeans():\n\nmodel_logit_fancy |&gt; \n  emmeans(~ public_sector_corruption + region, var = \"public_sector_corruption\",\n          at = list(public_sector_corruption = c(20, 80),\n                    region = regions_to_use),\n          regrid = \"response\") \n##  public_sector_corruption region                            prob     SE  df asymp.LCL asymp.UCL\n##                        20 Western Europe and North America 0.381 0.2975 Inf   -0.2021     0.964\n##                        80 Western Europe and North America 0.000 0.0005 Inf   -0.0009     0.001\n##                        20 Latin America and the Caribbean  0.397 0.1827 Inf    0.0395     0.755\n##                        80 Latin America and the Caribbean  0.055 0.0776 Inf   -0.0975     0.207\n##                        20 Middle East and North Africa     0.658 0.2505 Inf    0.1671     1.149\n##                        80 Middle East and North Africa     0.001 0.0024 Inf   -0.0042     0.005\n## \n## Confidence level used: 0.95\n\nThe results from the two packages are identical because we’re using a data grid—in both cases we’re averaging before plugging stuff into the model.\nInstead of setting corruption to 20 and 80, we’ll use a whole range of values so we can plot it.\n\nlogit_predictions &lt;- model_logit_fancy |&gt; \n  emmeans(~ public_sector_corruption + region, var = \"public_sector_corruption\",\n          at = list(public_sector_corruption = seq(0, 90, 1)),\n          regrid = \"response\") |&gt; \n  as_tibble()\n\nggplot(logit_predictions, aes(x = public_sector_corruption, y = prob, color = region)) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Public sector corruption\", y = \"Predicted probability of having\\na campaign finance disclosure law\", color = NULL) +\n  scale_y_continuous(labels = percent_format()) +\n  scale_color_manual(values = c(clrs, \"grey30\")) +\n  theme_mfx() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n(Alternatively, you can use marginaleffects’s built-in plot_predictions() to make this plot with one line of code):\n\nplot_predictions(model_logit_fancy, condition = c(\"public_sector_corruption\", \"region\"))\n\nThat’s such a cool plot! Each region has a different shape of predicted probabilities across public sector corruption.\nEarlier we calculated a bunch of instantaneous slopes when corruption was set to 20 and 80 in a few different regions, so let’s put those slopes and their tangent lines on the plot:\n\nlogit_slopes &lt;- model_logit_fancy |&gt; \n  emtrends(~ public_sector_corruption + region, var = \"public_sector_corruption\",\n           at = list(public_sector_corruption = c(20, 80),\n                     region = regions_to_use),\n           regrid = \"response\", delta.var = 0.001) |&gt; \n  as_tibble() |&gt; \n  mutate(panel = glue(\"Corruption set to {public_sector_corruption}\"))\n\nslopes_to_plot &lt;- logit_predictions |&gt; \n  filter(public_sector_corruption %in% c(20, 80),\n         region %in% regions_to_use) |&gt; \n  left_join(select(logit_slopes, public_sector_corruption, region, public_sector_corruption.trend, panel),\n            by = c(\"public_sector_corruption\", \"region\")) |&gt; \n  mutate(intercept = find_intercept(public_sector_corruption, prob, public_sector_corruption.trend)) |&gt; \n  mutate(round_slope = label_number(accuracy = 0.001, style_negative = \"minus\")(public_sector_corruption.trend * 100),\n         nice_slope = glue(\"Slope: {round_slope} pct pts\"))\n\nggplot(logit_predictions, aes(x = public_sector_corruption, y = prob, color = region)) +\n  geom_line(linewidth = 1) +\n  geom_point(data = slopes_to_plot, size = 2, show.legend = FALSE) +\n  geom_abline(data = slopes_to_plot, \n              aes(slope = public_sector_corruption.trend, intercept = intercept, color = region), \n              linewidth = 0.5, linetype = \"21\", show.legend = FALSE) +\n  geom_label_repel(data = slopes_to_plot, aes(label = nice_slope),\n                   fontface = \"bold\", seed = 123, show.legend = FALSE,\n                   size = 3, direction = \"y\") +\n  labs(x = \"Public sector corruption\", \n       y = \"Predicted probability of having\\na campaign finance disclosure law\", \n       color = NULL) +\n  scale_y_continuous(labels = percent_format()) +\n  scale_color_manual(values = c(\"grey30\", clrs)) +\n  facet_wrap(vars(panel)) +\n  theme_mfx() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAHH this is delightful! This helps us understand and visualize all these marginal effects. Let’s interpret them:\n\nIn both the Middle East and Western Europe/North America, an increase in public sector corruption in countries with low levels of corruption (20) is associated with a −2.040 and −2.493 percentage point decrease in the probability of seeing a disclosure law, while in low-corruption countries in Latin America, an increase in public sector corruption doesn’t do much to the probability (it increases it slightly by 0.083 percentage points)\nIn countries with high levels of corruption (80), on the other hand, a small increase in corruption doesn’t do much to the probability of having a disclosure law in the Middle East (−0.011 percentage point decrease) or Western Europe (−0.001 percentage point decrease). In Latin America, though, a small increase in corruption is associated with a −0.437 percentage point decrease in the probability of having a disclosure law.\n\nMAJOR CAVEAT: None of these marginal effects are statistically significant, so there’s a good chance that they’re possibly zero, or positive, or more negative, or whatever. We can plot just these marginal slopes to show this:\n\nggplot(logit_slopes, aes(x = public_sector_corruption.trend * 100, y = region, color = region)) +\n  geom_vline(xintercept = 0, linewidth = 0.5, linetype = \"24\", color = clrs[5]) +\n  geom_pointrange(aes(xmin = asymp.LCL * 100, xmax = asymp.UCL * 100)) +\n  scale_color_manual(values = c(clrs[4], clrs[1], clrs[2]), guide = \"none\") +\n  labs(x = \"Marginal effect (percentage points)\", y = NULL) +\n  facet_wrap(vars(panel), ncol = 1) +\n  theme_mfx()\n\n\n\n\n\n\n\nAverage marginal effects at counterfactual user-specified values\n\n\n\n\n\n\nCounterfactual grids in emmeans\n\n\n\nWhen I first wrote this in 2022, emmeans did not have support for counterfactual grids, but marginaleffects did, so here I only show this example with marginaleffects. Later in 2022, however, emmeans added a counterfactuals argument to ref_grid(), so you can actually calculate these same average marginal effects at counterfactual user-specified values with both marginaleffects and emmeans now.\n\n\nCalculating marginal effects at representative values is useful and widespread—plugging different values into the model while holding others constant is the best way to see how all the different moving parts of a model work, especially when there interactions, exponents, or non-linear outcomes. We’re using the full mixing panel here!\nHowever, creating a hypothetical data or reference grid creates hypothetical observations that might never exist in real life. This was the main difference behind the average marginal effect (AME) and the marginal effect at the mean (MEM) that we looked at earlier. Passing average covariate values into a model creates average predictions, but those averages might not reflect reality.\nFor example, we used this data grid to look at the effect of corruption on the probability of having a campaign finance disclosure law across different regions:\n\ndatagrid(model = model_logit_fancy,\n         public_sector_corruption = c(20, 80),\n         region = regions_to_use)\n##   disclose_donations polyarchy log_gdp_percapita public_sector_corruption                           region\n## 1              FALSE      52.8              8.57                       20 Western Europe and North America\n## 2              FALSE      52.8              8.57                       20  Latin America and the Caribbean\n## 3              FALSE      52.8              8.57                       20     Middle East and North Africa\n## 4              FALSE      52.8              8.57                       80 Western Europe and North America\n## 5              FALSE      52.8              8.57                       80  Latin America and the Caribbean\n## 6              FALSE      52.8              8.57                       80     Middle East and North Africa\n\nPolyarchy (democracy) and GDP per capita here are set at their dataset-level means, but that’s not how the world actually works. Levels of democracy and personal wealth vary a lot by region:\n\ncorruption |&gt; \n  filter(region %in% regions_to_use) |&gt; \n  group_by(region) |&gt; \n  summarize(avg_polyarchy = mean(polyarchy),\n            avg_log_gdp_percapita = mean(log_gdp_percapita))\n## # A tibble: 3 × 3\n##   region                           avg_polyarchy avg_log_gdp_percapita\n##   &lt;fct&gt;                                    &lt;dbl&gt;                 &lt;dbl&gt;\n## 1 Latin America and the Caribbean           62.9                  8.77\n## 2 Middle East and North Africa              27.4                  9.01\n## 3 Western Europe and North America          86.5                 10.7\n\nWestern Europe is far more democratic (average polyarchy = 86.50) than the Middle East (average polyarchy = 27.44). But in our calculations for finding region-specific marginal effects, we’ve been using a polyarchy value of 52.79 for all the regions.\nFortunately we can do something neat to work with observed covariate values and thus create an AME-flavored marginal effect at representative values instead of the current MEM-flavored marginal effect at representative values. Here’s the general process:\n\n\n\n\n\n\n\n\nInstead of creating a data or reference grid, we create multiple copies of our original dataset. In each copy we change the columns that we want to set to specific values and we leave all the other columns at their original values. We then feed all the copies of the dataset into the model and generate a ton of fitted values, which we then collapse into average effects.\nThat sounds really complex, but it’s only a matter of adding one argument to marginaleffects::datagrid(). We’ll take region out of datagrid here so that we keep all the original regions—we’ll take the average across those regions after the fact.\n\ncfct_data &lt;- datagrid(model = model_logit_fancy,\n                      public_sector_corruption = c(20, 80),\n                      grid_type = \"counterfactual\")\n\nThis new data grid has twice the number of rows that we have in the original data, since there are now two copies of the data stacked together:\n\nnrow(corruption)\n## [1] 168\nnrow(cfct_data)\n## [1] 336\n\nTo verify, lets look at the first 5 rows in each of the copies:\n\ncfct_data[c(1:5, nrow(corruption) + 1:5), ]\n##     rowidcf disclose_donations polyarchy log_gdp_percapita                           region public_sector_corruption\n## 1         1               TRUE      64.7              9.10  Latin America and the Caribbean                       20\n## 2         2              FALSE      76.1              8.93  Latin America and the Caribbean                       20\n## 3         3              FALSE      90.8             10.85 Western Europe and North America                       20\n## 4         4              FALSE      89.4             11.36 Western Europe and North America                       20\n## 5         5              FALSE      72.0              7.61               Sub-Saharan Africa                       20\n## 169       1               TRUE      64.7              9.10  Latin America and the Caribbean                       80\n## 170       2              FALSE      76.1              8.93  Latin America and the Caribbean                       80\n## 171       3              FALSE      90.8             10.85 Western Europe and North America                       80\n## 172       4              FALSE      89.4             11.36 Western Europe and North America                       80\n## 173       5              FALSE      72.0              7.61               Sub-Saharan Africa                       80\n\nThat’s neat! These 5 countries all have their original values of polyarchy, GDP per capita, and region, but have their public sector corruption indexes set to 20 (in the first copy) and 80 (in the second copy).\nWe can feed this stacked data to slopes() to get an instantaneous slope (estimate) for each row:\n\n# Specify variables = \"public_sector_corruption\" so that it doesn't calculate\n# slopes and contrasts for all the other covariates\nmfx_cfct &lt;- model_logit_fancy |&gt; \n  slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                            grid_type = \"counterfactual\"),\n         variables = \"public_sector_corruption\")\n\nhead(mfx_cfct)\n## \n##  rowidcf                     Term  Estimate Std. Error       z Pr(&gt;|z|)   2.5 %  97.5 % polyarchy log_gdp_percapita                           region public_sector_corruption\n##        1 public_sector_corruption  0.000860    0.00898  0.0958    0.924 -0.0167 0.01846      64.7              9.10 Latin America and the Caribbean                        20\n##        2 public_sector_corruption  0.000861    0.00900  0.0956    0.924 -0.0168 0.01851      76.1              8.93 Latin America and the Caribbean                        20\n##        3 public_sector_corruption -0.024669    0.02425 -1.0174    0.309 -0.0722 0.02285      90.8             10.85 Western Europe and North America                       20\n##        4 public_sector_corruption -0.024566    0.02460 -0.9986    0.318 -0.0728 0.02365      89.4             11.36 Western Europe and North America                       20\n##        5 public_sector_corruption -0.014743    0.01034 -1.4264    0.154 -0.0350 0.00551      72.0              7.61 Sub-Saharan Africa                                     20\n##        6 public_sector_corruption -0.014592    0.01034 -1.4107    0.158 -0.0349 0.00568      70.3              8.64 Sub-Saharan Africa                                     20\n## \n## Columns: rowid, rowidcf, term, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, disclose_donations, polyarchy, log_gdp_percapita, region, public_sector_corruption\n\nFinally we can calculate group averages for each of the levels of public_sector_corruption to get AME-flavored effects:\n\nmfx_cfct |&gt; \n  group_by(public_sector_corruption) |&gt; \n  summarize(avg_slope = mean(estimate))\n## # A tibble: 2 × 2\n##   public_sector_corruption avg_slope\n##                      &lt;dbl&gt;     &lt;dbl&gt;\n## 1                       20  -0.0128 \n## 2                       80  -0.00307\n\nOr we can let marginaleffects deal with the averaging so that we can get standard errors and confidence intervals:\n\nmodel_logit_fancy |&gt; \n  avg_slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                                grid_type = \"counterfactual\"),\n             variables = \"public_sector_corruption\",\n             by = \"public_sector_corruption\")\n## \n##                      Term    Contrast public_sector_corruption Estimate Std. Error     z Pr(&gt;|z|)    2.5 %    97.5 %\n##  public_sector_corruption mean(dY/dX)                       20 -0.01277    0.00659 -1.94   0.0527 -0.02568  0.000149\n##  public_sector_corruption mean(dY/dX)                       80 -0.00307    0.00123 -2.50   0.0125 -0.00547 -0.000661\n## \n## Columns: term, contrast, public_sector_corruption, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nWe can also calculate group averages across each region to get region-specific AME-flavored effects:\n\nmfx_cfct |&gt; \n  filter(region %in% regions_to_use) |&gt; \n  group_by(public_sector_corruption, region) |&gt; \n  summarize(avg_slope = mean(estimate))\n## `summarise()` has grouped output by 'public_sector_corruption'. You can override using the `.groups` argument.\n## # A tibble: 6 × 3\n## # Groups:   public_sector_corruption [2]\n##   public_sector_corruption region                            avg_slope\n##                      &lt;dbl&gt; &lt;fct&gt;                                 &lt;dbl&gt;\n## 1                       20 Latin America and the Caribbean   0.000816 \n## 2                       20 Middle East and North Africa     -0.0218   \n## 3                       20 Western Europe and North America -0.0252   \n## 4                       80 Latin America and the Caribbean  -0.00570  \n## 5                       80 Middle East and North Africa     -0.0000711\n## 6                       80 Western Europe and North America -0.0000370\n\nOr again, we can get standard errors and confidence intervals:\n\nmodel_logit_fancy |&gt; \n  avg_slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                                grid_type = \"counterfactual\"),\n             variables = \"public_sector_corruption\",\n             by = c(\"public_sector_corruption\", \"region\"))\n## \n##                      Term    Contrast public_sector_corruption                           region  Estimate Std. Error      z Pr(&gt;|z|)     2.5 %    97.5 %\n##  public_sector_corruption mean(dY/dX)                       20 Eastern Europe and Central Asia  -1.84e-03   0.005413 -0.340   0.7337 -0.012452  0.008769\n##  public_sector_corruption mean(dY/dX)                       20 Latin America and the Caribbean   8.16e-04   0.008500  0.096   0.9235 -0.015844  0.017477\n##  public_sector_corruption mean(dY/dX)                       20 Middle East and North Africa     -2.18e-02   0.016457 -1.323   0.1860 -0.054021  0.010490\n##  public_sector_corruption mean(dY/dX)                       20 Sub-Saharan Africa               -1.43e-02   0.011347 -1.262   0.2071 -0.036554  0.007925\n##  public_sector_corruption mean(dY/dX)                       20 Western Europe and North America -2.52e-02   0.023444 -1.077   0.2815 -0.071197  0.020703\n##  public_sector_corruption mean(dY/dX)                       20 Asia and Pacific                 -1.62e-02   0.010530 -1.536   0.1246 -0.036811  0.004467\n##  public_sector_corruption mean(dY/dX)                       80 Eastern Europe and Central Asia  -1.28e-02   0.006090 -2.104   0.0353 -0.024751 -0.000879\n##  public_sector_corruption mean(dY/dX)                       80 Latin America and the Caribbean  -5.70e-03   0.004395 -1.296   0.1951 -0.014309  0.002919\n##  public_sector_corruption mean(dY/dX)                       80 Middle East and North Africa     -7.11e-05   0.000254 -0.279   0.7799 -0.000570  0.000427\n##  public_sector_corruption mean(dY/dX)                       80 Sub-Saharan Africa               -2.21e-04   0.000454 -0.487   0.6261 -0.001112  0.000669\n##  public_sector_corruption mean(dY/dX)                       80 Western Europe and North America -3.70e-05   0.000217 -0.171   0.8646 -0.000463  0.000389\n##  public_sector_corruption mean(dY/dX)                       80 Asia and Pacific                 -2.73e-04   0.000682 -0.401   0.6884 -0.001610  0.001063\n## \n## Columns: term, contrast, public_sector_corruption, region, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nLet’s compare these counterfactual marginal effects with the region-specific marginal effects at representative values that we calculated earlier:\n\name_flavored &lt;- model_logit_fancy |&gt; \n  slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                            grid_type = \"counterfactual\"),\n         variables = \"public_sector_corruption\",\n         by = c(\"public_sector_corruption\", \"region\")) |&gt; \n  filter(region %in% regions_to_use)\n\nmem_flavored &lt;- model_logit_fancy |&gt; \n  slopes(newdata = datagrid(public_sector_corruption = c(20, 80),\n                            region = regions_to_use),\n         variables = \"public_sector_corruption\",\n         by = c(\"public_sector_corruption\", \"region\"))\n\nmfx_to_plot &lt;- bind_rows(`Counterfactual stacked data` = ame_flavored, \n                         `Average values` = mem_flavored, \n                         .id = \"approach\") |&gt; \n  mutate(panel = glue(\"Corruption set to {public_sector_corruption}\"))\n\nggplot(mfx_to_plot, aes(x = estimate * 100, y = region, color = region, \n                        linetype = approach, shape = approach)) +\n  geom_vline(xintercept = 0, linewidth = 0.5, linetype = \"24\", color = clrs[5]) +\n  geom_pointrange(aes(xmin = conf.low * 100, xmax = conf.high * 100),\n                  position = position_dodge(width = -0.6)) +\n  scale_color_manual(values = c(clrs[4], clrs[1], clrs[2]), guide = \"none\") +\n  labs(x = \"Marginal effect (percentage points)\", y = NULL, linetype = NULL, shape = NULL) +\n  facet_wrap(vars(panel), ncol = 1) +\n  theme_mfx() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nIn this case there are no huge differences 🤷. BUT STILL this is really neat!"
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#categorical-contrasts-as-statisticalmarginal-effects",
    "href": "blog/2022/05/20/marginalia/index.html#categorical-contrasts-as-statisticalmarginal-effects",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "Categorical contrasts as statistical/marginal effects",
    "text": "Categorical contrasts as statistical/marginal effects\nConfusingly, people sometimes also use the term “marginal effect” to talk about group averages or predicted values (I myself am guilty of this!). Technically speaking, a marginal effect is only a partial derivative, or a slope—not a predicted value or a difference in group means.\nBut regression lends itself well to group means, and predictions and fitted values are fundamental to calculating instantaneous slopes, so both marginaleffects and emmeans are used for adjusted predictions and marginal means and contrasts. They also use different approaches for calculating these averages, either averaging before putting values in the model (emmeans) or averaging after (marginaleffects’s default setting).\nWe’ve already seen two different functions for generating predictions when we plotted the predicted probabilities of having a disclosure law for each region: marginaleffects::predictions() and emmeans::emmeans().\nI won’t go into a ton of detail here about the differences between the two approaches to predictions and contrasts, mostly because pretty much everything we’ve looked at so far applies to both. Instead, you should look at Vincent’s excellent vignettes for marginaleffects:\n\nPredictions\nComparisons\n\nAnd the equally excellent vignettes for emmeans:\n\nPrediction in emmeans\nComparisons and contrasts in emmeans\n\nYou should also check out this Twitter thread tutorial by Alex Hayes on categorical contrasts and means—it’s a fantastic illustration of this same process.\nIn general, the two packages follow the same overall approach that we’ve seen with slopes() and emtrends():\n\nPrediction and comparison functions in marginaleffects try to calculate predictions and averages for each row, then collapses them to single average values (either globally or for specific groups). This approach is AME-flavored (though marginaleffects can also do MEM-flavored operations and average first).\nPrediction and contrast functions in emmeans collapse values into averages first, then feeds those average values into the model to generate average predictions and means (either globally or for specific groups). This approach is MEM-flavored."
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#tldr-overall-summary-of-all-these-marginal-effects-approaches",
    "href": "blog/2022/05/20/marginalia/index.html#tldr-overall-summary-of-all-these-marginal-effects-approaches",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "tl;dr: Overall summary of all these marginal effects approaches",
    "text": "tl;dr: Overall summary of all these marginal effects approaches\nPHEW we just did a lot of marginal work. This is important stuff. Unless you’re working with a linear OLS model without any fancy extra things like interactions, polynomials, logs, and so on, don’t try to talk about marginal effects based on just the output of a regression table—it’s not possible unless you do a lot of manual math!\nBoth marginaleffects and emmeans provide all sorts of neat and powerful ways to calculate marginal effects without needing to resort to calculus, but as we’ve seen here, there are some subtle and extremely important differences in how they calculate their different effects.\nThe main takeaway from this whole post is this: If you take the average before plugging values into the model, you compute average marginal effects for a combination of covariates that might not actually exist in reality. If you take the average after plugging values into the model, each original observation reflects combinations of covariates that definitely exist in reality, so the average marginal effect reflects that reality.\nTo remember all these differences, here’s a table summarizing all their different approaches:\n\n\n\n\n\n\n\n\n\n\nType\nProcess\nmarginaleffects\nemmeans\n\n\n\nAverage marginal effects (AME)\nGenerate predictions for each row of the original data, then collapse to averages\navg_slopes(model)\nNot supported\n\n\nGroup average marginal effects (G-AME)\nGenerate predictions for each row of the original data, then collapse to grouped averages\navg_slopes(model, by = \"some_group\")\nNot supported\n\n\nMarginal effects at the mean (MEM)\nCollapse data to averages, then generate predictions using those averages\navg_slopes(model, newdata = \"mean\")\nemtrends(model, ...)\n\n\nMarginal effects at user-specified or representative values (MER)\nCreate a grid of specific and average or typical values, then generate predictions\nslopes(\n  model,\n  newdata = datagrid(some_x = c(10, 20))\n)\nemtrends(\n  model, ...,\n  at = list(some_x = c(10, 20))\n)\n\n\nAverage marginal effects at counterfactual user-specified values\nCreate multiple copies of the original data with some columns set to specific values, then generate predictions for each row of each copy of the original data, then collapse to averages\nslopes(\n  model,\n  newdata = datagrid(\n    some_x = c(10, 20),\n    grid_type = \"counterfactual\"\n  )\n)\nemtrends(\n  model, \"some_x\",\n  counterfactuals = \"some_x\"\n)\n\n\n\n\n\nAnd here’s an image with all five of the diagrams at the same time:\n\n\n\n\n\n\nDiagrams!\n\n\n\nYou can download PDF, SVG, and PNG versions of the marginal effects diagrams in this guide, as well as the original Adobe Illustrator file, here:\n\nPDFs, SVGs, and PNGs\nIllustrator .ai file\n\nDo whatever you want with them! They’re licensed under Creative Commons Attribution-ShareAlike (BY-SA 4.0)."
  },
  {
    "objectID": "blog/2022/05/20/marginalia/index.html#which-approach-is-best",
    "href": "blog/2022/05/20/marginalia/index.html#which-approach-is-best",
    "title": "Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are",
    "section": "Which approach is best?",
    "text": "Which approach is best?\nWho even knows.\nBoth kinds of averaging approaches are pretty widespread. The tidymodels ecosystem encourages the use of modelr::data_grid() and plugging various combinations of specific and typical variables into models to look at slopes and group contrasts. That’s marginal effects at the mean (MEM) and marginal effects at representative values (MER), which both use average values before putting them in model. And it’s fine—tidymodels is used in data science production pipelines around the world.\nemmeans is also incredibly popular in data science and academia. I use it in a few of my blog post guides (like this one where I talk about average marginal effects the whole time even though technically none of the effects there are true AMEs—they’re all MEMs!). emmeans just calculates MEMs and MERs.\nThe idea of average marginal effects (AMEs)—calculating averages after plugging values into models—is incredibly popular in the social sciences. marginaleffects, its predecessor margins, and its Stata counterpart margins are all used in research in political science, public policy, economics, and other fields.\nI’m sure there are super smart people in the world who know when AMEs or MEMs are most appropriate (like this article here!), and people who have even better and robust ways to account for the typicalness and/or uncertainty of the original data (see here for an averaging approach using a Bayesian bootstrap, for instance), but I’m not one of those super smart people."
  },
  {
    "objectID": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html",
    "href": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html",
    "title": "Visualizing the differences between Bayesian posterior predictions, linear predictions, and the expectation of posterior predictions",
    "section": "",
    "text": "Downloadable cheat sheets!\n\n\n\nYou can download PDF, SVG, and PNG versions of the diagrams and cheat sheets in this post, as well as the original Adobe Illustrator and InDesign files, at the bottom of this post\nDo whatever you want with them! They’re licensed under Creative Commons Attribution-ShareAlike (BY-SA 4.0).\nI’ve been working with Bayesian models and the Stan-based brms ecosystem (tidybayes, ggdist, marginaleffects, and friends) for a few years now, and I’m currently finally working through formal materials on Bayesianism and running an independent readings class with a PhD student at GSU where we’re reading Richard McElreath’s Statistical Rethinking and Alicia Johnson, Miles Ott, and Mine Dogucu’s Bayes Rules!, both of which are fantastic books (check out my translation of their materials to tidyverse/brms here).\nSomething that has always plagued me about working with Bayesian posterior distributions, but that I’ve always waved off as too hard to think about, has been the differences between posterior predictions, the expectation of the posterior predictive distribution, and the posterior of the linear predictor (or posterior_predict(), posterior_epred(), and posterior_linpred() in the brms world). But reading these two books has forced me to finally figure it out.\nSo here’s an explanation of my mental model of the differences between these types of posterior distributions. It’s definitely not 100% correct, but it makes sense for me.\nFor bonus fun, skip down to the incredibly useful diagrams and cheat sheets at the bottom of this post.\nLet’s load some packages, load some data, and get started!\nlibrary(tidyverse)        # ggplot, dplyr, and friends\nlibrary(patchwork)        # Combine ggplot plots\nlibrary(ggtext)           # Fancier text in ggplot plots\nlibrary(scales)           # Labeling functions\nlibrary(brms)             # Bayesian modeling through Stan\nlibrary(tidybayes)        # Manipulate Stan objects in a tidy way\nlibrary(marginaleffects)  # Calculate marginal effects\nlibrary(modelr)           # For quick model grids\nlibrary(extraDistr)       # For dprop() beta distribution with mu/phi\nlibrary(distributional)   # For plotting distributions with ggdist\nlibrary(palmerpenguins)   # Penguins!\nlibrary(kableExtra)       # For nicer tables\n\n# Make random things reproducible\nset.seed(1234)\n\n# Bayes stuff\n# Use the cmdstanr backend for Stan because it's faster and more modern than\n# the default rstan. You need to install the cmdstanr package first\n# (https://mc-stan.org/cmdstanr/) and then run cmdstanr::install_cmdstan() to\n# install cmdstan on your computer.\noptions(mc.cores = 4,  # Use 4 cores\n        brms.backend = \"cmdstanr\")\nbayes_seed &lt;- 1234\n\n# Colors from MetBrewer\nclrs &lt;- MetBrewer::met.brewer(\"Java\")\n\n# Custom ggplot themes to make pretty plots\n# Get Roboto Condensed at https://fonts.google.com/specimen/Roboto+Condensed\n# Get Roboto Mono at https://fonts.google.com/specimen/Roboto+Mono\ntheme_pred &lt;- function() {\n  theme_minimal(base_family = \"Roboto Condensed\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          plot.title = element_text(face = \"bold\"),\n          strip.text = element_text(face = \"bold\"),\n          strip.background = element_rect(fill = \"grey80\", color = NA),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 0),\n          legend.title = element_text(face = \"bold\"))\n}\n\ntheme_pred_dist &lt;- function() {\n  theme_pred() +\n    theme(plot.title = element_markdown(family = \"Roboto Condensed\", face = \"plain\"),\n          plot.subtitle = element_text(family = \"Roboto Mono\", size = rel(0.9), hjust = 0),\n          axis.text.y = element_blank(),\n          panel.grid.major.y = element_blank(),\n          panel.grid.minor.y = element_blank())\n}\n\ntheme_pred_range &lt;- function() {\n  theme_pred() +\n    theme(plot.title = element_markdown(family = \"Roboto Condensed\", face = \"plain\"),\n          plot.subtitle = element_text(family = \"Roboto Mono\", size = rel(0.9), hjust = 0),\n          panel.grid.minor.y = element_blank())\n}\n\nupdate_geom_defaults(\"text\", list(family = \"Roboto Condensed\", lineheight = 1))\n# Add a couple new variables to the penguins data:\n#  - is_gentoo: Indicator for whether or not the penguin is a Gentoo\n#  - bill_ratio: The ratio of a penguin's bill depth (height) to its bill length\npenguins &lt;- penguins |&gt; \n  drop_na(sex) |&gt; \n  mutate(is_gentoo = species == \"Gentoo\") |&gt; \n  mutate(bill_ratio = bill_depth_mm / bill_length_mm)"
  },
  {
    "objectID": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#normal-gaussian-model",
    "href": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#normal-gaussian-model",
    "title": "Visualizing the differences between Bayesian posterior predictions, linear predictions, and the expectation of posterior predictions",
    "section": "Normal Gaussian model",
    "text": "Normal Gaussian model\nFirst we’ll look at basic linear regression. Normal or Gaussian models are roughly equivalent to frequentist ordinary least squares (OLS) regression. We estimate an intercept and a slope and draw a line through the data. If we include multiple explanatory variables or predictors, we’ll have multiple slopes, or partial derivatives or marginal effects (see here for more about that). But to keep things as simple and basic and illustrative as possible, we’ll just use one explanatory variable here.\nIn this example, we’re interested in the relationship between penguin flipper length and penguin body mass. Do penguins with longer flippers weigh more? Here’s what the data looks like:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(size = 1, alpha = 0.7) +\n  geom_smooth(method = \"lm\", color = clrs[5], se = FALSE) +\n  scale_y_continuous(labels = label_comma()) +\n  coord_cartesian(ylim = c(2000, 6000)) +\n  labs(x = \"Flipper length (mm)\", y = \"Body mass (g)\") +\n  theme_pred()\n\n\n\n\n\n\n\nIt seems like there’s a pretty clear relationship between the two. As flipper length increases, body mass also increases.\nWe can create a more formal model for the distribution of body mass, conditional on different values of flipper length, like this:\n\\[\n\\begin{aligned}\n\\text{Body mass}_i &\\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\ \\text{Flipper length}_i\n\\end{aligned}\n\\]\nOr more generally:\n\\[\n\\begin{aligned}\ny_i &\\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i\n\\end{aligned}\n\\]\nThis implies that body mass follows a normal (or Gaussian) distribution with some average (\\(\\mu\\)) and some amount of spread (\\(\\sigma\\)), and that the \\(\\mu\\) parameter is conditional on (or based on, or dependent on) flipper length.\nLet’s run that model in Stan through brms (with all the default priors; in real life you’d want to set more official priors for the intercept \\(\\alpha\\), the coefficient \\(\\beta\\), and the overall model spread \\(\\sigma\\))\n\nmodel_normal &lt;- brm(\n  bf(body_mass_g ~ flipper_length_mm),\n  family = gaussian(),\n  data = penguins\n)\n## Start sampling\n\nIf we look at the model results, we can see the means of the posterior distributions of each of the model’s parameters (\\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\)). The intercept (\\(\\alpha\\)) is huge and negative because flipper length is far away from 0, so it’s pretty uninterpretable. The \\(\\beta\\) coefficient shows that a one-mm increase in flipper length is associated with a 50 gram increase in body mass. And the overall model standard deviation \\(\\sigma\\) shows that there’s roughly 400 grams of deviation around the mean body mass.\n\nbroom.mixed::tidy(model_normal) |&gt; \n  bind_cols(parameter = c(\"α\", \"β\", \"σ\")) |&gt; \n  select(parameter, term, estimate, std.error, conf.low, conf.high)\n## # A tibble: 3 × 6\n##   parameter term              estimate std.error conf.low conf.high\n##   &lt;chr&gt;     &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 α         (Intercept)        -5874.     311.    -6466.    -5257. \n## 2 β         flipper_length_mm     50.2      1.54     47.1      53.1\n## 3 σ         sd__Observation      394.      15.7     366.      426.\n\nThat table shows just the posterior means for each of these parameters, but these are technically all complete distributions. In this post we’re not interested in these actual values—we’re concerned with the outcome, or penguin weight here. (But you can see this post or this post or this post or this documentation for more about working with these coefficients and calculating marginal effects)\nGoing back to the formal model, so far we’ve looked at \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\), but what about \\(\\mu\\) and the overall posterior distribution of the outcome \\(y\\) (or \\(\\operatorname{Normal}(\\mu_i, \\sigma)\\))? This is where life gets a little trickier (and why this guide exists in the first place!). Both \\(\\mu\\) and the posterior for \\(y\\) represent penguin body mass, but conceptually they’re different things. We’ll extract these different distributions with three different brms functions: posterior_predict(), posterior_epred(), and posterior_linpred() (the code uses predicted_draws(), epred_draws(), and linpred_draws(); these are tidybayes’s wrappers for the corresponding brms functions).\nNote the newdata argument here. We have to feed a data frame of values to plug into to make these different posterior predictions. We could feed the original dataset with newdata = penguins, which would plug each row of the data into the model and generate 4000 posterior draws for it. Given that there are 333 rows in penguins data, using newdata = penguins would give us 333 × 4,000 = 1,332,000 rows. That’s a ton of data, and looking at it all together like that isn’t super useful unless we look at predictions across a range of possible predictors. We’ll do that later in this section and see the posterior predictions of weights across a range of flipper lengths. But here we’re just interested in the prediction of the outcome based on a single value of flipper lengths. We’ll use the average (200.967 mm), but it could easily be the median or whatever arbitrary number we want.\n\n# Make a little dataset of just the average flipper length\npenguins_avg_flipper &lt;- penguins |&gt; \n  summarize(flipper_length_mm = mean(flipper_length_mm))\n\n# Extract different types of posteriors\nnormal_linpred &lt;- model_normal |&gt; \n  linpred_draws(newdata = penguins_avg_flipper)\n\nnormal_epred &lt;- model_normal |&gt; \n  epred_draws(newdata = penguins_avg_flipper)\n\nnormal_predicted &lt;- model_normal |&gt; \n  predicted_draws(newdata = penguins_avg_flipper,\n                  seed = 12345)  # So that the manual results with rnorm() are the same later\n\nThese each show the posterior distribution of penguin weight, and each corresponds to a different part of the formal mathematical model with. We can explore these nuances if we look at these distributions’ means, medians, standard deviations, and overall shapes:\n\nCodesummary_normal_linpred &lt;- normal_linpred |&gt; \n  ungroup() |&gt; \n  summarize(across(.linpred, lst(mean, sd, median), .names = \"{.fn}\"))\n\nsummary_normal_epred &lt;- normal_epred |&gt; \n  ungroup() |&gt; \n  summarize(across(.epred, lst(mean, sd, median), .names = \"{.fn}\"))\n\nsummary_normal_predicted &lt;- normal_predicted |&gt; \n  ungroup() |&gt; \n  summarize(across(.prediction, lst(mean, sd, median), .names = \"{.fn}\"))\n\ntribble(\n  ~Function, ~`Model element`,\n  \"&lt;code&gt;posterior_linpred()&lt;/code&gt;\", \"\\\\(\\\\mu\\\\) in the model\",\n  \"&lt;code&gt;posterior_epred()&lt;/code&gt;\", \"\\\\(\\\\operatorname{E(y)}\\\\) and \\\\(\\\\mu\\\\) in the model\",\n  \"&lt;code&gt;posterior_predict()&lt;/code&gt;\", \"Random draws from posterior \\\\(\\\\operatorname{Normal}(\\\\mu_i, \\\\sigma)\\\\)\"\n) |&gt; \n  bind_cols(bind_rows(summary_normal_linpred, summary_normal_epred, summary_normal_predicted)) |&gt; \n  kbl(escape = FALSE) |&gt; \n  kable_styling()\n\n\nTable 1: Normal posteriors\n\n\n\n\nFunction\nModel element\nmean\nsd\nmedian\n\n\n\nposterior_linpred()\n\\(\\mu\\) in the model\n4206\n21.8\n4207\n\n\nposterior_epred()\n\\(\\operatorname{E(y)}\\) and \\(\\mu\\) in the model\n4206\n21.8\n4207\n\n\nposterior_predict()\nRandom draws from posterior \\(\\operatorname{Normal}(\\mu_i, \\sigma)\\)\n4207\n386.7\n4209\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(normal_linpred, aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[3]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(4100, 4300)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Linear predictor** &lt;span style='font-size: 14px;'&gt;*µ* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_linpred(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist() +\n  theme(plot.title = element_markdown())\n\np2 &lt;- ggplot(normal_epred, aes(x = .epred)) +\n  stat_halfeye(fill = clrs[2]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(4100, 4300)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Expectation of the posterior** &lt;span style='font-size: 14px;'&gt;E[*y*] and *µ* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_epred(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist()\n\np3 &lt;- ggplot(normal_predicted, aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[1]) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(2900, 5500)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Normal(*µ*, *σ*)&lt;/span&gt;\",\n       subtitle = \"posterior_predict(..., tibble(flipper_length_mm = 201))\") +\n  theme_pred_dist()\n\n(p1 / plot_spacer() / p2 / plot_spacer() / p3) +\n  plot_layout(heights = c(0.3, 0.05, 0.3, 0.05, 0.3))\n\n\n\n\n\n\n\nThe most obvious difference between these different posterior predictions is the range of predictions. For posterior_linpred() and posterior_epred(), the standard error is tiny and the range of plausible predicted values is really narrow. For posterior_predict(), the standard error is substantially bigger, and the corresponding range of predicted values is huge.\nTo understand why, let’s explore the math going on behind the scenes in these functions. Both posterior_linpred() and posterior_epred() correspond to the \\(\\mu\\) part of the model. They’re the average penguin weight as predicted by the linear model (hence linpred; linear predictor). We can see this if we plug a 201 mm flipper length into each row of the posterior and calculate mu by hand with \\(\\beta_0 + (\\beta_1 \\times 201)\\):\n\nlinpred_manual &lt;- model_normal |&gt; \n  spread_draws(b_Intercept, b_flipper_length_mm) |&gt; \n  mutate(mu = b_Intercept + \n           (b_flipper_length_mm * penguins_avg_flipper$flipper_length_mm))\nlinpred_manual\n## # A tibble: 4,000 × 6\n##    .chain .iteration .draw b_Intercept b_flipper_length_mm    mu\n##     &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;               &lt;dbl&gt; &lt;dbl&gt;\n##  1      1          1     1      -6152.                51.5 4204.\n##  2      1          2     2      -5872                 50.2 4221.\n##  3      1          3     3      -6263.                52.1 4202.\n##  4      1          4     4      -6066.                51.1 4213.\n##  5      1          5     5      -5740.                49.4 4191.\n##  6      1          6     6      -5678.                49.2 4213.\n##  7      1          7     7      -6107.                51.1 4160.\n##  8      1          8     8      -5422.                48.0 4235.\n##  9      1          9     9      -6303.                52.1 4177.\n## 10      1         10    10      -6193.                51.6 4184.\n## # ℹ 3,990 more rows\n\nThat mu column is identical to what we calculate with posterior_linpred(). Just to confirm, we can plot the two distributions:\n\np1_manual &lt;- linpred_manual |&gt; \n  ggplot(aes(x = mu)) + \n  stat_halfeye(fill = colorspace::lighten(clrs[3], 0.5)) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(4100, 4300)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Linear predictor** &lt;span style='font-size: 14px;'&gt;*µ* in the model&lt;/span&gt;\",\n       subtitle = \"b_Intercept + (b_flipper_length_mm * 201)\") +\n  theme_pred_dist() +\n  theme(plot.title = element_markdown())\n\np1_manual | p1\n\n\n\n\n\n\n\nImportantly, the distribution of the \\(\\mu\\) part of the model here does not incorporate information about \\(\\sigma\\). That’s why the distribution is so narrow.\nThe results from posterior_predict(), on the other hand, correspond to the \\(y\\) part of the model. Officially, they are draws from a random normal distribution using both the estimated \\(\\mu\\) and the estimated \\(\\sigma\\). These results contain the full uncertainty of the posterior distribution of penguin weight. To help with the intuition, we can do the same thing by hand when plugging in a 201 mm flipper length:\n\nset.seed(12345)  # To get the same results as posterior_predict() from earlier\n\npostpred_manual &lt;- model_normal |&gt; \n  spread_draws(b_Intercept, b_flipper_length_mm, sigma) |&gt; \n  mutate(mu = b_Intercept + \n           (b_flipper_length_mm * \n              penguins_avg_flipper$flipper_length_mm),  # This is posterior_linpred()\n         y_new = rnorm(n(), mean = mu, sd = sigma))  # This is posterior_predict()\n\npostpred_manual |&gt; \n  select(.draw:y_new)\n## # A tibble: 4,000 × 6\n##    .draw b_Intercept b_flipper_length_mm sigma    mu y_new\n##    &lt;int&gt;       &lt;dbl&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1     1      -6152.                51.5  384. 4204. 4429.\n##  2     2      -5872                 50.2  401. 4221. 4506.\n##  3     3      -6263.                52.1  390. 4202. 4159.\n##  4     4      -6066.                51.1  409. 4213. 4027.\n##  5     5      -5740.                49.4  362. 4191. 4411.\n##  6     6      -5678.                49.2  393. 4213. 3499.\n##  7     7      -6107.                51.1  417. 4160. 4423.\n##  8     8      -5422.                48.0  351. 4235. 4138.\n##  9     9      -6303.                52.1  426. 4177. 4055.\n## 10    10      -6193.                51.6  426. 4184. 3793.\n## # ℹ 3,990 more rows\n\nThat y_new column here is the \\(y\\) part of the model and should have a lot more uncertainty than the mu column, which is just the \\(\\mu\\) part of the model. Notably, the y_new column is the same as what we get when using posterior predict(). We’ll plot the two distributions to confirm:\n\np3_manual &lt;- postpred_manual |&gt; \n  ggplot(aes(x = y_new)) + \n  stat_halfeye(fill = colorspace::lighten(clrs[1], 0.5)) +\n  scale_x_continuous(labels = label_comma()) +\n  coord_cartesian(xlim = c(2900, 5500)) +\n  labs(x = \"Body mass (g)\", y = NULL,\n       title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Normal(*µ*, *σ*)&lt;/span&gt;\",\n       subtitle = \"rnorm(b_Intercept + (b_flipper_length_mm * 201), sigma)\") +\n  theme_pred_dist() +\n  theme(plot.title = element_markdown())\n\np3_manual | p3\n\n\n\n\n\n\n\nThe results from posterior_predict() and posterior_linpred() have the same mean, but the full posterior predictions that incorporate the estimated \\(\\sigma\\) have a much wider range of plausible values.\nThe results from posterior_epred() are a little strange to understand, and in the case of normal/Gaussian regression (and many other types of regression models!), they’re identical to the linear predictor (posterior_linpred()). These are the posterior draws of the expected value or mean of the the posterior distribution, or \\(E(y_i)\\) in the model. Behind the scenes, this is calculated by taking the average of each row’s posterior distribution and then taking the average of that.\nOnce again, a quick illustration can help. As before, we’ll manually plug a flipper length of 201 mm into the posterior estimates of the intercept and slope to calculate the \\(\\mu\\) part of the model. We’ll then use that \\(\\mu\\) along with the estimated \\(\\sigma\\) to in rnorm() to generate the posterior predictive distribution, or the \\(y\\) part of the model. Finally, we’ll take the average of the y_new posterior predictive distribution to get the expectation of the posterior predictive distribution, or epred. It’s the same as what we get when using posterior_epred(); the only differences are because of randomness.\n\nepred_manual &lt;- model_normal |&gt; \n  spread_draws(b_Intercept, b_flipper_length_mm, sigma) |&gt; \n  mutate(mu = b_Intercept + \n           (b_flipper_length_mm * \n              penguins_avg_flipper$flipper_length_mm),  # This is posterior_linpred()\n         y_new = rnorm(n(), mean = mu, sd = sigma))  # This is posterior_predict()\n\n# This is posterior_epred()\nepred_manual |&gt; \n  summarize(epred = mean(y_new))\n## # A tibble: 1 × 1\n##   epred\n##   &lt;dbl&gt;\n## 1 4204.\n\n# It's essentially the same as the actual posterior_epred()\nnormal_epred |&gt; \n  ungroup() |&gt;\n  summarize(epred = mean(.epred))\n## # A tibble: 1 × 1\n##   epred\n##   &lt;dbl&gt;\n## 1 4206.\n\nFor mathy reasons, in Gaussian regression, this \\(\\operatorname{E(y)}\\) happens to be identical to the linear predictor \\(\\mu\\), so the results from posterior_linpred() and posterior_epred() are identical. And—fun fact—the brms code for posterior_epred() for Gaussian models doesn’t recalculate the average of the posterior. It just returns the linear predictor \\(\\mu\\).\nWe can also look at these different types of posterior predictions across a range of possible flipper lengths. There’s a lot more uncertainty in the full posterior, since it incorporates the uncertainty of both \\(\\mu\\) and \\(\\sigma\\), while the uncertainty of the linear predictor/expected value of the posterior is much more narrow (and equivalent in this case):\n\np1 &lt;- penguins |&gt; \n  data_grid(flipper_length_mm = seq_range(flipper_length_mm, n = 100)) |&gt; \n  add_linpred_draws(model_normal, ndraws = 100) |&gt; \n  ggplot(aes(x = flipper_length_mm)) +\n  stat_lineribbon(aes(y = .linpred), .width = 0.95,\n                  alpha = 0.5, color = clrs[3], fill = clrs[3]) +\n  geom_point(data = penguins, aes(y = body_mass_g), size = 1, alpha = 0.7) +\n  scale_y_continuous(labels = label_comma()) +\n  coord_cartesian(ylim = c(2000, 6000)) +\n  labs(x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n       title = \"**Linear predictor** &lt;span style='font-size: 14px;'&gt;*µ* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_linpred()\") +\n  theme_pred_range()\n\np2 &lt;- penguins |&gt; \n  data_grid(flipper_length_mm = seq_range(flipper_length_mm, n = 100)) |&gt; \n  add_epred_draws(model_normal, ndraws = 100) |&gt; \n  ggplot(aes(x = flipper_length_mm)) +\n  stat_lineribbon(aes(y = .epred), .width = 0.95,\n                  alpha = 0.5, color = clrs[2], fill = clrs[2]) +\n  geom_point(data = penguins, aes(y = body_mass_g), size = 1, alpha = 0.7) +\n  scale_y_continuous(labels = label_comma()) +\n  coord_cartesian(ylim = c(2000, 6000)) +\n  labs(x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n       title = \"**Expectation of the posterior** &lt;span style='font-size: 14px;'&gt;E[*y*] and *µ* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_epred()\") +\n  theme_pred_range()\n\np3 &lt;- penguins |&gt; \n  data_grid(flipper_length_mm = seq_range(flipper_length_mm, n = 100)) |&gt; \n  add_predicted_draws(model_normal, ndraws = 100) |&gt; \n  ggplot(aes(x = flipper_length_mm)) +\n  stat_lineribbon(aes(y = .prediction), .width = 0.95,\n                  alpha = 0.5, color = clrs[1], fill = clrs[1]) +\n  geom_point(data = penguins, aes(y = body_mass_g), size = 1, alpha = 0.7) +\n  scale_y_continuous(labels = label_comma()) +\n  coord_cartesian(ylim = c(2000, 6000)) +\n  labs(x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n       title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Normal(*µ*, *σ*)&lt;/span&gt;\",\n       subtitle = \"posterior_predict()\") +\n  theme_pred_range()\n\n(p1 / plot_spacer() / p2 / plot_spacer() / p3) +\n  plot_layout(heights = c(0.3, 0.05, 0.3, 0.05, 0.3))\n\n\n\n\n\n\n\nPhew. There are a lot of moving parts here with different types of posteriors and averages and variances. Here’s a helpful diagram that shows how everything is connected and which R functions calculate which parts:"
  },
  {
    "objectID": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#generalized-linear-models-with-link-transformations",
    "href": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#generalized-linear-models-with-link-transformations",
    "title": "Visualizing the differences between Bayesian posterior predictions, linear predictions, and the expectation of posterior predictions",
    "section": "Generalized linear models with link transformations",
    "text": "Generalized linear models with link transformations\nGeneralized linear models (e.g., logistic, probit, ordered logistic, exponential, Poisson, negative binomial, etc.) use special link functions (e.g. logit, log, etc.) to transform the likelihood of an outcome into a scale that is more amenable to linear regression.\nEstimates from these models can be used in their transformed scales (e.g., log odds in logistic regression) or can be back-transformed into their original scale (e.g., probabilities in logistic regression).\nWhen working with links, the various Bayesian prediction functions return values on different scales, each corresponding to different parts of the model.\nLogistic regression example\nTo show how different link functions work with posteriors from generalized linear models, we’ll use logistic regression with a single explanatory variable (again, for the sake of illustrative simplicity). We’re interested in whether a penguin’s bill length can predict if a penguin is a Gentoo or not. Here’s what the data looks like—Gentoos seem to have taller bills than their Chinstrap and Adélie counterparts.\n\nggplot(penguins, aes(x = bill_length_mm, y = as.numeric(is_gentoo))) +\n  geom_dots(aes(side = ifelse(is_gentoo, \"bottom\", \"top\")), \n            pch = 19, color = \"grey20\", scale = 0.2) +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial(link = \"logit\")),\n              color = clrs[5], se = FALSE) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(x = \"Bill length (mm)\", y = \"Probability of being a Gentoo\") +\n  theme_pred()\n\n\n\n\n\n\n\nWe ultimately want to model that curvy line, but working with regular slopes and intercepts makes it tricky, since the data is all constrained between 0% and 100% and the line is, um, curvy. If we were economists we could just stick a straight line on that graph, call it a linear probability model, and be done. But that’s weird.\nInstead, we can transform the outcome variable from 0s and 1s into logged odds or logits, which creates a nice straight line that we can use with regular old linear regression. Again, I won’t go into the details of how logistic regression works here (see this example or this tutorial or this post or this post for lots more about it).\nJust know that logits (or log odds) are a transformation of probabilities (\\(p\\)) into a different scale using on this formula:\n\\[\n\\operatorname{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n\\]\nThis plot shows the relationship between the two scales. Probabilities range from 0 to 1, while logits typically range from −4 to 4ish, where logit of 0 is a \\(p\\) of 0.5. There are big changes in probability between −4ish and 4ish, but once you start getting into the 5s and beyond, the probability is all essentially the same.\n\ntibble(x = seq(-8, 8, by = 0.1)) |&gt; \n  mutate(y = plogis(x)) |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_line(size = 1, color = clrs[4]) +\n  labs(x = \"Logit scale\", y = \"Probability scale\") +\n  theme_pred()\n\n\n\n\n\n\n\nWe can create a formal model for the probability of being a Gentoo following a binomial distribution with a size of 1 (i.e. the distribution contains only 0s and 1s—either the penguin is a Gentoo or it is not), and a probability \\(\\pi\\) that is conditional on different values of bill length:\n\\[\n\\begin{aligned}\n\\text{Is Gentoo?}_i &\\sim \\operatorname{Binomial}(1, \\pi_i) \\\\\n\\operatorname{logit}(\\pi_i) &= \\alpha + \\beta \\ \\text{Bill length}_i\n\\end{aligned}\n\\]\nOr more generally,\n\\[\n\\begin{aligned}\ny_i &\\sim \\operatorname{Binomial}(1, \\pi_i) \\\\\n\\operatorname{logit}(\\pi_i) &= \\alpha + \\beta x_i\n\\end{aligned}\n\\]\nModel time! Again, we’re using all the default priors here—in real life you’d want to set more official priors for the intercept \\(\\alpha\\) and the coefficient \\(\\beta\\), especially since \\(\\beta\\) is on the logit scale and unlikely to ever be bigger than 3 or 4.\n\nmodel_logit &lt;- brm(\n  bf(is_gentoo ~ bill_length_mm),\n  family = bernoulli(link = \"logit\"),\n  data = penguins\n)\n## Start sampling\n\nWe could look at these coefficients and interpret their marginal effects, but here we’re more interested in the distribution of the outcome, not the coefficients (see here or here or here for examples of how to interpret logistic regression coefficients).\nLet’s again extract these different posterior distributions with the three main brms functions: posterior_linpred(), posterior_epred(), and posterior_predict(). We’ll look at the posterior distribution when bill_length_mm is its average value, or 43.993:\n\n# Make a little dataset of just the average bill length\npenguins_avg_bill &lt;- penguins |&gt; \n  summarize(bill_length_mm = mean(bill_length_mm))\n\n# Extract different types of posteriors\nlogit_linpred &lt;- model_logit |&gt; \n  linpred_draws(newdata = penguins_avg_bill)\n\nlogit_epred &lt;- model_logit |&gt; \n  epred_draws(newdata = penguins_avg_bill)\n\nlogit_predicted &lt;- model_logit |&gt; \n  predicted_draws(newdata = penguins_avg_bill)\n\nThese each show the posterior distribution of being a Gentoo, but unlike the Gaussian posteriors we looked at earlier, each of these is measured completely differently now!\n\nCodesummary_logit_linpred &lt;- logit_linpred |&gt; \n  ungroup() |&gt; \n  summarize(across(.linpred, lst(mean, sd, median), .names = \"{.fn}\"))\n\nsummary_logit_epred &lt;- logit_epred |&gt; \n  ungroup() |&gt; \n  summarize(across(.epred, lst(mean, sd, median), .names = \"{.fn}\"))\n\nsummary_logit_predicted &lt;- logit_predicted |&gt; \n  ungroup() |&gt; \n  summarize(across(.prediction, lst(mean), .names = \"{.fn}\"))\n\ntribble(\n  ~Function, ~`Model element`, ~Values,\n  \"&lt;code&gt;posterior_linpred()&lt;/code&gt;\", \"\\\\(\\\\operatorname{logit}(\\\\pi)\\\\) in the model\", \"Logits or log odds\",\n  \"&lt;code&gt;posterior_linpred(transform = TRUE)&lt;/code&gt; or &lt;code&gt;posterior_epred()&lt;/code&gt;\", \"\\\\(\\\\operatorname{E(y)}\\\\) and \\\\(\\\\pi\\\\) in the model\", \"Probabilities\",\n  \"&lt;code&gt;posterior_predict()&lt;/code&gt;\", \"Random draws from posterior \\\\(\\\\operatorname{Binomial}(1, \\\\pi)\\\\)\", \"0s and 1s\"\n) |&gt; \n  bind_cols(bind_rows(summary_logit_linpred, summary_logit_epred, summary_logit_predicted)) |&gt; \n  kbl(escape = FALSE) |&gt; \n  kable_styling()\n\n\nTable 2: Logit posteriors\n\n\n\n\nFunction\nModel element\nValues\nmean\nsd\nmedian\n\n\n\nposterior_linpred()\n\\(\\operatorname{logit}(\\pi)\\) in the model\nLogits or log odds\n-0.798\n0.138\n-0.796\n\n\n\nposterior_linpred(transform = TRUE) or posterior_epred()\n\n\\(\\operatorname{E(y)}\\) and \\(\\pi\\) in the model\nProbabilities\n0.311\n0.029\n0.311\n\n\nposterior_predict()\nRandom draws from posterior \\(\\operatorname{Binomial}(1, \\pi)\\)\n0s and 1s\n0.306\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(logit_linpred, aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[3]) +\n  coord_cartesian(xlim = c(-1.5, -0.2)) +\n  labs(x = \"Logit-transformed probability of being a Gentoo\", y = NULL,\n       title = \"**Linear predictor** &lt;span style='font-size: 14px;'&gt;logit(*π*) in the model&lt;/span&gt;\",\n       subtitle = \"posterior_linpred(..., tibble(bill_length_mm = 44))\") +\n  theme_pred_dist()\n\np2 &lt;- ggplot(logit_epred, aes(x = .epred)) +\n  stat_halfeye(fill = clrs[2]) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(xlim = c(0.2, 0.45)) +\n  labs(x = \"Probability of being a Gentoo\", y = NULL,\n       title = \"**Expectation of the posterior** &lt;span style='font-size: 14px;'&gt;E[*y*] and *π* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_epred(..., tibble(bill_length_mm = 44))\") +\n  theme_pred_dist()\n\np3 &lt;- logit_predicted |&gt; \n  count(is_gentoo = .prediction) |&gt; \n  mutate(prop = n / sum(n),\n         prop_nice = label_percent(accuracy = 0.1)(prop)) |&gt; \n  ggplot(aes(x = factor(is_gentoo), y = n)) +\n  geom_col(fill = clrs[1]) +\n  geom_text(aes(label = prop_nice), nudge_y = -300, color = \"white\", size = 3) +\n  scale_x_discrete(labels = c(\"Not Gentoo (0)\", \"Gentoo (1)\")) +\n  scale_y_continuous(labels = label_comma()) +\n  labs(x = \"Prediction of being a Gentoo\", y = NULL,\n       title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Binomial(1, *π*)&lt;/span&gt;\",\n       subtitle = \"posterior_predict(..., tibble(bill_length_mm = 44))\") +\n  theme_pred_range() +\n  theme(panel.grid.major.x = element_blank())\n\n(p1 / plot_spacer() / p2 / plot_spacer() / p3) +\n  plot_layout(heights = c(0.3, 0.05, 0.3, 0.05, 0.3))\n\n\n\n\n\n\n\nUnlike the Gaussian/normal regression from earlier, the results from posterior_epred() and posterior_linpred() are not identical here. They still both correspond to the \\(\\pi\\) part of the model, but on different scales. posterior_epred() provides results on the probability scale, un-logiting and back-transforming the results from posterior_linpred() (which provides results on the logit scale).\nAgain, technically, posterior_epred() isn’t just the back-transformed linear predictor (if you want that, you can use posterior_linpred(..., transform = TRUE)). More formally, posterior_epred() returns the expected values of the posterior, or \\(\\operatorname{E(y)}\\), or the average of the posterior’s averages. But as with Gaussian regression, for mathy reasons this average-of-averages happens to be the same as the back-transformed \\(\\pi\\), so \\(E(y) = \\operatorname{inverse logit}(\\pi)\\).\nThe results from posterior_predict() are draws from a random binomial distribution using the estimated \\(\\pi\\), and they consist of only 0s and 1s (not Gentoo and Gentoo).\nShowing these posterior predictions across a range of bill lengths also helps with the intuition here and illustrates the different scales and values that these posterior functions return:\n\n\nposterior_linpred() returns the value of \\(\\pi\\) on the logit scale\n\nposterior_epred() returns the value of \\(\\pi\\) on the probability scale (technically it’s returning \\(\\operatorname{E(y)}\\), but in practice those are identical here)\n\nposterior_predict() returns 0s and 1s, plotted here as points at bill lengths of 35, 45, and 55 mm\n\n\npred_logit_gentoo &lt;- tibble(bill_length_mm = c(35, 45, 55)) |&gt; \n  add_predicted_draws(model_logit, ndraws = 500)\n\npred_logit_gentoo_summary &lt;- pred_logit_gentoo |&gt; \n  group_by(bill_length_mm) |&gt; \n  summarize(prop = mean(.prediction),\n            prop_nice = paste0(label_percent(accuracy = 0.1)(prop), \"\\nGentoos\"))\n\np1 &lt;- penguins |&gt; \n  data_grid(bill_length_mm = seq_range(bill_length_mm, n = 100)) |&gt; \n  add_linpred_draws(model_logit, ndraws = 100) |&gt; \n  ggplot(aes(x = bill_length_mm)) +\n  stat_lineribbon(aes(y = .linpred), .width = 0.95,\n                  alpha = 0.5, color = clrs[3], fill = clrs[3]) +\n  coord_cartesian(xlim = c(30, 60)) +\n  labs(x = \"Bill length (mm)\", y = \"Logit-transformed\\nprobability of being a Gentoo\",\n       title = \"**Linear predictor posterior** &lt;span style='font-size: 14px;'&gt;logit(*π*) in the model&lt;/span&gt;\",\n       subtitle = \"posterior_linpred()\") +\n  theme_pred_range()\n\np2 &lt;- penguins |&gt; \n  data_grid(bill_length_mm = seq_range(bill_length_mm, n = 100)) |&gt; \n  add_epred_draws(model_logit, ndraws = 100) |&gt; \n  ggplot(aes(x = bill_length_mm)) +\n  geom_dots(data = penguins, aes(y = as.numeric(is_gentoo), x = bill_length_mm, \n                                 side = ifelse(is_gentoo, \"bottom\", \"top\")), \n            pch = 19, color = \"grey20\", scale = 0.2) +\n  stat_lineribbon(aes(y = .epred), .width = 0.95,\n                  alpha = 0.5, color = clrs[2], fill = clrs[2]) +\n  scale_y_continuous(labels = label_percent()) +\n  coord_cartesian(xlim = c(30, 60)) +\n  labs(x = \"Bill length (mm)\", y = \"Probability of\\nbeing a Gentoo\",\n       title = \"**Expectation of the posterior** &lt;span style='font-size: 14px;'&gt;E[*y*] and *π* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_epred()\") +\n  theme_pred_range()\n\np3 &lt;- ggplot(pred_logit_gentoo, aes(x = factor(bill_length_mm), y = .prediction)) +\n  geom_point(position = position_jitter(width = 0.2, height = 0.1, seed = 1234),\n             size = 0.75, alpha = 0.3, color = clrs[1]) +\n  geom_text(data = pred_logit_gentoo_summary, aes(y = 0.5, label = prop_nice), size = 3) +  \n  scale_y_continuous(breaks = c(0, 1), labels = c(\"Not\\nGentoo\", \"Gentoo\")) +\n  labs(x = \"Bill length (mm)\", y = \"Prediction of\\nbeing a Gentoo\",\n       title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Binomial(1, *π*)&lt;/span&gt;\",\n       subtitle = \"posterior_predict()\") +\n  theme_pred_range() +\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = 0.5))\n\n(p1 / plot_spacer() / p2 / plot_spacer() / p3) +\n  plot_layout(heights = c(0.3, 0.05, 0.3, 0.05, 0.3))\n\n\n\n\n\n\n\nThere are a lot more moving parts here than with Gaussian regression, with different types of posteriors measured on three different scales! This diagram summarizes everything:"
  },
  {
    "objectID": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#distributional-models-with-link-transformations",
    "href": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#distributional-models-with-link-transformations",
    "title": "Visualizing the differences between Bayesian posterior predictions, linear predictions, and the expectation of posterior predictions",
    "section": "Distributional models with link transformations",
    "text": "Distributional models with link transformations\nRegression models often focus solely on the location parameter of the model (e.g., \\(\\mu\\) in \\(\\operatorname{Normal}(\\mu, \\sigma)\\); \\(\\pi\\) in \\(\\operatorname{Binomial}(n, \\pi)\\)). However, it is also possible to specify separate predictors for the scale or shape parameters of models (e.g., \\(\\sigma\\) in \\(\\operatorname{Normal}(\\mu, \\sigma)\\), \\(\\phi\\) in \\(\\operatorname{Beta}(\\mu, \\phi)\\)). In the world of brms, these are called distributional models.\nMore complex models can use a collection of distributional parameters. Zero-inflated beta models estimate a mean \\(\\mu\\), precision \\(\\phi\\), and a zero-inflated parameter zi, while hurdle lognormal models estimate a mean \\(\\mu\\), scale \\(\\sigma\\), and a hurdle parameter hu. Even plain old Gaussian models become distributional models when a set of predictors is specified for \\(\\sigma\\) (e.g. brm(y ~ x1 + x2, sigma ~ x2 + x3)).\nWhen working with extra distributional parameters, the various Bayesian posterior prediction functions return values on different scales for each different component of the model, making life even more complex! Estimates and distributional parameters (what brms calls dpar in its functions) from these models can be used in their transformed scales or can be back-transformed into their original scale.\nBeta regression example\nTo show how different link functions and distributional parameters work with posteriors from distributional models, we’ll use beta regression with a single explanatory variable. The penguin data we’ve been using doesn’t have any variables that are proportions or otherwise constrained between 0 and 1, so we’ll make one up. Here we’re interested in the the ratio of penguin bill depth (equivalent to the height of the bill; see this illustration) to bill length and whether flipper length influences that ratio. I know nothing about penguins (or birds, for that matter), so I don’t know if biologists even care about the depth/length ratio in bills, but it makes a nice proportion so we’ll go with it.\nHere’s what the relationship looks like—as flipper length increases, the bill ratio decreases. Longer-flippered penguins have shorter and longer bills; shorter-flippered penguins have taller bills in proportion to their lengths. Or something like that.\n\nggplot(penguins, aes(x = flipper_length_mm, y = bill_ratio)) +\n  geom_point(size = 1, alpha = 0.7) +\n  geom_smooth(method = \"lm\", color = clrs[5], se = FALSE) +\n  labs(x = \"Flipper length (mm)\", y = \"Ratio of bill depth / bill length\") +\n  theme_pred()\n\n\n\n\n\n\n\nWe want to model that green line, and in this case it appears nice and straight and could probably be modeled with regular Gaussian regression, but we also want to make sure any predictions are constrained between 0 and 1 since we’re working with a proportion. Beta regression is perfect for this. Once again, I won’t go into detail about how beta models work—I have a whole detailed guide to it here.\nWith beta regression, we need to model two parameters of the beta distribution—the mean \\(\\mu\\) and the precision \\(\\phi\\). Ordinarily beta distributions are actually defined by two other parameters, called either shape 1 and shape 2 or \\(\\alpha\\) and \\(\\beta\\). The two systems of parameters are closely related and you can switch between them with a little algebra—see this guide for an example of how.\nWe can create a formal model for the distribution of the ratio of bill depth to bill length with a beta distribution with a mean \\(\\mu\\) and precision \\(\\phi\\), each of which are conditional on different values of flipper length. The models for \\(\\mu\\) and \\(\\phi\\) don’t have to use the same explanatory variables—I’m just doing that here for the sake of simplicity.\n\\[\n\\begin{aligned}\n\\text{Bill ratio}_i &\\sim \\operatorname{Beta}(\\mu_i, \\phi_i) \\\\\n\\operatorname{logit}(\\mu_i) &= \\alpha_{\\mu} + \\beta_{\\mu} \\ \\text{Flipper length}_i \\\\\n\\log({\\phi}) &= \\alpha_{\\phi} + \\beta_{\\phi} \\ \\text{Flipper length}_i\n\\end{aligned}\n\\]\nOr more generally,\n\\[\n\\begin{aligned}\ny_i &\\sim \\operatorname{Beta}(\\mu_i, \\phi_i) \\\\\n\\operatorname{logit}(\\mu_i) &= \\alpha_{\\mu} + \\beta_{\\mu} x_i \\\\\n\\log({\\phi}) &= \\alpha_{\\phi} + \\beta_{\\phi} x_i\n\\end{aligned}\n\\]\nLet’s fit the model! But first, we’ll actually set more specific priors this time instead of relying on the defaults. Since \\(\\mu\\) is on the logit scale, it’s unlikely to ever have any huge numbers (i.e. anything beyond ±4; recall the probability scale/logit scale plot earlier). The default brms priors for coefficients in beta regression models are flat and uniform, resulting in some potentially huge and implausible priors that lead to really bad model fit (and really slow sampling!). So we’ll help Stan a little here and explicitly tell it that the coefficients will be small (normal(0, 1)) and that \\(\\phi\\) must be positive (exponential(1) with a lower bound of 0).\n\nmodel_beta &lt;- brm(\n  bf(bill_ratio ~ flipper_length_mm,\n     phi ~ flipper_length_mm),\n  family = Beta(),\n  init = \"0\",\n  data = penguins,\n  prior = c(prior(normal(0, 1), class = \"b\"),\n            prior(exponential(1), class = \"b\", dpar = \"phi\", lb = 0))\n)\n## Start sampling\n\nAgain, we don’t care about the coefficients or marginal effects here—see this guide for more about how to work with those. Let’s instead extract these different posterior distributions of bill ratios with the three main brms functions: posterior_linpred(), posterior_epred(), and posterior_predict(). And once again, we’ll use a single value flipper length (the average, 200.967 mm) to explore these distributions.\n\n# Make a little dataset of just the average flipper length\npenguins_avg_flipper &lt;- penguins |&gt; \n  summarize(flipper_length_mm = mean(flipper_length_mm))\n\n# Extract different types of posteriors\nbeta_linpred &lt;- model_beta |&gt; \n  linpred_draws(newdata = penguins_avg_flipper)\n\nbeta_linpred_phi &lt;- model_beta |&gt; \n  linpred_draws(newdata = penguins_avg_flipper, dpar = \"phi\")\n\nbeta_linpred_trans &lt;- model_beta |&gt; \n  linpred_draws(newdata = penguins_avg_flipper, transform = TRUE)\n\nbeta_linpred_phi_trans &lt;- model_beta |&gt; \n  linpred_draws(newdata = penguins_avg_flipper, dpar = \"phi\", transform = TRUE)\n\nbeta_epred &lt;- model_beta |&gt; \n  epred_draws(newdata = penguins_avg_flipper)\n\nbeta_predicted &lt;- model_beta |&gt; \n  predicted_draws(newdata = penguins_avg_flipper)\n\nNotice the addition of two new posteriors here: linpred_draws(..., dpar = \"phi\") and linpred_draws(..., dpar = \"phi\", transform = TRUE). These give us the posterior distributions of the precision (\\(\\phi\\)) distributional parameter, measured on different scales.\nImportantly, for weird historical reasons, it is possible to use posterior_epred(..., dpar = \"phi\") to get the unlogged \\(\\phi\\) parameter. However, conceptually this is wrong. An epred is the expected value, or average, of the posterior predictive distribution, or \\(y\\). It is not the expected value of the \\(\\phi\\) part of the model. brms (or tidybayes) happily spits out the unlogged posterior distribution of \\(\\phi\\) when you use posterior_epred(..., dpar = \"phi\"), but it’s technically not an epred despite its name. To keep the terminology consistent, it’s best to use posterior_linpred() when working with distributional parameters, using either transform = FALSE or transform = TRUE for the logged or the unlogged scale.\n\nCodesummary_beta_linpred &lt;- beta_linpred |&gt; \n  ungroup() |&gt; \n  summarize(across(.linpred, lst(mean, sd, median), .names = \"{.fn}\"))\n\nsummary_beta_linpred_phi &lt;- beta_linpred_phi |&gt; \n  ungroup() |&gt; \n  summarize(across(phi, lst(mean, sd, median), .names = \"{.fn}\"))\n\nsummary_beta_linpred_phi_trans &lt;- beta_linpred_phi_trans |&gt; \n  ungroup() |&gt; \n  summarize(across(phi, lst(mean, sd, median), .names = \"{.fn}\"))\n\nsummary_beta_epred &lt;- beta_epred |&gt; \n  ungroup() |&gt; \n  summarize(across(.epred, lst(mean, sd, median), .names = \"{.fn}\"))\n\nsummary_beta_predicted &lt;- beta_predicted |&gt; \n  ungroup() |&gt; \n  summarize(across(.prediction, lst(mean, sd, median), .names = \"{.fn}\"))\n\ntribble(\n  ~Function, ~`Model element`, ~Values,\n  \"&lt;code&gt;posterior_linpred()&lt;/code&gt;\", \"\\\\(\\\\operatorname{logit}(\\\\mu)\\\\) in the model\", \"Logits or log odds\",\n  \"&lt;code&gt;posterior_linpred(transform = TRUE)&lt;/code&gt; or &lt;code&gt;posterior_epred()&lt;/code&gt;\", \"\\\\(\\\\operatorname{E(y)}\\\\) and \\\\(\\\\mu\\\\) in the model\", \"Probabilities\",\n  '&lt;code&gt;posterior_linpred(dpar = \"phi\")&lt;/code&gt;', \"\\\\(\\\\log(\\\\phi)\\\\) in the model\", \"Logged precision values\",\n  '&lt;code&gt;posterior_linpred(dpar = \"phi\", transform = TRUE)&lt;/code&gt;', \"\\\\(\\\\phi\\\\) in the model\", \"Unlogged precision values\",\n  \"&lt;code&gt;posterior_predict()&lt;/code&gt;\", \"Random draws from posterior \\\\(\\\\operatorname{Beta}(\\\\mu, \\\\phi)\\\\)\", \"Values between 0–1\"\n) |&gt; \n  bind_cols(bind_rows(summary_beta_linpred, summary_beta_epred, \n                      summary_beta_linpred_phi, summary_beta_linpred_phi_trans,\n                      summary_beta_predicted)) |&gt; \n  kbl(escape = FALSE) |&gt; \n  kable_styling()\n\n\nTable 3: Beta posteriors\n\n\n\n\nFunction\nModel element\nValues\nmean\nsd\nmedian\n\n\n\nposterior_linpred()\n\\(\\operatorname{logit}(\\mu)\\) in the model\nLogits or log odds\n-0.423\n0.011\n-0.423\n\n\n\nposterior_linpred(transform = TRUE) or posterior_epred()\n\n\\(\\operatorname{E(y)}\\) and \\(\\mu\\) in the model\nProbabilities\n0.396\n0.003\n0.396\n\n\nposterior_linpred(dpar = \"phi\")\n\\(\\log(\\phi)\\) in the model\nLogged precision values\n4.672\n0.078\n4.675\n\n\nposterior_linpred(dpar = \"phi\", transform = TRUE)\n\\(\\phi\\) in the model\nUnlogged precision values\n107.284\n8.329\n107.259\n\n\nposterior_predict()\nRandom draws from posterior \\(\\operatorname{Beta}(\\mu, \\phi)\\)\nValues between 0–1\n0.397\n0.048\n0.397\n\n\n\n\n\n\n\n\nNeat! We have a bunch of different pieces here, all measured differently. Let’s look at all these different pieces simultaneously:\n\np1 &lt;- ggplot(beta_linpred, aes(x = .linpred)) +\n  stat_halfeye(fill = clrs[3]) +\n  labs(x = \"Logit-scale ratio of bill depth / bill length\", y = NULL,\n       title = \"**Linear predictor** &lt;span style='font-size: 14px;'&gt;logit(*µ*) in the model&lt;/span&gt;\",\n       subtitle = \"posterior_linpred(\\n  ..., tibble(flipper_length_mm = 201))\\n\") +\n  theme_pred_dist()\n\np1a &lt;- ggplot(beta_linpred_phi, aes(x = phi)) +\n  stat_halfeye(fill = colorspace::lighten(clrs[3], 0.3)) +\n  labs(x = \"Log-scale precision parameter\", y = NULL,\n       title = \"**Precision parameter** &lt;span style='font-size: 14px;'&gt;log(*φ*) in the model&lt;/span&gt;\",\n       subtitle = 'posterior_linpred(\\n  ..., tibble(flipper_length_mm = 201),\\n  dpar = \"phi\")') +\n  theme_pred_dist()\n\np2 &lt;- ggplot(beta_epred, aes(x = .epred)) +\n  stat_halfeye(fill = clrs[2]) +\n  labs(x = \"Ratio of bill depth / bill length\", y = NULL,\n       title = \"**Expectation of the posterior** &lt;span style='font-size: 14px;'&gt;E[*y*] or *µ* in the model&lt;/span&gt;\",\n       subtitle = \"posterior_epred(\\n  ..., tibble(flipper_length_mm = 201)) # or \\nposterior_linpred(..., transform = TRUE)\") +\n  theme_pred_dist()\n\np2a &lt;- ggplot(beta_linpred_phi_trans, aes(x = phi)) +\n  stat_halfeye(fill = colorspace::lighten(clrs[2], 0.4)) +\n  labs(x = \"Precision parameter\", y = NULL,\n       title = \"**Precision parameter** &lt;span style='font-size: 14px;'&gt;*φ* in the model&lt;/span&gt;\",\n       subtitle = 'posterior_linpred(\\n  ..., tibble(flipper_length_mm = 201),\\n  dpar = \"phi\", transform = TRUE)\\n') +\n  theme_pred_dist()\n\np3 &lt;- ggplot(beta_predicted, aes(x = .prediction)) +\n  stat_halfeye(fill = clrs[1]) +\n  coord_cartesian(xlim = c(0.2, 0.6)) +\n  labs(x = \"Ratio of bill depth / bill length\", y = NULL,\n       title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Beta(*µ*, *φ*)&lt;/span&gt;\",\n       subtitle = \"posterior_predict()\") +\n  theme_pred_dist()\n\nlayout &lt;- \"\nAB\nCC\nDE\nFF\nGG\n\"\n\np1 + p1a + plot_spacer() + p2 + p2a + plot_spacer() + p3 +\n  plot_layout(design = layout, heights = c(0.3, 0.05, 0.3, 0.05, 0.3))\n\n\n\n\n\n\n\nAs with logistic regression, the results from posterior_epred() and posterior_linpred() are not identical. They still both correspond to the \\(\\mu\\) part of the model, but on different scales. posterior_epred() provides results on the probability or proportion scale, un-logiting and back-transforming the logit-scale results from posterior_linpred().\nAnd once again, posterior_epred() isn’t technically the back-transformed linear predictor (if you want that, you can use posterior_linpred(..., transform = TRUE)). Instead it shows the expected values of the posterior, or \\(\\operatorname{E(y)}\\), or the average of the posterior’s averages. But just like Gaussian regression and logistic regression, this average-of-averages still happens to be the same as the back-transformed \\(\\mu\\), so \\(E(y) = \\operatorname{inverse logit}(\\mu)\\).\nWe can extract the \\(\\phi\\) parameter by including the dpar = \"phi\" argument (or technically just dpar = TRUE, which returns all possible distributional parameters, which is helpful in cases with lots of them like zero-one-inflated beta regression). posterior_linpred(..., dpar = \"phi\", transform = TRUE) provides \\(\\phi\\) on the original precision scale (however that’s measured), while posterior_linpred(..., dpar = \"phi\") returns a log-transformed version.\nAnd finally, the results from posterior_predict() are draws from a random beta distribution using the estimated \\(\\mu\\) and \\(\\phi\\), and they consist of values ranging between 0 and 1.\nShowing the posterior predictions for these different parameters across a range of flipper lengths will help with the intuition and illustrate the different scales, values, and parameters that these posterior functions return:\n\n\nposterior_linpred() returns the value of \\(\\mu\\) on the logit scale\n\nposterior_epred() returns the value of \\(\\mu\\) on the probability scale (technically it’s returning \\(\\operatorname{E(y)}\\), but in practice those are identical here)\n\nposterior_linpred(..., dpar = \"phi\") returns the logged value of \\(\\phi\\)\n\n\nposterior_linpred(..., dpar = \"phi\", transform = TRUE) returns the value of \\(\\phi\\) on its original scale\n\nposterior_predict() returns probabilities or proportions\n\n\np1 &lt;- penguins |&gt; \n  data_grid(flipper_length_mm = seq_range(flipper_length_mm, n = 100)) |&gt; \n  add_linpred_draws(model_beta, ndraws = 100) |&gt; \n  ggplot(aes(x = flipper_length_mm)) +\n  geom_point(data = penguins, aes(y = qlogis(bill_ratio)), size = 1, alpha = 0.7) +\n  stat_lineribbon(aes(y = .linpred), .width = 0.95,\n                  alpha = 0.5, color = clrs[3], fill = clrs[3]) +\n  coord_cartesian(xlim = c(170, 230)) +\n  labs(x = \"Flipper length (mm)\", y = \"Logit-scale ratio of\\nbill depth / bill length\",\n       title = \"**Linear predictor posterior** &lt;span style='font-size: 14px;'&gt;logit(*µ*) in the model&lt;/span&gt;\",\n       subtitle = \"posterior_linpred()\") +\n  theme_pred_range()\n\np1a &lt;- penguins |&gt; \n  data_grid(flipper_length_mm = seq_range(flipper_length_mm, n = 100)) |&gt; \n  add_linpred_draws(model_beta, ndraws = 100, dpar = \"phi\") |&gt; \n  ggplot(aes(x = flipper_length_mm)) +\n  stat_lineribbon(aes(y = phi), .width = 0.95, alpha = 0.5, \n                  color = colorspace::lighten(clrs[3], 0.3), fill = colorspace::lighten(clrs[3], 0.3)) +\n  coord_cartesian(xlim = c(170, 230)) +\n  labs(x = \"Flipper length (mm)\", y = \"Log-scale\\nprecision parameter\",\n       title = \"**Precision parameter** &lt;span style='font-size: 14px;'&gt;log(*φ*) in the model&lt;/span&gt;\",\n       subtitle = 'posterior_linpred(dpar = \"phi\")') +\n  theme_pred_range()\n\np2 &lt;- penguins |&gt; \n  data_grid(flipper_length_mm = seq_range(flipper_length_mm, n = 100)) |&gt; \n  add_epred_draws(model_beta, ndraws = 100) |&gt; \n  ggplot(aes(x = flipper_length_mm)) +\n  geom_point(data = penguins, aes(y = bill_ratio), size = 1, alpha = 0.7) +\n  stat_lineribbon(aes(y = .epred), .width = 0.95,\n                  alpha = 0.5, color = clrs[2], fill = clrs[2]) +\n  coord_cartesian(xlim = c(170, 230)) +\n  labs(x = \"Flipper length (mm)\", y = \"Ratio of\\nbill depth / bill length\",\n       title = \"**Expectation of the posterior** &lt;span style='font-size: 14px;'&gt;E[*y*] or *µ* in the model&lt;/span&gt;\",\n       subtitle = 'posterior_epred()\\nposterior_linpred(transform = TRUE)') +\n  theme_pred_range()\n\np2a &lt;- penguins |&gt; \n  data_grid(flipper_length_mm = seq_range(flipper_length_mm, n = 100)) |&gt; \n  add_epred_draws(model_beta, ndraws = 100, dpar = \"phi\") |&gt; \n  ggplot(aes(x = flipper_length_mm)) +\n  stat_lineribbon(aes(y = phi), .width = 0.95, alpha = 0.5, \n                  color = colorspace::lighten(clrs[2], 0.4), fill = colorspace::lighten(clrs[2], 0.4)) +\n  coord_cartesian(xlim = c(170, 230)) +\n  labs(x = \"Flipper length (mm)\", y = \"Precision parameter\",\n       title = \"**Precision parameter** &lt;span style='font-size: 14px;'&gt;*φ* in the model&lt;/span&gt;\",\n       subtitle = 'posterior_linpred(dpar = \"phi\",\\n                  transform = TRUE)') +\n  theme_pred_range()\n\np3 &lt;- penguins |&gt; \n  data_grid(flipper_length_mm = seq_range(flipper_length_mm, n = 100)) |&gt; \n  add_predicted_draws(model_beta, ndraws = 500) |&gt; \n  ggplot(aes(x = flipper_length_mm)) +\n  geom_point(data = penguins, aes(y = bill_ratio), size = 1, alpha = 0.7) +\n  stat_lineribbon(aes(y = .prediction), .width = 0.95,\n                  alpha = 0.5, color = clrs[1], fill = clrs[1]) +\n  coord_cartesian(xlim = c(170, 230)) +\n  labs(x = \"Flipper length (mm)\", y = \"Ratio of\\nbill depth / bill length\",\n       title = \"**Posterior predictions** &lt;span style='font-size: 14px;'&gt;Random draws from posterior Beta(*µ*, *φ*)&lt;/span&gt;\",\n       subtitle = \"posterior_predict()\") +\n  theme_pred_range()\n\nlayout &lt;- \"\nAB\nCC\nDE\nFF\nGG\n\"\n\np1 + p1a + plot_spacer() + p2 + p2a + plot_spacer() + p3 +\n  plot_layout(design = layout, heights = c(0.3, 0.05, 0.3, 0.05, 0.3))\n\n\n\n\n\n\n\nSo many moving parts in these distributional models! This diagram summarizes all these different posteriors, scales, and distributional parameters:\n\n\n\n\n\n\n\n\nBonus: Playing with posterior beta parameters\nBefore finishing with beta regression, we can play around with some of these posterior parameters to better understand what this kind of distributional model is actually doing. First, we can plot the posterior distribution using the means of the posterior \\(\\mu\\) and \\(\\phi\\) parameters instead of using the results from posterior_predict(), creating a pseudo-analytical posterior distribution. We’ll use the dprop() function from the extraDistr package instead of dbeta(), since dprop uses \\(\\mu\\) and \\(\\phi\\) instead of shape 1 and shape 2.\nIt’s not the greatest model at all—the actual distribution of bill ratios is bimodal (probably because of species-specific differences), but using the posterior values for \\(\\mu\\) and \\(\\phi\\) creates a distribution that picks up the average ratio.\nIn practice we typically don’t actually want to use these two parameters like this—we can use the results from posterior_predict() instead—but it’s cool that we can produce the same distribution with these parameters. That’s the magic of these distributional models!\n\nmu &lt;- summary_beta_epred$mean\nphi &lt;- summary_beta_linpred_phi_trans$mean\n\nggplot(penguins, aes(x = bill_ratio)) +\n  geom_density(aes(fill = \"Actual data\"), color = NA) +\n  stat_function(\n    aes(fill = glue::glue(\"Beta(µ = {round(mu, 3)}, φ = {round(phi, 2)})\")),\n    geom = \"area\", fun = ~ extraDistr::dprop(., mean = mu, size = phi),\n    alpha = 0.7\n  ) +\n  scale_fill_manual(values = c(clrs[5], clrs[1]), name = NULL) +\n  xlim(c(0.2, 0.65)) +\n  labs(x = \"Ratio of bill depth / bill length\", y = NULL,\n       title = \"**Analytical posterior predictions** &lt;span style='font-size: 14px;'&gt;Average posterior *µ* and *φ* from the model&lt;/span&gt;\") +\n  theme_pred_dist() +\n  theme(legend.position = c(0, 0.9),\n        legend.justification = \"left\",\n        legend.key.size = unit(0.75, \"lines\"))\n\n\n\n\n\n\n\nFor even more fun, because we modeled the \\(\\phi\\) parameter as conditional on flipper length, it changes depending on different flipper lengths. This means that the actual posterior beta distribution is shaped differently across a whole range of lengths. Here’s what that looks like, with analytical distributions plotted at 180, 200, and 200 mm. As the precision increases, the distributions become more narrow and precise (which is also reflected in the size of the posterior_predict()-based credible intervals around the points)\n\nmuphi_to_shapes &lt;- function(mu, phi) {\n  shape1 &lt;- mu * phi\n  shape2 &lt;- (1 - mu) * phi\n  return(lst(shape1 = shape1, shape2 = shape2))\n}\n\nbeta_posteriors &lt;- tibble(flipper_length_mm = c(180, 200, 220)) |&gt; \n  add_linpred_draws(model_beta, ndraws = 500, dpar = TRUE, transform = TRUE) |&gt; \n  group_by(flipper_length_mm) |&gt; \n  summarize(across(c(mu, phi), ~mean(.))) |&gt; \n  ungroup() |&gt; \n  mutate(shapes = map2(mu, phi, ~as_tibble(muphi_to_shapes(.x, .y)))) |&gt; \n  unnest(shapes) |&gt; \n  mutate(nice_label = glue::glue(\"Beta(µ = {round(mu, 3)}, φ = {round(phi, 2)})\"))\n\n# Here are the parameters we'll use\n# We need to convert the mu and phi values to shape1 and shape2 so that we can\n# use dist_beta() to plot the halfeye distributions correctly\nbeta_posteriors\n## # A tibble: 3 × 6\n##   flipper_length_mm    mu   phi shape1 shape2 nice_label                \n##               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;glue&gt;                    \n## 1               180 0.485  57.3   27.8   29.5 Beta(µ = 0.485, φ = 57.29)\n## 2               200 0.400 104.    41.5   62.4 Beta(µ = 0.4, φ = 103.92) \n## 3               220 0.320 191.    61.3  130.  Beta(µ = 0.32, φ = 191.31)\n\npenguins |&gt; \n  data_grid(flipper_length_mm = seq_range(flipper_length_mm, n = 100)) |&gt; \n  add_predicted_draws(model_beta, ndraws = 500) |&gt; \n  ggplot(aes(x = flipper_length_mm)) +\n  geom_point(data = penguins, aes(y = bill_ratio), size = 1, alpha = 0.7) +\n  stat_halfeye(data = beta_posteriors, aes(ydist = dist_beta(shape1, shape2), y = NULL), \n               side = \"bottom\", fill = clrs[1], alpha = 0.75) +\n  stat_lineribbon(aes(y = .prediction), .width = 0.95,\n                  alpha = 0.1, color = clrs[1], fill = clrs[1]) +\n  geom_text(data = beta_posteriors, \n            aes(x = flipper_length_mm, y = 0.9, label = nice_label),\n            hjust = 0.5) +\n  coord_cartesian(xlim = c(170, 230)) +\n  labs(x = \"Flipper length (mm)\", y = \"Ratio of\\nbill depth / bill length\",\n       title = \"**Analytical posterior predictions** &lt;span style='font-size: 14px;'&gt;Average posterior *µ* and *φ* from the model&lt;/span&gt;\") +\n  theme_pred_range()"
  },
  {
    "objectID": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#when-posterior_epred-isnt-just-the-back-transformed-linear-predictor",
    "href": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#when-posterior_epred-isnt-just-the-back-transformed-linear-predictor",
    "title": "Visualizing the differences between Bayesian posterior predictions, linear predictions, and the expectation of posterior predictions",
    "section": "When posterior_epred() isn’t just the back-transformed linear predictor",
    "text": "When posterior_epred() isn’t just the back-transformed linear predictor\nIn all the examples in this guide, the results from posterior_epred() have been identical to the back-transformed results from posterior_linpred() (or posterior_linpred(..., transform = TRUE) if there are link functions). With logistic regression, posterior_epred() returned the probability-scale values of \\(\\pi\\); with beta regression, posterior_epred() returned the proportion/probability-scale values of \\(\\mu\\). This is the case for many model families in Stan and brms—for mathy reasons that go beyond my skills, the average of averages \\(\\operatorname{E(y)}\\) is the same as the back-transformed linear predictor for lots of distributions.\nThis isn’t always the case though! In some families, like lognormal models, posterior_epred() and posterior_linpred(..., transform = TRUE) give different estimates. For lognormal models \\(\\operatorname{E(y)}\\) isn’t just one of the distribution’s parameters—it’s this:\n\\[\n\\operatorname{E}(y | \\mid \\mu, \\sigma) = \\exp \\left( \\mu + \\frac{\\sigma^2}{2} \\right)\n\\]\nI won’t show any examples of that here—this guide is already too long—but Matthew Kay has an example here that shows the differences between expected posterior values and back-transformed linear posterior values.\nTo see which kinds of families use fancier epreds, look at the source for brms::posterior_epred() here. Most of the families just use the back-transformed mu (prep\\$dpars\\$mu in the code), but some have special values, like lognormal’s with(prep$dpars, exp(mu + sigma^2 / 2))"
  },
  {
    "objectID": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#tldr-diagrams-and-cheat-sheets",
    "href": "blog/2022/09/26/guide-visualizing-types-posteriors/index.html#tldr-diagrams-and-cheat-sheets",
    "title": "Visualizing the differences between Bayesian posterior predictions, linear predictions, and the expectation of posterior predictions",
    "section": "tl;dr: Diagrams and cheat sheets",
    "text": "tl;dr: Diagrams and cheat sheets\nKeeping track of which kinds of posterior predictions you’re working with, on which scales, and for which parameters, can be tricky, especially with more complex models with lots of moving parts. To make life easier, here are all the summary diagrams in one place:\nNormal Gaussian models\n(Download a PDF) or (download original Adobe Illustrator file)\n\n\n\n\n\n\n\n\nGeneralized linear models with link transformations (logistic regression example)\n(Download a PDF) or (download original Adobe Illustrator file)\n\n\n\n\n\n\n\n\nDistributional models with link transformations (beta regression example)\n(Download a PDF) or (download original Adobe Illustrator file)\n\n\n\n\n\n\n\n\nComplete cheat sheet\nAnd here’s an even more detailed summary cheat sheet as a printable PDF:\n(Download a PDF) or (download the original Adobe InDesign file)"
  },
  {
    "objectID": "blog/2022/12/08/log10-natural-log-scales-ggplot/index.html",
    "href": "blog/2022/12/08/log10-natural-log-scales-ggplot/index.html",
    "title": "How to use natural and base 10 log scales in ggplot2",
    "section": "",
    "text": "I always forget how to deal with logged values in ggplot—particularly things that use the natural log. The {scales} package was invented in part to allow users to adjust axes and scales in plots, including adjusting axes to account for logged values, but there have been some new developments in {scales} that have made existing answers (like this one on StackOverflow) somewhat obsolete (e.g. the trans_breaks() and trans_format() functions used there are superceded and deprecated).\nSo here’s a quick overview of how to use 2022-era {scales} to adjust axis breaks and labels to use both base 10 logs and natural logs. I’ll use data from the Gapminder project, since it has a nice exponentially-distributed measure of GDP per capita.\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(gapminder)\nlibrary(patchwork)\n\n# Just look at one year\ngapminder_2007 &lt;- gapminder |&gt;\n  filter(year == 2007)\n\ntheme_set(theme_bw() + theme(plot.title = element_text(face = \"bold\")))"
  },
  {
    "objectID": "blog/2022/12/08/log10-natural-log-scales-ggplot/index.html#original-unlogged-values",
    "href": "blog/2022/12/08/log10-natural-log-scales-ggplot/index.html#original-unlogged-values",
    "title": "How to use natural and base 10 log scales in ggplot2",
    "section": "Original unlogged values",
    "text": "Original unlogged values\nThe distribution of GDP per capita is heavily skewed, with most countries reporting less than $10,000. As a result, the scatterplot makes an upside-down L shape. Try sticking a regression line on that and you’ll get in trouble.\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  guides(color = \"none\") +\n  labs(title = \"GDP per capita\",\n       subtitle = \"Original non-logged values\")\n\n\n\nScatterplot of GDP per capita and life expectancy. GDP per capita is exponentially distributed so it is heavily skewed with most observations under $10,000. The resulting shape of the plot is not linear."
  },
  {
    "objectID": "blog/2022/12/08/log10-natural-log-scales-ggplot/index.html#log-base-10",
    "href": "blog/2022/12/08/log10-natural-log-scales-ggplot/index.html#log-base-10",
    "title": "How to use natural and base 10 log scales in ggplot2",
    "section": "Log base 10",
    "text": "Log base 10\nggplot comes with a built-in scale_x_log10() to transform the x-axis into logged values. It will automatically create pretty, logical breaks based on the data. Here, the breaks automatically go from 300 → 1000 → 3000 → 10000, and so on:\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10() +\n  guides(color = \"none\") +\n  labs(title = \"GDP per capita, log base 10\",\n       subtitle = \"scale_x_log10()\") +\n  theme(panel.grid.minor = element_blank())\n\n\n\nThe x-axis now shows GDP per capita scaled to log base 10, with axis breaks at 300, 1000, 3000, 100000, and 30000. The relationship is much more linear now.\n\n\n\nIf we want to be mathy about the labels, we can format them as base 10 exponents using label_log():\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10(labels = label_log(digits = 2)) +\n  guides(color = \"none\") +\n  labs(title = \"GDP per capita, log base 10\",\n       subtitle = \"scale_x_log10() with exponentiated labels\") +\n  theme(panel.grid.minor = element_blank())\n\n\n\nThe x-axis shows logged values, but instead of displaying dollar amounts like 300, 1000, etc., it displays exponents like \\(10^{2.5}\\) and \\(10^3\\).\n\n\n\nWhat if we don’t want the default 300, 1000, 3000, etc. breaks? In the interactive plot at gapminder.org, the breaks start at 500 and double after that: 500, 1000, 2000, 4000, 8000, etc. We can control our axis breaks by feeding a list of numbers to scale_x_log10() with the breaks argument. Instead of typing out every possible break, we can generate a list of numbers starting at 500 and then doubling (\\(500 \\times 2^0\\), \\(500 \\times 2^1\\), \\(500 \\times 2^2\\), and so on):\n\n500 * 2^seq(0, 8, by = 1)\n## [1]    500   1000   2000   4000   8000  16000  32000  64000 128000\n\nFor bonus fun, we’ll format the breaks as dollars and use the new-as-of-{scales}-1.2.0 cut_short_scale() to shorten the values:\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10(breaks = 500 * 2^seq(0, 9, by = 1),\n                labels = label_dollar(scale_cut = cut_short_scale())) +\n  guides(color = \"none\") +\n  labs(title = \"GDP per capita, log base 10\",\n       subtitle = \"scale_x_log10() + more logical breaks\") +\n  theme(panel.grid.minor = element_blank())\n\n\n\nThe x-axis shows logged values, but instead using the default automatic breaks at 300, 1000, etc., it has breaks at 500, 1000, 2000, 4000, etc."
  },
  {
    "objectID": "blog/2022/12/08/log10-natural-log-scales-ggplot/index.html#log-base-e-or-the-natural-log",
    "href": "blog/2022/12/08/log10-natural-log-scales-ggplot/index.html#log-base-e-or-the-natural-log",
    "title": "How to use natural and base 10 log scales in ggplot2",
    "section": "Log base \\(e\\), or the natural log",
    "text": "Log base \\(e\\), or the natural log\nLog base 10 makes sense for visualizing things. Seeing the jumps from $500 → $1000 → $2000 is generally easy for people to understand (especially in today’s world of exponentially growing COVID cases). When working with logged values for statistical modeling, analysts prefer to use the natural log, or log base \\(e\\) instead.\n\n\n\n\n\n\nWhat the heck is \\(e\\)?\n\n\n\nHere are a bunch of helpful resources explaining what \\(e\\) and the natural log are and why analysts use them all the time:\n\nNumberphile, “e (Euler’s Number)”\n“An intuitive guide to exponential functions and e”\n“Demystifying the natural logarithm (ln)”\n\n\n\nThe default logging function in R, log(), calculates the natural log (you have to use log10() or log(base = 10) to get base 10 logs).\nPlotting natural logged values is a little trickier than base 10 values, since ggplot doesn’t have anything like scale_x_log_e(). But it’s still doable.\nFirst, we can log the value on our own and just use the default scale_x_continuous() for labeling:\n\nggplot(gapminder_2007, aes(x = log(gdpPercap), y = lifeExp, color = continent)) +\n  geom_point() +\n  guides(color = \"none\") +\n  labs(title = \"GDP per capita, natural log (base e)\",\n       subtitle = \"GDP per capita logged manually\")\n\n\n\nThe x-axis now shows GDP per capita scaled to log base \\(e\\), or the natural log, with axis breaks at 6, 7, 8, 9, 10, and 11. The relationship still linear, just like log base 10, but the values are less interpretable. The values on the x-axis were logged before being fed to ggplot.\n\n\n\nThose 6, 7, 8, etc. breaks in the x-axis represent the power \\(e\\) is raised to, like \\(e^6\\) and \\(e^8\\). We can format these labels as exponents to make that clearer:\n\nggplot(gapminder_2007, aes(x = log(gdpPercap), y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_continuous(labels = label_math(e^.x)) +\n  guides(color = \"none\") +\n  labs(title = \"GDP per capita, natural log (base e)\",\n       subtitle = \"GDP per capita logged manually, exponentiated labels\")\n\n\n\nThe x-axis labels show natural log values as exponents for \\(e\\): \\(e^6\\), \\(e^7\\), and so on. They’re still tricky to interpret, but now it shows that they’re at least based on \\(e\\) instead of being actual values like 6. The values on the x-axis were logged before being fed to ggplot.\n\n\n\nTo get these labels, we have to pre-log GDP per capita. We didn’t need to pre-log the varialb when using scale_x_log10(), since that logs things for us. We can have the scale_x_*() function handle the natural logging for us too by specifying trans = log_trans():\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_continuous(trans = log_trans()) +\n  guides(color = \"none\") +\n  labs(title = \"GDP per capita, natural log (base e)\",\n       subtitle = \"trans = log_trans()\")\n\n\n\nThe values on the x-axis are now logged by ggplot. The x-axis labels are on the dollar scale instead of the log scale. This makes it a little easier to interpret, but the numbers are gross: 1096.633, 8103.084, and 59874.142, or \\(e^7\\), \\(e^9\\), and \\(e^{11}\\)\n\n\n\nEverything is logged as expected, but those labels are gross—they’re \\(e^7\\), \\(e^9\\), and \\(e^{11}\\), but on the dollar scale:\n\nexp(c(7, 9, 11))\n## [1]  1096.633  8103.084 59874.142\n\nWe can format these breaks as \\(e\\)-based exponents instead with label_math() (with the format = log argument to make the formatting function log the values first):\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_continuous(trans = log_trans(),\n                     # This breaks_log() thing happens behind the scenes and\n                     # isn't strictly necessary here\n                     # breaks = breaks_log(base = exp(1)),\n                     labels = label_math(e^.x, format = log)) +\n  guides(color = \"none\") +\n  labs(title = \"GDP per capita, natural log (base e)\",\n       subtitle = \"trans = log_trans(), exponentiated labels\")\n\n\n\nThe x-axis now shows GDP per capita scaled to log base \\(e\\), or the natural log, with automatic axis breaks at 7, 9, and 11. The values on the x-axis are logged automatically with trans = log_trans().\n\n\n\nIf we want more breaks than 7, 9, 11, we can feed the scaling function a list of exponentiated breaks:\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_continuous(trans = log_trans(),\n                     breaks = exp(6:11),\n                     labels = label_math(e^.x, format = log)) +\n  guides(color = \"none\") +\n  labs(title = \"GDP per capita, natural log (base e)\",\n       subtitle = \"trans = log_trans(), exponentiated labels, custom breaks\")\n\n\n\nThe x-axis now shows GDP per capita scaled to log base \\(e\\), or the natural log, with axis breaks at 6, 7, 8, 9, 10, and 11. The values on the x-axis are logged automatically with trans = log_trans()."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "",
    "text": "Pandoc-flavored Markdown makes it really easy to cite and reference things. You can write something like this (assuming you use this references.bib BibTeX file):\nAnd it’ll convert to this after running the document through pandoc:\nThis is all great and ideal when working with documents that have a single bibliography at the end."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html#the-limits-of-default-in-text-citations",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html#the-limits-of-default-in-text-citations",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "The limits of default in-text citations",
    "text": "The limits of default in-text citations\nSome documents—like course syllabuses and readings lists—don’t have a final bibliography. Instead they have lists of things people should read. However, if you try to insert citations like normal, you’ll get the inline references and a final bibliography:\n---\ntitle: \"Some course syllabus\"\nbibliography: references.bib\n---\n\n## Course schedule\n\n### Week 1\n\n- [@Lovelace:1842]\n- [@Turing:1936]\n\n### Week 2\n\n- [@Keynes:1937]\n\n\n\n\n\n\nRendered document\n\n\n\n\nSome course syllabus\n\n\nCourse schedule\n\n\nWeek 1\n\n\n(Lovelace 1842)\n(Turing 1936)\n\n\nWeek 2\n\n\n(Keynes 1937)\n\nReferences\n\n\nKeynes, John Maynard. 1937. “The General Theory of Employment.” The Quarterly Journal of Economics 51 (2): 209–23.\n\n\nLovelace, Augusta Ada. 1842. “Sketch of the Analytical Engine Invented by Charles Babbage, by LF Menabrea, Officer of the Military Engineers, with Notes Upon the Memoir by the Translator.” Taylor’s Scientific Memoirs 3: 666–731.\n\n\nTuring, Alan Mathison. 1936. “On Computable Numbers, with an Application to the Entscheidungsproblem.” Journal of Math 58 (345-363): 230–65.\n\n\n\n\nThe full citations are all in the document, but not in a very convenient location. Readers have to go to the back of the document to see what they actually need to read (especially if there’s a website or DOI URL they need to click on)."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html#making-note-based-styles-appear-in-the-text",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html#making-note-based-styles-appear-in-the-text",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "Making note-based styles appear in the text",
    "text": "Making note-based styles appear in the text\nIt would be great if the full citation could be included in the lists in the document instead of at the end of the document.\nAnd it’s possible, with just a minor tweak to the Citation Style Language (CSL) style file that you’re using (thanks to adam.smith at StackOverflow for pointing out how).\nBy default pandoc uses Chicago author-date for bibiliographic references—hence the (Lovelace 1842) style of references. You can download any other CSL file from Zotero’s searchable style repository, from the Citation Styles project’s searchable list, or clone the full massive GitHub repository of styles to find others, like Chicago notes, APA, MLA, and so on.\nThe easiest way to get full citations inline is to find a CSL that uses note-based citations, like the Chicago full note style and edit the CSL file to tell it to be an inline style instead of a note style.\nThe second line of all CSL files contains a &lt;style&gt; XML element with a class attribute. Inline styles like APA and Chicago author date have class=\"in-text\":\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"display-and-sort\" page-range-format=\"chicago\"&gt;\n  &lt;info&gt;\n    &lt;title&gt;Chicago Manual of Style 17th edition (author-date)&lt;/title&gt;\n    ...\n…while note-based styles like Chicago notes have class=\"note\":\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"note\" version=\"1.0\" demote-non-dropping-particle=\"display-and-sort\" page-range-format=\"chicago\"&gt;\n  &lt;info&gt;\n    &lt;title&gt;Chicago Manual of Style 17th edition (full note)&lt;/title&gt;\n    ...\nIf you download a note-based CSL style and manually change it to be in-text, the footnotes that it inserts will get inserted in the text itself instead of as foonotes.\nHere I downloaded Chicago full note, edited the second line to say class=\"in-text\", and saved it as chicago-syllabus.csl:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;style xmlns=\"http://purl.org/net/xbiblio/csl\" class=\"in-text\" version=\"1.0\" demote-non-dropping-particle=\"display-and-sort\" page-range-format=\"chicago\"&gt;\n  &lt;info&gt;\n    &lt;title&gt;Chicago Manual of Style 17th edition (full note, but in-text)&lt;/title&gt;\n    ...\nI can then tell pandoc to use that CSL when rendering the document:\n---\ntitle: \"Some course syllabus\"\nbibliography: references.bib\ncsl: chicago-syllabus.csl\n---\n\n## Course schedule\n\n### Week 1\n\n- [@Lovelace:1842]\n- [@Turing:1936]\n\n### Week 2\n\n- [@Keynes:1937]\n…and the full references are included in the document itself!\n\n\n\n\n\n\nRendered document\n\n\n\n\nSome course syllabus\n\n\nCourse schedule\n\n\nWeek 1\n\n\nAugusta Ada Lovelace, “Sketch of the Analytical Engine Invented by Charles Babbage, by LF Menabrea, Officer of the Military Engineers, with Notes Upon the Memoir by the Translator,” Taylor’s Scientific Memoirs 3 (1842): 666–731.\nAlan Mathison Turing, “On Computable Numbers, with an Application to the Entscheidungsproblem,” Journal of Math 58, no. 345-363 (1936): 230–65.\n\n\nWeek 2\n\n\nJohn Maynard Keynes, “The General Theory of Employment,” The Quarterly Journal of Economics 51, no. 2 (1937): 209–23.\n\nReferences\n\n\nKeynes, John Maynard. “The General Theory of Employment.” The Quarterly Journal of Economics 51, no. 2 (1937): 209–23.\n\n\nLovelace, Augusta Ada. “Sketch of the Analytical Engine Invented by Charles Babbage, by LF Menabrea, Officer of the Military Engineers, with Notes Upon the Memoir by the Translator.” Taylor’s Scientific Memoirs 3 (1842): 666–731.\n\n\nTuring, Alan Mathison. “On Computable Numbers, with an Application to the Entscheidungsproblem.” Journal of Math 58, no. 345-363 (1936): 230–65."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html#a-few-minor-tweaks-to-perfect-the-output",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html#a-few-minor-tweaks-to-perfect-the-output",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "A few minor tweaks to perfect the output",
    "text": "A few minor tweaks to perfect the output\nThis isn’t quite perfect, though. There are three glaring problems with this:\n\nWe have a bibliography at the end, since Chicago notes-bibliography requires it. This makes sense for regular documents where you have footnotes throughout the body of the text with a list of references at the end, but it’s not necessary here.\nThe in-text references all have hyperlinks to their corresponding references in the final bibliography. We don’t need those since the linked text is the bibliography.\nIf you render this in Quarto, you get helpful popups that contain the full reference when you hover over the link. But again, the link is the full reference, so that extra hover information is redundant.\n\n\n\nCitation reference hovering popup\n\nAll these problems are easy to fix with some additional YAML settings that suppress the final bibliography, turn off citation links, and disable Quarto’s hovering:\n---\ntitle: \"Some course syllabus\"\nbibliography: references.bib\ncsl: chicago-syllabus.csl\nsuppress-bibliography: true\nlink-citations: false\ncitations-hover: false\n---\n\n## Course schedule\n\n### Week 1\n\n- [@Lovelace:1842]\n- [@Turing:1936]\n\n### Week 2\n\n- [@Keynes:1937]\nPerfect!\n\n\n\n\n\n\nPerfect final rendered document\n\n\n\n\nSome course syllabus\n\n\nCourse schedule\n\n\nWeek 1\n\n\nAugusta Ada Lovelace, “Sketch of the Analytical Engine Invented by Charles Babbage, by LF Menabrea, Officer of the Military Engineers, with Notes Upon the Memoir by the Translator,” Taylor’s Scientific Memoirs 3 (1842): 666–731.\nAlan Mathison Turing, “On Computable Numbers, with an Application to the Entscheidungsproblem,” Journal of Math 58, no. 345-363 (1936): 230–65.\n\n\nWeek 2\n\n\nJohn Maynard Keynes, “The General Theory of Employment,” The Quarterly Journal of Economics 51, no. 2 (1937): 209–23."
  },
  {
    "objectID": "blog/2023/01/09/syllabus-csl-pandoc/index.html#using-other-styles",
    "href": "blog/2023/01/09/syllabus-csl-pandoc/index.html#using-other-styles",
    "title": "One Simple Trick™ to create inline bibliography entries with Markdown and pandoc",
    "section": "Using other styles",
    "text": "Using other styles\nThis is all great and super easy if you (like me) are fond of Chicago. What if you want to use APA, though? Or MLA? Or any other style that doesn’t use footnotes?\nFor APA, you’re in luck! There’s an APA (curriculum vitae) CSL style that you can use, and you don’t need to edit it beforehand—it just works:\n---\ntitle: \"Some course syllabus with APA\"\nbibliography: references.bib\ncsl: apa-cv.csl\nsuppress-bibliography: true\nlink-citations: false\ncitations-hover: false\n---\n\n## Course schedule\n\n### Week 1\n\n- [@Lovelace:1842]\n- [@Turing:1936]\n\n### Week 2\n\n- [@Keynes:1937]\n\n\n\n\n\n\nFinal rendered document using APA CV\n\n\n\n\nSome course syllabus with APA\n\n\nCourse schedule\n\n\nWeek 1\n\n\nLovelace, A. A. (1842). Sketch of the analytical engine invented by Charles Babbage, by LF Menabrea, officer of the military engineers, with notes upon the memoir by the translator. Taylor’s Scientific Memoirs, 3, 666–731.\nTuring, A. M. (1936). On computable numbers, with an application to the Entscheidungsproblem. Journal of Math, 58(345-363), 230–265.\n\n\nWeek 2\n\n\nKeynes, J. M. (1937). The general theory of employment. The Quarterly Journal of Economics, 51(2), 209–223.\n\n\n\nFor any other style though, you’re (somewhat) out of luck. The simple trick of switching class=\"note\" to class=\"in-text\" doesn’t work if the underlying style is already in-text like APA or Chicago author-date. You’d have to do some major editing and rearranging in the CSL file to force the bibliography entries to show up as inline citations, which goes beyond my skills.\nAs a workaround you can use the {RefManageR} package in R to read the bibliography file with R and output the bibliography part of the citations as Markdown. Steve Miller has a helpful guide for this here."
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html",
    "title": "Making Middle Earth maps with R",
    "section": "",
    "text": "I’ve taught a course on data visualization with R since 2017, and it’s become one of my more popular classes, especially since it’s all available asynchronously online with hours of Creative Commons-licensed videos and materials. One of the most popular sections of the class (as measured by my server logs and by how often I use it myself) is a section on GIS-related visualization, or how to work with maps in {ggplot2}. Nowadays, since the advent of the {sf} package, I find that making maps with R is incredibly easy and fun.\nI’m also a huge fan of J. R. R. Tolkien and his entire Legendarium (as evidenced by my previous blog post here simulating Aragorn’s human-scale age based on an obscure footnote in Tolkien’s writings about Númenor).\nBack in 2020, as I was polishing up my data visualization course page on visualizing spatial data, I stumbled across a set of shapefiles for Middle Earth, meaning that it was possible to use R and ggplot to make maps of Tolkien’s fictional world. I whipped up a quick example and tweeted about it back then, but then kind of forgot about it.\nWith Twitter dying, and with my recent read of The Fall of Númenor, Middle Earth maps have been on my mind again, so I figured I’d make a more formal didactic blog post about how to make and play with these maps. So consider this blog post a fun little playground for learning more about doing GIS work with {sf} and ggplot, and learn some neat data visualization tricks along the way.\nLet’s put the R in “J. R. R.”"
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#getting-geographic-data",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#getting-geographic-data",
    "title": "Making Middle Earth maps with R",
    "section": "Getting geographic data",
    "text": "Getting geographic data\nShapefiles are everywhere. They’re one of the de facto standard formats for GIS data, and most government agencies provide them for their jurisdictions (see here for a list of some different sources). You can view and edit them graphically with the free and open source QGIS or with the expensive and industry-standard ArcGIS.\nWe’ve already seen how to load shapefiles into R with sf::read_sf(), and that works great. But doing that requires that you go and find and download the shapefiles that you want, which can involve hunting through complicated websites. There are also lots of different R packages that let you get shapefiles directly from different websites’ APIs.\nFor example, we’ve already loaded the 2022 US Census maps by downloading and unzipping the shapefile and using read_sf(). We could have also used the {tigris} package to access the data directly from the Census, like this:\n\nCodelibrary(tigris)\n\nus_states &lt;- states(resolution = \"20m\", year = 2022, cb = TRUE)\n\nlower_48 &lt;- us_states %&gt;% \n  filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n\n\nFor world-level data, Natural Earth has incredibly well-made shapefiles. We could download the 1:50m cultural data from their website, unzip it, and load it with read_sf():\n\nCode# Medium scale data, 1:50m Admin 0 - Countries\n# Download from https://www.naturalearthdata.com/downloads/50m-cultural-vectors/\nworld_map &lt;- read_sf(\"ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\") %&gt;% \n  filter(iso_a3 != \"ATA\")  # Remove Antarctica\n\n\nOr we can use the {rnaturalearth} package to do the same thing:\n\nCodelibrary(rnaturalearth)\n\n# rerturnclass = \"sf\" makes it so the resulting dataframe has the special\n# sf-enabled geometry column\nworld_map &lt;- ne_countries(scale = 50, returnclass = \"sf\") %&gt;% \n  filter(iso_a3 != \"ATA\")  # Remove Antarctica\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThroughout this post, I use {rnaturalearth} for world-level shapefiles and downloaded shapefiles for the US, but that’s just for the sake of illustration. Both can be done with packages or through downloading.\n\n\nAnd finally, for fun, here are some examples of different maps and projections and ggplot tinkering. I’m perpetually astounded by how easy it is to plot GIS data with geom_sf()! That geometry list column is truly magical.\n\nCodelibrary(patchwork)\n\np1 &lt;- ggplot() + \n  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +  # NAD83\n  labs(title = \"NAD83 projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np2 &lt;- ggplot() + \n  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  labs(title = \"Albers projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np3 &lt;- ggplot() +\n  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n  coord_sf(crs = st_crs(\"EPSG:3395\")) +  # Mercator\n  labs(title = \"Mercator projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np4 &lt;- ggplot() +\n  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n  coord_sf(crs = st_crs(\"ESRI:54030\")) +  # Robinson\n  labs(title = \"Robinson projection\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\n(p1 | p2) / (p3 | p4)\n\n\n\nExamples of different North American and world map projections"
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#in-the-united-states",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#in-the-united-states",
    "title": "Making Middle Earth maps with R",
    "section": "In the United States",
    "text": "In the United States\nFirst, let’s stick a scaled-up version of Middle Earth in the United States. For fun, we’ll put the Shire in the geographic center of the US, and we’ll calculate the coordinates for that with R just to show that it’s possible.\nCurrently we have a dataset with 49 rows (48 states + DC). We can use the st_centroid() function to find the center of geographic areas, but if we use it on our current data, we’ll get 49 separate centers. So instead, we’ll melt all the states into one big geographic shape with group_by() and summarize() (using summarize() on the geometry column in an sf dataset combines the geographic areas), and then use st_centroid() on that:\n\nCode# Melt the lower 48 states into one big shape first, then use st_centroid()\nus_dissolved &lt;- lower_48 %&gt;% \n  mutate(country = \"US\") %&gt;%  # Create new column with the country name \n  group_by(country) %&gt;%  # Group by that country name column\n  summarize()  # Collapse all the geographic data into one big blob\nus_dissolved\n## Simple feature collection with 1 feature and 1 field\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -125 ymin: 24.5 xmax: -66.9 ymax: 49.4\n## Geodetic CRS:  WGS 84\n## # A tibble: 1 × 2\n##   country                                                                                geometry\n##   &lt;chr&gt;                                                                        &lt;MULTIPOLYGON [°]&gt;\n## 1 US      (((-68.9 43.8, -68.9 43.8, -68.8 43.8, -68.9 43.9, -68.9 43.9, -68.9 43.8)), ((-71.6...\n\nus_center &lt;- us_dissolved %&gt;% \n  st_geometry() %&gt;%  # Extract the geometry column\n  st_centroid()  # Find the center\nus_center\n## Geometry set for 1 feature \n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -99 ymin: 39.8 xmax: -99 ymax: 39.8\n## Geodetic CRS:  WGS 84\n## POINT (-99 39.8)\n\n\nAccording to these calculations, the center of the contiguous US is 39.751441°N -98.965620°W. Technically that’s not 100% correct—the true location is at 39.833333°N -98.583333°W, but this is close enough (according to Google, it’s 25 miles off). I’m guessing the discrepancy is due to differences in the shapefile—I’m not using the highest resolution possible, and there might be islands I need to account for (or not account for). Who knows.\nHere’s where that is. I’m using the {leaflet} package just for fun here (this post is a showcase of different R-based GIS things, so let’s showcase!):\n\nCodeus_center_plot &lt;- us_dissolved %&gt;% \n  st_centroid() %&gt;% \n  mutate(fancy_coords = format_coords(geometry)) %&gt;% \n  mutate(label = glue(\"&lt;span style='display: block; text-align: center;'&gt;&lt;strong&gt;Roughly of the center of the contiguous US&lt;/strong&gt;\",\n                      \"&lt;br&gt;{fancy_coords}&lt;/span&gt;\"))\n\nleaflet(us_center_plot) %&gt;% \n  setView(lng = st_geometry(us_center_plot)[[1]][1], \n          lat = st_geometry(us_center_plot)[[1]][2], \n          zoom = 4) %&gt;%\n  addTiles() %&gt;% \n  addCircleMarkers(label = ~htmltools::HTML(label),\n                   labelOptions = labelOptions(noHide = TRUE, \n                                               direction = \"top\", \n                                               textOnly = FALSE))\n\n\n\n\n\nNext, we need to transform the Middle Earth data so that it fits on the US map. We need to do a few things to make this work:\n\nDouble all the distances so they match Real World miles\nChange the projection of each of the Middle Earth-related datasets to match the projection of lower_48, or WGS 84, or EPSG:4326\n\nShift the Middle Earth-related datasets so that Hobbiton aligns with the center of the US.\n\nChanging the projection of an {sf}-enabled dataset is super easy with st_transform(). Let’s first transform the CRS for the Hobbiton coordinates:\n\nCodehobbiton_in_us &lt;- hobbiton %&gt;% \n  st_transform(st_crs(lower_48))\n\nhobbiton_in_us %&gt;% st_geometry()\n## Geometry set for 1 feature \n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 3.15 ymin: 9.44 xmax: 3.15 ymax: 9.44\n## Geodetic CRS:  WGS 84\n## POINT (3.15 9.44)\n\n\nNote how the coordinates are now on the decimal degrees scale (3.15, 9.44) instead of the meter scale (515948, 1043820). That’s how the US map is set up, so now we can do GIS math with the two maps.\nNext, we need to calculate the offset from the center of the US and Hobbiton by finding the difference between the two sets of coordinates:\n\nCodeme_to_us &lt;- st_coordinates(us_center) - st_coordinates(hobbiton_in_us)\nme_to_us\n##         X    Y\n## [1,] -102 30.3\n\n\nNow we can use that offset to redefine the geometry column in any Middle Earth-related {sf}-enabled dataset we have. Here’s the process for the places data—it’ll be the same for any of the other shapefiles.\n\nCodeme_places_in_us &lt;- places %&gt;% \n  # Make the Middle Earth data match the US projection\n  st_transform(st_crs(lower_48)) %&gt;%\n  # Just look at a handful of places\n  filter(NAME %in% c(\"Hobbiton\", \"Rivendell\", \"Edoras\", \"Minas Tirith\")) %&gt;% \n  # Double the distances\n  st_set_geometry((st_geometry(.) - st_geometry(hobbiton_in_us)) * 2 + st_geometry(hobbiton_in_us)) %&gt;% \n  # Shift everything around so that Hobbiton is in the center of the US\n  st_set_geometry(st_geometry(.) + me_to_us) %&gt;% \n  # All the geometry math made us lose the projection metadata; set it again\n  st_set_crs(st_crs(lower_48))\n\n\nWe can now stick this US-transformed set of place locations insde a map of the US. (Note the ±70000 values for nudging. I have no idea what scale these are on—they’re not meters or miles (maybe feet? maybe decimal degrees?). I had to tinker with different values until it looked okay.)\n\nCodeggplot() + \n  geom_sf(data = lower_48, fill = \"#FF851B\", color = \"white\", linewidth = 0.25) +\n  geom_sf(data = me_places_in_us) +\n  geom_sf_label(data = filter(me_places_in_us, NAME %in% c(\"Hobbiton\", \"Edoras\")),\n                aes(label = NAME), nudge_x = -70000, hjust = 1) +\n  geom_sf_label(data = filter(me_places_in_us, NAME %in% c(\"Rivendell\", \"Minas Tirith\")),\n                aes(label = NAME), nudge_x = 70000, hjust = 0) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_void()\n\n\n\nMiddle Earth locations placed in the US\n\n\n\nAssuming the Shire is in the middle of Kansas, Rivendell would be near the Mississippi River in Missouri. Rohan is down in southern Arkansas, while Gondor is in southern Alabama.\nWe could be even fancier and reshift all the Middle Earth shapefiles to fit in the US, and then overlay all of Middle Earth on the US, but I won’t do that here. I’ll just stick the coastline on so we can compare the relative sizes of the US and Middle Earth:\n\nCodecoastline_in_us &lt;- coastline %&gt;% \n  st_transform(st_crs(lower_48)) %&gt;%\n  st_set_geometry((st_geometry(.) - st_geometry(hobbiton_in_us)) * 2 + st_geometry(hobbiton_in_us)) %&gt;% \n  st_set_geometry(st_geometry(.) + me_to_us) %&gt;% \n  st_set_crs(st_crs(lower_48))\n\nggplot() + \n  geom_sf(data = lower_48, fill = \"#FF851B\", color = \"white\", linewidth = 0.25) +\n  geom_sf(data = coastline_in_us, linewidth = 0.25) +\n  geom_sf(data = me_places_in_us) +\n  geom_sf_label(data = filter(me_places_in_us, NAME %in% c(\"Hobbiton\", \"Edoras\")),\n                aes(label = NAME), nudge_x = -70000, hjust = 1) +\n  geom_sf_label(data = filter(me_places_in_us, NAME %in% c(\"Rivendell\", \"Minas Tirith\")),\n                aes(label = NAME), nudge_x = 70000, hjust = 0) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_void()\n\n\n\nMiddle Earth locations and borders placed in the US"
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#in-europe",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#in-europe",
    "title": "Making Middle Earth maps with R",
    "section": "In Europe",
    "text": "In Europe\nSticking Middle Earth in the US makes sense because I live in the US, so these relative distances are straightforward to me. (I’m in Georgia, which is the middle of Mordor in the maps above).\nBut Tolkien was from England and lived in Oxford—at 20 Northmoor Road to be precise, or at 51.771004°N -1.260142°W to be even more precise (I found this by going to Google Maps, right clicking on Tolkien’s home, and copying the coordinates). Here’s where that is:\n\nCodetolkien_home &lt;- tribble(\n  ~place, ~lat, ~long,\n  \"Tolkien's home\", 51.771003605142724, -1.2601418874304429\n) %&gt;% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\")) \n\ntolkien_home_plot &lt;- tolkien_home %&gt;% \n  mutate(fancy_coords = format_coords(geometry)) %&gt;% \n  mutate(label = glue(\"&lt;span style='display: block; text-align: center;'&gt;&lt;strong&gt;{place}&lt;/strong&gt;\",\n                      \"&lt;br&gt;{fancy_coords}&lt;/span&gt;\"))\n\nleaflet(tolkien_home_plot) %&gt;% \n  setView(lng = st_geometry(tolkien_home_plot)[[1]][1], \n          lat = st_geometry(tolkien_home_plot)[[1]][2], \n          zoom = 14) %&gt;%\n  addTiles() %&gt;% \n  addCircleMarkers(label = ~htmltools::HTML(label),\n                   labelOptions = labelOptions(noHide = TRUE, \n                                               direction = \"top\", \n                                               textOnly = FALSE))\n\n\n\n\n\nWe can put Hobbiton in Tolkien’s home and then see the relative distances to the rest of Middle Earth from Oxford.\nWe’ll use the Natural Earth data that we loaded at the beginning of this post. We could theoretically filter it to only look at European countries, since it includes a column for continent, but doing so causes all sorts of issues:\n\nRussia is huuuuge\nFrench Guiana is officially part of France, so the map includes a part of South America\nOther countries like Denmark, Norway, and the UK have similar overseas province-like territories, so the map gets even more expanded\n\n\nCodeeurope &lt;- world_map %&gt;% \n  filter(continent == \"Europe\")\n\nggplot() +\n  geom_sf(data = europe)\n\n\n\nUgly map of all European countries\n\n\n\nWe could do some fancy filtering and use more detailed data that splits places like France into separate subdivisions (i.e. one row for continental Europe France, one row for French Guiana, etc.), but that’s a lot of work. So instead, we’ll use coord_sf() to define a window so we can zoom in on just a chunk of Europe. Before, we added some arbitrary number of miles around the coordinates for Hobbiton. This time we’ll use a helpful tool from OpenStreetMap that lets you draw a bounding box on a world map to get coordinates to work with:\n\n\n\n\nOpenStreetMap’s site for exporting bounding box coordinates\n\n\n\nWe can then create a little matrix of coordinates. We’re ultimately going to use the PTRA08 / LAEA Europe projection, which is centered in Portugal and is a good Europe-centric projection, so we’ll convert the list of coordinates to that projection.\n\nCodeeurope_window &lt;- st_sfc(\n  st_point(c(-12.4, 29.31)),  # left (west), bottom (south)\n  st_point(c(44.74, 64.62)),  # right (east), top (north)\n  crs = st_crs(\"EPSG:4326\")   # WGS 84\n) %&gt;% \n  st_transform(crs = st_crs(\"EPSG:5633\")) %&gt;%  # LAEA Europe, centered in Portugal\n  st_coordinates()\neurope_window\n##            X       Y\n## [1,] 2135398 1019399\n## [2,] 5912220 5020959\n\n\nNow we can plot the full world map data and use coord_sf() to limit it to just this window:\n\nCodeggplot() +\n  geom_sf(data = world_map) +\n  coord_sf(crs = st_crs(\"EPSG:5633\"),\n           xlim = europe_window[, \"X\"],\n           ylim = europe_window[, \"Y\"],\n           expand = FALSE)\n\n\n\nWorld map cropped to just show part of Europe\n\n\n\nNeat. Now that we know how to zoom in on Europe, we can go through the same process we did with the US—we’ll convert the Middle Earth shapefiles to the European projection, center Hobbiton on Tolkien’s home in Oxford, double all the distances, and shift everything around.\n\nCode# Convert the Tolkien home coordinates to European coordinates\ntolkien_home &lt;- tolkien_home %&gt;% \n  st_transform(crs = st_crs(\"EPSG:5633\"))\n\n# Convert the Hobbiton coordinates to European coordinates\nhobbiton_in_europe &lt;- hobbiton %&gt;% \n  st_transform(st_crs(\"EPSG:5633\"))\n\n# Find the offset between Tolkien's home and Hobbiton\nme_to_europe &lt;- st_coordinates(tolkien_home) - st_coordinates(hobbiton_in_europe)\n\nme_places_in_europe &lt;- places %&gt;% \n  # Make the Middle Earth data match the Europe projection\n  st_transform(st_crs(\"EPSG:5633\")) %&gt;%\n  # Just look at a handful of places\n  filter(NAME %in% c(\"Hobbiton\", \"Rivendell\", \"Edoras\", \"Minas Tirith\")) %&gt;% \n  # Double the distances\n  st_set_geometry((st_geometry(.) - st_geometry(hobbiton_in_europe)) * 2 + st_geometry(hobbiton_in_europe)) %&gt;% \n  # Shift everything around so that Hobbiton is in Oxford\n  st_set_geometry(st_geometry(.) + me_to_europe) %&gt;% \n  # All the geometry math made us lose the projection metadata; set it again\n  st_set_crs(st_crs(\"EPSG:5633\"))\n\ncoastline_in_europe &lt;- coastline %&gt;% \n  st_transform(st_crs(\"EPSG:5633\")) %&gt;%\n  st_set_geometry((st_geometry(.) - st_geometry(hobbiton_in_europe)) * 2 + st_geometry(hobbiton_in_europe)) %&gt;% \n  st_set_geometry(st_geometry(.) + me_to_europe) %&gt;% \n  st_set_crs(st_crs(\"EPSG:5633\"))\n\n\n\nCodeggplot() + \n  geom_sf(data = world_map, fill = \"#39CCCC\", color = \"white\", linewidth = 0.25) +\n  geom_sf(data = coastline_in_europe, linewidth = 0.25) +\n  geom_sf(data = me_places_in_europe) +\n  geom_sf_label(data = filter(me_places_in_europe, NAME %in% c(\"Hobbiton\", \"Edoras\")),\n                aes(label = NAME), nudge_x = -70000, hjust = 1) +\n  geom_sf_label(data = filter(me_places_in_europe, NAME %in% c(\"Rivendell\", \"Minas Tirith\")),\n                aes(label = NAME), nudge_x = 70000, hjust = 0) +\n  coord_sf(crs = st_crs(\"EPSG:5633\"),\n           xlim = europe_window[, \"X\"],\n           ylim = europe_window[, \"Y\"],\n           expand = FALSE) +\n  theme_void()\n\n\n\nMiddle Earth locations and borders placed in Europe\n\n\n\nWith Hobbiton in Oxford, Rivendell is in north central Germany (near Hanover?), with Rohan in Switzerland and Gondor on the border of Croatia and Bosnia."
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#paths",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#paths",
    "title": "Making Middle Earth maps with R",
    "section": "Paths",
    "text": "Paths\nIt would be really cool to be able to plot the pathways different characters took in each of the books (Bilbo and Thorin’s company; Frodo and Sam; Aragorn, Legolas, and Gimli, etc.). This data exists! The LOTR Project has detailed maps with the pathways of all of the main characters’ journeys. However, it’s not (as far as I can tell) open source or Creative Commons-licensed, and I don’t think the coordinates are directly comparable to the shapefiles from the ME-GIS project. Alas."
  },
  {
    "objectID": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#first-and-second-ages",
    "href": "blog/2023/04/26/middle-earth-mapping-sf-r-gis/index.html#first-and-second-ages",
    "title": "Making Middle Earth maps with R",
    "section": "First and Second Ages",
    "text": "First and Second Ages\nIn addition to The Lord of the Rings and The Hobbit, Tolkien wrote a ton about other ages in Middle Earth (see this super quick crash course in the Ages of Arda from my post on Númenorean ages). A separate mapping project—Arda-Maps—has shapefiles for all three ages of Tolkien’s world, including Valinor, Beleriand, and Númenor.\nHowever, the maps aren’t as detailed as the ME-GIS project, and they’re on a completely different scale. For example, here’s the island of Númenor (featured in Amazon’s The Rings of Power). I downloaded the shapefiles from their GitHub repository—the Second Age shapefiles are buried in QGIS/second age/arda2\nHere I use st_bbox() to create a bounding box of coordinates that I then use to crop the underlying data. This is different from what we did with Europe, where we plotted the whole world map and then zoomed in on just a chunk of western Europe. Here, st_crop() cuts out the geographic data that doesn’t fall within the box (similar to filtering).\n\nCodenumenor_box &lt;- st_bbox(c(xmin = 0.007, xmax = 0.017, ymin = -0.025, ymax = -0.015))\n\nnumenor_outlines &lt;- read_sf(\"data/Arda-Maps/QGIS/second age/arda2/poly_outline.shp\") %&gt;% \n  filter(name == \"Numenor\")\n\nnumenor_rivers &lt;- read_sf(\"data/Arda-Maps/QGIS/second age/arda2/line_river.shp\") %&gt;% \n  st_crop(numenor_box)\n\nnumenor_cities &lt;- read_sf(\"data/Arda-Maps/QGIS/second age/arda2/point_city.shp\") %&gt;% \n  st_crop(numenor_box)\n\nggplot() +\n  geom_sf(data = numenor_outlines, fill = \"#F2CB9B\") +\n  geom_sf(data = numenor_rivers, linewidth = 0.4, color = clr_blue) +\n  geom_sf(data = numenor_cities) +\n  # Use geom_label_repel with the geometry column!\n  ggrepel::geom_label_repel(\n    data = numenor_cities, \n    aes(label = eventname, geometry = geometry),\n    stat = \"sf_coordinates\", seed = 1234,\n    family = \"Overpass ExtraBold\") +\n  annotation_scale(location = \"tl\", bar_cols = c(\"grey30\", \"white\"),\n                   text_family = \"Overpass\",\n                   unit_category = \"imperial\") +\n  annotation_north_arrow(\n    location = \"tl\", pad_y = unit(1.5, \"lines\"),\n    style = north_arrow_fancy_orienteering(fill = c(\"grey30\", \"white\"), \n                                           line_col = \"grey30\",\n                                           text_family = \"Overpass\")) +\n  labs(title = \"Númenor\") +\n  theme_void() +\n  theme(plot.background = element_rect(fill = clr_yellow),\n        plot.title = element_text(family = \"Aniron\", size = rel(2), \n                                  hjust = 0.02))\n\n\n\nFancy map of Númenor\n\n\n\nThe map looks fantastic! But notice the scale bar in the top left corner—in this data, Númenor is only a couple thousand feet wide—less than half a mile. The distances are all way wrong. I could probably scale it up by comparing the projection distances in the Arda Maps’ version of regular Middle Earth with the ME-GIS project’s version of regular Middle Earth and then do some fancy math, but that goes beyond my skills."
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "",
    "text": "In a couple days, I’m going to drive across the country to Utah, my home state. I haven’t been out west with my whole family in four years—not since 2019 when we moved from Spanish Fork, Utah to Atlanta, Georgia. According to Google Maps, it’s a mere 1,900 miles (3,000 kilometers) through the middle of the United States and should take 28 hours, assuming no stops.\nFor bonus fun though, my wife and I decided to make this trip even more adventurous and hit as many exciting stops as possible. So rather than drive through the middle of Kansas, we’re going along the southern border on the way there, visiting New Orleans, Louisiana; San Antonio, Texas; Carlsbad Caverns, New Mexico; the Grand Canyon in Arizona; and then camping for a couple days at Capitol Reef National Park in Southern Utah before finally arriving at my aunt’s house in Spanish Fork, Utah. On the way home, we’re going across the northern part of the United States, visiting my sister in Idaho before heading towards Yellowstone in Wyoming; Devil’s Tower in Wyoming; Mount Rushmore in South Dakota; and Nauvoo, Illinois (Mormons!). It’s going to be an awesome—but way longer—mega road trip.\nTo help keep six kids entertained beyond just plugging them into iPads, my wife made some road trip journals to make the experience more memorable and educational, and we’re bringing a bunch of books and extra materials about each of the stops we’ll be making so they can do research along the way.\nI just finished revamping my online data visualization class this week since the summer semester is starting next week, so visualizing data through maps has been on my mind. In my session on visualizing maps, I added a section on making and visualizing your own geocoded data (i.e. plotting specific cities on a map), so I figured it would be neat to visualize this huge road trip somehow to make some maps to include in the kids’ journals.\nGetting the latitude and longitude coordinates for specific cities or addresses is super easy—you can either right click on Google Maps and copy specific coordinates, or you can automate it with the {tidygeocoder} package in R.\nGetting route coordinates is a lot trickier though, since it involves all sorts of complex algorithms (accounting for road locations, speed limits, traffic, etc.). This is why Google Maps is so invaluable—it does an excellent job of figuring this out. But Google’s data is proprietary.\nThere’s an R package named {mapsapi} that makes it really easy to get data from the Google Maps API, and if you get an API key from Google (following these instructions), you can extract geocoded {sf}-compatible route data from Google with the mp_directions() function. It’s neat and quick and easy, but it’s expensive. When I first started playing with the maps API in R, I racked up $0.50 in charges just with testing a bunch of routing options. The Google Maps API gives you $200 in free credits every month and I was well below that amount, but it still was a little nervewracking to see that it was that expensive in just an hour of tinkering. Also, setting up the API key was fairly complicated (creating a Google Cloud project, setting up billing, enabling specific services, storing the API key securely on my computer, etc.), and I don’t want to go through that hassle in the future.\nI’m a fan of the OpenStreetMap project, and it’s one of the backends (through Nominatim) for {tidygeocoder} (and I used it in a previous blog post showing some coordinates in interactive Leaflet maps). In searching around the internet for open source alternatives to the Google Maps API, I discovered that OpenStreetMap can also do directions and routing through the Open Source Routing Machine (OSRM) project. You can use OSRM’s public demo server for small-scale routing things, or you can install your own local instance through Docker. And super conveniently, there’s also an R package called {osrm} for accessing the OSRM API. This means that it’s possible to pull geocoded routing and directions data into R in an open source (and free!) way.\nSo let’s see how to use {osrm} and make some neat road trip maps with {sf} and {ggplot2}!"
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#who-this-post-is-for",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#who-this-post-is-for",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "Who this post is for",
    "text": "Who this post is for\nHere’s what I assume you know:\n\nYou’re familiar with R and the tidyverse (particularly {dplyr} and {ggplot2}).\nYou’re somewhat familiar with {sf} for working with geographic data. I have a whole tutorial here and a simplified one here and the {sf} documentation has a ton of helpful vignettes and blog posts, and there are also two free books about it: Spatial Data Science and Geocomputation with R. Also check this fantastic post out to learn more about the anatomy of a geometry column with {sf}."
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#packages-and-functions",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#packages-and-functions",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "Packages and functions",
    "text": "Packages and functions\nBefore officially getting started, let’s load all the packages we need and create some helpful functions and variables:\n\nCodelibrary(tidyverse)     # ggplot, dplyr, and friends\nlibrary(sf)            # Handle spatial data in R in a tidy way\nlibrary(tigris)        # Access geographic data from the US Census\nlibrary(tidygeocoder)  # Automated geocoding\nlibrary(osrm)          # Access OSRM through R\nlibrary(ggrepel)       # Nicer non-overlapping labels\nlibrary(glue)          # Easier string interpolation\nlibrary(scales)        # Nicer labeling functions\nlibrary(patchwork)     # Combine plots nicely\nlibrary(ggspatial)     # Nicer map features like scale bars\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Overpass\ntheme_roadtrip &lt;- function() {\n  theme_void(base_family = \"Overpass Light\") +\n    theme(\n      plot.title = element_text(family = \"Overpass\", face = \"bold\", hjust = 0.5),\n      strip.text = element_text(\n        family = \"Overpass ExtraBold\", face = \"plain\",\n        size = rel(1.1), hjust = 0.5)\n    )\n}\n\n# Make labels use Overpass by default\nupdate_geom_defaults(\"label_repel\", \n                     list(family = \"Overpass\",\n                          fontface = \"plain\"))\nupdate_geom_defaults(\"label\", \n                     list(family = \"Overpass\",\n                          fontface = \"plain\"))\n\nupdate_geom_defaults(\"text_repel\", \n                     list(family = \"Overpass\",\n                          fontface = \"plain\"))\nupdate_geom_defaults(\"text\", \n                     list(family = \"Overpass\",\n                          fontface = \"plain\"))\n\n# Yellowstone colors\nclrs &lt;- NatParksPalettes::natparks.pals(\"Yellowstone\")\n\n\n\nCode#' Format duration in minutes and hours\n#'\n#' This function takes a numeric input \\code{x} representing a duration in minutes,\n#' rounds it to the nearest 15 minutes, and formats the result as a string\n#' indicating the number of hours and minutes in the duration.\n#'\n#' @param x A numeric input representing a duration in minutes.\n#' @return A character vector of formatted duration strings.\n#' @examples\n#' fmt_duration(c(93, 1007, 3056))\nfmt_duration &lt;- function(x) {\n  # Round to the nearest 15 minutes\n  n_seconds &lt;- round(seconds(x * 60) / (15 * 60)) * (15 * 60)\n  n_seconds &lt;- seconds_to_period(n_seconds)\n  \n  out &lt;- map_chr(n_seconds, \\(n) {\n    if (seconds(n) &lt;= 59) {\n      # If this is less than an hour, don't format anything with hours\n      glue(\"{MM} minutes\", MM = minute(n))\n    } else {\n      # I only want to format this as a number of hours. If the duration is\n      # longer than 24 hours, seconds_to_period() rolls over into days (i.e.\n      # seconds_to_period(60 * 60 * 24) returns \"1d 0H 0M 0S\"), and it shows\n      # zero hours. So we extract the day part of the period, multiply it by 24,\n      # and add it to the hour component that we want to display\n      extra_day_hours &lt;- day(n) * 24\n  \n      glue(\"{HH} hour{s} {MM} minutes\",\n        HH = scales::label_comma()(hour(n) + extra_day_hours),\n        MM = minute(n),\n        s = ifelse(hour(n) == 1, \"\", \"s\")\n      )\n    }\n  })\n  \n  return(out)\n}\n\nfmt_miles &lt;- scales::label_number(accuracy = 10, suffix = \" miles\", big.mark = \",\")\n\nmiles_to_meters &lt;- function(x) {\n  x * 1609.344\n}\n\nmeters_to_miles &lt;- function(x) {\n  x / 1609.344\n}\n\nkm_to_miles &lt;- function(x) {\n  meters_to_miles(x * 1000)\n}"
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#fixing-state-highlighting",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#fixing-state-highlighting",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "Fixing state highlighting",
    "text": "Fixing state highlighting\nFirst we need to change how we identify the states we’ll cross through. Before, we kept the unique state names, but this stripped away the details about the direction of the trip, so we’ll find all the distinct combinations of direction and state and then join that little dataset to lower_48 to make the visited column:\n\nCodestates_crossed_through_better &lt;- st_intersection(\n  st_transform(lower_48, st_crs(routes_geocoded)),\n  routes_geocoded\n) %&gt;% \n  distinct(direction, NAME)\nstates_crossed_through_better\n##       direction         NAME\n## 4         There      Georgia\n## 10        There    Louisiana\n## 32        There  Mississippi\n## 41        There      Alabama\n## 1         There        Texas\n## 36        There   New Mexico\n## 47        There      Arizona\n## 35        There         Utah\n## 12   Back again        Idaho\n## 35.2 Back again         Utah\n## 15   Back again      Montana\n## 25   Back again      Wyoming\n## 23   Back again South Dakota\n## 16   Back again    Minnesota\n## 14   Back again     Illinois\n## 18   Back again         Iowa\n## 3    Back again     Kentucky\n## 4.1  Back again      Georgia\n## 7    Back again     Missouri\n## 9    Back again    Tennessee\n\nlower_48_highlighted_better &lt;- lower_48 %&gt;% \n  left_join(states_crossed_through_better, by = join_by(NAME)) %&gt;% \n  mutate(visited = !is.na(direction))\nas_tibble(lower_48_highlighted_better)\n## # A tibble: 51 × 12\n##    STATEFP STATENS  AFFGEOID    GEOID STUSPS NAME       LSAD         ALAND      AWATER direction                                                                                 geometry visited\n##    &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;                                                                           &lt;MULTIPOLYGON [°]&gt; &lt;lgl&gt;  \n##  1 48      01779801 0400000US48 48    TX     Texas      00    676685555821 18974391187 There      (((-107 31.9, -107 32, -107 32, -107 32, -106 32, -106 32, -106 32, -106 32, -105 32... TRUE   \n##  2 06      01779778 0400000US06 06    CA     California 00    403673617862 20291712025 &lt;NA&gt;       (((-119 33.5, -118 33.5, -118 33.4, -118 33.4, -118 33.3, -118 33.3, -118 33.3, -118... FALSE  \n##  3 21      01779786 0400000US21 21    KY     Kentucky   00    102266581101  2384240769 Back again (((-89.5 36.6, -89.5 36.6, -89.4 36.6, -89.4 36.6, -89.3 36.6, -89.3 36.6, -89.3 36.... TRUE   \n##  4 13      01705317 0400000US13 13    GA     Georgia    00    149486268417  4418716153 There      (((-85.6 35, -85.5 35, -85.4 35, -85.4 35, -85.3 35, -85.3 35, -85 35, -85 35, -85 3... TRUE   \n##  5 13      01705317 0400000US13 13    GA     Georgia    00    149486268417  4418716153 Back again (((-85.6 35, -85.5 35, -85.4 35, -85.4 35, -85.3 35, -85.3 35, -85 35, -85 35, -85 3... TRUE   \n##  6 55      01779806 0400000US55 55    WI     Wisconsin  00    140292518676 29343193162 &lt;NA&gt;       (((-86.9 45.4, -86.8 45.5, -86.8 45.4, -86.9 45.4, -86.9 45.3, -87 45.4, -86.9 45.4)... FALSE  \n##  7 41      01155107 0400000US41 41    OR     Oregon     00    248630319014  6169061220 &lt;NA&gt;       (((-125 42.8, -124 43, -124 43, -124 43.1, -124 43.2, -124 43.2, -124 43.3, -124 43.... FALSE  \n##  8 29      01779791 0400000US29 29    MO     Missouri   00    178052253239  2487526202 Back again (((-95.8 40.6, -95.5 40.6, -95.4 40.6, -95.3 40.6, -95.2 40.6, -95.1 40.6, -94.9 40.... TRUE   \n##  9 51      01779803 0400000US51 51    VA     Virginia   00    102258178227  8528072639 &lt;NA&gt;       (((-76 37.3, -76 37.4, -76 37.4, -75.9 37.5, -75.9 37.6, -75.9 37.6, -75.9 37.7, -75... FALSE  \n## 10 47      01325873 0400000US47 47    TN     Tennessee  00    106792368794  2322190840 Back again (((-90.3 35, -90.3 35, -90.2 35.1, -90.2 35.1, -90.2 35.1, -90.1 35.1, -90.1 35.2, -... TRUE   \n## # ℹ 41 more rows\n\n\nLet’s see how it works by adding a new geom_sf() layer for the highlighted states. We need to double up here with both lower_48 and lower_48_highlighted_better because if we only plot the highlighted states, each facet will only contain those states and not the underlying US map, which we don’t want.\n\nCodeggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = na.omit(lower_48_highlighted_better), aes(fill = visited)) +\n  geom_sf(data = routes_geocoded, color = clrs[1]) +\n  geom_sf(data = all_stops_unique) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  scale_fill_manual(values = c(\"grey80\"), guide = \"none\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip() +\n  facet_wrap(vars(direction), ncol = 1)\n\n\n\nEach facet now only shows the states corresponding to each direction\n\n\n\nThere, we fixed issue #1. ✅\nIssue #2—the missing endpoint in each facet—is a little trickier to deal with, but still doable. Before messing with the real data, let’s look at a simpler toy example first. Here’s a little dataset with three different “routes” between cities. The coordinates here aren’t actually coordinates—they’re just some arbitrary numbers. But that’s okay—this basically mirrors what we have in routes_geocoded"
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#fixing-missing-destination-cities",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#fixing-missing-destination-cities",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "Fixing missing destination cities",
    "text": "Fixing missing destination cities\n\nCodeexample &lt;- tribble(\n  ~day, ~origin_city, ~destination_city, ~origin_coords, ~destination_coords,\n  1,    \"Atlanta\",    \"Boston\",          1234,           5678,\n  2,    \"Boston\",     \"Chicago\",         5678,           91011,\n  3,    \"Chicago\",    \"Denver\",          91011,          131415\n) \nexample\n## # A tibble: 3 × 5\n##     day origin_city destination_city origin_coords destination_coords\n##   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;                    &lt;dbl&gt;              &lt;dbl&gt;\n## 1     1 Atlanta     Boston                    1234               5678\n## 2     2 Boston      Chicago                   5678              91011\n## 3     3 Chicago     Denver                   91011             131415\n\n\nThe reason we’re not getting the final point in each facet or subset is because there are only three rows here, but four cities. If we plotted the origin_coords column, Atlanta would be missing; if we plotted the destination_coords column, Denver would be missing. We need to manipulate this data so that it has 4 rows (Atlanta, Boston, Chicago, Denver), with the correct coordinates for each city.\nThere’s an easy way to do this with pivot_longer() from {tidyr}. We can pivot with multiple columns that follow a pattern in their names. Here, all four of the columns we care about are are prefixed destination or origin, followed by a _. If we specify these name settings in pivot_longer(), it’ll automatically create a column named type for destination and origin and it’ll put the rest of the data in corresponding columns:\n\nCodeexample %&gt;% \n  pivot_longer(\n    cols = c(origin_city, destination_city, origin_coords, destination_coords),\n    names_to = c(\"type\", \".value\"),\n    names_sep = \"_\"\n  )\n## # A tibble: 6 × 4\n##     day type        city    coords\n##   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;\n## 1     1 origin      Atlanta   1234\n## 2     1 destination Boston    5678\n## 3     2 origin      Boston    5678\n## 4     2 destination Chicago  91011\n## 5     3 origin      Chicago  91011\n## 6     3 destination Denver  131415\n\n\nNow we have one row per city, which is close to what we need. Boston and Chicago are duplicated (since they’re both destination and origin cities), so we need to filter out duplicate cities. There are lots of ways to do this—my preferred way is to group by city and keep the first row in each group using slice():\n\nCodeexample %&gt;% \n  pivot_longer(\n    cols = c(origin_city, destination_city, origin_coords, destination_coords),\n    names_to = c(\"type\", \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  group_by(city) %&gt;% \n  slice(1)\n## # A tibble: 4 × 4\n## # Groups:   city [4]\n##     day type        city    coords\n##   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;\n## 1     1 origin      Atlanta   1234\n## 2     1 destination Boston    5678\n## 3     2 destination Chicago  91011\n## 4     3 destination Denver  131415\n\n\nWoohoo! A dataset with four rows and the correct coordinates for each city. If we could plot this, we’d get both endpoints (Atlanta and Denver).\nThis same double pivot approach (magically!!) works with the special geometry columns in our actual routes data:\n\nCodeall_stops_endpoints &lt;- routes_geocoded %&gt;% \n  pivot_longer(\n    cols = c(origin_city, destination_city, origin_geometry, destination_geometry),\n    names_to = c(\"type\", \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  group_by(direction, city) %&gt;% \n  slice(1) %&gt;% \n  arrange(direction, day, desc(type)) %&gt;% \n  # Use \"geometry\" as the default geometry column instead of the routes\n  st_set_geometry(\"geometry\")\n\nall_stops_endpoints\n## Simple feature collection with 15 features and 11 fields\n## Active geometry column: geometry\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -112 ymin: 29.4 xmax: -84.4 ymax: 44.6\n## Geodetic CRS:  WGS 84\n## # A tibble: 15 × 13\n## # Groups:   direction, city [15]\n##    direction    day route_src route_dst route_duration route_distance                                                                            route_geometry distance_miles distance_text duration_text       type        city                             geometry\n##    &lt;fct&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;                                                                          &lt;LINESTRING [°]&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;               &lt;chr&gt;       &lt;chr&gt;                         &lt;POINT [°]&gt;\n##  1 There          1 src       dst                 513.           753. (-84.4 33.7, -84.5 33.6, -84.8 33.4, -84.9 33.1, -85.1 32.9, -85.4 32.6, -85.7 32.5, -...           468. 470 miles     8 hours 30 minutes  origin      Atlanta, Georgia             (-84.4 33.7)\n##  2 There          1 src       dst                 513.           753. (-84.4 33.7, -84.5 33.6, -84.8 33.4, -84.9 33.1, -85.1 32.9, -85.4 32.6, -85.7 32.5, -...           468. 470 miles     8 hours 30 minutes  destination New Orleans, Louisiana         (-90.1 30)\n##  3 There          2 src       dst                 596.           870. (-90.1 30, -90.3 30, -90.5 30.1, -90.9 30.2, -91.1 30.4, -91.3 30.5, -92.1 30.2, -93.2...           541. 540 miles     10 hours 0 minutes  destination San Antonio, Texas           (-98.5 29.4)\n##  4 There          3 src       dst                 458.           727. (-98.5 29.4, -98.7 29.7, -98.9 30, -99.5 30.3, -99.8 30.5, -99.9 30.5, -100 30.4, -101...           452. 450 miles     7 hours 45 minutes  destination Carlsbad, New Mexico          (-104 32.4)\n##  5 There          4 src       dst                 749.          1100. (-104 32.4, -104 32.5, -104 32.6, -104 32.9, -104 32.9, -104 33.3, -105 33.4, -105 33....           684. 680 miles     12 hours 30 minutes destination Grand Canyon NP, Arizona      (-112 36.1)\n##  6 There          5 src       dst                 510.           628. (-112 36.1, -112 36, -112 36, -112 36, -112 36, -112 35.9, -112 35.9, -112 35.9, -111 ...           390. 390 miles     8 hours 30 minutes  destination Grover, Utah                  (-111 38.2)\n##  7 There          6 src       dst                 197.           273. (-111 38.2, -111 38.2, -111 38.3, -112 38.3, -112 38.4, -112 38.4, -112 38.4, -112 38....           169. 170 miles     3 hours 15 minutes  destination Spanish Fork, Utah            (-112 40.1)\n##  8 Back again     1 src       dst                 262.           411. (-112 40.1, -112 40.2, -112 40.3, -112 40.5, -112 41, -112 41.1, -112 41.2, -112 41.6,...           256. 260 miles     4 hours 15 minutes  origin      Spanish Fork, Utah            (-112 40.1)\n##  9 Back again     1 src       dst                 262.           411. (-112 40.1, -112 40.2, -112 40.3, -112 40.5, -112 41, -112 41.1, -112 41.2, -112 41.6,...           256. 260 miles     4 hours 15 minutes  destination Shelley, Idaho                (-112 43.4)\n## 10 Back again     1 src       dst                 199.           240. (-112 43.4, -112 43.8, -112 43.9, -112 43.9, -112 44, -112 44, -111 44.1, -111 44.2, -...           149. 150 miles     3 hours 15 minutes  destination Yellowstone NP, Wyoming       (-111 44.5)\n## 11 Back again     2 src       dst                 552.           694. (-111 44.5, -111 44.4, -111 44.5, -111 44.5, -110 44.5, -110 44.6, -110 44.5, -110 44....           431. 430 miles     9 hours 15 minutes  destination Devil's Tower, Wyoming        (-105 44.6)\n## 12 Back again     2 src       dst                 161.           215. (-105 44.6, -105 44.6, -105 44.6, -105 44.6, -105 44.5, -105 44.5, -105 44.5, -105 44....           133. 130 miles     2 hours 45 minutes  destination Mount Rushmore, South Dakota  (-103 43.9)\n## 13 Back again     3 src       dst                 548.           866. (-103 43.9, -103 43.9, -103 44, -103 44, -103 44.1, -103 44.1, -103 44.1, -102 44.1, -...           538. 540 miles     9 hours 15 minutes  destination Albert Lea, Minnesota        (-93.4 43.6)\n## 14 Back again     4 src       dst                 359.           487. (-93.4 43.6, -93.3 43.1, -92.7 43.1, -92.7 43, -92.7 43, -92.6 43, -92.5 42.7, -92.5 4...           303. 300 miles     6 hours 0 minutes   destination Nauvoo, Illinois             (-91.4 40.6)\n## 15 Back again     5 src       dst                 847.          1199. (-91.4 40.6, -91.4 40.4, -91.6 40.4, -91.5 39.7, -91.4 39.7, -91.4 39.6, -91 39.2, -90...           745. 750 miles     14 hours 0 minutes  destination Atlanta, Georgia             (-84.4 33.7)\n\n\nAnd plotted:\n\nCodeggplot() +\n  geom_sf(data = lower_48) +\n  geom_sf(data = na.omit(lower_48_highlighted_better), aes(fill = visited)) +\n  geom_sf(data = routes_geocoded, color = clrs[1]) +\n  geom_sf(data = all_stops_endpoints) +\n  geom_label_repel(\n    data = all_stops_endpoints,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\", seed = 1234,\n    size = 3, segment.color = clrs[3], min.segment.length = 0\n  ) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  scale_fill_manual(values = c(\"grey80\"), guide = \"none\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip() +\n  facet_wrap(vars(direction), ncol = 1)\n\n\n\nEach facet now shows the destination city\n\n\n\nIssue #2 fixed. ✅"
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#one-day-of-the-trip",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#one-day-of-the-trip",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "One day of the trip",
    "text": "One day of the trip\nFirst we’ll extract the route, cities, and states for the first day of the trip out there:\n\nCoderoute_day1 &lt;- routes_geocoded %&gt;% \n  filter(direction == \"There\", day == 1)\n\nstops_day1 &lt;- all_stops_endpoints %&gt;% \n  filter(direction == \"There\", day == 1)\n\nstates_crossed_day1 &lt;- st_intersection(\n  st_transform(lower_48, st_crs(route_day1)),\n  route_day1\n) %&gt;% \n  distinct(NAME)\n\nlower_48_highlighted_day1 &lt;- lower_48 %&gt;% \n  mutate(visited = NAME %in% states_crossed_day1$NAME)\n\n\nLet’s plot these to make sure it worked:\n\nCodeggplot() +\n  geom_sf(data = lower_48_highlighted_day1, aes(fill = visited)) +\n  geom_sf(data = route_day1, color = clrs[1]) +\n  geom_sf(data = stops_day1) +\n  geom_sf_label(\n    data = stops_day1, \n    aes(label = city),\n    nudge_y = miles_to_meters(80)\n  ) +\n  scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_roadtrip()\n\n\n\nDrive for the first day on the full US map\n\n\n\nYep, it worked. But we don’t need to show the whole map—we want to zoom in on just the area around the route for the day. To do this, we can use st_bbox() to extract a rectangle of coordinates around the route, creating a bounding box for the map that we can then use with coord_sf() to zoom in.\nst_bbox() returns a named vector of numbers with the x- and y-axis limits:\n\nCodebbox &lt;- st_bbox(route_day1)\nbbox\n##  xmin  ymin  xmax  ymax \n## -90.1  30.0 -84.4  33.7\n\n\nSince the elements are all named, we can use them to specify the different values in coord_sf(). First we’ll temporarily stop using the Albers projection, since the numbers we’ve extracted with st_bbox() are on the WGS 84 (−180 to 180) scale and Albers uses meters, which will make the map not work. (But we’ll fix that in a minute!)\n\nCodeggplot() +\n  geom_sf(data = lower_48_highlighted_day1, aes(fill = visited)) +\n  geom_sf(data = route_day1, color = clrs[1]) +\n  geom_sf(data = stops_day1) +\n  geom_sf_label(\n    data = stops_day1, \n    aes(label = city),\n    # We're not using Albers, so this isn't measured in meters; it's decimal degrees\n    nudge_y = 0.15\n  ) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\"\n  ) +\n  scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n  coord_sf(\n    xlim = c(bbox[\"xmin\"], bbox[\"xmax\"]), \n    ylim = c(bbox[\"ymin\"], bbox[\"ymax\"])\n  ) +\n  theme_roadtrip()\n\n\n\nTrip for the first day, but zoomed in a little too far\n\n\n\nWe’re zoomed in (yay) but the edges of the bounding box window are too close to the points and the route, and the labels are cropped funny. Also, to shift the labels up we had to think in decimal degrees instead of meters, which I can’t do naturally. And also, we can’t change projections—because we’re specifying the bounding box coordinates in decimal degrees, we’re stuck with WGS 84 instead of Albers or any other projection. These are all fixable issues, fortunately.\nTo fix all this, we’ll expand the bounding box window by 150 miles on all sides using st_buffer() and then convert the coordinates to Albers before extracting the coordinates of the window for plotting:\n\nCodebbox_nice &lt;- route_day1 %&gt;% \n  st_bbox() %&gt;%  # Extract the bounding box of the coordinates\n  st_as_sfc() %&gt;%  # Convert the bounding box matrix back to an sf object\n  st_buffer(miles_to_meters(150)) %&gt;%  # Add 150 miles to all sides\n  st_transform(\"ESRI:102003\") %&gt;%  # Switch to Albers\n  st_bbox()  # Extract the bounding box of the expanded box\nbbox_nice\n##     xmin     ymin     xmax     ymax \n##   301851 -1071395  1366321  -105313\n\n\nThese are no longer in decimal degrees, so we can use them with the Albers projection. We’ve also added a 150-mile buffer around the window, so we should have room for all the labels now. Let’s check it:\n\nCodeggplot() +\n  geom_sf(data = lower_48_highlighted_day1, aes(fill = visited)) +\n  geom_sf(data = route_day1, color = clrs[1]) +\n  geom_sf(data = stops_day1) +\n  geom_sf_label(\n    data = stops_day1, \n    aes(label = city),\n    # We're in Albers again, so we can work with meters (or miles)\n    nudge_y = miles_to_meters(30)\n  ) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\",\n    width_hint = 0.4\n  ) +\n  scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n  coord_sf(\n    xlim = c(bbox_nice[\"xmin\"], bbox_nice[\"xmax\"]), \n    ylim = c(bbox_nice[\"ymin\"], bbox_nice[\"ymax\"]),\n    crs = st_crs(\"ESRI:102003\")\n  ) +\n  theme_roadtrip()\n\n\n\nTrip for the first day, correctly zoomed and using the Albers projection\n\n\n\nHeck yeah.\nHowever, the shapes look a little curved and distorted because we’re zoomed in so much. Albers is great for big countries, but on a small scale like this, WGS 84 is probably fine. That’s what Google Maps does—it only starts getting weird and curvy along horizontal latitude lines when you zoom out really far. Let’s do the first day map one last time without the Albers conversion:\n\nCodebbox_nice &lt;- route_day1 %&gt;% \n  st_bbox() %&gt;%  # Extract the bounding box of the coordinates\n  st_as_sfc() %&gt;%  # Convert the bounding box matrix back to an sf object\n  st_buffer(miles_to_meters(150)) %&gt;%  # Add 150 miles to all sides\n  st_bbox()  # Extract the bounding box of the expanded box\n\nggplot() +\n  geom_sf(data = lower_48_highlighted_day1, aes(fill = visited)) +\n  geom_sf(data = route_day1, color = clrs[1]) +\n  geom_sf(data = stops_day1) +\n  geom_sf_label(\n    data = stops_day1, \n    aes(label = city),\n    # We're in WGS 84, so these are decimal degrees\n    nudge_y = 0.5\n  ) +\n  geom_label_repel(\n    data = route_day1,\n    aes(label = distance_text, geometry = route_geometry),\n    stat = \"sf_coordinates\", seed = 12345,\n    family = \"Overpass ExtraBold\", fontface = \"plain\",\n    size = 3.5, fill = colorspace::lighten(clrs[5], 0.8), color = clrs[1], \n    segment.color = \"grey50\", min.segment.length = 0\n  ) +\n  annotation_scale(\n    location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n    unit_category = \"imperial\", text_family = \"Overpass\",\n    width_hint = 0.3\n  ) +\n  scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n  coord_sf(\n    xlim = c(bbox_nice[\"xmin\"], bbox_nice[\"xmax\"]), \n    ylim = c(bbox_nice[\"ymin\"], bbox_nice[\"ymax\"]),\n    crs = st_crs(\"EPSG:4326\")\n  ) +\n  theme_roadtrip()\n\n\n\nTrip for the first day, correctly zoomed and using the WGS 84 projection\n\n\n\nNeat. The northern borders of Georgia, Alabama, and Mississippi are all flat here, which is great."
  },
  {
    "objectID": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#all-the-days-of-the-trip",
    "href": "blog/2023/06/01/geocoding-routing-openstreetmap-r/index.html#all-the-days-of-the-trip",
    "title": "How to make fancy road trip maps with R and OpenStreetMap",
    "section": "All the days of the trip",
    "text": "All the days of the trip\nWe successfully plotted one day of the trip. But we’ll be driving for 11 days (!) and need 11 plots. I don’t want to copy/paste this all code 11 times, so we’ll stick each major step into a function:\n\nCode# Take a set of routes and do some pivoting to create a dataset that includes\n# the start and end points\nexpand_endpoints &lt;- function(routes) {\n  routes %&gt;% \n    pivot_longer(\n    cols = c(origin_city, destination_city, origin_geometry, destination_geometry),\n    names_to = c(\"type\", \".value\"),\n    names_sep = \"_\"\n  ) %&gt;% \n  group_by(city) %&gt;% \n  slice(1) %&gt;% \n  arrange(desc(type)) %&gt;% \n  st_set_geometry(\"geometry\")\n}\n\n# Take a set of routes and return a dataset with a flag marking which states they cross\nfind_states &lt;- function(routes) {\n  # st_intersection() complains about innocuous things so suppress the warnings\n  suppressWarnings({\n    states_crossed &lt;- st_intersection(\n      st_transform(lower_48, st_crs(routes)),\n      routes\n    ) %&gt;% \n      distinct(NAME)\n  })\n  \n  lower_48 %&gt;% \n    mutate(visited = NAME %in% states_crossed$NAME)\n}\n\n# Use routes, stopes, and states to build a plot\nbuild_day_map &lt;- function(routes, stops, states, direction, day) {\n  bbox_nice &lt;- routes %&gt;%\n    st_bbox() %&gt;%  # Extract the bounding box of the coordinates\n    st_as_sfc() %&gt;%  # Convert the bounding box matrix back to an sf object\n    st_buffer(miles_to_meters(150)) %&gt;%  # Add 150 miles to all sides\n    st_bbox()  # Extract the bounding box of the expanded box\n\n  ggplot() +\n    geom_sf(data = states, aes(fill = visited)) +\n    geom_sf(data = routes, color = clrs[1]) +\n    geom_sf(data = stops) +\n    geom_sf_label(\n      data = stops,\n      aes(label = city),\n      # We're in WGS 84, so these are decimal degrees\n      nudge_y = 0.4\n    ) +\n    geom_label_repel(\n      data = routes,\n      aes(label = distance_text, geometry = route_geometry),\n      stat = \"sf_coordinates\", seed = 12345,\n      family = \"Overpass ExtraBold\", fontface = \"plain\",\n      size = 3.5, fill = colorspace::lighten(clrs[5], 0.8), color = clrs[1], \n      segment.color = \"grey50\", min.segment.length = 0\n    ) +\n    annotation_scale(\n      location = \"bl\", bar_cols = c(\"grey30\", \"white\"),\n      unit_category = \"imperial\", text_family = \"Overpass\",\n      width_hint = 0.4\n    ) +\n    scale_fill_manual(values = c(\"grey90\", \"grey80\"), guide = \"none\") +\n    coord_sf(\n      xlim = c(bbox_nice[\"xmin\"], bbox_nice[\"xmax\"]),\n      ylim = c(bbox_nice[\"ymin\"], bbox_nice[\"ymax\"]),\n      crs = st_crs(\"EPSG:4326\")\n    ) +\n    labs(title = glue(\"{direction}, day {day}\")) +\n    theme_roadtrip()\n}\n\n\nWith everything functionalized, we can do some super wild {purrr} magic. If we take routes_geocoded and group it by direction and day (so there’s a group per driving day), we’ll get a list column that contains the geocded coordinates for each day:\n\nCodedaily_plots &lt;- routes_geocoded %&gt;% \n  group_by(direction, day) %&gt;% \n  nest(.key = \"routes\")\n\ndaily_plots\n## # A tibble: 11 × 3\n## # Groups:   direction, day [11]\n##    direction    day routes       \n##    &lt;fct&gt;      &lt;dbl&gt; &lt;list&gt;       \n##  1 There          1 &lt;sf [1 × 12]&gt;\n##  2 There          2 &lt;sf [1 × 12]&gt;\n##  3 There          3 &lt;sf [1 × 12]&gt;\n##  4 There          4 &lt;sf [1 × 12]&gt;\n##  5 There          5 &lt;sf [1 × 12]&gt;\n##  6 There          6 &lt;sf [1 × 12]&gt;\n##  7 Back again     1 &lt;sf [2 × 12]&gt;\n##  8 Back again     2 &lt;sf [2 × 12]&gt;\n##  9 Back again     3 &lt;sf [1 × 12]&gt;\n## 10 Back again     4 &lt;sf [1 × 12]&gt;\n## 11 Back again     5 &lt;sf [1 × 12]&gt;\n\n# Check the first day\ndaily_plots$routes[[1]]\n## Simple feature collection with 1 feature and 9 fields\n## Active geometry column: route_geometry\n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -90.1 ymin: 30 xmax: -84.4 ymax: 33.7\n## Geodetic CRS:  WGS 84\n## # A tibble: 1 × 12\n##   origin_city      origin_geometry destination_geometry destination_city       route_src route_dst route_duration route_distance                                                                            route_geometry distance_miles distance_text duration_text     \n##   &lt;chr&gt;                &lt;POINT [°]&gt;          &lt;POINT [°]&gt; &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;                                                                          &lt;LINESTRING [°]&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;             \n## 1 Atlanta, Georgia    (-84.4 33.7)           (-90.1 30) New Orleans, Louisiana src       dst                 513.           753. (-84.4 33.7, -84.5 33.6, -84.8 33.4, -84.9 33.1, -85.1 32.9, -85.4 32.6, -85.7 32.5, -...           468. 470 miles     8 hours 30 minutes\n\n# Check the first day of the return trip\ndaily_plots$routes[[7]]\n## Simple feature collection with 2 features and 9 fields\n## Active geometry column: route_geometry\n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -112 ymin: 40.1 xmax: -111 ymax: 44.7\n## Geodetic CRS:  WGS 84\n## # A tibble: 2 × 12\n##   origin_city        origin_geometry destination_geometry destination_city        route_src route_dst route_duration route_distance                                                                            route_geometry distance_miles distance_text duration_text     \n##   &lt;chr&gt;                  &lt;POINT [°]&gt;          &lt;POINT [°]&gt; &lt;chr&gt;                   &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;                                                                          &lt;LINESTRING [°]&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;             \n## 1 Spanish Fork, Utah     (-112 40.1)          (-112 43.4) Shelley, Idaho          src       dst                 262.           411. (-112 40.1, -112 40.2, -112 40.3, -112 40.5, -112 41, -112 41.1, -112 41.2, -112 41.6,...           256. 260 miles     4 hours 15 minutes\n## 2 Shelley, Idaho         (-112 43.4)          (-111 44.5) Yellowstone NP, Wyoming src       dst                 199.           240. (-112 43.4, -112 43.8, -112 43.9, -112 43.9, -112 44, -112 44, -111 44.1, -111 44.2, -...           149. 150 miles     3 hours 15 minutes\n\n\nWe can then use purrr::map() and purrr::pmap() to feed that nested list-column data frame through the different functions to expand endpoints, find crossed-through states, and build the daily plot.\n\nCodedaily_plots &lt;- routes_geocoded %&gt;% \n  group_by(direction, day) %&gt;% \n  nest(.key = \"routes\") %&gt;% \n  mutate(\n    stops = map(routes, expand_endpoints),\n    states = map(routes, find_states),\n    plot = pmap(list(routes, stops, states, direction, day), build_day_map)\n  )\n\n\nBefore looking at the plots, check out all these nested datasets! Each day has a dataset for the routes, stops, states, and final plot, and everything is contained here in a data frame. MAGICAL.\n\nCodedaily_plots\n## # A tibble: 11 × 6\n## # Groups:   direction, day [11]\n##    direction    day routes        stops         states         plot  \n##    &lt;fct&gt;      &lt;dbl&gt; &lt;list&gt;        &lt;list&gt;        &lt;list&gt;         &lt;list&gt;\n##  1 There          1 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  2 There          2 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  3 There          3 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  4 There          4 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  5 There          5 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  6 There          6 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  7 Back again     1 &lt;sf [2 × 12]&gt; &lt;sf [3 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  8 Back again     2 &lt;sf [2 × 12]&gt; &lt;sf [3 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n##  9 Back again     3 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n## 10 Back again     4 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;  \n## 11 Back again     5 &lt;sf [1 × 12]&gt; &lt;sf [2 × 11]&gt; &lt;sf [49 × 11]&gt; &lt;gg&gt;\n\n\nWe can extract the plot for any single day with indexing:\n\nCodedaily_plots$plot[[10]]\n\n\n\nTrip for the day 4 of the return trip\n\n\n\nOr we can filter, pull, and pluck like good denizens of the tidyverse:\n\nCodedaily_plots %&gt;% \n  filter(direction == \"Back again\", day == 5) %&gt;% \n  pull(plot) %&gt;% pluck(1)\n\n\n\nTrip for the day 5 of the return trip\n\n\n\nHere’s all of them simultaneously inside a Quarto tabset, just for fun:\n\n\nThe trip there\nThe trip back again\n\n\n\n\n\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5\nDay 6\n\n\n\n\nCodedaily_plots$plot[[1]]\n\n\n\n\n\n\n\n\n\n\nCodedaily_plots$plot[[2]]\n\n\n\n\n\n\n\n\n\n\nCodedaily_plots$plot[[3]]\n\n\n\n\n\n\n\n\n\n\nCodedaily_plots$plot[[4]]\n\n\n\n\n\n\n\n\n\n\nCodedaily_plots$plot[[5]]\n\n\n\n\n\n\n\n\n\n\nCodedaily_plots$plot[[6]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5\n\n\n\n\nCodedaily_plots$plot[[7]]\n\n\n\n\n\n\n\n\n\n\nCodedaily_plots$plot[[8]]\n\n\n\n\n\n\n\n\n\n\nCodedaily_plots$plot[[9]]\n\n\n\n\n\n\n\n\n\n\nCodedaily_plots$plot[[10]]\n\n\n\n\n\n\n\n\n\n\nCodedaily_plots$plot[[11]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nOr, instead of extracting each plot individually like this, we can use wrap_plots() from {patchwork} to combine a whole list of plots automatically, though we have a lot less control over the plots this way—some of the maps use a landscape orientation and some use a portrait orientation, so they’re a big mess when combined like this:\n\nCodedaily_plots %&gt;%\n  pull(plot) %&gt;% \n  wrap_plots()\n\n\n\n\n\n\n\n\nTime to add some maps to road trip journals and finish packing!"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "",
    "text": "In my research, I study international nongovernmental organizations (INGOs) and look at how lots of different institutional and organizational factors influence INGO behavior. For instance, many authoritarian regimes have passed anti-NGO laws and engaged in other forms of legal crackdown, which has forced NGOs to change their programming strategies and their sources of funding.\nIn one ongoing project with Suparna Chaudhry and Marc Dotson, we look at how individual private donors feel about NGOs that face legal crackdown abroad and how the experience of crackdown interacts with donor characteristics (like how much education a person has, whether they’ve traveled abroad, what their income is, etc.) with organizational characteristics (like whether it deals with humanitarian or human rights issues, whether it has an open commitment to transparency, whether it faces legal crackdown abroad, etc.) to shape an individual’s decision to give money to an NGO.1\nWe use a conjoint experiment to test a bunch of different hypotheses related to INGO characteristics and donor behavior2 However, in all my previous stats training, I never learned how to analyze conjoint data or how to test specific causal claims and estimate causal effects with this kind of data. So, in the course of analyzing our INGO conjoint data, I’ve been making a guide for future-me (and anyone else who’s interested) in how to analyze conjoint survey data.\nThis is that guide.\nI explore three main things here:\nThroughout this example, I’ll use data from a political candidate conjoint experiment from Hainmueller, Hopkins, and Yamamoto (2014). The data is available at the Harvard Dataverse and it’s public domain, so you can also download it here if you want to follow along:\nLet’s load some packages and data, create some helper functions and nice theme stuff, and get started!\nlibrary(tidyverse)        # ggplot, dplyr, and friends\nlibrary(haven)            # Read Stata files\nlibrary(broom)            # Convert model objects to tidy data frames\nlibrary(cregg)            # Automatically calculate frequentist conjoint AMCEs and MMs\nlibrary(survey)           # Panel-ish regression models\nlibrary(scales)           # Nicer labeling functions\nlibrary(marginaleffects)  # Calculate marginal effects\nlibrary(broom.helpers)    # Add empty reference categories to tidy model data frames\nlibrary(ggforce)          # For facet_col()\nlibrary(brms)             # The best formula-based interface to Stan\nlibrary(tidybayes)        # Manipulate Stan results in tidy ways\nlibrary(ggdist)           # Fancy distribution plots\nlibrary(patchwork)        # Combine ggplot plots\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Jost\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Jost\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(family = \"Jost\", face = \"bold\"),\n          axis.title = element_text(family = \"Jost Medium\"),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 1),\n          strip.text = element_text(family = \"Jost\", face = \"bold\",\n                                    size = rel(0.75), hjust = 0),\n          strip.background = element_rect(fill = \"grey90\", color = NA))\n}\n\n# Set default theme and font stuff\ntheme_set(theme_nice())\nupdate_geom_defaults(\"text\", list(family = \"Jost\", fontface = \"plain\"))\nupdate_geom_defaults(\"label\", list(family = \"Jost\", fontface = \"plain\"))\n\n# Party colors from the Urban Institute's data visualization style guide, for fun\n# http://urbaninstitute.github.io/graphics-styleguide/\nparties &lt;- c(\"#1696d2\", \"#db2b27\")\n\n# Functions for formatting things as percentage points\nlabel_pp &lt;- label_number(accuracy = 1, scale = 100, \n                         suffix = \" pp.\", style_negative = \"minus\")\n\nlabel_amce &lt;- label_number(accuracy = 0.1, scale = 100, suffix = \" pp.\", \n                           style_negative = \"minus\", style_positive = \"plus\")\n\n# Data from https://doi.org/10.7910/DVN/THJYQR\n# It's public domain\ncandidate &lt;- read_stata(\"data/candidate.dta\") %&gt;% \n  as_factor()  # Convert all the Stata categories to factors\n\n# Make a little lookup table for nicer feature labels\nvariable_lookup &lt;- tribble(\n  ~variable,    ~variable_nice,\n  \"atmilitary\", \"Military\",\n  \"atreligion\", \"Religion\",\n  \"ated\",       \"Education\",\n  \"atprof\",     \"Profession\",\n  \"atmale\",     \"Gender\",\n  \"atinc\",      \"Income\",\n  \"atrace\",     \"Race\",\n  \"atage\",      \"Age\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#average-marginal-component-effect-amce",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#average-marginal-component-effect-amce",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Average marginal component effect (AMCE)",
    "text": "Average marginal component effect (AMCE)\nThis distinction between causal and descriptive estimands makes sense if we look at the notation for the estimands themselves. In the world of do-calculus, causal questions are asked using the \\(\\operatorname{do}()\\) operator, which represents a direct intervention into a data generating process. In this case, the researcher randomly sets the attributes to specific levels—the respondent does not self-select into different conditions or decide for themselves that Candidate 1 is a Catholic lawyer or that Candidate 2 is a Jewish farmer. This thus eliminates selection bias and other external confounding, leaving us with an average causal effect.\nWe can calculate (1) the average outcome when a feature is set to a level we’re interested in (e.g., when religion = Mormon), (2) the average outcome when a feature is set to some reference level (e.g., when religion = none), and (3) find the difference between the two:\n\\[\n\\begin{aligned}\n\\theta =\\ &P [\\text{Candidate selection} \\mid \\operatorname{do}(\\text{Religion} = \\text{none})]\\ - \\\\\n&P[\\text{Candidate selection} \\mid \\operatorname{do}(\\text{Religion} = \\text{Mormon})]\n\\end{aligned}\n\\]\nPractically speaking, the easiest way to think about the average marginal component effect (AMCE) is as categorical coefficients in a linear regression model.\nIn my favorite analogy for thinking about regression, model covariates can either be sliders or switches:\n\n\n\n\n\n\n\n\nNumeric covariates are sliders—a one unit change in X is associated with a \\(\\beta\\) unit change in Y. You can slide that X variable up and down and influence Y accordingly (a 10 unit change in X is associated with a \\(10  \\times \\beta\\) change in Y; a −1 unit change in X is associated with a \\(-\\beta\\) change in Y; and so on). The \\(\\beta\\) is a slider that you can move up and down and see what happens to the outcome.\nCategorical covariates, on the other hand, are switches. One of the categories is omitted and represents the baseline average for that category, or the average when the switch is off. The other category coefficients represent shifts in that baseline, or what happens when the switch is flipped on.\nHere’s a quick super basic reference example with data from {palmerpenguins} (this is not actual conjoint data!). We’ll model penguin weight based on penguin species and sex:\n\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins %&gt;% drop_na(sex)\n\npenguin_model &lt;- lm(body_mass_g ~ species + sex, data = penguins)\ntidy(penguin_model)\n## # A tibble: 4 × 5\n##   term             estimate std.error statistic   p.value\n##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)        3372.       31.4   107.    4.34e-258\n## 2 speciesChinstrap     26.9      46.5     0.579 5.63e-  1\n## 3 speciesGentoo      1378.       39.1    35.2   1.05e-113\n## 4 sexmale             668.       34.7    19.2   8.73e- 56\n\nThere are three species and two sexes of penguins, but we only get coefficients for two species (Chinstrap and Gentoo) and one sex (male) because of how regression works—one category is omitted. The coefficients here represent changes in average body mass when switching on the corresponding category. For sex, the omitted category is “female,” so the coefficient for sexmale shows that male penguins are 667 grams heavier than female penguins, on average. Imagine a “sex” switch—when it’s flipped up to the male position, body mass goes up. The same idea works for species—the omitted species is Adélie, so on average Chinstraps are 27 grams heavier than Adélies, and Gentoos are 1,378 grams heavier than Adélies. We can flip the “Chinstrap” or “Gentoo” switches on and increase body mass accordingly.\nThe nice thing about the slider and switch analogy is that it makes it easier to think about holding everything constant—we have switches for both species and sex, but if we just tinker with one, we’ll get the effect of being a Chinstrap or a Gentoo or a male. It’s like a mixer board:\n\n\n\n\n\n\n\n\nThat’s all standard regression-with-categorical-variables stuff.\nThe magic of AMCEs is that in the case of ordinary least squares (OLS) linear regression without any squared or nonlinear terms, AMCEs are just coefficients. It’s a little more complicated with non-linear terms or models with nonlinear link functions like logistic regression—we’d need to calculate marginal effects to get the AMCEs in those cases (but I’ll explore that a lot more below)."
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Marginal means",
    "text": "Marginal means\nThe descriptive estimand, on the other hand, does not have a \\(\\operatorname{do}()\\) operator, which means that there’s no experimental intervention or causal effect. Instead, we’re working with the observed averages of different levels.\nFor instance, if we wanted to know what proportion of respondents support Mormon candidates, we could calculate this estimand:\n\\[\n\\theta = P(\\text{Candidate selection} \\mid \\text{Religion = Mormon})\n\\]\nOr if we wanted to know the percentage point difference between the probabilities of Mormon and Catholic candidates, we could calculate this estimand:\n\\[\n\\begin{aligned}\n\\theta =\\ &P[\\text{Candidate selection} \\mid \\text{Religion = Mormon}]\\ - \\\\\n&P[\\text{Candidate selection} \\mid \\text{Religion = Catholic}]\n\\end{aligned}\n\\]\nImportantly these aren’t causal estimands—they’re descriptive. They’re also not relative to any baseline level. They’re not regression-style “switches” but instead are group averages.\nWe can see this really quick with the penguin data (again, this isn’t related to conjoint stuff! this is just to help with the intuition). To find these marginal means, we calculate the category-specific means. We can do that without regression with some grouping and summarizing:\n\n# Marginal means for species\npenguins %&gt;% \n  group_by(species) %&gt;% \n  summarize(avg = mean(body_mass_g))\n## # A tibble: 3 × 2\n##   species     avg\n##   &lt;fct&gt;     &lt;dbl&gt;\n## 1 Adelie    3706.\n## 2 Chinstrap 3733.\n## 3 Gentoo    5092.\n\n# Marginal means for sex\npenguins %&gt;% \n  group_by(sex) %&gt;% \n  summarize(avg = mean(body_mass_g))\n## # A tibble: 2 × 2\n##   sex      avg\n##   &lt;fct&gt;  &lt;dbl&gt;\n## 1 female 3862.\n## 2 male   4546.\n\nOr we could use a couple intercept-free models to get the same values. Mathematically this is the same as grouping and summarizing, since regression is ultimately just fancy averaging.\n\n\n\n\n“Many things got easier once I accepted that regression is just fancy averaging.” —@ajeanstevenson\n\n\n\n\nbind_rows(\n  tidy(lm(body_mass_g ~ 0 + species, data = penguins)),\n  tidy(lm(body_mass_g ~ 0 + sex, data = penguins))\n)\n## # A tibble: 5 × 5\n##   term             estimate std.error statistic   p.value\n##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 speciesAdelie       3706.      38.1      97.2 6.88e-245\n## 2 speciesChinstrap    3733.      55.9      66.8 8.16e-194\n## 3 speciesGentoo       5092.      42.2     121.  6.31e-275\n## 4 sexfemale           3862.      56.8      68.0 1.70e-196\n## 5 sexmale             4546.      56.3      80.7 8.39e-220\n\nOr even better, we can use marginal_means() from {marginaleffects} (way more on that below!)\n\nmarginal_means(penguin_model, newdata = c(\"species\", \"sex\"), wts = \"cells\")\n## \n##     Term     Value Mean Std. Error     z Pr(&gt;|z|) 2.5 % 97.5 %\n##  species Adelie    3706       26.2 141.4   &lt;0.001  3655   3758\n##  species Chinstrap 3733       38.4  97.2   &lt;0.001  3658   3808\n##  species Gentoo    5092       29.0 175.5   &lt;0.001  5036   5149\n##  sex     female    3862       24.6 156.7   &lt;0.001  3814   3911\n##  sex     male      4546       24.4 186.1   &lt;0.001  4498   4594\n## \n## Results averaged over levels of: species, sex \n## Columns: term, value, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nRegardless of how we calculate these, the numbers are the same, and they represent average penguin weights in each of the species and both of the sexes. We can answer descriptive questions about the average weight of female penguins or the difference in average weights between Gentoo and Chinstrap penguins. There’s no reference level—there are no regression switches or sliders—these are just averages.\n\n\n\n\n\n\nBonus: Market simulations\n\n\n\nThere’s an extra third option that I didn’t include in Table 1 because it’s not used often in my worlds of public policy and political science, but it is used a lot in marketing. In this approach, researchers use conjoint experiments to simulate the data generating process for an overall “market” of “products” (or candidates in this running example).\nResearchers build rich multilevel models to capture all the dynamics of the different respondent-level characteristics and the experimental features and levels and then create hypothetical “purchasers” with different covariate levels and see which combinations of individual characteristics and product characteristics influence the overall market share of products. Using the candidate experiment example, the market simulation would model the effect of different candidate features (religion, military experience, education, and so on) on the probability of choosing a candidate across individual respondent characteristics like ideology, political preferences, age, education, and so.\nFrom what I’ve seen, this market simulation-based approach is really rare in social science. In fact, this paper (Chaudhry, Dotson, and Heiss 2021) coauthored by Suparna Chaudhry, Marc Dotson, and me is the only political science-y one I know of! In it, we were interested in a host of different factors that drive individual preferences for charitable donations. We generated dozens of hypothetical donor personas and looked at how different categories of respondents felt about different features and how those preferences then influenced the market share of hypothetical nonprofits/NGOs.\nFor example, here we can see the average predicted donation market shares across all donor personas, segmented by persona public affairs knowledge, political ideology, and social trust across different NGO–host government relationships. NGOs with friendly relationships with their host governments will receive a greater share of donations from the market, regardless of individual political ideology or political knowledge and travelling experience.\n\n\nFigure 4 from Chaudhry, Dotson, and Heiss (2021)\n\nReturning to the slider + switch analogy, this kind of market simulation is like an ultimate-super-mega-huge mixer board. We had to create a Shiny dashboard (accessible here) to explore all the different possible outcomes. It’s wild.\n\n\nScreenshot of market simulator"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#relationship-between-amces-and-marginal-means",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#relationship-between-amces-and-marginal-means",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Relationship between AMCEs and marginal means",
    "text": "Relationship between AMCEs and marginal means\nTechnically AMCEs and marginal means measure the same thing, just in different ways. Thinking about this in regression terms helps—with categorical marginal effects, the intercept for the model represents the average outcome for the omitted category, while the coefficient represents the offset from the average. The coefficient is the switch—turning it on changes the baseline reference category average by \\(\\beta\\) amount.\nPenguin example\nHere’s one last example from the non-conjoint penguins data, just to drive the point home. Let’s make a super basic model that predicts weight based on sex only:\n\nmodel_sex_only &lt;- lm(body_mass_g ~ sex, data = penguins)\ntidy(model_sex_only)\n## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    3862.      56.8     68.0  1.70e-196\n## 2 sexmale         683.      80.0      8.54 4.90e- 16\n\nThe intercept shows average weight for female penguins (3862 grams), while the coefficient for sexmale shows the change from that average when the “male” switch is turned on (683 more grams, on average).\nWe can actually use these two pieces of information to find the average penguin weight across both sexes: females are 3862.3 grams, males are 3862.3 + 683.4 = 4546 grams. We can confirm this with a quick group_by() %&gt;% summarize():\n\npenguins %&gt;% \n  group_by(sex) %&gt;% \n  summarize(avg_weight = mean(body_mass_g))\n## # A tibble: 2 × 2\n##   sex    avg_weight\n##   &lt;fct&gt;       &lt;dbl&gt;\n## 1 female      3862.\n## 2 male        4546.\n\nVisualizing this should help even more with the general intuition. The horizontal distance between these two points is the same in each panel (683 grams). In the left panel, female weight is set at 0 since it’s the omitted reference category. In the right panel, both males and females have specific group averages. These two panels are analgous to the conjoint idea of AMCEs and marginal means.\n\nCodep1 &lt;- model_sex_only %&gt;%\n  tidy_and_attach() %&gt;%\n  tidy_add_reference_rows() %&gt;%\n  tidy_add_estimate_to_reference_rows() %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  ggplot(aes(x = estimate, y = term)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  annotate(\n    geom = \"errorbar\",\n    x = 0, xmin = 0, xmax = 683.4, y = 1.5,\n    width = 0.1, color = \"grey70\"\n  ) +\n  annotate(\n    geom = \"label\",\n    x = 683.4 / 2, y = 1.5,\n    label = \"683.4 grams\",\n    size = 3\n  ) +\n  labs(\n    x = \"Grams\", y = NULL,\n    title = \"Relative shift in average\",\n    subtitle = \"Similar to AMCEs\"\n  )\n\np2 &lt;- lm(body_mass_g ~ 0 + sex, data = penguins) %&gt;%\n  tidy(conf.int = TRUE) %&gt;%\n  ggplot(aes(x = estimate, y = term)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  annotate(\n    geom = \"errorbar\",\n    x = 3862, xmin = 3862, xmax = 4546, y = 1.5,\n    width = 0.1, color = \"grey70\"\n  ) +\n  annotate(\n    geom = \"label\",\n    x = 4546 - (683.4/2), y = 1.5,\n    label = \"683.4 grams\",\n    size = 3\n  ) +\n  scale_x_continuous(labels = label_comma()) +\n  labs(\n    x = \"Grams\", y = NULL, \n    title = \"Group averages\",\n    subtitle = \"Similar to marginal means\"\n  )\n\np1 | p2\n\n\n\n\n\n\n\nConjoint AMCEs and marginal means (finally!)\nOkay, with that intuition nailed down, we can finally look at conjoint results. We’ll look at the results from the candidate experiment in Hainmueller, Hopkins, and Yamamoto (2014), which Leeper, Hobolt, and Tilley (2020) replicate and explore in their paper distinguishing between AMCEs and marginal means, and which we explored at the beginning of this post to show how conjoints work (i.e. there are seven candidate attributes like military service history, religion, gender, age, and so on).\nHere’s an excerpt from Figure 1 in Leeper, Hobolt, and Tilley (2020), which shows both the published AMCEs and the Leeper, et al.-calculated marginal means from Hainmueller, Hopkins, and Yamamoto’s original candidate study:\n\n\n\n\n\n\n\n\n\n\nThe AMCEs in the left panel have a direct causal interpretation. In this case, holding all else equal, changing a candidate from not having military service to serving in the military increases the probability of support (or overall favorability) by 0.09, or 9 percentage points. Similarly, changing from a nonreligious candidate to a Mormon candidate decreases the probability of support by 14 percentage points (!!!).4 There is no sex-based effect—changing a candidate from male to female has an AMCE of 0.0.\n4 This study was written and published in 2013, right after the 2012 presidential election between Barack Obama and Mitt Romney, a Mormon who did surprisingly well considering longstanding anti-Mormon sentiment in American politics (for more on Mormons and American politics, see Reeve (2015) and McBride, Rogers, and Erekson (2020)).The marginal means in the right panel don’t rely on a reference category and are all centered around 50%—if there’s no difference between the the levels in a two-level feature, we’d expect the averages for each level to be 50%. We can see this with sex, where both male and female candidates have a 50% probability of selection (or 50% favorability, or however we want to interpret the outcome here).\nWith the AMCEs, we saw a 9 percentage point increase in favorability caused by military service. That same 9-point difference is visible in the marginal means: candidates without military service history have 46% favorability compared to the 54% favorability among those with military history (54−46 = 9).\nThe presence of a reference category doesn’t matter much when dealing with binary treatments like sex and military service here. If we reversed the reference category, we’d get the same causal effect but in reverse—not serving in the military causes a 9 percentage point drop in favorability.\nThe reference category matters a lot, however, in attributes with more than two levels, like religion. In the AMCE panel, all the causal effects are in reference to a candidate with no religion. Being Mormon causes a 14 percentage point drop from having no religion; being Evangelical causes a 12 percentage point drop from having no religion; and so on. Deciding which level to use as the omitted reference category matters and can seriously change the causal interpretation of the AMCEs. For instance, here it looks like there’s a serious penalty for being Mormon or Evangelical, while being Protestant, Catholic, or Jewish doesn’t matter. But that’s only the case from a certain point of view. If an Evangelical candidate were the reference category, there wouldn’t be any significant Mormon effect (but there would be a Protestant, Catholic, Jewish, and None effect).\n\n\n\n\n“What I told you was true… from a certain point of view”—Obi-Wan Kenobi\n\nThe reference category gets in the way of making descriptive statements like “Mormon candidates see 42% favorability, while Jewish candidates see 52% favorability.” You can’t get numbers like that from the AMCEs alone unless you know the underlying favorability of the reference category and then reconstruct the other categories’ averages by hand. But researchers working with conjoint experiments try to do this with AMCEs all the time. Leeper, Hobolt, and Tilley (2020) argue that\n\nAMCEs are relative, not absolute, statements about preferences. As such, there is simply no predictable connection between subgroup causal effects and the levels of underlying subgroup preferences. Yet, analysts and their readers frequently interpret differences in conditional AMCEs as differences in underlying preferences (Leeper, Hobolt, and Tilley 2020, 214).\n\nThe key point is this: AMCEs are relative statements, not absolute statements. If we want to talk about the causal effect of moving one attribute level (None → Mormon, None → Catholic, Male → Female, etc.), we can use AMCEs. If we want to talk about general preferences or favorabilities or probabilities or average outcomes, we need to talk about marginal means."
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces-and-marginal-means-across-subgroups",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces-and-marginal-means-across-subgroups",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "AMCEs and marginal means across subgroups",
    "text": "AMCEs and marginal means across subgroups\nThis issue with relative vs. absolute estimands is especially complex when thinking about subgroups or controlling for respondent-level characteristics like political ideology, sex, education, and so on. Suppose we want to know if this Mormon penalty is bigger among Democratic respondents than Republican respondents. We could control for respondent ideology, or calculate two different AMCEs—one when Democrat = true and one when Republican = true. This seems all good and logical. And it it’s fine if you’re talking about causal effects. But once you start trying to compare overall trends across respondent-level subgroups, AMCEs will give you wrong estimates! The main argument in Leeper, Hobolt, and Tilley (2020) is that looking at AMCEs across respondent subgroups doesn’t work people think it does because AMCEs are relative and not absolute. The difference between the relative Mormon candidate AMCE among Democratic-leaning and Republican-leaning respondents isn’t really comparable to the party-based differences in other levels (Evangelicals, Catholics, etc.).\nLeeper, Hobolt, and Tilley (2020) illustrate this idea in a neat way in their Figure 2 (included below). This figure recreates the results for the “candidate sex” feature in a different candidate conjoint experiment done by Teele, Kalla, and Rosenbluth (2018). In this case, unlike our running example from Hainmueller, Hopkins, and Yamamoto (2014)’s study, there is a sex effect—a candidate being female causes a 4.5 percentage point increase in favorability. We can see this in the top two panels, both as an AMCE of 0.045 and as marginal means (female = 0.52; male = 0.48; difference = AMCE = 0.045).\n\n\n\n\n\n\n\n\nThe third panel shows two different AMCEs conditional on respondent political party, and it appeared in the published study. Because AMCEs require a reference category as the baseline, and because AMCEs are relative quantities, it looks like there’s a huge party-based difference in favorability toward women candidates—being a woman causes a 7ish percentage point boost in favorability among Democrats, while it does nothing (or maybe something negative) among Republicans. These are conditional AMCEs (or CAMCEs), or the causal effect of turning on the “female” switch for a hypothetical candidate across Republican and Democratic respondents.\nConditional AMCEs are fine for causal work, but what trips people up often is that it’s tempting to use those conditional effects to describe actual overall patterns of preferences. Because there’s a reference category involved (male candidates), we can’t really say anything about the general respondent-party-ID-based preferences for male and female candidates. The conditional AMCE here combines the party-based difference in favorability toward female candidates (53.7% among Democrats; 49.2% among Republicans; difference of 4.5 percentage points) and the party-based difference in favorability toward male candidates (46.3% among Democrats; 50.8% among Republicans; difference of 4.5 percentage points). According to Leeper, et al.:\n\nBecause Democrats and Republicans actually differ in their views of profiles containing the reference (male) category, AMCEs sum the true differences in preferences for a given feature level with the difference in preferences toward the reference category. Visual or numerical similarity of subgroup AMCEs is therefore an analytical artifact, not an accurate statement of the similarity of patterns of preferences (Leeper, Hobolt, and Tilley 2020, 215).\n\nIf we’re interested not in describing a causal effect (i.e. the effect of switching from male → female) but instead describing general preferences (i.e. the difference in overall candidate sex favorability between Republicans and Democrats), we have to look at marginal means (and the difference in marginal means) instead of conditional AMCEs. It is really tempting to look at the distance between Republicans and Democrats in the “Female” level in the third panel and say that there’s a nearly 10 percentage point difference in favorability across parties, but that’s wrong! It only looks that way because the effect sizes are all relative to the reference “Male” level.\n\n\n\n\n\n\n\n\nIn reality, Democrats are 4.5 percentage points less likely to select a male candidate and 4.5 percentage points more likely to select a female candidate. The differences in marginal means within party subgroups would look like this:\n\nCode# Teele, Kalla, and Rosenbluth 2018 data from https://doi.org/10.7910/DVN/FVCGHC\ncandidate_parties &lt;- read_stata(\"data/conjoint_data.dta\") %&gt;% \n  as_factor() %&gt;%\n  mutate(female = factor(orig_cand_gender_string)) %&gt;% \n  mutate(female = fct_relevel(female, \"Male\")) %&gt;% \n  mutate(party_respondent = case_when(\n    democrat_respondent == 1 ~ \"Democrat\",\n    republican_respondent == 1 ~ \"Republican\"\n  )) %&gt;% \n  mutate(party_respondent = factor(party_respondent)) %&gt;% \n  filter(sample == \"usa voter\")\n\nmm_diffs(\n  candidate_parties,\n  winner ~ female,\n  by = ~party_respondent\n) %&gt;%\n  ggplot(aes(x = estimate, y = level)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(\n    xmin = lower, xmax = upper, \n    color = \"Republican marginal mean − Democrat marginal mean\"\n  )) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = \"#85144b\") +\n  labs(\n    x = \"Difference in marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Difference in marginal means between\\nRepublican and Democratic respondents\",\n    subtitle = \"Positive differences = Republicans prefer the level\"\n  ) +\n  facet_wrap(vars(\"Candidate sex\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\n\n\n\n\n\n\n(For more about this, see Leeper, Hobolt, and Tilley (2020) for a few other examples of the substantive differences between subgroup conditional AMCEs and subgroup differences-between-marginal-means.)\nLong story short: because AMCEs are relative estimands, they get weird (and can’t really be used) when using them for descriptive estimands across subgroups or when controlling for other respondent characteristics. To account for this weirdness, calculate marginal means instead and find the subgroup differences in marginal means for each level."
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nFirst we’ll calculate the average marginal component effects (AMCEs), which are the partial derivatives (or coefficients) from a linear regression model. These represent the causal effect of switching from some reference level to a level of interest, while holding everything else constant.\nAutomatic estimates with cregg::amce()\n\nThe amce() function from {cregg} calculates AMCEs automatically and returns them in a nice tidy data frame:\n\nmodel_candidate_amce &lt;- amce(\n  candidate,\n  selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale,\n  id = ~resID\n)\n\nmodel_candidate_amce %&gt;% as_tibble()\n## # A tibble: 40 × 10\n##    outcome  statistic feature          level                  estimate std.error      z            p   lower   upper\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;fct&gt;            &lt;fct&gt;                     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1 selected amce      Military Service Did Not Serve            0        NA      NA     NA           NA      NA     \n##  2 selected amce      Military Service Served                   0.0873    0.0177  4.95   0.000000761  0.0527  0.122 \n##  3 selected amce      Religion         None                     0        NA      NA     NA           NA      NA     \n##  4 selected amce      Religion         Jewish                  -0.0373    0.0263 -1.42   0.156       -0.0889  0.0143\n##  5 selected amce      Religion         Catholic                -0.0156    0.0278 -0.561  0.575       -0.0702  0.0389\n##  6 selected amce      Religion         Mainline protestant     -0.0149    0.0308 -0.484  0.629       -0.0753  0.0455\n##  7 selected amce      Religion         Evangelical protestant  -0.117     0.0309 -3.78   0.000157    -0.178  -0.0563\n##  8 selected amce      Religion         Mormon                  -0.137     0.0307 -4.46   0.00000838  -0.197  -0.0767\n##  9 selected amce      College          No BA                    0        NA      NA     NA           NA      NA     \n## 10 selected amce      College          Baptist college          0.139     0.0289  4.82   0.00000143   0.0827  0.196 \n## # ℹ 30 more rows\n\nIt also provides an automatic plot function. Compare this with the original—the results are identical.\n\n\nplot(cregg::amce())\nOriginal AMCE coefficent plot\n\n\n\n\nplot(model_candidate_amce) + \n  guides(color = \"none\") +\n  theme_nice() +\n  labs(title = \"AMCEs from cregg::amce()\")\n\n\n\n\n\n\n\n\n\nRight panel of Figure 1 in Leeper, Hobolt, and Tilley (2020):\n\n\n\n\nOLS coefficients\nBehind the scenes, {cregg} uses survey::svyglm() to run OLS with ID-specific adjustments to standard errors:\n\ncandidate_svy_design &lt;- svydesign(\n  ids = ~resID,\n  weights = ~1,\n  data = candidate\n)\n\nmodel_svy &lt;- svyglm(\n  selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale,\n  design = candidate_svy_design\n)\n\ntidy(model_svy)\n## # A tibble: 33 × 5\n##    term                             estimate std.error statistic  p.value\n##    &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n##  1 (Intercept)                        0.397     0.0484     8.20  9.18e-15\n##  2 atmilitaryServed                   0.0873    0.0177     4.95  1.32e- 6\n##  3 atreligionJewish                  -0.0373    0.0263    -1.42  1.58e- 1\n##  4 atreligionCatholic                -0.0156    0.0278    -0.561 5.75e- 1\n##  5 atreligionMainline protestant     -0.0149    0.0308    -0.484 6.29e- 1\n##  6 atreligionEvangelical protestant  -0.117     0.0309    -3.78  1.92e- 4\n##  7 atreligionMormon                  -0.137     0.0307    -4.46  1.22e- 5\n##  8 atedBaptist college                0.139     0.0289     4.82  2.36e- 6\n##  9 atedCommunity college              0.150     0.0290     5.17  4.44e- 7\n## 10 atedState university               0.188     0.0277     6.77  7.80e-11\n## # ℹ 23 more rows\n\nThese estimates and standard errors are the same results that we get from cregg::amce():\n\n# Combine the estimates from cregg::amce() with the estimates from\n# survey::svglm() just to check that they're the same (they are)\namce_estimates &lt;- model_candidate_amce %&gt;% \n  as_tibble() %&gt;% \n  drop_na(std.error) %&gt;% \n  select(level, amce_estimate = estimate, amce_std.error = std.error)\n\nsvy_estimates &lt;- model_svy %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  select(svy_estimate = estimate, svy_std.error = std.error)\n\nbind_cols(amce_estimates, svy_estimates)\n## # A tibble: 32 × 5\n##    level                  amce_estimate amce_std.error svy_estimate svy_std.error\n##    &lt;fct&gt;                          &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n##  1 Served                        0.0873         0.0177       0.0873        0.0177\n##  2 Jewish                       -0.0373         0.0263      -0.0373        0.0263\n##  3 Catholic                     -0.0156         0.0278      -0.0156        0.0278\n##  4 Mainline protestant          -0.0149         0.0308      -0.0149        0.0308\n##  5 Evangelical protestant       -0.117          0.0309      -0.117         0.0309\n##  6 Mormon                       -0.137          0.0307      -0.137         0.0307\n##  7 Baptist college               0.139          0.0289       0.139         0.0289\n##  8 Community college             0.150          0.0290       0.150         0.0290\n##  9 State university              0.188          0.0277       0.188         0.0277\n## 10 Small college                 0.178          0.0273       0.178         0.0273\n## # ℹ 22 more rows\n\nMarginal effects instead of coefficients\nSo these OLS coefficients here are the AMCEs. That’s super straightforward.\nThat’s only true because we’re estimating a linear probability model (LPM) here. selected is a binary 0/1 variable, but we’re pretending it’s numeric and using OLS with it. In this special case, the marginal effects (slopes) are just the partial derivatives reported in the regression table.\nBut if we’re going to have interaction terms, or nonlinear terms (like polynomials), or if we’re going to use a model family that’s not OLS (like logistic regression), raw coefficients won’t work. Instead we need to calculate actual marginal effects (see this post and the “Get Started” page at the {marginaleffects} documentation for more details about these).\nAt one point, {cregg} supported this by using the {margins} package (which makes sense—Thomas Leeper developed both {cregg} and {margins}), but starting with {cregg} version 0.1.14, the {margins} support disappeared, leaving only support for OLS linear probability models.\nWe can use the {marginaleffects} package (the successor to {margins}) to calculate the average marginal effects/slopes for each of these effects. Here we’ll just hold all the predictors at their means (but there are a ton of different estimands we could calculate). In this case they’re the same as the raw coefficients, since it’s just a linear model:\n\nmfx_svy &lt;- model_svy %&gt;% \n  avg_slopes(newdata = \"mean\")\n\n# Combine the average marginal effects with the original coefficients just to\n# check that they're the same (they are)\nmfx_svy %&gt;% \n  select(contrast, mfx_estimate = estimate, mfx_std.error = std.error) %&gt;% \n  cbind(svy_estimates) %&gt;% \n  as_tibble()\n## # A tibble: 32 × 5\n##    contrast                      mfx_estimate mfx_std.error svy_estimate svy_std.error\n##    &lt;chr&gt;                                &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n##  1 45 - 36                            0.0259         0.0295       0.0873        0.0177\n##  2 52 - 36                            0.0236         0.0303      -0.0373        0.0263\n##  3 60 - 36                            0.00414        0.0288      -0.0156        0.0278\n##  4 68 - 36                           -0.0639         0.0294      -0.0149        0.0308\n##  5 75 - 36                           -0.146          0.0289      -0.117         0.0309\n##  6 Baptist college - No BA            0.139          0.0289      -0.137         0.0307\n##  7 Community college - No BA          0.150          0.0290       0.139         0.0289\n##  8 Ivy League university - No BA      0.269          0.0291       0.150         0.0290\n##  9 Small college - No BA              0.178          0.0273       0.188         0.0277\n## 10 State university - No BA           0.188          0.0277       0.178         0.0273\n## # ℹ 22 more rows\n\nCoefficient plots\nWhat about that neat coefficient plot with the reference categories? tidy() can’t return empty rows for the reference levels like cregg::amce() does, but we can make it work in a couple different ways: (1) automatically with broom.helpers::tidy_add_reference_rows(), or (2) manually with some data wrangling.\nAutomatically with {broom.helpers}\nThe easiest way to do this is to use broom.helpers::tidy_add_reference_rows(), which adds the reference levels automatically with estimates of 0, so we can use geom_pointrange() and make basically the same plot as {cregg}:\n\nplot_data_manual &lt;- model_svy %&gt;% \n  tidy_and_attach() %&gt;% \n  tidy_add_reference_rows() %&gt;% \n  tidy_add_estimate_to_reference_rows() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(term_nice = str_remove(term, variable)) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(term_nice, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_data_manual,\n  aes(x = estimate, y = term_nice, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"AMCEs plotted with tidy_add_reference_rows()\"\n  ) +\n  # Automatically resize each facet height with ggforce::facet_col()\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\n\n\n\nManually with {dplyr} wrangling magic\nHowever, tidy_add_reference_rows() doesn’t work with {marginaleffects} objects or multilevel models or Bayesian models, so ultimately I can’t use this in the real project I’m working on :(\nBut we can get the same thing with some fancy data wrangling:\n\n# Extract all the right-hand variables\nrhs_vars &lt;- all.vars(stats::update(formula(model_svy), 0 ~ .))\n\n# Make a data frame of all the levels of all the non-numeric things\nmodel_variable_levels &lt;- tibble(\n  variable = rhs_vars\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- candidate[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Extract model results\nmodel_results &lt;- tidy(model_svy, conf.int = TRUE)\n\n# Combine full dataset of factor levels with model results\nplot_data &lt;- model_variable_levels %&gt;%\n  left_join(model_results, by = join_by(term)) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_data,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"AMCEs from fancy data wrangling\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\n\n\n\nThe process is a little different with marginal effects because the term column is for the overarching variable (e.g., taxrate1) and the individual levels are built as contrasts in a column named contrast, like \"&lt;10k: 5% - &lt;10k: 0%\". We need to clean up that contrast column for joining with model_variable_levels.\n\n# Extract marginal effects\nmodel_results_mfx &lt;- model_svy %&gt;%\n  avg_slopes(newdata = \"mean\") %&gt;%\n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  )\n\n# Combine full dataset of factor levels with marginal effects\nplot_data_mfx &lt;- model_variable_levels %&gt;%\n  left_join(\n    model_results_mfx,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_data_mfx,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"AMCEs from OLS marginal effects\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\n\n\n\n\nLogistic regression instead of OLS\nWhat if you don’t want use a linear probability model and instead want to use a different family like logistic regression (which treats the outcome as actual 0 and 1 categories instead of numbers)? Or what if you have 3+ possible choices for outcomes and need to use a multinomial logistic regression model? We can still calculate AMCEs, but we can’t use raw regression coefficients. Instead we have to calculate response-scale (i.e. probability scale) marginal effects.\nLet’s make a logistic regression model:\n\nmodel_logit &lt;- survey::svyglm(\n  selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale,\n  design = candidate_svy_design,\n  family = binomial(link = \"logit\")\n)\n\nCheck out those sweet sweet uninterpretable log odds:\n\ntidy(model_logit)\n## # A tibble: 33 × 5\n##    term                             estimate std.error statistic  p.value\n##    &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n##  1 (Intercept)                       -0.466     0.215     -2.17  3.07e- 2\n##  2 atmilitaryServed                   0.385     0.0775     4.96  1.22e- 6\n##  3 atreligionJewish                  -0.162     0.116     -1.40  1.62e- 1\n##  4 atreligionCatholic                -0.0651    0.123     -0.531 5.96e- 1\n##  5 atreligionMainline protestant     -0.0634    0.136     -0.468 6.40e- 1\n##  6 atreligionEvangelical protestant  -0.510     0.137     -3.72  2.44e- 4\n##  7 atreligionMormon                  -0.600     0.137     -4.39  1.64e- 5\n##  8 atedBaptist college                0.619     0.131      4.74  3.43e- 6\n##  9 atedCommunity college              0.663     0.131      5.06  7.73e- 7\n## 10 atedState university               0.828     0.126      6.57  2.43e-10\n## # ℹ 23 more rows\n\nWe can convert these log odds into probability-scale marginal effects with slopes() or avg_slopes() from {marginaleffects}. Conceptually this involves plugging in different covariate values—here we’re plugging in all the original values in the data into the model and then collapsing the predictions into averages. (Again, see this and this for more details about average marginal effects/slopes.)\n\nmfx_logit &lt;- model_logit %&gt;% \n  avg_slopes()\n\nmfx_logit %&gt;% as_tibble()\n## # A tibble: 32 × 8\n##    term  contrast                      estimate std.error statistic  p.value conf.low conf.high\n##    &lt;chr&gt; &lt;chr&gt;                            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 atage 45 - 36                        0.0258     0.0296     0.874 3.82e- 1  -0.0321   0.0838 \n##  2 atage 52 - 36                        0.0232     0.0303     0.765 4.44e- 1  -0.0362   0.0826 \n##  3 atage 60 - 36                        0.00406    0.0289     0.141 8.88e- 1  -0.0526   0.0607 \n##  4 atage 68 - 36                       -0.0639     0.0294    -2.17  3.00e- 2  -0.122   -0.00619\n##  5 atage 75 - 36                       -0.145      0.0289    -5.03  4.86e- 7  -0.202   -0.0888 \n##  6 ated  Baptist college - No BA        0.140      0.0290     4.82  1.45e- 6   0.0828   0.196  \n##  7 ated  Community college - No BA      0.150      0.0291     5.15  2.56e- 7   0.0928   0.207  \n##  8 ated  Ivy League university - No BA  0.269      0.0291     9.26  2.11e-20   0.212    0.326  \n##  9 ated  Small college - No BA          0.178      0.0274     6.50  8.07e-11   0.124    0.231  \n## 10 ated  State university - No BA       0.188      0.0277     6.80  1.04e-11   0.134    0.243  \n## # ℹ 22 more rows\n\nheck yes these percentage-point-scale estimates are basically the same as what we get from the LPM model!\n\n# Extract marginal effects\nmodel_results_logit_mfx &lt;- mfx_logit %&gt;%\n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  )\n\n# Combine full dataset of factor levels with marginal effects\nplot_data_logit_mfx &lt;- model_variable_levels %&gt;%\n  left_join(\n    model_results_logit_mfx,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_data_logit_mfx,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_pp) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"AMCEs from logistic regression marginal effects\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means-1",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means-1",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Marginal means",
    "text": "Marginal means\nNext we’ll calculate marginal means for these different attributes and levels, which are conditional averages and not regression coefficients. These are descriptive estimands/quantities of interest and they don’t rely on any reference category or baseline level.\nAutomatic estimates with cregg::mm()\n\nThe mm() function from {cregg} calculates marginal means automatically and returns them in a nice tidy data frame. I include the package namespace prefix here (i.e. cregg::mm() instead of just mm()) because I’ve loaded the {brms} package and it has its own mm() function that’s used for creating multi-membership grouping terms (whatever those are).\n\ncandidate_mms_auto &lt;- cregg::mm(\n  candidate,\n  selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale,\n  id = ~ resID\n)\n\ncandidate_mms_auto %&gt;% as_tibble()\n## # A tibble: 40 × 10\n##    outcome  statistic feature          level                  estimate std.error     z         p lower upper\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;fct&gt;            &lt;fct&gt;                     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 selected mm        Military Service Did Not Serve             0.458   0.00907  50.5 0         0.440 0.475\n##  2 selected mm        Military Service Served                    0.543   0.00914  59.4 0         0.525 0.560\n##  3 selected mm        Religion         None                      0.556   0.0200   27.8 1.88e-170 0.517 0.595\n##  4 selected mm        Religion         Jewish                    0.520   0.0186   28.0 2.77e-172 0.484 0.557\n##  5 selected mm        Religion         Catholic                  0.526   0.0177   29.7 2.82e-193 0.491 0.560\n##  6 selected mm        Religion         Mainline protestant       0.543   0.0194   28.0 3.93e-172 0.505 0.581\n##  7 selected mm        Religion         Evangelical protestant    0.437   0.0200   21.9 4.11e-106 0.398 0.476\n##  8 selected mm        Religion         Mormon                    0.417   0.0194   21.5 7.40e-103 0.379 0.455\n##  9 selected mm        College          No BA                     0.340   0.0186   18.3 1.12e- 74 0.303 0.376\n## 10 selected mm        College          Baptist college           0.482   0.0200   24.1 3.89e-128 0.443 0.521\n## # ℹ 30 more rows\n\n\n\nplot(cregg::mm())\nOriginal AMCE coefficent plot\n\n\n\n\nplot(candidate_mms_auto, vline = 0.5) + \n  guides(color = \"none\") +\n  theme_nice() +\n  labs(title = \"Marginal means from from cregg::mm()\")\n\n\n\n\n\n\n\n\n\nLeft panel of Figure 1 in Leeper, Hobolt, and Tilley (2020):\n\n\n\n\nQuick-and-dirty marginal means\nMarginal means are just conditional group averages, so we can actually get the same estimates with some basic {dplyr} grouping and summarizing. I’ll just show the averages for military and religion here for the sake of space, but the averages are the same as what we get with cregg::mm():\n\ncandidate %&gt;% \n  group_by(atmilitary) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 2 × 2\n##   atmilitary      avg\n##   &lt;fct&gt;         &lt;dbl&gt;\n## 1 Did Not Serve 0.458\n## 2 Served        0.543\n\ncandidate %&gt;% \n  group_by(atreligion) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 6 × 2\n##   atreligion               avg\n##   &lt;fct&gt;                  &lt;dbl&gt;\n## 1 None                   0.556\n## 2 Jewish                 0.520\n## 3 Catholic               0.526\n## 4 Mainline protestant    0.543\n## 5 Evangelical protestant 0.437\n## 6 Mormon                 0.417\n\nBehind the scenes, cregg::mm() creates simple intercept-less models for each of the categorical terms in the model and then returns their coefficients. Again, for the sake of space, here are the regression-based averages for just military and religion:\n\nbind_rows(\n  tidy(lm(selected ~ 0 + atmilitary, data = candidate)),\n  tidy(lm(selected ~ 0 + atreligion, data = candidate))\n)\n## # A tibble: 8 × 5\n##   term                             estimate std.error statistic   p.value\n##   &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 atmilitaryDid Not Serve             0.458    0.0120      38.3 1.16e-267\n## 2 atmilitaryServed                    0.543    0.0120      45.3 0        \n## 3 atreligionNone                      0.556    0.0205      27.1 3.39e-147\n## 4 atreligionJewish                    0.520    0.0208      25.0 9.62e-127\n## 5 atreligionCatholic                  0.526    0.0205      25.6 1.34e-132\n## 6 atreligionMainline protestant       0.543    0.0209      26.0 1.36e-136\n## 7 atreligionEvangelical protestant    0.437    0.0209      20.9 1.53e- 91\n## 8 atreligionMormon                    0.417    0.0207      20.2 8.15e- 86\n\nRegardless of how we calculate them, we can see that these estimates are just group averages. 46% of respondents chose candidates who didn’t serve in the military; 54% chose candidates who did; and so on.\nMarginal means with {marginaleffects}\nCombining a bunch of smaller group_by() %&gt;% summarize() datasets, or combining a bunch of intercept-less models involves a bit of extra code and can get tedious. Plus it can get more complex when not using a linear model, or if you want interaction terms, or if you want group averages across multiple groups (i.e. average proportions for military across Republican and Democratic respondents). Additionally, the standard errors from these basic averages are wrong since they don’t take into account the nested structure of the data (i.e. respondents each have 6–12 responses).\nTo make life easier and more flexible, we can use marginal_means() from {marginaleffects} to calculate the unique categorical group means from a regression model.\nHow marginal_means() works\nBut first we need to look at a quick example to build the intuition behind what happens with marginal_means(). First we’ll make a simpler regresison model with just the military and religion features. We’ll then calculate predicted values for all the levels of those features using predictions()—this essentially involves plugging in all the unique values of atmilitary and atreligion into the model and finding the predicted values. We’ll then reshape the results a little so that we can see the average proportions of religion conditional on military service:\n\nmodel_candidate_simple &lt;- lm(\n  selected ~ atmilitary + atreligion, \n  data = candidate\n)\n\npredictions_simple &lt;- predictions(\n  model_candidate_simple,\n  by = c(\"atreligion\", \"atmilitary\")\n)\n\npredictions_simple_wide &lt;- predictions_simple %&gt;% \n  select(estimate, atmilitary, atreligion) %&gt;% \n  pivot_wider(names_from = \"atmilitary\", values_from = \"estimate\")\npredictions_simple_wide\n## # A tibble: 6 × 3\n##   atreligion             `Did Not Serve` Served\n##   &lt;fct&gt;                            &lt;dbl&gt;  &lt;dbl&gt;\n## 1 None                             0.514  0.598\n## 2 Jewish                           0.479  0.563\n## 3 Catholic                         0.483  0.567\n## 4 Mainline protestant              0.500  0.584\n## 5 Evangelical protestant           0.394  0.478\n## 6 Mormon                           0.376  0.460\n\nGreat. The average proportion for non-military Mormons is 37.6% and for military Mormons is 46%, and so on.\nIf we find the average of these averages and add a column and row in the literal right and bottom margins of the table, we’ll have marginal means of religion and military service:\n\npredictions_simple_wide %&gt;% \n  mutate(`Religion marginal mean` = (`Did Not Serve` + Served) / 2) %&gt;% \n  add_row(\n    atreligion = \"Military marginal mean\", \n    `Did Not Serve` = mean(predictions_simple_wide$`Did Not Serve`),\n    Served = mean(predictions_simple_wide$Served)\n  )\n## # A tibble: 7 × 4\n##   atreligion             `Did Not Serve` Served `Religion marginal mean`\n##   &lt;chr&gt;                            &lt;dbl&gt;  &lt;dbl&gt;                    &lt;dbl&gt;\n## 1 None                             0.514  0.598                    0.556\n## 2 Jewish                           0.479  0.563                    0.521\n## 3 Catholic                         0.483  0.567                    0.525\n## 4 Mainline protestant              0.500  0.584                    0.542\n## 5 Evangelical protestant           0.394  0.478                    0.436\n## 6 Mormon                           0.376  0.460                    0.418\n## 7 Military marginal mean           0.458  0.542                   NA\n\nTo me this is wild. The marginal mean for Mormons is 41%, which is basically what we found with group_by(atreligion) %&gt;% summarize() earlier. The marginal mean for candidates who served in the military is 54.2%, again roughly the same as what we found with group_by(atmilitary) %&gt;% summarize(...).\nInstead of manually creating these marginal rows and columns, the marginal_means() function will find those values automatically for us, based on a grid of values (in newdata) that it’ll plug into the model. Here it’ll plug all combinations of atreligion and atmilitary into the simple model. The wts = \"cells\" argument here makes it so that the marginal mean is weighted by the actual distribution of the levels of religion and military. For instance, imagine that only 30% of rows served in the military and 70% did not. Calculating the average of those two averages by just doing (Did Not Serve + Served) / 2 wouldn’t take that underlying distribution into account. Weighting by cells make it so that marginal_means() computes a weighted marginal mean proportional to each level’s frequency in the original data.\nLet’s let marginal_means() work its magic:\n\nmarginal_means_simple &lt;- marginal_means(\n  model_candidate_simple,\n  newdata = c(\"atreligion\", \"atmilitary\"),\n  wts = \"cells\"\n)\nmarginal_means_simple\n## \n##        Term                  Value  Mean Std. Error    z Pr(&gt;|z|) 2.5 % 97.5 %\n##  atmilitary Did Not Serve          0.458     0.0119 38.5   &lt;0.001 0.434  0.481\n##  atmilitary Served                 0.543     0.0119 45.5   &lt;0.001 0.519  0.566\n##  atreligion None                   0.556     0.0204 27.2   &lt;0.001 0.516  0.596\n##  atreligion Jewish                 0.520     0.0208 25.1   &lt;0.001 0.479  0.561\n##  atreligion Catholic               0.526     0.0205 25.7   &lt;0.001 0.485  0.566\n##  atreligion Mainline protestant    0.543     0.0208 26.1   &lt;0.001 0.502  0.584\n##  atreligion Evangelical protestant 0.437     0.0208 21.0   &lt;0.001 0.396  0.477\n##  atreligion Mormon                 0.417     0.0206 20.3   &lt;0.001 0.377  0.458\n## \n## Results averaged over levels of: atmilitary, atreligion \n## Columns: term, value, estimate, std.error, statistic, p.value, conf.low, conf.high\n\nEt voilà, the averages here are basically the same as what we found in the manual version we did earlier. The only differences are due to the weighted averaging—by default marginal_means() assumes equal weights in the columns, so it’s literally going to just calculate (Did Not Serve + Served) / 2. With wts = \"cells\", it creates a weighted average: (Did Not Serve * cell_weight + Served * other_cell_weight) / 2.\n\nmarginal_means() with the full actual model\nNow that we understand what’s happening with marginal_means(), we can use it with the full model_svy model. We’ll find the marginal means for all the different features and make sure that we weight by cells so that we get weighted averages. Because there are so many combinations of attribute levels here, it takes a while to run:\n\n# This takes a while...\ntictoc::tic()\nmm_mfx &lt;- marginal_means(\n  model_svy, \n  newdata = c(\n    \"atmilitary\", \"atreligion\", \"ated\", \"atprof\", \n    \"atinc\", \"atrace\", \"atage\", \"atmale\"\n  ),\n  wts = \"cells\"\n)\ntictoc::toc()\n## 82.563 sec elapsed\n\nThe results are identical to what we find when using group_by() %&gt;% summarize(), but\n\nthe standard errors are correct (and actually present; we’d need to calculate those on our own with summarize() and that’s a pain), and\nwe have the ability to use other extra {marginaleffects} things like hypothesis tests, counterfactual estimates, p-value adjustments for multiple comparisons, clustered robust standard errors, and so on\n\n\n\nmarginal_means()\ngroup_by() %&gt;% summarize()\n\n\n\n\nmm_mfx %&gt;% as_tibble()\n## # A tibble: 40 × 8\n##    term       value                  estimate std.error statistic   p.value conf.low conf.high\n##    &lt;chr&gt;      &lt;fct&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 atmilitary Did Not Serve             0.458   0.00913      50.1 0            0.440     0.476\n##  2 atmilitary Served                    0.543   0.00928      58.4 0            0.524     0.561\n##  3 atreligion None                      0.556   0.0195       28.5 1.64e-178    0.518     0.594\n##  4 atreligion Jewish                    0.520   0.0175       29.7 1.90e-194    0.486     0.554\n##  5 atreligion Catholic                  0.526   0.0177       29.7 3.14e-193    0.491     0.560\n##  6 atreligion Mainline protestant       0.543   0.0184       29.6 1.86e-192    0.507     0.579\n##  7 atreligion Evangelical protestant    0.437   0.0198       22.0 1.07e-107    0.398     0.475\n##  8 atreligion Mormon                    0.417   0.0189       22.1 8.22e-108    0.380     0.454\n##  9 ated       No BA                     0.340   0.0184       18.5 5.02e- 76    0.304     0.376\n## 10 ated       Baptist college           0.482   0.0198       24.4 1.63e-131    0.443     0.521\n## # ℹ 30 more rows\n\n\n\n\ncandidate %&gt;% \n  group_by(atmilitary) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 2 × 2\n##   atmilitary      avg\n##   &lt;fct&gt;         &lt;dbl&gt;\n## 1 Did Not Serve 0.458\n## 2 Served        0.543\n\ncandidate %&gt;% \n  group_by(atreligion) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 6 × 2\n##   atreligion               avg\n##   &lt;fct&gt;                  &lt;dbl&gt;\n## 1 None                   0.556\n## 2 Jewish                 0.520\n## 3 Catholic               0.526\n## 4 Mainline protestant    0.543\n## 5 Evangelical protestant 0.437\n## 6 Mormon                 0.417\n\n\n\n\nSince there’s no reference level to deal with, plotting these these marginal means is pretty straightforward:\n\nplot_mm_mfx &lt;- mm_mfx %&gt;% \n  as_tibble() %&gt;% \n  mutate(value = fct_inorder(value)) %&gt;%\n  left_join(variable_lookup, by = join_by(term == variable)) %&gt;% \n  mutate(across(c(value, variable_nice), ~fct_inorder(.)))\n\nggplot(\n  plot_mm_mfx,\n  aes(x = estimate, y = value, color = variable_nice)\n) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  scale_x_continuous(labels = label_percent()) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Marginal mean\",\n    y = NULL,\n    title = \"Marginal means with marginaleffects::marginal_means()\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#subgroup-differences-in-amces-and-marginal-means",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#subgroup-differences-in-amces-and-marginal-means",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Subgroup differences in AMCEs and marginal means",
    "text": "Subgroup differences in AMCEs and marginal means\nHainmueller, Hopkins, and Yamamoto (2014) did not include any individual respondent-level characteristics in their data, so we can’t look at how these causal effects differ across individual traits like party identification (Republicans and Democrats) or education or income or anything else like that.\nSo instead we’ll invent a pretend column for respondent-level party ID! To simplify life, we’ll look at differences between fake-party-ID within the military service history attribute. Arbitrarily (and super stereotypically), we’ll say that if a respondent selected a candidate more than 60% of the time when seeing that they had served in the military, there’s a 90% chance they’re a Republican. If they didn’t select the military candidate 60% of the time, there’s a 75% chance they’re a Democrat. AGAIN THESE PROBABILITIES ARE COMPLETELY ARBITRARY AND JUST CAME FROM MY HEAD—THEY ARE NOT REAL. We’ll make a new dataset called candidate_fake with a column named respondent_party for each respondent’s fake party:\n\nCode for generating fake respondent_party column# Wrap this in withr::with_seed() so that the randomness is reproducible but the\n# overall document seed doesn't get set or messed up\nwithr::with_seed(1234, {\n  respondents_party_military &lt;- candidate %&gt;%\n    group_by(resID, atmilitary) %&gt;%\n    # Find the proportion of times each respondent selected the candidate when\n    # military service history was \"Served\"\n    summarize(prob_select = mean(selected)) %&gt;%\n    filter(atmilitary == \"Served\") %&gt;%\n    select(-atmilitary) %&gt;%\n    ungroup() %&gt;%\n    # If a respondent selected the candidate more than 60% of the time when\n    # seeing that they had served in the military, there's a 90% chance they're\n    # a Republican. If they didn't select the military candidate 60% of the\n    # time, there's a 75% chance they're a Democrat.\n    mutate(respondent_party = case_when(\n      prob_select &gt;= 0.6 ~\n        sample(\n          c(\"Democrat\", \"Republican\"), n(),\n          replace = TRUE, prob = c(0.1, 0.9)\n        ),\n      prob_select &lt; 0.6 ~\n        sample(\n          c(\"Democrat\", \"Republican\"), n(),\n          replace = TRUE, prob = c(0.75, 0.25)\n        )\n    )) %&gt;%\n    mutate(respondent_party = factor(respondent_party))\n})\n\ncandidate_fake &lt;- candidate %&gt;% \n  left_join(respondents_party_military, by = join_by(resID))\n\n\nLet’s check the average of selected across both the candidate military attribute and our new fake respondent party to see how all that random assignment shook out:\n\ncandidate_fake %&gt;% \n  group_by(atmilitary, respondent_party) %&gt;% \n  summarize(avg = mean(selected))\n## # A tibble: 4 × 3\n## # Groups:   atmilitary [2]\n##   atmilitary    respondent_party   avg\n##   &lt;fct&gt;         &lt;fct&gt;            &lt;dbl&gt;\n## 1 Did Not Serve Democrat         0.551\n## 2 Did Not Serve Republican       0.378\n## 3 Served        Democrat         0.447\n## 4 Served        Republican       0.619\n\nCool cool. Republican respondents are way more favorable towards candidates who served in the military (61.9%) than Democratic respondents (44.8%). And that kind of interpretation is actually mathematically and conceptually legal and recommended by Leeper, Hobolt, and Tilley (2020)—technically these are subgroup marginal means and what we should be looking at for descriptive purposes already, but more on that below.\nConditional AMCEs\nLet’s first look at the two different party-based causal effects of switching a candidate from having no military history to having served in the military. For the sake of simplicity we’ll just use regular OLS instead of logistic regression, and we’ll use marginaleffects::avg_slopes() to find the marginal since the regression model involves interaction terms. We’ll also only look at the atmilitary feature instead of all the features, since it’s the only one where we built in a party effect.\nWe can find conditional AMCEs with {cregg} using the cj() function, which is a general function for all of the different estimands that {cregg} can calculate:\n\ncregg::cj(\n  candidate_fake, \n  selected ~ atmilitary, \n  id = ~resID, \n  estimate = \"amce\", \n  by = ~respondent_party\n)\n##           BY  outcome statistic          feature         level estimate std.error      z         p   lower    upper respondent_party\n## 1   Democrat selected      amce Military Service Did Not Serve   0.0000        NA     NA        NA      NA       NA         Democrat\n## 2   Democrat selected      amce Military Service        Served  -0.1032   0.02063 -5.003 5.655e-07 -0.1437 -0.06278         Democrat\n## 3 Republican selected      amce Military Service Did Not Serve   0.0000        NA     NA        NA      NA       NA       Republican\n## 4 Republican selected      amce Military Service        Served   0.2405   0.02246 10.710 9.095e-27  0.1965  0.28455       Republican\n\nWe can do the same thing using regression if we use an interaction term for the subgroup:\n\nmodel_military_party &lt;- lm(\n  selected ~ atmilitary * respondent_party, \n  data = candidate_fake\n)\n\ntidy(model_military_party)\n## # A tibble: 4 × 5\n##   term                                        estimate std.error statistic   p.value\n##   &lt;chr&gt;                                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)                                    0.551    0.0174     31.7  7.90e-194\n## 2 atmilitaryServed                              -0.103    0.0248     -4.16 3.21e-  5\n## 3 respondent_partyRepublican                    -0.172    0.0236     -7.28 3.97e- 13\n## 4 atmilitaryServed:respondent_partyRepublican    0.344    0.0335     10.3  2.48e- 24\n\nThose coefficients by themselves aren’t super informative without doing some tricky algebra to piece everything together. Instead, we can use avg_slopes() from {marginaleffects} to find the partial derivative (or AMCE) for atmilitary across each party. These are the same results we get from {cregg}:\n\nparty_amces &lt;- model_military_party %&gt;% \n  avg_slopes(\n    variables = \"atmilitary\",\n    by = \"respondent_party\"\n  )\nparty_amces\n## \n##        Term                           Contrast respondent_party Estimate Std. Error     z Pr(&gt;|z|)  2.5 %  97.5 %\n##  atmilitary mean(Served) - mean(Did Not Serve)       Democrat     -0.103     0.0248 -4.16   &lt;0.001 -0.152 -0.0546\n##  atmilitary mean(Served) - mean(Did Not Serve)       Republican    0.241     0.0226 10.66   &lt;0.001  0.196  0.2847\n## \n## Columns: term, contrast, respondent_party, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\n\nClick to show the code since it’s so long; you can make a quick basic version of it with two calls to plot(amce(...))—one on data filtered to only include Democrats and one that only includes Republicans# Clean up marginal effects\nparty_amces_mfx &lt;- party_amces %&gt;%\n  # The contrast column contains values like this:\n  #   mean(Served) - mean(Did Not Serve)\n  # I could probably use some fancy regex to extract those things, but here I'll\n  # just brute force it and remove \"mean(\" and \")\" with two separate\n  # str_remove()s\n  mutate(\n    contrast = str_remove_all(contrast, \"mean\\\\(\"),\n    contrast = str_remove_all(contrast, \"\\\\)\")\n  ) %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  )\n\n# Combine full dataset of factor levels with marginal effects\nplot_amces_party_mfx &lt;- expand_grid(\n  respondent_party = levels(candidate_fake$respondent_party),\n  filter(model_variable_levels, variable == \"atmilitary\")\n) %&gt;%\n  left_join(\n    party_amces_mfx,\n    by = join_by(variable == term, levels == variable_level, respondent_party)\n  ) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.))) %&gt;% \n  mutate(estimate_nice = case_when(\n    estimate != 0 ~ label_amce(estimate),\n    estimate == 0 ~ NA\n  ))\n\np_mfx &lt;- ggplot(\n  plot_amces_party_mfx, \n  aes(x = estimate, y = levels, color = respondent_party)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(\n    aes(xmin = conf.low, xmax = conf.high),\n    position = position_dodge(width = 0.15)\n  ) +\n  geom_label(\n    aes(label = estimate_nice), \n    position = position_dodge(width = -1.2),\n    size = 3.5, show.legend = FALSE\n  ) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = parties) +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    color = NULL,\n    title = \"AMCEs by respondent party\"\n  ) +\n  facet_wrap(vars(variable_nice)) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\np_mfx\n\n\n\n\n\n\n\nThese have a straightforward causal interpretation. Among Democratic-identifying respondents, flipping a hypothetical candidate’s military status from “did not serve” to “serve” causes a 10 percentage point drop in favorability (or in the probability of being selected). Among Republican-identifying respondents, a candidate’s military service causes a 24 percentage point increase in favorability. Both effects are statistically significantly different from zero.\nConditional marginal means\nIf we’re describing overall trends in the data, though, these conditional AMCEs are misleading. It’s really tempting to look at this and conclude that there’s a 34ish percentage point difference between Democrats and Republicans when they’re presented with a candidate with military service, since that’s the distance between the two AMCEs in the plot. But that’s wrong! That distance is an illusion—a mirage caused by the fact that the AMCEs are based on a reference category that is set to zero.\n\nCodep_mfx +\n  annotate(\n    geom = \"errorbar\",\n    x = 0, xmin = -0.103, xmax = 0.241, y = 2,\n    width = 0.1, color = \"black\"\n  ) +\n  annotate(\n    geom = \"label\", \n    x = 0.241 - (0.241 - -0.103) / 2, y = 2,\n    label = \"This difference isn't\\nwhat you think it is!\", \n    size = 5,\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 0.05, xend = 0.01, y = 1, yend = 1,\n    arrow = arrow(angle = 30, length = grid::unit(0.5, \"lines\"))\n  ) +\n  annotate(\n    geom = \"label\", \n    x = 0, y = 1,\n    label = \"These 0s mess things up!\", \n    size = 5,\n    hjust = -0.2\n  ) \n\n\n\n\n\n\n\nTo get the correct party-based difference in support for candidates, we need to find the party-based marginal means of support for the different levels of the military feature and then talk about those differences. As discussed above, this technically just involves finding the conditional averages across groups—in this case the average outcome within the two levels of “Served” and “Did not serve” between Republican and Democratic respondents. We can find these marginal means with some basic group_by() %&gt;% summarize() and the difference between Republican and Democratic marginal means with some pivoting and subtracting:\n\nparty_mms_manual &lt;- candidate_fake %&gt;% \n  group_by(atmilitary, respondent_party) %&gt;% \n  summarize(avg = mean(selected))\nparty_mms_manual\n## # A tibble: 4 × 3\n## # Groups:   atmilitary [2]\n##   atmilitary    respondent_party   avg\n##   &lt;fct&gt;         &lt;fct&gt;            &lt;dbl&gt;\n## 1 Did Not Serve Democrat         0.551\n## 2 Did Not Serve Republican       0.378\n## 3 Served        Democrat         0.447\n## 4 Served        Republican       0.619\n\nparty_mms_manual %&gt;% \n  pivot_wider(names_from = \"respondent_party\", values_from = \"avg\") %&gt;% \n  mutate(mm_diff = Republican - Democrat)\n## # A tibble: 2 × 4\n## # Groups:   atmilitary [2]\n##   atmilitary    Democrat Republican mm_diff\n##   &lt;fct&gt;            &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n## 1 Did Not Serve    0.551      0.378  -0.172\n## 2 Served           0.447      0.619   0.172\n\nOr we can use cregg::cj(..., estimate = \"mm_diff\") to do that automatically:\n\ncj(\n  candidate_fake, \n  selected ~ atmilitary, \n  id = ~resID, \n  estimate = \"mm_diff\", \n  by = ~respondent_party\n)\n##                      BY     statistic  outcome          feature         level estimate std.error      z         p   lower   upper respondent_party\n## 1 Republican - Democrat mm_difference selected Military Service Did Not Serve  -0.1722   0.01531 -11.25 2.361e-29 -0.2022 -0.1422       Republican\n## 2 Republican - Democrat mm_difference selected Military Service        Served   0.1715   0.01574  10.89 1.219e-27  0.1407  0.2024       Republican\n\nOr, to be extra thorough and allow for any type of regression family (logisitic! multinomial!) with any other types of covariates, we can find these manually with our own regression model fed through marginaleffects::marginal_means(). By specifying cross = TRUE, we get all combinations of military service and party (without it, we’d get separate marginal means for just the two levels of military and the two levels of party).\n\nparty_mms_mfx &lt;- marginal_means(\n  model_military_party,\n  newdata = c(\"atmilitary\", \"respondent_party\"),\n  cross = TRUE,\n  wts = \"cells\"\n)\nparty_mms_mfx %&gt;% as_tibble()\n## # A tibble: 4 × 11\n##   rowid atmilitary    respondent_party estimate std.error statistic   p.value conf.low conf.high selected   wts\n##   &lt;int&gt; &lt;fct&gt;         &lt;fct&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 Did Not Serve Democrat            0.551    0.0174      31.7 1.68e-220    0.517     0.585      0.5 0.231\n## 2     2 Did Not Serve Republican          0.378    0.0160      23.6 3.56e-123    0.347     0.410      0.5 0.271\n## 3     3 Served        Democrat            0.447    0.0177      25.3 3.11e-141    0.413     0.482      0.5 0.222\n## 4     4 Served        Republican          0.619    0.0159      39.0 0            0.588     0.650      0.5 0.276\n\nTo find the differences in those marginal means, we could pivot wider and subtract the Democrat column from the Republican column, or we can use the hypothesis argument in marginal_means() to have {marginaleffects} automatically calculate differences between categories for us without needing to pivot. If we were only concerned with one contrast, like Republican − Democrat, we could specify hypothesis = \"pairwise\" and it would subtract the two groups’ marginal means. However, we want two differences: Republican − Democrat in both the “served” and the “did not serve” levels. As seen above, party_mms_mfx has four rows in it. We want the differences between rows 2 and 1 (Republican − Democrat for “Did not serve”) and between rows 4 and 3 (Republican − Democrat for “Served”). We can control which rows are used when calculating differences with a vector of linear combinations. If we use a vector like c(-1, 1, 0, 0), {marginaleffects} will essentially make the first row negative, leave the second row as is, and give no weight to (or ignore) the third and fourth row, which will calculate the difference between rows 2 and 1. Similarly, c(0, 0, -1, 1) will ignore rows 1 and 2 and find the difference between row 4 and 3. If we feed marginal_means() a matrix of these two vectors, with each vector as a column, it’ll find the differences for us:\n\ngroup_diffs_terms &lt;- matrix(\n  c(-1, 1, 0, 0,\n    0, 0, -1, 1),\n  ncol = 2\n) %&gt;% \n  magrittr::set_colnames(levels(candidate_fake$atmilitary))\ngroup_diffs_terms\n##      Did Not Serve Served\n## [1,]            -1      0\n## [2,]             1      0\n## [3,]             0     -1\n## [4,]             0      1\n\nparty_mms_mfx_diff &lt;- marginal_means(\n  model_military_party,\n  newdata = c(\"atmilitary\", \"respondent_party\"),\n  cross = TRUE,\n  wts = \"cells\",\n  hypothesis = group_diffs_terms\n) %&gt;% \n  as_tibble()\nparty_mms_mfx_diff\n## # A tibble: 2 × 7\n##   term          estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 Did Not Serve   -0.172    0.0236     -7.28 3.22e-13   -0.219    -0.126\n## 2 Served           0.172    0.0238      7.22 5.23e-13    0.125     0.218\n\nWe can plot these conditional marginal means along with the party-based differences:\n\nClick to see all the plotting codemm_party_plot1 &lt;- party_mms_mfx %&gt;% \n  as_tibble() %&gt;% \n  mutate(estimate_nice = case_when(\n    estimate != 0 ~ label_percent()(estimate),\n    estimate == 0 ~ NA\n  )) %&gt;% \n  ggplot(aes(x = estimate, y = atmilitary, color = respondent_party)) +\n  geom_vline(xintercept = 0.5) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_label(\n    aes(label = estimate_nice), \n    position = position_dodge(width = -1.2),\n    size = 3.5, show.legend = FALSE\n  ) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_color_manual(values = parties) +\n  labs(\n    x = \"Marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Marginal means by respondent party\"\n  ) +\n  facet_wrap(vars(\"Military\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\nmm_party_plot2 &lt;- party_mms_mfx_diff %&gt;% \n  mutate(estimate_nice = label_amce(estimate)) %&gt;% \n  ggplot(aes(x = estimate, y = term)) +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(\n    xmin = conf.low, xmax = conf.high, \n    color = \"Republican marginal mean − Democrat marginal mean\"\n  )) +\n  geom_label(\n    aes(label = estimate_nice), size = 3.5,\n    nudge_y = 0.3, color = \"#85144b\"\n  ) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = \"#85144b\") +\n  labs(\n    x = \"Difference in marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Difference in marginal means by respondent party\",\n    subtitle = \"Positive differences = Republicans prefer the level\"\n  ) +\n  facet_wrap(vars(\"Military\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\nmm_party_plot1 | mm_party_plot2\n\n\n\n\n\n\n\nThese marginal means have a direct descriptive interpretation. In the left panel above we can see that Democratic respondents tend to prefer candidates that haven’t served in the military (55%) to those that have (45%), but compared to Republican respondents, this divergence isn’t as dramatic. Republicans tend to strongly prefer candidates with a military history, with 62% expressing favorability. In general, Republicans are more extreme in their preferences for and against candidates with and without military service history.\nIn the right panel we can see the differences between the parties’ marginal means within each level. There’s a 17 percentage point distance between Republican and Democratic marginal means within the served level and a 17 percentage point distance between their marginal means in the did not serve level. Overall, Republicans prefer candidates with military service; Democrats prefer candidates without military service."
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#the-overall-model",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#the-overall-model",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "The overall model",
    "text": "The overall model\nThe easiest way for me to think about all these estimands is as moving parts (marginal effects or partial derivatives) in a regression model. We can model the whole data-generating system (i.e. all the candidate features and levels + any subgroups or covariates we’re interested in) and then look at individual parts of that system for the different estimands we’re interested in.\nEach survey respondent saw multiple pairs of hypothetical candidates—some saw 4, some 5, some 6, and so on:\n\ncandidate %&gt;% \n  count(resID) %&gt;% \n  mutate(pairs_seen = n / 2)\n## # A tibble: 311 × 3\n##    resID              n pairs_seen\n##    &lt;fct&gt;          &lt;int&gt;      &lt;dbl&gt;\n##  1 A10ZOUOZZ3EOAJ    12          6\n##  2 A11F3HMX0N23V4    10          5\n##  3 A12H2RTXSAQPRH     8          4\n##  4 A13DPXX91VQ49Q    12          6\n##  5 A13KTOZC30NBS6    10          5\n##  6 A142W1RAF1TBWP    12          6\n##  7 A15A4X84A1CJPF    12          6\n##  8 A15Q5F9YWO45EI    12          6\n##  9 A1610EPXM31F9D    12          6\n## 10 A1650FELH3UL2F    10          5\n## # ℹ 301 more rows\n\nThis means that we have a natural multilevel structure in our data. Individual candidate selection choices are nested inside respondents:\n\n\n\n\nMultilevel conjoint data structure, with candidate choices \\(y\\) nested in respondents\n\n\n\nWe want to model candidate selection (selected) based on candidate characteristics (and maybe individual respondent characteristics, if we had those). We’ll use the subscript \\(i\\) to refer to individual candidate choices and \\(j\\) to refer to respondents, which each contain multiple \\(i\\)s.\nSince candidate selection selected is binary, we can model it as a Bernoulli process that has a probability \\(\\pi_{i_j}\\) of success. We’ll model that \\(\\pi_{i_j}\\) using a logistic regression model with covariates for each of the levels of each candidate feature. To account for respondent-level differences in probabilities, we’ll use respondent-specific offsets (\\(b_{0_j}\\)) from the global success rate, thus creating random intercepts. We’ll specify priors for each of the logit-scale coefficients/partial derivatives and the between respondent variability (\\(\\sigma_0\\)). If we really wanted we could specify priors for each individual coefficient, but for simplicity we’ll just use a normal distribution with a mean of 0 and a standard deviation of 1 for all of them (since logit-scale coefficients don’t ever get really big). We’ll use an exponential prior for (\\(\\sigma_0\\)) because we don’t know much about it.\nHere’s what that all looks like more formally:\n\n\\[\n\\begin{aligned}\n\\text{Selection}_{i_j} \\sim&\\ \\operatorname{Bernoulli}(\\pi_{i_j}) & \\text{Probability of selection for choice}_i \\text{ in respondent}_j \\\\\n\\operatorname{logit}(\\pi_{i_j}) =&\\ (\\beta_0 + b_{0_j}) + \\beta_1\\, \\text{Military[Served]}_{i_j} + & \\text{Model for probability}\\\\\n&\\ \\beta_2\\, \\text{Religion[Mormon]}_{i_j} + \\\\\n&\\ \\beta_3\\, \\text{Religion[Evangelical]}_{i_j} + \\\\\n&\\ \\dots +\\ \\\\\n&\\ \\beta_n\\, \\text{Sex[Female]}_{i_j} \\\\\nb_{0_j} \\sim&\\ \\mathcal{N}(0, \\sigma_0) & \\text{Respondent-specific offsets from global success rate} \\\\\n\\\\\n\\beta_{0_c} \\sim&\\ \\mathcal{N}(0, 1) & \\text{Prior for global average success rate} \\\\\n\\beta_1 \\dots \\beta_n \\sim&\\ \\mathcal{N}(0, 1) & \\text{Prior for candidate feature levels} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) & \\text{Prior for between-respondent variability}\n\\end{aligned}\n\\]\n\nLet’s build the model!\n\npriors &lt;- c(\n  prior(normal(0, 1), class = Intercept),\n  prior(normal(0, 1), class = b),\n  prior(exponential(1), class = sd)\n)\n\nmodel_brms &lt;- brm(\n  bf(selected ~ atmilitary + atreligion + ated +\n    atprof + atinc + atrace + atage + atmale +\n    (1 | resID)),\n  data = candidate,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"candidate_model_brms\"\n)"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces-1",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#amces-1",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nIf we’re interested in the causal effect of specific candidate levels, we need to find the average marginal component effect (AMCE). Here’s the formal definition of the no religion → Mormon AMCE, for example:\n\\[\n\\begin{aligned}\n\\theta =\\ &P [\\text{Candidate selection} \\mid \\operatorname{do}(\\text{Religion} = \\text{none})]\\ - \\\\\n&P[\\text{Candidate selection} \\mid \\operatorname{do}(\\text{Religion} = \\text{Mormon})]\n\\end{aligned}\n\\]\nWe can find this AMCE (and all the other AMCEs) by calculating the marginal effect/partial derivative of each candidate-level covariate. marginaleffects::avg_slopes() makes this easy and gives us percentage-point-scale estimates instead of logit-scale estimates\n\n\n\n\n\n\nDealing with respondent offsets\n\n\n\nWhen plugging values into avg_slopes (or predictions() or marginal_means() or any function that calculates predictions from a model), we have to decide how to handle the random respondent offsets (\\(b_{0_j}\\)). I have a whole other blog post guide about this and how absolutely maddening the nomenclature for all this is.\nBy default, avg_slopes() and friends will calculate the effects for a typical respondent, or a respondent where the random offset is set to 0. It invisibly uses the re_formula = NA argument to do this. This is also called a conditional effect.\nWe could also use re_formula = NULL to calculate the effect for respondents on average. This is also called a marginal effect. (ARGH I HATE THESE NAMES.). This estimate includes details from the random offsets, either by integrating them out or by using the mean and standard deviation of the random offsets to generate a simulated average respondent.\n\nConditional effect = average respondent = re_formula = NA (default)\nMarginal effect = respondents on average = re_formula = NULL + existing respondent levels or a new simulated respondent\n\nAgain, see this guide for way more about these distinctions. In this example here, we’ll just use conditional effects, or the effect for an average respondent.\n\n\n\nposterior_mfx &lt;- model_brms %&gt;% \n  avg_slopes(newdata = \"mean\", allow_new_levels = TRUE) %&gt;% \n  posteriordraws() \n\nposterior_mfx_nested &lt;- posterior_mfx %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\n# Combine full dataset of factor levels with model results\nplot_data_bayes &lt;- model_variable_levels %&gt;%\n  left_join(\n    posterior_mfx_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(draw = 0, estimate = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\nggplot(plot_data_bayes, aes(x = draw, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  guides(fill = \"none\") +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    title = \"Posterior AMCEs\"\n  )\n\n\n\n\n\n\n\nThe results here are all basically the same as what we found with all the frequentist approaches earlier, but now we have full posteriors for each of these AMCEs so we can do all sorts of neat Bayesian inference things, like calculating the probability that the effect is larger than zero. For instance, there’s a 100% posterior probability that the None → Mormon effect is negative. The Business owner → Lawyer effect, on the other hand, is generally negative, but sometimes positive—there’s a 78% posterior probability that it’s negative.\n\nexample_amces &lt;- posterior_mfx %&gt;% \n  filter(\n    term == \"atreligion\" & contrast == \"Mormon - None\" |\n    term == \"atprof\" & contrast == \"Lawyer - Business owner\"\n  )\n\nexample_amces_p_direction &lt;- example_amces %&gt;% \n  group_by(contrast) %&gt;% \n  summarize(prop_lt_0 = sum(draw &lt; 0) / n()) %&gt;% \n  mutate(\n    prob_nice = label_percent()(prop_lt_0),\n    label = glue::glue(\"P(θ &lt; 0) = {prob_nice}\")\n  )\n\nggplot(example_amces, aes(x = draw)) +\n  stat_halfeye(aes(fill_ramp = after_stat(x &gt; 0)), fill = \"grey50\") +\n  geom_vline(xintercept = 0) +\n  geom_text(\n    data = example_amces_p_direction, \n    aes(x = -0.21, y = 0.9, label = label),\n    hjust = 0\n  ) +\n  scale_x_continuous(labels = label_pp) +\n  scale_fill_ramp_discrete(from = \"#FF851B\", guide = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\", \n    y = NULL, \n    title = \"Two example AMCEs\"\n  ) +\n  facet_wrap(vars(fct_rev(contrast))) +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    panel.background = element_rect(color = \"grey90\", linewidth = 0.5)\n  )"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means-2",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#marginal-means-2",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Marginal means",
    "text": "Marginal means\nInstead of working with causal AMCEs, which are all relative to an omitted feature level, we can work with absolute marginal means to be more descriptive with our estimands. We could find the overall level of favorability of Mormon candidates…\n\\[\n\\theta = P(\\text{Candidate selection} \\mid \\text{Religion = Mormon})\n\\]\n…or the difference between Mormon and Catholic favorability…\n\\[\n\\begin{aligned}\n\\theta =\\ &P[\\text{Candidate selection} \\mid \\text{Religion = Mormon}]\\ - \\\\\n&P[\\text{Candidate selection} \\mid \\text{Religion = Catholic}]\n\\end{aligned}\n\\]\nUnfortunately marginaleffects::marginal_means() doesn’t work with brms models. BUT we can fake it by finding the posterior marginal means for each of the features individually and then combining them into one big data frame.\n\nClick to show the code. It’s hidden because it’s long and repetitive.# There's probably a more efficient way to do with with mapping or loops or\n# whatever but I don't want to figure it out right now, so we brute force it\nposterior_mms &lt;- bind_rows(\n  atmilitary = predictions(\n    model_brms,\n    by = \"atmilitary\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atmilitary) %&gt;% posteriordraws(),\n  atreligion = predictions(\n    model_brms,\n    by = \"atreligion\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atreligion) %&gt;% posteriordraws(),\n  ated = predictions(\n    model_brms,\n    by = \"ated\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = ated) %&gt;% posteriordraws(),\n  atprof = predictions(\n    model_brms,\n    by = \"atprof\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atprof) %&gt;% posteriordraws(),\n  atprof = predictions(\n    model_brms,\n    by = \"atprof\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atprof) %&gt;% posteriordraws(),\n  atinc = predictions(\n    model_brms,\n    by = \"atinc\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atinc) %&gt;% posteriordraws(),\n  atrace = predictions(\n    model_brms,\n    by = \"atrace\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atrace) %&gt;% posteriordraws(),\n  atage = predictions(\n    model_brms,\n    by = \"atage\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atage) %&gt;% posteriordraws(),\n  atmale = predictions(\n    model_brms,\n    by = \"atmale\",\n    allow_new_levels = TRUE\n  ) %&gt;% rename(value = atmale) %&gt;% posteriordraws(),\n  .id = \"term\"\n) %&gt;% \n  as_tibble()\nposterior_mms\n## # A tibble: 184,000 × 7\n##    term       drawid  draw value         estimate conf.low conf.high\n##    &lt;chr&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n##  1 atmilitary 1      0.455 Did Not Serve    0.458    0.436     0.481\n##  2 atmilitary 1      0.546 Served           0.542    0.520     0.564\n##  3 atmilitary 2      0.459 Did Not Serve    0.458    0.436     0.481\n##  4 atmilitary 2      0.531 Served           0.542    0.520     0.564\n##  5 atmilitary 3      0.463 Did Not Serve    0.458    0.436     0.481\n##  6 atmilitary 3      0.543 Served           0.542    0.520     0.564\n##  7 atmilitary 4      0.450 Did Not Serve    0.458    0.436     0.481\n##  8 atmilitary 4      0.544 Served           0.542    0.520     0.564\n##  9 atmilitary 5      0.468 Did Not Serve    0.458    0.436     0.481\n## 10 atmilitary 5      0.554 Served           0.542    0.520     0.564\n## # ℹ 183,990 more rows\n\n\nHere are the marginal means for all the levels of all candidate features:\n\nplot_posterior_mms &lt;- posterior_mms %&gt;% \n  left_join(variable_lookup, by = join_by(term == variable)) %&gt;% \n  mutate(across(c(value, variable_nice), ~fct_inorder(.)))\n  \nggplot(\n  plot_posterior_mms,\n  aes(x = draw, y = value, fill = variable_nice)\n) +\n  geom_vline(xintercept = 0.5) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_percent()) +\n  guides(fill = \"none\") +\n  labs(\n    x = \"Marginal means of probabilities\",\n    y = NULL,\n    title = \"Posterior marginal means\"\n  )\n\n\n\n\n\n\n\nAnd here are the two we mentioned at the beginning of this section—overall favorability of Mormon candidates and the difference between Mormon and Catholic favorability. Mormon candidates have a median posterior favorability of 41.8%, with a 95% credible interval of 38–46%. The median posterior difference between Mormon and Catholic favorability is 10.8 percentage points, with a 95% credible interval of 5–16 percentage points. Neat.\n\nmms_mormon &lt;- posterior_mms %&gt;% \n  filter(term == \"atreligion\", value == \"Mormon\")\nmms_mormon %&gt;% median_qi(draw)\n## # A tibble: 1 × 6\n##    draw .lower .upper .width .point .interval\n##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 0.418  0.381  0.456   0.95 median qi\n\nmms_mormon_catholic &lt;- posterior_mms %&gt;% \n  filter(term == \"atreligion\", value %in% c(\"Mormon\", \"Catholic\")) %&gt;% \n  select(drawid, draw, value) %&gt;% \n  pivot_wider(names_from = \"value\", values_from = \"draw\") %&gt;% \n  mutate(diff = Mormon - Catholic)\nmms_mormon_catholic %&gt;% median_qi(diff)\n## # A tibble: 1 × 6\n##     diff .lower  .upper .width .point .interval\n##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 -0.107 -0.160 -0.0537   0.95 median qi\n\n\nmm1 &lt;- ggplot(mms_mormon, aes(x = draw)) +\n  stat_halfeye(fill = \"#FF851B\") +\n  facet_wrap(vars(\"Mormon favorability\")) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Marginal mean\", y = NULL) +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    panel.background = element_rect(color = \"grey90\", linewidth = 0.5)\n  )\n\nmm2 &lt;- ggplot(mms_mormon_catholic, aes(x = diff)) +\n  stat_halfeye(fill = \"#B10DC9\") +\n  facet_wrap(vars(\"Difference between Mormons and Catholics\")) +\n  scale_x_continuous(labels = label_pp) +\n  labs(x = \"Difference in marginal means\", y = NULL) +\n  theme(\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    panel.background = element_rect(color = \"grey90\", linewidth = 0.5)\n  )\n\nmm1 | mm2"
  },
  {
    "objectID": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#subgroup-differences",
    "href": "blog/2023/07/25/conjoint-bayesian-frequentist-guide/index.html#subgroup-differences",
    "title": "The ultimate practical guide to conjoint analysis with R",
    "section": "Subgroup differences",
    "text": "Subgroup differences\nFinally, if we’re interested in subgroup differences, we can run a separate model with an interaction term for the subgroups we’re interested in. Again, this candidate experiment didn’t include any respondent-level covariates, so we’ll use the made-up, fake respondent political party that we used earlier.\n\npriors &lt;- c(\n  prior(normal(0, 1), class = Intercept),\n  prior(normal(0, 1), class = b),\n  prior(exponential(1), class = sd)\n)\n\nmodel_brms_military_party &lt;- brm(\n  bf(selected ~ atmilitary * respondent_party + (1 | resID)),\n  data = candidate_fake,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"candidate_model_brms_interaction\"\n)\n\nConditional AMCEs\nWe can calculate two different causal estimands, the conditional AMCE across each respondent political party of switching a candidate from having no military history to having served in the military. Because this model (1) uses logit-scale coefficients and (2) splits the causal effects across a bunch of different regression terms, we’ll use marginaleffects::avg_slopes() to find the group-specific probability-scale marginal effects.\nAmong Republican respondents, the causal effect of a candidate switching from no military history to having military service history has a posterior median of 23.8 percentage points, with a 95% credible interval of 19–28 percentage points. For Democrats, the same causal effect is negative, with a posterior median of −9.8 percentage points and a 95% credible interval of −5 to −14.7 percentage points.\n\nposterior_party_amces &lt;- model_brms_military_party %&gt;% \n  avg_slopes(\n    newdata = \"mean\",\n    variables = \"atmilitary\",\n    by = \"respondent_party\",\n    allow_new_levels = TRUE\n  ) %&gt;% \n  posteriordraws()\n\nposterior_party_amces %&gt;% \n  group_by(respondent_party) %&gt;% \n  median_qi(draw)\n## # A tibble: 2 × 7\n##   respondent_party    draw .lower  .upper .width .point .interval\n##   &lt;fct&gt;              &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Democrat         -0.0982 -0.148 -0.0481   0.95 median qi       \n## 2 Republican        0.238   0.194  0.279    0.95 median qi\n\n\nClick to show the code since it’s so longposterior_party_amces_wide &lt;- posterior_party_amces %&gt;%\n  # The contrast column contains values like this:\n  #   mean(Served) - mean(Did Not Serve)\n  # I could probably use some fancy regex to extract those things, but here I'll\n  # just brute force it and remove \"mean(\" and \")\" with two separate\n  # str_remove()s\n  mutate(\n    contrast = str_remove_all(contrast, \"mean\\\\(\"),\n    contrast = str_remove_all(contrast, \"\\\\)\")\n  ) %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  )\n\n# Combine full dataset of factor levels with marginal effects\nplot_posterior_party_amces &lt;- expand_grid(\n  respondent_party = levels(candidate_fake$respondent_party),\n  filter(model_variable_levels, variable == \"atmilitary\")\n) %&gt;%\n  left_join(\n    posterior_party_amces_wide,\n    by = join_by(variable == term, levels == variable_level, respondent_party)\n  ) %&gt;%\n  # Make these zero\n  mutate(\n    across(\n      c(draw, estimate),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(variable_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.))) %&gt;% \n  mutate(estimate_nice = case_when(\n    estimate != 0 ~ label_amce(estimate),\n    estimate == 0 ~ NA\n  ))\n\nggplot(\n  plot_posterior_party_amces, \n  aes(x = draw, y = levels, color = respondent_party, fill = respondent_party)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(position = position_dodge(width = 0.15)) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(\n    values = parties,\n    guide = guide_legend(\n      override.aes = list(linetype = 0, fill = parties),\n      keywidth = 0.8, keyheight = 0.8)\n  ) +\n  scale_fill_manual(\n    values = colorspace::lighten(parties, 0.4), \n    guide = \"none\"\n  ) +\n  labs(\n    x = \"Percentage point change in probability of candidate selection\",\n    y = NULL,\n    color = NULL,\n    title = \"Posterior AMCEs by respondent party\"\n  ) +\n  facet_wrap(vars(variable_nice)) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\n\n\n\n\n\n\nConditional marginal means\nAnd finally, we can calculate subgroup conditional marginal means to describe general party-based trends (since the relative conditional AMCEs create an illusion of difference). Again, since marginaleffects::marginal_means() doesn’t work with brms models, we can instead use marginaleffects::predictions(), which still works with the neat hypothesis argument for calculating contrasts.\nIn the left panel below, Democratic respondents prefer candidates that haven’t served in the military (55% median posterior; 95% credible interval: 51–58%) to those that have (45% median posterior; 95% credible interval: 41–48%). Republicans strongly prefer candidates with a military history, with a posterior median 62% expressing favorability (95% credible interval: 59–65%). In general, Republicans are more extreme in their preferences for and against candidates with and without military service history.\nIn the right panel we can see the differences between the parties’ marginal means within each level. There’s a posterior median 17 percentage point distance between Republican and Democratic marginal means within the served level and a posterior median 17 percentage point distance between their marginal means in the did not serve level (95% credible interval: 12–21 percentage points). Overall, Republicans prefer candidates with military service; Democrats prefer candidates without military service.\n\nposterior_party_mms &lt;- predictions(\n  model_brms_military_party,\n  by = c(\"atmilitary\", \"respondent_party\"),\n  allow_new_levels = TRUE\n) %&gt;% \n  posterior_draws()\n\nposterior_party_mms %&gt;% \n  group_by(atmilitary, respondent_party) %&gt;% \n  median_qi(draw)\n## # A tibble: 4 × 8\n##   atmilitary    respondent_party  draw .lower .upper .width .point .interval\n##   &lt;fct&gt;         &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Did Not Serve Democrat         0.548  0.513  0.582   0.95 median qi       \n## 2 Did Not Serve Republican       0.380  0.351  0.412   0.95 median qi       \n## 3 Served        Democrat         0.450  0.415  0.485   0.95 median qi       \n## 4 Served        Republican       0.618  0.587  0.648   0.95 median qi\n\ngroup_diffs_terms &lt;- matrix(\n  c(-1, 1, 0, 0,\n    0, 0, -1, 1),\n  ncol = 2\n) %&gt;% \n  magrittr::set_colnames(levels(candidate_fake$atmilitary))\n\nposterior_party_mms_diff &lt;- predictions(\n  model_brms_military_party,\n  by = c(\"atmilitary\", \"respondent_party\"),\n  allow_new_levels = TRUE,\n  hypothesis = group_diffs_terms\n) %&gt;% \n  posterior_draws()\n\nposterior_party_mms_diff %&gt;% \n  group_by(term) %&gt;% \n  median_qi(draw)\n## # A tibble: 2 × 7\n##   term            draw .lower .upper .width .point .interval\n##   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 Did Not Serve -0.167 -0.213 -0.121   0.95 median qi       \n## 2 Served         0.168  0.120  0.215   0.95 median qi\n\n\nClick to show the code since it’s so longmm_posterior_party1 &lt;- ggplot(\n  posterior_party_mms, \n  aes(x = draw, y = atmilitary, color = respondent_party, fill = respondent_party)\n) +\n  geom_vline(xintercept = 0.5) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_color_manual(\n    values = parties,\n    guide = guide_legend(\n      override.aes = list(linetype = 0, fill = parties),\n      keywidth = 0.8, keyheight = 0.8)\n  ) +\n  scale_fill_manual(\n    values = colorspace::lighten(parties, 0.4), \n    guide = \"none\"\n  ) +\n  labs(\n    x = \"Marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Posterior marginal means by respondent party\"\n  ) +\n  facet_wrap(vars(\"Military\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\nmm_posterior_party2 &lt;- ggplot(\n  posterior_party_mms_diff,\n  aes(x = draw, y = term)\n) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(aes(\n    color = \"Republican marginal mean − Democrat marginal mean\",\n    fill = \"Republican marginal mean − Democrat marginal mean\"\n  )) +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(\n    values = \"#85144b\",\n    guide = guide_legend(\n      override.aes = list(linetype = 0, fill = \"#85144b\"),\n      keywidth = 0.8, keyheight = 0.8)\n  ) +\n  scale_fill_manual(\n    values = colorspace::lighten(\"#85144b\", 0.4), \n    guide = \"none\"\n  ) +\n  labs(\n    x = \"Difference in marginal means\",\n    y = NULL,\n    color = NULL,\n    title = \"Difference in marginal means by respondent party\",\n    subtitle = \"Positive differences = Republicans prefer the level\"\n  ) +\n  facet_wrap(vars(\"Military\")) +\n  theme(\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = -5)\n  )\n\nmm_posterior_party1 | mm_posterior_party2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "",
    "text": "I recently posted a guide (mostly for future-me) about how to analyze conjoint survey data with R. I explore two different estimands that social scientists are interested in—causal average marginal component effects (AMCEs) and descriptive marginal means—and show how to find them with R, with both frequentist and Bayesian approaches.\nHowever, that post is a little wrong. It’s not wrong wrong, but it is a bit oversimplified.\nWhen political scientists, psychologists, economists, and other social scientists analyze conjoint data, they overwhelmingly do it with ordinary least squares (OLS) regression, or just standard linear regression (lm(y ~ x) in R; reg y x in Stata). Even if the outcome is binary, they’ll use OLS and call it a linear probability model. The main R package for working with conjoint data in a frequentist way ({cregg}) uses OLS and linear probability models. Social scientists (and economists in particular) adore OLS.\nIn my earlier guide, I showed how to analyze the data with logistic regression, but even that is still overly simplified. In reality, conjoint choice-based experiments are more complex than what regular old OLS regression—or even logistic regression—can handle (though I’m sure some econometrician somewhere has a proof showing that OLS works just fine for multinomial conjoint data :shrug:).\nA recent paper published in Political Science Research and Methods (Jensen et al. 2021) does an excellent job explaining the problem with using plain old OLS to estimate AMCEs and marginal means with conjoint data (access the preprint here). Their main argument boils down to this: OLS throws away too much useful information about (1) the relationships and covariance between the different combinations of feature levels offered to respondents, and (2) individual-specific differences in how respondents react to different feature levels.\nJensen et al. explain three different approaches to analyzing data that has a natural hierarchical structure like conjoint data (where lots of choices related to different “products” are nested within individuals). This also is the same argument from chapter 15 of Bayes Rules!.\nBy using a multilevel hierarchical model, Jensen et al. (2021) show that we can still find AMCEs and causal effects, just like in my previous guide, but we can take advantage of the far richer heterogeneity that we get from these complex statements. We can make cool statements like this (in an experiment that varied policies related to unions):\nUsing hierarchical models for conjoint experiments in political science is new and exciting and revolutionary and neat. That’s the whole point of Jensen et al.’s paper—it’s a call to stop using OLS for everything.\nI’ve been working on a conjoint experiment with my coauthors Marc Dotson and Suparna Chaudhry. Suparna and I are political scientists and this multilevel stuff in general is still relatively new and wildly underused in the discipline. Marc, though, is a marketing scholar. The marketing world has been using hierarchical models for conjoint experiments for a long time and it’s standard practice in that discipline. There’s a whole textbook about the hierarchical model approach in marketing (Chapman and Feit 2019), and these fancy conjoint multilevel models are used widely throughout the marketing industry.\nlol at political science, just now discovering this.\nSo, I need to expand my previous conjoint guide. That’s what this post is for.\nI’ll do three things in this guide:\nThroughout this example, I’ll use data from two different simulated conjoint choice experiments. You can download these files and follow along:\nAdditionally, in Part 3, I fit a huge Stan model with {brms} that takes ≈30 minutes to run on my fast laptop. If you want to follow along and not melt your CPU for half an hour, you can download an .rds file of that fitted model that I stuck in an OSF project. The code for brm() later in this guide will load the .rds file automatically instead of rerunning the model as long as you put it in a folder named “models” in your working directory. This code uses the {osfr} package to download the .rds file from OSF automatically and places it where it needs to go:\nlibrary(osfr)  # Interact with OSF via R\n\n# Make a \"models\" folder if it doesn't exist already\nif (!file.exists(\"models\")) { dir.create(\"models\") }\n\n# Download model_minivans_mega_mlm_brms.rds from OSF\nosf_retrieve_file(\"https://osf.io/zp6eh\") |&gt;\n  osf_download(path = \"models\", conflicts = \"overwrite\", progress = TRUE)\nLet’s load some libraries, create some helper functions, load the data, and get started!\nlibrary(tidyverse)        # ggplot, dplyr, and friends\nlibrary(broom)            # Convert model objects to tidy data frames\nlibrary(parameters)       # Show model results as nice tables\nlibrary(survey)           # Panel-ish frequentist regression models\nlibrary(mlogit)           # Frequentist multinomial regression models\nlibrary(dfidx)            # Structure data for {mlogit} models\nlibrary(scales)           # Nicer labeling functions\nlibrary(marginaleffects)  # Calculate marginal effects\nlibrary(ggforce)          # For facet_col()\nlibrary(brms)             # The best formula-based interface to Stan\nlibrary(tidybayes)        # Manipulate Stan results in tidy ways\nlibrary(ggdist)           # Fancy distribution plots\nlibrary(patchwork)        # Combine ggplot plots\nlibrary(rcartocolor)      # Color palettes from CARTOColors (https://carto.com/carto-colors/)\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://github.com/intel/clear-sans\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Clear Sans\") +\n    theme(panel.grid.minor = element_blank(),\n          plot.title = element_text(family = \"Clear Sans\", face = \"bold\"),\n          axis.title.x = element_text(hjust = 0),\n          axis.title.y = element_text(hjust = 1),\n          strip.text = element_text(family = \"Clear Sans\", face = \"bold\",\n                                    size = rel(0.75), hjust = 0),\n          strip.background = element_rect(fill = \"grey90\", color = NA))\n}\n\ntheme_set(theme_nice())\n\nclrs &lt;- carto_pal(name = \"Prism\")\n\n# Functions for formatting things as percentage points\nlabel_pp &lt;- label_number(accuracy = 1, scale = 100, \n                         suffix = \" pp.\", style_negative = \"minus\")\nchocolate &lt;- read_csv(\"data/choco_candy.csv\") %&gt;% \n  mutate(\n    dark = case_match(dark, 0 ~ \"Milk\", 1 ~ \"Dark\"),\n    dark = factor(dark, levels = c(\"Milk\", \"Dark\")),\n    soft = case_match(soft, 0 ~ \"Chewy\", 1 ~ \"Soft\"),\n    soft = factor(soft, levels = c(\"Chewy\", \"Soft\")),\n    nuts = case_match(nuts, 0 ~ \"No nuts\", 1 ~ \"Nuts\"),\n    nuts = factor(nuts, levels = c(\"No nuts\", \"Nuts\"))\n  )\n\nminivans &lt;- read_csv(\"data/rintro-chapter13conjoint.csv\") %&gt;% \n  mutate(\n    across(c(seat, cargo, price), factor),\n    carpool = factor(carpool, levels = c(\"no\", \"yes\")),\n    eng = factor(eng, levels = c(\"gas\", \"hyb\", \"elec\"))\n  )"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The setup",
    "text": "The setup\nIn this experiment, respondents are asked to choose which of these kinds of candies they’d want to buy. Respondents only see this question one time and all possible options are presented simultaneously.\n\n\n\n\n\n\nExample survey question\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\n\n\n\nChocolate\nMilk\nMilk\nMilk\nMilk\nDark\nDark\nDark\nDark\n\n\nCenter\nChewy\nChewy\nSoft\nSoft\nChewy\nChewy\nSoft\nSoft\n\n\nNuts\nNo nuts\nNuts\nNo nuts\nNuts\nNo nuts\nNuts\nNo nuts\nNuts\n\n\nChoice"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The data",
    "text": "The data\nThe data for this kind of experiment looks like this, with one row for each possible alternative (so eight rows per person, or subj), with the alternative that was selected marked as 1 in choice. Here, Subject 1 chose option E (dark, chewy, no nuts). There were 10 respondents, with 8 rows each, so there are 10 × 8 = 80 rows.\n\nchocolate\n## # A tibble: 80 × 6\n##     subj choice alt   dark  soft  nuts   \n##    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  \n##  1     1      0 A     Milk  Chewy No nuts\n##  2     1      0 B     Milk  Chewy Nuts   \n##  3     1      0 C     Milk  Soft  No nuts\n##  4     1      0 D     Milk  Soft  Nuts   \n##  5     1      1 E     Dark  Chewy No nuts\n##  6     1      0 F     Dark  Chewy Nuts   \n##  7     1      0 G     Dark  Soft  No nuts\n##  8     1      0 H     Dark  Soft  Nuts   \n##  9     2      0 A     Milk  Chewy No nuts\n## 10     2      0 B     Milk  Chewy Nuts   \n## # ℹ 70 more rows"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The model",
    "text": "The model\nRespondents were shown eight different options and asked to select one. While this seems like a binary yes/no choice that could work with just regular plain old logistic regression, we want to account for the features and levels in all the unchosen categories too. To do this, we can use multinomial logistic regression, where the outcome variable is an unordered categorical variable with more than two categories. In this case we have eight different possible outcomes: alternatives A through H.\nOriginal SAS model as a baseline\n\n\n\n\n\n\nlol SAS\n\n\n\nI know nothing about SAS. I have never opened SAS in my life. It is a mystery to me.\nI copied these results directly from p. 297 in SAS’s massive “Discrete Choice” technical note (Kuhfeld 2010).\nI only have this SAS output here as a baseline reference for what the actual correct coefficients are supposed to be.\n\n\nSAS apparently fits these models with proportional hazard survival-style models, which feels weird, but there’s probably a mathematical or statistical reason for it. You use PROC PHREG to do it:\nproc phreg data=chocs outest=betas;\n   strata subj set;\n   model c*c(2) = dark soft nuts / ties=breslow;\n   run;\nIt gives these results:\n                   Choice of Chocolate Candies\n\n                       The PHREG Procedure\n\n              Multinomial Logit Parameter Estimates\n              \n                      Parameter     Standard\n                 DF    Estimate        Error  Chi-Square   Pr &gt; ChiSq\nDark Chocolate   1      1.38629      0.79057      3.0749       0.0795\nSoft Center      1     -2.19722      1.05409      4.3450       0.0371\nWith Nuts        1      0.84730      0.69007      1.5076       0.2195\nSurvival model\nEw, enough SAS. Let’s do this with R instead.\nWe can recreate the same proportional hazards model with coxph() from the {survival} package. Again, this feels weird and not like an intended purpose of survival models and not like multinomial logit at all—in my mind it is neither (1) multinomial nor (2) logit, but whatever. People far smarter than me invented these things, so I’ll just trust them.\n\nmodel_chocolate_survival &lt;- coxph(\n  Surv(subj, choice) ~ dark + soft + nuts, \n  data = chocolate, \n  ties = \"breslow\"  # This is what SAS uses\n)\n\nmodel_parameters(model_chocolate_survival, digits = 4, p_digits = 4)\n## Parameter   | Coefficient |     SE |             95% CI |       z |      p\n## --------------------------------------------------------------------------\n## dark [Dark] |      1.3863 | 0.7906 | [-0.1632,  2.9358] |  1.7535 | 0.0795\n## soft [Soft] |     -2.1972 | 1.0541 | [-4.2632, -0.1312] | -2.0845 | 0.0371\n## nuts [Nuts] |      0.8473 | 0.6901 | [-0.5052,  2.1998] |  1.2279 | 0.2195\n\nThe coefficients, standard errors, and p-values are identical to the SAS output! The only difference is the statistic: in SAS they use a chi-square statistic, while survival:coxph() uses a z statistic. There’s probably a way to make coxph() use a chi-square statistic, but I don’t care about that. I never use survival models and I’m only doing this to replicate the SAS output and it just doesn’t matter.\nPoisson model\nAn alternative way to fit a multinomial logit model without resorting to survival models is to actually (mis?)use another model family. We can use a Poisson model, even though choice isn’t technically count data, because of obscure stats reasons. See here for an illustration of the relationship between multinomial and Poisson distributions; or see this 2011 Biometrika paper about using Poisson models to reduce bias in multinomial logit models. Richard McElreath has a subsection about this in Statistical Rethinking as well: “Multinomial in disguise as Poisson” (11.3.3). Or as he said over on the currently-walled-garden Bluesky, “All count distributions are just one or more Poisson distributions in a trench coat.”\nTo account for the repeated subjects in the data, we’ll use svyglm() from the {survey} package so that the standard errors are more accurate.\n\nmodel_chocolate_poisson &lt;- glm(\n  choice ~ dark + soft + nuts, \n  data = chocolate, \n  family = poisson()\n)\n\nmodel_parameters(model_chocolate_poisson, digits = 4, p_digits = 4)\n## Parameter   | Log-Mean |     SE |             95% CI |       z |      p\n## -----------------------------------------------------------------------\n## (Intercept) |  -2.9188 | 0.8628 | [-4.9727, -1.4905] | -3.3829 | 0.0007\n## dark [Dark] |   1.3863 | 0.7906 | [ 0.0023,  3.2772] |  1.7535 | 0.0795\n## soft [Soft] |  -2.1972 | 1.0541 | [-5.1119, -0.5256] | -2.0845 | 0.0371\n## nuts [Nuts] |   0.8473 | 0.6901 | [-0.4328,  2.3820] |  1.2279 | 0.2195\n\nLovely—the results are the same.\n\nmlogit model\nFinally, we can use the {mlogit} package to fit the model. Before using mlogit(), we need to transform our data a bit to specify which column represents the choice (choice) and how the data is indexed: subjects (subj) with repeated alternatives (alt).\n\nchocolate_idx &lt;- dfidx(\n  chocolate,\n  idx = list(\"subj\", \"alt\"),\n  choice = \"choice\",\n  shape = \"long\"\n)\n\nWe can then use this indexed data frame with mlogit(), which uses the familiar R formula interface, but with some extra features separated by |s\noutcome ~ features | individual-level variables | alternative-level variables\nIf we had columns related to individual-level characteristics or alternative-level characteristics, we could include those in the model—and we’ll do precisely that later in this post. (Incorporating individual-level covariates is the whole point of this post!)\nLet’s fit the model:\n\nmodel_chocolate_mlogit &lt;- mlogit(\n  choice ~ dark + soft + nuts | 0 | 0, \n  data = chocolate_idx\n)\n\nmodel_parameters(model_chocolate_mlogit, digits = 4, p_digits = 4)\n## Parameter   | Log-Odds |     SE |             95% CI |       z |      p\n## -----------------------------------------------------------------------\n## dark [Dark] |   1.3863 | 0.7906 | [-0.1632,  2.9358] |  1.7535 | 0.0795\n## soft [Soft] |  -2.1972 | 1.0541 | [-4.2632, -0.1312] | -2.0845 | 0.0371\n## nuts [Nuts] |   0.8473 | 0.6901 | [-0.5052,  2.1998] |  1.2279 | 0.2195\n\nDelightful. All the results are the same as the survival model and the Poisson model.\n\nmclogit model\nAs noted earlier, {marginaleffects} doesn’t support mlogit() because of its weird internal structure. It does support mclogit::mclogit() though.\nThe syntax requires two parts on the left-hand side of the formula: (1) the choice selected (0 or 1), and (2) a unique id for set of choices, or the question. In this case, since each subject only saw one question, the subj column doubles as the question ID, so we can just use that.\nThe results are the same:\n\nlibrary(mclogit)\n\nmodel_chocolate_mclogit &lt;- mclogit(\n  choice | subj ~ dark + soft + nuts,\n  data = chocolate\n)\n## \n## Iteration 1 - deviance = 29.16 - criterion = 0.1735\n## Iteration 2 - deviance = 28.74 - criterion = 0.01449\n## Iteration 3 - deviance = 28.73 - criterion = 0.0003555\n## Iteration 4 - deviance = 28.73 - criterion = 5.523e-07\n## Iteration 5 - deviance = 28.73 - criterion = 1.554e-12\n## converged\nmodel_parameters(model_chocolate_mclogit, digits = 4, p_digits = 4)\n## Parameter   | Log-Odds |     SE |             95% CI |       z |      p\n## -----------------------------------------------------------------------\n## dark [Dark] |   1.3863 | 0.7906 | [-0.1632,  2.9358] |  1.7535 | 0.0795\n## soft [Soft] |  -2.1972 | 1.0541 | [-4.2632, -0.1312] | -2.0845 | 0.0371\n## nuts [Nuts] |   0.8473 | 0.6901 | [-0.5052,  2.1998] |  1.2279 | 0.2195\n## \n## Uncertainty intervals (equal-tailed) and p-values (two-tailed) computed using a Wald z-distribution approximation.\n\nBayesian model\nWe can also fit this model in a Bayesian way using {brms}. Stan has a categorical distribution family for multinomial models, and we’ll use it in the next example. For now, for the sake of simplicity, we’ll use a Poisson family, since, as we saw above, that’s a legal way of parameterizing multinomial distributions.\nThe data has a natural hierarchical structure to it, with 8 choices (for alternatives A through H) nested inside each of the 10 subjects.\n\n\n\n\nMultilevel experimental structure, with candy choices \\(y_{\\text{A}\\dots\\text{H}}\\) nested in subjects\n\n\n\nWe want to model candy choice (choice) based on candy characteristics (dark, soft, and nuts). We’ll use the subscript \\(i\\) to refer to individual candy choices and \\(j\\) to refer to subjects.\nSince we can legally pretend that this multinomial selection process is actually Poisson, we’ll model it as a Poisson process that has a rate of \\(\\lambda_{i_j}\\). We’ll model that \\(\\lambda_{i_j}\\) with a log-linked regression model with covariates for each of the levels of each candy feature. To account for the multilevel structure, we’ll include subject-specific offsets (\\(b_{0_j}\\)) from the global average, thus creating random intercepts. We’ll specify fairly wide priors just because.\nHere’s the formal model for all this:\n\\[\n\\begin{aligned}\n&\\ \\textbf{Probability of selection of alternative}_i \\textbf{ in subject}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Poisson}(\\lambda_{i_j}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\log(\\lambda_{i_j}) =&\\ (\\beta_0 + b_{0_j}) + \\beta_1 \\text{Dark}_{i_j} + \\beta_2 \\text{Soft}_{i_j} + \\beta_3 \\text{Nuts}_{i_j} \\\\[5pt]\nb_{0_j} \\sim&\\ \\mathcal{N}(0, \\sigma_0) \\qquad\\qquad\\quad \\text{Subject-specific offsets from global choice probability} \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_0 \\sim&\\ \\mathcal{N}(0, 3) \\qquad\\qquad\\quad\\ \\ \\text{Prior for global average choice probability} \\\\\n\\beta_1, \\beta_2, \\beta_3 \\sim&\\ \\mathcal{N}(0, 3) \\qquad\\qquad\\quad\\ \\ \\text{Prior for candy feature levels} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) \\qquad \\text{Prior for between-subject variability}\n\\end{aligned}\n\\]\nAnd here’s the {brms} model:\n\nmodel_chocolate_brms &lt;- brm(\n  bf(choice ~ dark + soft + nuts + (1 | subj)),\n  data = chocolate,\n  family = poisson(),\n  prior = c(\n    prior(normal(0, 3), class = Intercept),\n    prior(normal(0, 3), class = b),\n    prior(exponential(1), class = sd)\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"models/model_chocolate_brms\"\n)\n\nThe results are roughly the same as what we found with all the other models—they’re slightly off because of random MCMC sampling.\n\nmodel_parameters(model_chocolate_brms)\n## Running MCMC with 4 sequential chains, with 2 thread(s) per chain...\n## \n## Chain 1 finished in 0.1 seconds.\n## Chain 2 finished in 0.1 seconds.\n## Chain 3 finished in 0.1 seconds.\n## Chain 4 finished in 0.1 seconds.\n## \n## All 4 chains finished successfully.\n## Mean chain execution time: 0.1 seconds.\n## Total execution time: 1.0 seconds.\n## # Fixed Effects\n## \n## Parameter   | Median |         95% CI |     pd |  Rhat |     ESS\n## ----------------------------------------------------------------\n## (Intercept) |  -3.04 | [-5.07, -1.60] |   100% | 1.000 | 3598.00\n## darkDark    |   1.35 | [-0.05,  3.16] | 96.92% | 1.000 | 4238.00\n## softSoft    |  -2.03 | [-4.35, -0.47] | 99.60% | 1.000 | 2867.00\n## nutsNuts    |   0.83 | [-0.40,  2.27] | 90.28% | 1.000 | 4648.00"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Predictions",
    "text": "Predictions\nIn the SAS technical note example, they use the model to generated predicted probabilities of the choice of each of the options. In the world of marketing, this can also be seen as the predicted market share for each option. To do this, they plug each of the eight different different combinations of dark, soft, and nuts into the model and calculate the predicted output on the response (i.e. probability) scale. They get these results, where dark, chewy, and nuts is the most likely and popular option (commanding a 50% market share).\n      Choice of Chocolate Candies\n\nObs    Dark    Soft     Nuts       p\n\n  1    Dark    Chewy    Nuts       0.50400\n  2    Dark    Chewy    No Nuts    0.21600\n  3    Milk    Chewy    Nuts       0.12600\n  4    Dark    Soft     Nuts       0.05600\n  5    Milk    Chewy    No Nuts    0.05400\n  6    Dark    Soft     No Nuts    0.02400\n  7    Milk    Soft     Nuts       0.01400\n  8    Milk    Soft     No Nuts    0.00600\nWe can do the same thing with R.\nFrequentist predictions\n{mlogit} model objects have predicted values stored in one of their slots (model_chocolate_mlogit$probabilities), but they’re in a weird non-tidy matrix form and I like working with tidy data. I’m also a huge fan of the {marginaleffects} package, which provides a consistent way to calculate predictions, comparisons, and slopes/marginal effects (with predictions(), comparisons(), and slopes()) for dozens of kinds of models, including mlogit() models. So instead of wrangling the built-in mlogit() probabilities, we’ll generate predictions by feeding the model the unique combinations of dark, soft, and nuts to marginaleffects::predictions(), which will provide us with probability- or proportion-scale predictions:\nWe can calculate probability-scale predictions by feeding the alternatives A–H (or the unique combinations of dark, soft, and nuts) to the model using predict(). We have to first create a little dataset of the alternatives (here I filter the data to include only Subject 1), and then feed that to predict(), which returns a named vector. To get that into a more usable data frame, we can convert the named vector into a data frame with tidyr::enframe() and then join it to the data frame of alternatives:\n\n# Unique combinations of dark, soft, and nuts across the 8 alternatives\nalts &lt;- chocolate %&gt;% \n  filter(subj == 1) %&gt;% \n  select(-subj, -choice)\n\n# Named vector of predicted values\npreds &lt;- predict(model_chocolate_mlogit, newdata = alts)\n\npreds_chocolate_mlogit &lt;- alts %&gt;% \n  left_join(enframe(preds, name = \"alt\", value = \"estimate\"), by = join_by(alt)) %&gt;% \n  arrange(desc(estimate))\npreds_chocolate_mlogit\n## # A tibble: 8 × 5\n##   alt   dark  soft  nuts    estimate\n##   &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;\n## 1 F     Dark  Chewy Nuts     0.504  \n## 2 E     Dark  Chewy No nuts  0.216  \n## 3 B     Milk  Chewy Nuts     0.126  \n## 4 H     Dark  Soft  Nuts     0.0560 \n## 5 A     Milk  Chewy No nuts  0.0540 \n## 6 G     Dark  Soft  No nuts  0.0240 \n## 7 D     Milk  Soft  Nuts     0.0140 \n## 8 C     Milk  Soft  No nuts  0.00600\n\nPerfect! They’re identical to the SAS output.\n\n\n\n\n\n\njklol they’re not perfect\n\n\n\nAs far as I can tell, predict() doesn’t provide standard errors for mlogit()-based models, and none of the more standard helper packages like marginaleffects::predictions() or broom::agument() really do either. I guess that’s good incentive to do this with Bayesian methods instead :)\n\n\nWe can play around with these predictions to describe the overall market for candy. Chewy candies dominate the market…\n\npreds_chocolate_mlogit %&gt;% \n  group_by(dark) %&gt;% \n  summarize(share = sum(estimate))\n## # A tibble: 2 × 2\n##   dark  share\n##   &lt;fct&gt; &lt;dbl&gt;\n## 1 Milk  0.200\n## 2 Dark  0.800\n\n…and dark chewy candies are by far the most popular:\n\npreds_chocolate_mlogit %&gt;% \n  group_by(dark, soft) %&gt;% \n  summarize(share = sum(estimate))\n## # A tibble: 4 × 3\n## # Groups:   dark [2]\n##   dark  soft   share\n##   &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n## 1 Milk  Chewy 0.180 \n## 2 Milk  Soft  0.0200\n## 3 Dark  Chewy 0.720 \n## 4 Dark  Soft  0.0800\n\nBayesian predictions\n{marginaleffects} supports {brms} models too, so we can use its predictions() function to generate predictions for our Bayesian model.\n\n\n\n\n\n\nlol subject offsets\n\n\n\nWhen plugging values into predictions() (or avg_slopes() or any function that calculates predictions from a model), we have to decide how to handle the random subject offsets (\\(b_{0_j}\\)). I have a whole other blog post guide about this and how absolutely maddening the nomenclature for all this is.\nBy default, predictions() and friends will calculate predictions for subjects on average by using the re_formula = NULL argument. This estimate includes details from the random offsets, either by integrating them out or by using the mean and standard deviation of the random offsets to generate a simulated average subject. When working with slopes, this is also called a marginal effect.\nWe could also use re_formula = NA to calculate predictions for a typical subject, or a subject where the random offset is set to 0. When working with slopes, this is also called a conditional effect.\n\nConditional predictions/effect = average subject = re_formula = NA\n\nMarginal predictions/effect = subjects on average = re_formula = NULL (default), using existing subject levels or a new simulated subject\n\nAgain, see this guide for way more about these distinctions. In this example here, we’ll just use the default marginal predictions/effects (re_formula = NULL), or the effect for subjects on average.\n\n\nThe predicted proportions aren’t identical to the SAS output, but they’re close enough, given that it’s a completely different modeling approach.\n\npreds_chocolate_brms &lt;- predictions(\n  model_chocolate_brms, \n  newdata = datagrid(dark = unique, soft = unique, nuts = unique)\n) \n\npreds_chocolate_brms %&gt;% \n  as_tibble() %&gt;%\n  arrange(desc(estimate)) %&gt;% \n  select(dark, soft, nuts, estimate, conf.low, conf.high)\n## # A tibble: 8 × 6\n##   dark  soft  nuts    estimate conf.low conf.high\n##   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 Dark  Chewy Nuts     0.432   0.144       1.09  \n## 2 Dark  Chewy No nuts  0.186   0.0424      0.571 \n## 3 Milk  Chewy Nuts     0.110   0.0168      0.419 \n## 4 Dark  Soft  Nuts     0.0553  0.00519     0.279 \n## 5 Milk  Chewy No nuts  0.0465  0.00552     0.219 \n## 6 Dark  Soft  No nuts  0.0230  0.00182     0.141 \n## 7 Milk  Soft  Nuts     0.0136  0.00104     0.0881\n## 8 Milk  Soft  No nuts  0.00556 0.000326    0.0414\n\nPlots\nSince predictions() returns a tidy data frame, we can plot these predicted probabilities (or market shares or however we want to think about them) with {ggplot2}:\n\np1 &lt;- preds_chocolate_mlogit %&gt;% \n  arrange(estimate) %&gt;% \n  mutate(label = str_to_sentence(glue::glue(\"{dark} chocolate, {soft} interior, {nuts}\"))) %&gt;% \n  mutate(label = fct_inorder(label)) %&gt;% \n  ggplot(aes(x = estimate, y = label)) +\n  geom_point(color = clrs[7]) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(\n    x = \"Predicted probability of selection\", y = NULL,\n    title = \"Frequentist {mlogit} predictions\") +\n  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank())\n\np2 &lt;- preds_chocolate_brms %&gt;% \n  posterior_draws() %&gt;%  # Extract the posterior draws of the predictions\n  arrange(estimate) %&gt;% \n  mutate(label = str_to_sentence(glue::glue(\"{dark} chocolate, {soft} interior, {nuts}\"))) %&gt;% \n  mutate(label = fct_inorder(label)) %&gt;% \n  ggplot(aes(x = draw, y = label)) +\n  stat_halfeye(normalize = \"xy\", fill = clrs[7])  +\n  scale_x_continuous(labels = label_percent()) +\n  labs(\n    x = \"Predicted probability of selection\", y = NULL,\n    title = \"Bayesian {brms} predictions\") +\n  theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank()) +\n  # Make the x-axis match the mlogit plot\n  coord_cartesian(xlim = c(-0.05, 0.78))\n\np1 / p2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nThe marketing world doesn’t typically look at coefficients or marginal effects, but the political science world definitely does. In political science, the estimand we often care about the most is the average marginal component effect (AMCE), or the causal effect of moving one feature level to a different value, holding all other features constant. I have a whole in-depth blog post about AMCEs and how to calculate them—go look at that for more details. Long story short—AMCEs are basically the coefficients in a regression model.\nInterpreting the coefficients is difficult with models that aren’t basic linear regression. Here, all these coefficients are on the log scale, so they’re not directly interpretable. The original SAS technical note also doesn’t really interpret any of these , they don’t really interpret these things anyway, since they’re more focused on predictions. All they say is this:\n\nThe parameter estimate with the smallest p-value is for soft center. Since the parameter estimate is negative, chewy is the more preferred level. Dark is preferred over milk, and nuts over no nuts, however only the p-value for Soft is less than 0.05.\n\nWe could exponentiate the coefficients to make them multiplicative (akin to odds ratios in logistic regression). For center = soft, \\(e^{-2.19722}\\) = 0.1111, which means that candies with a soft center are 89% less likely to be chosen than candies with a chewy center, relative to the average candy. But that’s weird to think about.\nSo instead we can turn to {marginaleffects} once again to calculate percentage-point scale estimands that we can interpret far more easily.\n\n\n\n\n\n\nlol marginal effects\n\n\n\nNobody is ever consistent about the word “marginal effect.” Some people use it to refer to averages; some people use it to refer to slopes. These are complete opposites. In calculus, averages = integrals and slopes = derivatives and they’re the inverse of each other.\nI like to think of marginal effects as what happens to the outcome when you move an explanatory variable a tiny bit. With continuous variables, that’s a slope; with categorical variables, that’s an offset in average outcomes. These correspond directly to how you normally interpret regression coefficients. Or returning to my favorite analogy about regression, with numeric variables we care what happens to the outcome when we slide the value up a tiny bit; with categorical variables we care about what happens to the outcome when we switch on a category.\nAdditionally, there are like a billion different ways to calculate marginal effects: average marginal effects (AMEs), group-average marginal effects (G-AMEs), marginal effects at user-specified values, marginal effects at the mean (MEM), and counterfactual marginal effects. See the documentation for {marginaleffects} + this mega blog post for more about these subtle differences.\n\n\nBayesian comparisons/contrasts\nWe can use avg_comparisons() to calculate the difference (or average marginal effect) for each of the categorical coefficients on the percentage-point scale, showing the effect of moving from milk → dark, chewy → soft, and nuts → no nuts.\n(Technically we can also use avg_slopes(), even though none of these coefficients are actually slopes. {marginaleffects} is smart enough to show contrasts for categorical variables and partial derivatives/slopes for continuous variables.)\n\navg_comparisons(model_chocolate_brms)\n## \n##  Term                   Contrast Estimate    2.5 %  97.5 %\n##  dark mean(Dark) - mean(Milk)       0.139 -0.00551  0.3211\n##  nuts mean(Nuts) - mean(No nuts)    0.092 -0.05221  0.2599\n##  soft mean(Soft) - mean(Chewy)     -0.182 -0.35800 -0.0523\n## \n## Type:  response \n## Columns: term, contrast, estimate, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx\n\nWhen holding all other features constant, moving from chewy → soft is associated with a posterior median 18 percentage point decrease in the probability of selection (or drop in market share if you want to think of it that way), on average.\nFrequentist comparisons/contrasts\nWe went out of order in this section and showed how to use avg_comparisons() with the Bayesian model first instead of the frequentist model. That’s because it was easy.\nmlogit() models don’t work with {marignaleffects} (or other model wrangling packages like {broom}), so there’s no good way to do it. We’re limited to calculating the contrasts of predictions by hand and manipulating and collapsing the data frame of predictions that we made previously.\nIt’s helpful to illustrate what exactly we’re looking at when collapsing these results. Remember that earlier we calculated predictions for all the unique combinations of dark, soft, and nuts:\n\n# Unique combinations of dark, soft, and nuts across the 8 alternatives\nalts &lt;- chocolate %&gt;% \n  filter(subj == 1) %&gt;% \n  select(-subj, -choice)\n\n# Named vector of predicted values\npreds &lt;- predict(model_chocolate_mlogit, newdata = alts, se.fit = TRUE)\n\npreds_chocolate_mlogit &lt;- alts %&gt;% \n  left_join(enframe(preds, name = \"alt\", value = \"estimate\"), by = join_by(alt)) %&gt;% \n  arrange(desc(estimate))\npreds_chocolate_mlogit\n## # A tibble: 8 × 5\n##   alt   dark  soft  nuts    estimate\n##   &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;dbl&gt;\n## 1 F     Dark  Chewy Nuts     0.504  \n## 2 E     Dark  Chewy No nuts  0.216  \n## 3 B     Milk  Chewy Nuts     0.126  \n## 4 H     Dark  Soft  Nuts     0.0560 \n## 5 A     Milk  Chewy No nuts  0.0540 \n## 6 G     Dark  Soft  No nuts  0.0240 \n## 7 D     Milk  Soft  Nuts     0.0140 \n## 8 C     Milk  Soft  No nuts  0.00600\n\nFour of the groups have dark = Milk and four have dark = Dark, with other varying characteristics across those groups (chewy/soft, nuts/no nuts). If we want the average proportion of all milk and dark chocolate options, we can group and summarize:\n\npreds_dark &lt;- preds_chocolate_mlogit %&gt;% \n  group_by(dark) %&gt;% \n  summarize(avg_pred = mean(estimate))\npreds_dark\n## # A tibble: 2 × 2\n##   dark  avg_pred\n##   &lt;fct&gt;    &lt;dbl&gt;\n## 1 Milk    0.0500\n## 2 Dark    0.200\n\nThe average market share for milk chocolate candies, holding all other features constant, is 5% (\\(\\frac{0.0540 + 0.126 + 0.006 + 0.014}{2} = 0.05\\)); the average market share for dark chocolate candies is 20% (\\(\\frac{0.216 + 0.504 + 0.024 + 0.056}{2} = 0.2\\)). These values are the averages of the predictions from the four groups where dark is either Milk or Dark.\nIf we find the difference between the two predictions, we’ll see the average causal effect of moving from dark → milk—holding all other features constant, switching the chocolate type from dark to milk causes a 15 percentage point decrease in the probability of selecting the candy, on average.\n\n# Using diff() (reversing it so that it does Milk - Dark)\ndiff(rev(preds_dark$avg_pred))\n## [1] -0.15\n\n# Tidyverse way\npreds_dark %&gt;% \n  pivot_wider(names_from = dark, values_from = avg_pred) %&gt;% \n  mutate(\n    term = \"Milk - Dark\",\n    estimate = Milk - Dark\n  ) %&gt;% \n  select(term, estimate)\n## # A tibble: 1 × 2\n##   term        estimate\n##   &lt;chr&gt;          &lt;dbl&gt;\n## 1 Milk - Dark   -0.150\n\nWe can use this same approach to make a big data frame of all the contrasts:\n\namces_chocolate_mlogit &lt;- bind_rows(\n  dark = preds_chocolate_mlogit %&gt;% \n    group_by(dark) %&gt;% \n    summarize(avg_pred = mean(estimate)) %&gt;% \n    pivot_wider(names_from = dark, values_from = avg_pred) %&gt;% \n    mutate(\n      term = \"Dark - Milk\",\n      estimate = Dark - Milk\n    ) %&gt;% \n    select(term, estimate),\n  soft = preds_chocolate_mlogit %&gt;% \n    group_by(soft) %&gt;% \n    summarize(avg_pred = mean(estimate)) %&gt;% \n    pivot_wider(names_from = soft, values_from = avg_pred) %&gt;% \n    mutate(\n      term = \"Soft - Chewy\",\n      estimate = Soft - Chewy\n    ) %&gt;% \n    select(term, estimate),\n  nuts = preds_chocolate_mlogit %&gt;% \n    group_by(nuts) %&gt;% \n    summarize(avg_pred = mean(estimate)) %&gt;% \n    pivot_wider(names_from = nuts, values_from = avg_pred) %&gt;% \n    mutate(\n      term = \"Nuts - No nuts\",\n      estimate = Nuts - `No nuts`\n    ) %&gt;% \n    select(term, estimate),\n  .id = \"variable\"\n)\namces_chocolate_mlogit\n## # A tibble: 3 × 3\n##   variable term           estimate\n##   &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt;\n## 1 dark     Dark - Milk       0.150\n## 2 soft     Soft - Chewy     -0.200\n## 3 nuts     Nuts - No nuts    0.1\n\nFor instance, what’s the causal effect of moving from milk → dark chocolate?\n\n# Unique combinations of dark, soft, and nuts across the 8 alternatives\nalts &lt;- chocolate %&gt;% \n  filter(subj == 1) %&gt;% \n  select(-subj, -choice)\n\n# Named vector of predicted values\npreds &lt;- predict(model_chocolate_mlogit, newdata = alts, se.fit = TRUE)\n\npreds_chocolate_mlogit &lt;- alts %&gt;% \n  left_join(enframe(preds, name = \"alt\", value = \"estimate\"), by = join_by(alt)) %&gt;% \n  arrange(desc(estimate))\n\npreds_chocolate_mlogit %&gt;% \n  group_by(dark) %&gt;% \n  summarize(estimate = mean(estimate))\n## # A tibble: 2 × 2\n##   dark  estimate\n##   &lt;fct&gt;    &lt;dbl&gt;\n## 1 Milk    0.0500\n## 2 Dark    0.200\n\nPlots\nPlotting these AMCEs requires a bit of data wrangling, but we get really neat plots, so it’s worth it. I’ve hidden all the code here for the sake of space.\n\nExtract variable labelschocolate_var_levels &lt;- tibble(\n  variable = c(\"dark\", \"soft\", \"nuts\")\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- chocolate[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Make a little lookup table for nicer feature labels\nchocolate_var_lookup &lt;- tribble(\n  ~variable, ~variable_nice,\n  \"dark\",    \"Type of chocolate\",\n  \"soft\",    \"Type of center\",\n  \"nuts\",    \"Nuts\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))\n\n\n\nCombine full dataset of factor levels with model comparisons and make {mlogit} plotamces_chocolate_mlogit_split &lt;- amces_chocolate_mlogit %&gt;% \n  separate_wider_delim(\n    term,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  rename(term = variable)\n\nplot_data &lt;- chocolate_var_levels %&gt;%\n  left_join(\n    amces_chocolate_mlogit_split,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;% \n  # Make these zero\n  mutate(\n    across(\n      c(estimate),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(chocolate_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np1 &lt;- ggplot(\n  plot_data,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_point() +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = clrs[c(1, 3, 8)]) +\n  guides(color = \"none\") +\n  labs(\n    x = \"Percentage point change in\\nprobability of candy selection\",\n    y = NULL,\n    title = \"Frequentist AMCEs from {mlogit}\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\nCombine full dataset of factor levels with posterior draws and make {brms} plot# This is much easier than the mlogit mess because we can use avg_comparisons() directly\nposterior_mfx &lt;- model_chocolate_brms %&gt;% \n  avg_comparisons() %&gt;% \n  posteriordraws() \n\nposterior_mfx_nested &lt;- posterior_mfx %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  # At some point marginaleffects() started adding \"mean(Blah)\" instead of\n  # \"Blah\" in the contrast column for avg_comparisons(), so this removes the\n  # function name and parentheses\n  mutate(variable_level = str_replace_all(variable_level, \".*\\\\(([^)]+)\\\\).*\", \"\\\\1\")) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\n# Combine full dataset of factor levels with model results\nplot_data_bayes &lt;- chocolate_var_levels %&gt;%\n  left_join(\n    posterior_mfx_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(draw = 0, estimate = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(chocolate_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np2 &lt;- ggplot(plot_data_bayes, aes(x = draw, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  guides(fill = \"none\") +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  scale_fill_manual(values = clrs[c(1, 3, 8)]) +\n  labs(\n    x = \"Percentage point change in\\nprobability of candy selection\",\n    y = NULL,\n    title = \"Posterior Bayesian AMCEs from {brms}\"\n  )\n\n\n\np1 | p2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The setup",
    "text": "The setup\nIn this experiment, respondents are asked to choose which of these minivans they’d want to buy, based on four different features/attributes with different levels:\n\n\nFeatures/Attributes\nLevels\n\n\n\nPassengers\n6, 7, 8\n\n\nCargo area\n2 feet, 3 feet\n\n\nEngine\nGas, electric, hybrid\n\n\nPrice\n$30,000; $35,000; $40,000\n\n\n\nRespondents see this a question similar to this fifteen different times, with three options with randomly shuffled levels for each of the features.\n\n\n\n\n\n\nExample survey question\n\n\n\n\n\n\n\n\n\n\n\n\nOption 1\nOption 2\nOption 3\n\n\n\nPassengers\n7\n8\n6\n\n\nCargo area\n3 feet\n3 feet\n2 feet\n\n\nEngine\nElectric\nGas\nHybrid\n\n\nPrice\n$40,000\n$40,000\n$30,000\n\n\nChoice"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-data-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The data",
    "text": "The data\nThe data for this kind of experiment has one row for each possible alternative (alt) within each set of 15 questions (ques), thus creating 3 × 15 = 45 rows per respondent (resp.id). There were 200 respondents, with 45 rows each, so there are 200 × 45 = 9,000 rows. Here, Respondent 1 chose a $30,000 gas van with 6 seats and 3 feet of cargo space in the first set of three options, a $35,000 gas van with 7 seats and 3 feet of cargo space in the second set of three options, and so on.\nThere’s also a column here for carpool indicating if the respondent carpools with others when commuting. It’s an individual respondent-level characteristic and is constant throughout all the questions and alternatives, and we’ll use it later.\n\nminivans\n## # A tibble: 9,000 × 9\n##    resp.id  ques   alt carpool seat  cargo eng   price choice\n##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n##  1       1     1     1 yes     6     2ft   gas   35         0\n##  2       1     1     2 yes     8     3ft   hyb   30         0\n##  3       1     1     3 yes     6     3ft   gas   30         1\n##  4       1     2     1 yes     6     2ft   gas   30         0\n##  5       1     2     2 yes     7     3ft   gas   35         1\n##  6       1     2     3 yes     6     2ft   elec  35         0\n##  7       1     3     1 yes     8     3ft   gas   35         1\n##  8       1     3     2 yes     7     3ft   elec  30         0\n##  9       1     3     3 yes     8     2ft   elec  40         0\n## 10       1     4     1 yes     7     3ft   elec  40         1\n## # ℹ 8,990 more rows"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-model-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The model",
    "text": "The model\nRespondents were shown three different options and asked to select one. We thus have three possible outcomes: a respondent could have selected option 1, option 2, or option 3. Because everything was randomized, there shouldn’t be any patterns in which options people choose—we don’t want to see that the first column is more common, since that would indicate that respondents are just repeatedly selecting the first column to get through the survey. Since there are three possible outcomes (option 1, 2, and 3), we’ll use multinomial logistic regression.\nOriginal model as a baseline\nIn the example in their textbook, Chapman and Feit (2019) use {mlogit} to estimate this model and they find these results. This will be our baseline throughout this example.\n\n\nOriginal results from Chapman and Feit (2019) p. 371\n\n\nmlogit model\nThis data is a little more complex now, since there are alternatives nested inside questions inside respondents. To account for this panel structure when using {mlogit}, we need to define two index columns: one for the unique set of alternatives offered to the respondent and one for the respondent ID. We still do this with dfidx(), but need to create a new column with an ID number for each unique combination of respondent ID and question number:\n\nminivans_idx &lt;- minivans %&gt;% \n  # mlogit() needs a column with unique question id numbers\n  group_by(resp.id, ques) %&gt;% \n  mutate(choice.id = cur_group_id()) %&gt;% \n  ungroup() %&gt;% \n  # Make indexed data frame for mlogit\n  dfidx(\n    idx = list(c(\"choice.id\", \"resp.id\"), \"alt\"),\n    choice = \"choice\",\n    shape = \"long\"\n  )\n\nNow we can fit the model. Note the 0 ~ seat syntax here. That suppresses the intercept for the model, which behaves weirdly with multinomial models. Since there are three categories for the outcome (options 1, 2, and 3), there are two intercepts, representing cutpoints-from-ordered-logit-esque shifts in the probability of selecting option 1 vs. option 2 and option 2 vs. option 3. We don’t want to deal with those, so we’ll suppress them.\n\nmodel_minivans_mlogit &lt;- mlogit(\n  choice ~ 0 + seat + cargo + eng + price | 0 | 0, \n  data = minivans_idx\n)\nmodel_parameters(model_minivans_mlogit, digits = 4, p_digits = 4)\n## Parameter   | Log-Odds |     SE |             95% CI |        z |           p\n## -----------------------------------------------------------------------------\n## seat [7]    |  -0.5353 | 0.0624 | [-0.6575, -0.4131] |  -8.5837 | 9.1863e-18 \n## seat [8]    |  -0.3058 | 0.0611 | [-0.4256, -0.1860] |  -5.0032 | 5.6376e-07 \n## cargo [3ft] |   0.4774 | 0.0509 | [ 0.3777,  0.5772] |   9.3824 | 6.4514e-21 \n## eng [hyb]   |  -0.8113 | 0.0601 | [-0.9291, -0.6934] | -13.4921 | 1.7408e-41 \n## eng [elec]  |  -1.5308 | 0.0675 | [-1.6630, -1.3985] | -22.6926 | 5.3004e-114\n## price [35]  |  -0.9137 | 0.0606 | [-1.0324, -0.7949] | -15.0765 | 2.3123e-51 \n## price [40]  |  -1.7259 | 0.0696 | [-1.8623, -1.5894] | -24.7856 | 1.2829e-135\n\nThese are the same results from p. 371 in Chapman and Feit (2019), so it worked. Again, the marketing world doesn’t typically do much with these coefficients beyond looking at their direction and magnitude. For instance, in Chapman and Feit (2019) they say that the estimate for seat [7] here is negative, which means that a 7-seat option is less preferred than 6-seat option, and that the estimate for price [40] is more negative than the already-negative estimate for price [35], which means that (1) respondents don’t like the $35,000 option compared to the baseline $30,000 and that (2) respondents really don’t like the $40,000 option. We could theoretically exponentiate these things—like, seeing 7 seats makes it \\(e^{-0.5353}\\) = 0.5855 = 41% less likely to select the option compared to 6 seats—but again, that’s weird.\n\nmclogit model\nAs noted earlier, {marginaleffects} doesn’t support mlogit() because of its weird internal structure. It does support mclogit::mclogit() though.\nThe syntax requires two parts on the left-hand side of the formula: (1) the choice selected (0 or 1), and (2) a unique id for set of choices, or the question. Like with mlogit(), we can create a choice.id column identifying the unique question numbers, and then use that in the formula.\nThe results are the same:\n\nlibrary(mclogit)\n\nminivans_mclogit &lt;- minivans %&gt;% \n  # mclogit() needs a column with unique question id numbers\n  group_by(resp.id, ques) %&gt;% \n  mutate(choice.id = cur_group_id()) %&gt;%\n  ungroup()\n\nmodel_minivans_mclogit &lt;- mclogit(\n  choice | choice.id ~ seat + cargo + eng + price,\n  data = minivans_mclogit\n)\n## \n## Iteration 1 - deviance = 5176 - criterion = 0.4078\n## Iteration 2 - deviance = 5163 - criterion = 0.00242\n## Iteration 3 - deviance = 5163 - criterion = 7.874e-06\n## Iteration 4 - deviance = 5163 - criterion = 1.033e-10\n## converged\n\nmodel_parameters(model_minivans_mclogit, digits = 4, p_digits = 4)\n## Parameter   | Log-Odds |     SE |             95% CI |        z |           p\n## -----------------------------------------------------------------------------\n## seat [7]    |  -0.5353 | 0.0624 | [-0.6575, -0.4131] |  -8.5837 | 9.1863e-18 \n## seat [8]    |  -0.3058 | 0.0611 | [-0.4256, -0.1860] |  -5.0032 | 5.6376e-07 \n## cargo [3ft] |   0.4774 | 0.0509 | [ 0.3777,  0.5772] |   9.3824 | 6.4514e-21 \n## eng [hyb]   |  -0.8113 | 0.0601 | [-0.9291, -0.6934] | -13.4921 | 1.7408e-41 \n## eng [elec]  |  -1.5308 | 0.0675 | [-1.6630, -1.3985] | -22.6926 | 5.3003e-114\n## price [35]  |  -0.9137 | 0.0606 | [-1.0324, -0.7949] | -15.0765 | 2.3123e-51 \n## price [40]  |  -1.7259 | 0.0696 | [-1.8623, -1.5894] | -24.7856 | 1.2828e-135\n## \n## Uncertainty intervals (equal-tailed) and p-values (two-tailed) computed using a Wald z-distribution approximation.\n\nBayesian model with {brms}\nWe can also fit this multinomial model in a Bayesian way using {brms}. Stan has a categorical family for dealing with mulitnomial/categorical outcomes. But first, we’ll look at the nested structure of this data and incorporate that into the model, since we won’t be using the weird {mlogit}-style indexed data frame. As with the chocolate experiment, the data has a natural hierarchy in it, with three questions nested inside 15 separate question sets, nested inside each of the 200 respondents.\n\n\n\n\nMultilevel experimental structure, with minivan choices \\(\\{y_1, y_2, y_3\\}\\) nested in sets of questions in respondents\n\n\n\nCurrently, our main outcome variable choice is binary. If we run the model with choice as the outcome with a categorical family, the model will fit, but it will go slow and {brms} will complain about it and recommend switching to regular logistic regression. The categorical family in Stan requires 2+ outcomes and a reference category. Here we have three possible options (1, 2, and 3), and we can imagine a reference category of 0 for rows that weren’t selected.\nWe can create a new outcome column (choice_alt) that indicates which option each respondent selected: 0 if they didn’t choose the option and 1–3 if they chose the first, second, or third option. Because of how the data is recorded, this only requires multiplying alt and choice:\n\nminivans_choice_alt &lt;- minivans %&gt;% \n  mutate(choice_alt = factor(alt * choice))\n\nminivans_choice_alt %&gt;% \n  select(resp.id, ques, alt, seat, cargo, eng, price, choice, choice_alt)\n## # A tibble: 9,000 × 9\n##    resp.id  ques   alt seat  cargo eng   price choice choice_alt\n##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;     \n##  1       1     1     1 6     2ft   gas   35         0 0         \n##  2       1     1     2 8     3ft   hyb   30         0 0         \n##  3       1     1     3 6     3ft   gas   30         1 3         \n##  4       1     2     1 6     2ft   gas   30         0 0         \n##  5       1     2     2 7     3ft   gas   35         1 2         \n##  6       1     2     3 6     2ft   elec  35         0 0         \n##  7       1     3     1 8     3ft   gas   35         1 1         \n##  8       1     3     2 7     3ft   elec  30         0 0         \n##  9       1     3     3 8     2ft   elec  40         0 0         \n## 10       1     4     1 7     3ft   elec  40         1 1         \n## # ℹ 8,990 more rows\n\nWe can now use the new four-category choice_alt column as our outcome with the categorical() family.\nIf we realllly wanted, we could add random effects for question sets nested inside respondents, like (1 | resp.id / ques). We’d want to do that if there were set-specific things that could influences choices. Like maybe we want to account for the possibility that everyone’s just choosing the first option, so it behaves differently? Or maybe the 5th set of questions is set to an extra difficult level on a quiz or something? Or maybe we have so many sets that we think the later ones will be less accurate because of respondent fatigue? idk. In this case, question set-specific effects don’t matter at all. Each question set is equally randomized and no different from the others, so we won’t bother modeling that layer of the hierarchy.\nWe want to model the choice of option 1, 2, or 3 (choice_alt) based on minivan characteristics (seat, cargo, eng, price). With the categorical model, we actually get a set of parameters to estimate the probability of selecting each of the options, which Stan calls \\(\\mu\\), so we have a set of three probabilities: \\(\\{\\mu_1, \\mu_2, \\mu_3\\}\\). We’ll use the subscript \\(i\\) to refer to individual minivan choices and \\(j\\) to refer to respondents. Here’s the fun formal model:\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ (\\beta_0 + b_{0_j}) + \\beta_1 \\text{Seat[7]}_{i_j} + \\beta_2 \\text{Seat[8]}_{i_j} + \\beta_3 \\text{Cargo[3ft]}_{i_j} + \\\\\n&\\ \\beta_4 \\text{Engine[hyb]}_{i_j} + \\beta_5 \\text{Engine[elec]}_{i_j} + \\beta_6 \\text{Price[35k]}_{i_j} + \\beta_7 \\text{Price[40k]}_{i_j} \\\\[5pt]\nb_{0_j} \\sim&\\ \\mathcal{N}(0, \\sigma_0) \\qquad\\quad\\quad \\text{Respondent-specific offsets from global probability} \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_{0 \\dots 7} \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ \\ \\text{Prior for choice-level coefficients} \\\\\n\\sigma_0 \\sim&\\ \\operatorname{Exponential}(1) \\quad \\text{Prior for between-respondent variability}\n\\end{aligned}\n\\]\nAnd here’s the {brms} model. Notice the much-more-verbose prior section—because the categorical family in Stan estimates separate parameters for each of the categories (\\(\\{\\mu_1, \\mu_2, \\mu_3\\}\\)), we have a mean and standard deviation for the probability of selecting each of those options. We need to specify each of these separately too instead of just doing something like prior(normal(0, 3), class = b). Also notice the refcat argument in categorical()—this makes it so that all the estimates are relative to not choosing an option (or when choice_alt is 0). And also notice the slightly different syntax for the random respondent intercepts: (1 | ID | resp.id). That new middle ID is special {brms} formula syntax that we can use when working with categorical or ordinal families, and it makes it so that the group-level effects for the different outcomes (here options 0, 1, 2, and 3) are correlated (see p. 4 of this {brms} vignette for more about this special syntax).\n\nmodel_minivans_categorical_brms &lt;- brm(\n  bf(choice_alt ~ 0 + seat + cargo + eng + price + (1 | ID | resp.id)),\n  data = minivans_choice_alt,\n  family = categorical(refcat = \"0\"),\n  prior = c(\n    prior(normal(0, 3), class = b, dpar = mu1),\n    prior(normal(0, 3), class = b, dpar = mu2),\n    prior(normal(0, 3), class = b, dpar = mu3),\n    prior(exponential(1), class = sd, dpar = mu1),\n    prior(exponential(1), class = sd, dpar = mu2),\n    prior(exponential(1), class = sd, dpar = mu3)\n  ),\n  chains = 4, cores = 4, iter = 2000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), refresh = 0,\n  file = \"models/model_minivans_categorical_brms\"\n)\n\nThis model gives us a ton of parameters! We get three estimates per feature level (i.e. mu1_cargo3ft, mu2_cargo3ft, and mu3_cargo3ft for the cargo3ft effect), since we’re actually estimating the effect of each covariate on the probability of selecting each of the three options.\n\nmodel_parameters(model_minivans_categorical_brms)\n## Parameter    | Median |         95% CI |     pd |  Rhat |     ESS\n## -----------------------------------------------------------------\n## mu1_seat6    |  -0.34 | [-0.52, -0.16] | 99.95% | 1.001 | 2673.00\n## mu1_seat7    |  -0.86 | [-1.04, -0.67] |   100% | 1.000 | 3167.00\n## mu1_seat8    |  -0.59 | [-0.77, -0.41] |   100% | 1.000 | 3373.00\n## mu1_cargo3ft |   0.46 | [ 0.32,  0.60] |   100% | 1.000 | 6314.00\n## mu1_enghyb   |  -0.76 | [-0.92, -0.60] |   100% | 1.001 | 4628.00\n## mu1_engelec  |  -1.51 | [-1.69, -1.33] |   100% | 0.999 | 4913.00\n## mu1_price35  |  -0.82 | [-0.99, -0.67] |   100% | 0.999 | 4574.00\n## mu1_price40  |  -1.74 | [-1.94, -1.56] |   100% | 1.000 | 4637.00\n## mu2_seat6    |  -0.39 | [-0.57, -0.20] |   100% | 1.000 | 2387.00\n## mu2_seat7    |  -0.95 | [-1.15, -0.77] |   100% | 1.001 | 2470.00\n## mu2_seat8    |  -0.67 | [-0.85, -0.49] |   100% | 1.001 | 2489.00\n## mu2_cargo3ft |   0.49 | [ 0.35,  0.63] |   100% | 1.000 | 4836.00\n## mu2_enghyb   |  -0.79 | [-0.95, -0.63] |   100% | 1.000 | 4421.00\n## mu2_engelec  |  -1.40 | [-1.57, -1.22] |   100% | 1.000 | 4261.00\n## mu2_price35  |  -0.79 | [-0.95, -0.63] |   100% | 1.001 | 3699.00\n## mu2_price40  |  -1.47 | [-1.65, -1.29] |   100% | 0.999 | 3978.00\n## mu3_seat6    |  -0.28 | [-0.46, -0.11] | 99.85% | 1.000 | 2077.00\n## mu3_seat7    |  -0.78 | [-0.96, -0.60] |   100% | 1.000 | 3025.00\n## mu3_seat8    |  -0.63 | [-0.81, -0.46] |   100% | 1.000 | 2483.00\n## mu3_cargo3ft |   0.36 | [ 0.23,  0.50] |   100% | 0.999 | 5327.00\n## mu3_enghyb   |  -0.73 | [-0.88, -0.58] |   100% | 1.000 | 4039.00\n## mu3_engelec  |  -1.41 | [-1.59, -1.23] |   100% | 1.001 | 3818.00\n## mu3_price35  |  -0.85 | [-1.01, -0.69] |   100% | 1.000 | 4315.00\n## mu3_price40  |  -1.56 | [-1.75, -1.39] |   100% | 0.999 | 4774.00\n\nImportantly, the estimates here are all roughly equivalent to what we get from {mlogit}: the {mlogit} estimate for cargo3ft was 0.4775, while the three median posterior {brms} estimates are 0.46 (95% credible interval: 0.32–0.60), 0.49 (0.35–0.63), and 0.36 (0.23–0.50)\nSince all the features are randomly shuffled between the three options each time, and each option is selected 1/3rd of the time, it’s probably maybe legal to pool these posterior estimates together (maaaaybeee???) so that we don’t have to work with three separate estimates for each parameter? To do this we’ll take the average of each of the three \\(\\mu\\) estimates within each draw, which is also called “marginalizing” across the three options.\nHere’s how we’d do that with {tidybayes}. The medians are all roughly the same now!\n\nminivans_cat_marginalized &lt;- model_minivans_categorical_brms %&gt;% \n  gather_draws(`^b_.*$`, regex = TRUE) %&gt;% \n  # Each variable name has \"mu1\", \"mu2\", etc. built in, like \"b_mu1_seat6\". This\n  # splits the .variable column into two parts based on a regular expression,\n  # creating one column for the mu part (\"b_mu1_\") and one for the rest of the\n  # variable name (\"seat6\")\n  separate_wider_regex(\n    .variable,\n    patterns = c(mu = \"b_mu\\\\d_\", .variable = \".*\")\n  ) %&gt;% \n  # Find the average of the three mu estimates for each variable within each\n  # draw, or marginalize across the three options, since they're randomized\n  group_by(.variable, .draw) %&gt;% \n  summarize(.value = mean(.value)) \n\nminivans_cat_marginalized %&gt;% \n  group_by(.variable) %&gt;% \n  median_qi()\n## # A tibble: 8 × 7\n##   .variable .value .lower .upper .width .point .interval\n##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 cargo3ft   0.439  0.340  0.532   0.95 median qi       \n## 2 engelec   -1.44  -1.56  -1.32    0.95 median qi       \n## 3 enghyb    -0.762 -0.871 -0.651   0.95 median qi       \n## 4 price35   -0.823 -0.935 -0.713   0.95 median qi       \n## 5 price40   -1.59  -1.72  -1.47    0.95 median qi       \n## 6 seat6     -0.337 -0.464 -0.208   0.95 median qi       \n## 7 seat7     -0.862 -0.994 -0.734   0.95 median qi       \n## 8 seat8     -0.629 -0.753 -0.503   0.95 median qi\n\nAnd for fun, here’s what the posterior for new combined/collapsed/marginalized cargo3ft looks like. Great.\n\nminivans_cat_marginalized %&gt;% \n  filter(.variable == \"cargo3ft\") %&gt;% \n  ggplot(aes(x = .value, y = .variable)) +\n  stat_halfeye(fill = clrs[4]) +\n  labs(x = \"Posterior distribution of β (logit-scale)\", y = NULL)"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Predictions",
    "text": "Predictions\nAs we saw in the first example with chocolates, the marketing world typically uses predictions from these kinds of models to estimate the predicted market share for products with different constellations of features. That was a pretty straightforward task with the chocolate model since respondents were shown all 8 options simultaneously. It’s a lot trickier with the minivan example where respondents were shown 15 sets of 3 options. Dealing with multinomial predictions is a bear of a task because these models are a lot more complex.\nFrequentist predictions\nWith the chocolate model, we could use predict(model_chocolate_mlogit) and automatically get predictions for all 8 options. That’s not the case here:\n\npredict(model_minivans_mlogit)\n##      1      2      3 \n## 0.3333 0.3333 0.3333\n\nWe get three predictions, and they’re all 33ish%. That’s because respondents were presented with three randomly shuffled options and chose one of them. All these predictions tell us is that across all 15 iterations of the questions, 1/3 of respondents selected the first option, 1/3 the second, and 1/3 the third. That’s a good sign in this case—there’s no evidence that people were just repeatedly choosing the first option. But in the end, these predictions aren’t super useful.\nWe instead want to be able to get predicted market shares (or predicted probabilities) for any given mix of products. For instance, here are six arbitrary hypothetical products with different combinations of seats, cargo space, engines, and prices:\n\nexample_product_mix &lt;- tribble(\n  ~seat, ~cargo, ~eng, ~price,\n  \"7\", \"2ft\", \"hyb\", \"30\",\n  \"6\", \"2ft\", \"gas\", \"30\",\n  \"8\", \"2ft\", \"gas\", \"30\",\n  \"7\", \"3ft\", \"gas\", \"40\",\n  \"6\", \"2ft\", \"elec\", \"40\",\n  \"7\", \"2ft\", \"hyb\", \"35\"\n) %&gt;% \n  mutate(across(everything(), factor)) %&gt;% \n  mutate(eng = factor(eng, levels = levels(minivans$eng)))\nexample_product_mix\n## # A tibble: 6 × 4\n##   seat  cargo eng   price\n##   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;\n## 1 7     2ft   hyb   30   \n## 2 6     2ft   gas   30   \n## 3 8     2ft   gas   30   \n## 4 7     3ft   gas   40   \n## 5 6     2ft   elec  40   \n## 6 7     2ft   hyb   35\n\nIf we were working with any other type of model, we could plug this data into the newdata argument of predict() and get predicted values. That doesn’t work here though. We get predicted values, but these don’t correspond to each row of the newdata dataset—instead, each row sums to 100%, showing the probability of choosing column 1, 2 or 3 in the original survey, which isn’t really all that helpful here.\n\npreds_wrong &lt;- predict(model_minivans_mlogit, newdata = example_product_mix)\npreds_wrong\n##        1      2      3\n## 1 0.1303 0.5008 0.3689\n## 2 0.5405 0.1239 0.3356\n\nFurther complicating things, it only returns two rows because it assumes that these six hypothetical product offerings were two different choice sets, or that rows 1–3 were shown together to the respondent, followed by rows 4–6. But that’s not the case, and we’re not interested in that.\n\n# Rearrange the predictions to be based on choice sets\npreds_wrong_df &lt;- as.data.frame(preds_wrong) %&gt;%\n  mutate(choice_set = row_number()) %&gt;%\n  pivot_longer(cols = -choice_set, names_to = \"alternative\", values_to = \"probability\") %&gt;% \n  mutate(alternative = as.integer(alternative))\n\n# The probabilities within each fake choice set add to 100%\nexample_product_mix %&gt;%\n  mutate(choice_set = rep(1:2, each = 3), alternative = rep(1:3, 2)) %&gt;%\n  left_join(preds_wrong_df, by = c(\"choice_set\", \"alternative\"))\n## # A tibble: 6 × 7\n##   seat  cargo eng   price choice_set alternative probability\n##   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;int&gt;       &lt;int&gt;       &lt;dbl&gt;\n## 1 7     2ft   hyb   30             1           1       0.130\n## 2 6     2ft   gas   30             1           2       0.501\n## 3 8     2ft   gas   30             1           3       0.369\n## 4 7     3ft   gas   40             2           1       0.540\n## 5 6     2ft   elec  40             2           2       0.124\n## 6 7     2ft   hyb   35             2           3       0.336\n\nInstead, following Chapman and Feit (2019) (and this Stan forum post), we can manually multiply the covariates in example_product_mix with the model coefficients to calculate “utility” (or predicted vales on the logit scale), which we can then exponentiate and divide to calculate market shares.\n\n# Create a matrix of 0s and 1s for the values in `example_product_mix`, omitting\n# the first column (seat6)\nexample_product_dummy_encoded &lt;- model.matrix(\n  update(model_minivans_mlogit$formula, 0 ~ .),\n  data = example_product_mix\n)[, -1]\nexample_product_dummy_encoded\n##   seat7 seat8 cargo3ft enghyb engelec price35 price40\n## 1     1     0        0      1       0       0       0\n## 2     0     0        0      0       0       0       0\n## 3     0     1        0      0       0       0       0\n## 4     1     0        1      0       0       0       1\n## 5     0     0        0      0       1       0       1\n## 6     1     0        0      1       0       1       0\n\n# Matrix multiply the matrix of 0s and 1s with the model coefficients to get\n# logit-scale predictions, or utility\nutility &lt;- example_product_dummy_encoded %*% coef(model_minivans_mlogit)\n\n# Divide each exponentiated utility by the sum of the exponentiated utilities to\n# get the market share\nshare &lt;- exp(utility) / sum(exp(utility))\n\n# Stick all of these in one final dataset\nbind_cols(share = share, logits = utility, example_product_mix)\n## # A tibble: 6 × 6\n##   share[,1] logits[,1] seat  cargo eng   price\n##       &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;\n## 1    0.113      -1.35  7     2ft   hyb   30   \n## 2    0.433       0     6     2ft   gas   30   \n## 3    0.319      -0.306 8     2ft   gas   30   \n## 4    0.0728     -1.78  7     3ft   gas   40   \n## 5    0.0167     -3.26  6     2ft   elec  40   \n## 6    0.0452     -2.26  7     2ft   hyb   35\n\n\n\n\n\n\n\nFunction version of this kind of prediction\n\n\n\n\n\nOn p. 375 of Chapman and Feit (2019) (and at this Stan forum post), there’s a function called predict.mnl() that does this utility and share calculation automatically. Because this post is more didactic and because I’m more interested in the Bayesian approach, I didn’t use it earlier, but it works just the same.\n\npredict.mnl &lt;- function(model, data) {\n  # Function for predicting shares from a multinomial logit model \n  # model: mlogit object returned by mlogit()\n  # data: a data frame containing the set of designs for which you want to \n  #       predict shares. Same format at the data used to estimate model. \n  data.model &lt;- model.matrix(update(model$formula, 0 ~ .), data = data)[ , -1]\n  utility &lt;- data.model %*% model$coef\n  share &lt;- exp(utility) / sum(exp(utility))\n  cbind(share, data)\n}\n\npredict.mnl(model_minivans_mlogit, example_product_mix)\n##     share seat cargo  eng price\n## 1 0.11273    7   2ft  hyb    30\n## 2 0.43337    6   2ft  gas    30\n## 3 0.31918    8   2ft  gas    30\n## 4 0.07281    7   3ft  gas    40\n## 5 0.01669    6   2ft elec    40\n## 6 0.04521    7   2ft  hyb    35\n\n\n\n\nThis new predicted share column sums to one, and it shows us the predicted market share assuming these are the only six products available. The $30,000 six-seater 2ft gas van and the $30,000 eight-seater 2ft gas van would comprise more than 75% (0.43337 + 0.31918) of a market consisting of these six products.\nBayesian predictions\nIf we use the categorical multinomial {brms} model we run into the same issue of getting weird predictions. Using marginaleffects::avg_predictions(), we see that that 2/3rds of predictions are 0, which makes sense—if a respondent is offered 10 iterations of 3 possible choices, that would be 30 total choices, but they can only choose one option per iteration, so 20 choices (or 20/30 or 2/3) wouldn’t be selected. The other three groups are each 11%, since that’s the remaining 33% divided evenly across three options. Neat, I guess, but still not super helpful.\n\navg_predictions(model_minivans_categorical_brms)\n## \n##  Group Estimate 2.5 % 97.5 %\n##      0    0.667 0.657  0.676\n##      1    0.109 0.103  0.115\n##      2    0.112 0.105  0.118\n##      3    0.113 0.106  0.119\n## \n## Columns: group, estimate, conf.low, conf.high \n## Type:  response\n\nInstead of going through the manual process of matrix-multiplying a dataset of some mix of products with a single set of coefficients, we can use predictions(..., type = \"link\") to get predicted values on the log-odds scale, or that utility value that we found before.\n\n\n\n\n\n\nmarginaleffects::predictions() vs. {tidybayes} functions\n\n\n\nWe can actually use either marginaleffects::predictions() or {tidybayes}’s *_draw() functions for these posterior predictions. They do the same thing, with slightly different syntax:\n\n# Logit-scale predictions with marginaleffects::predictions()\nmodel_minivans_categorical_brms %&gt;% \n  predictions(newdata = example_product_mix, re_formula = NA, type = \"link\") %&gt;% \n  posterior_draws()\n\n# Logit-scale predictions with tidybayes::add_linpred_draws()\nmodel_minivans_categorical_brms %&gt;% \n  add_linpred_draws(newdata = example_product_mix, re_formula = NA)\n\nEarlier in the chocolate example, I used marginaleffects::predictions() with the Bayesian {brms} model. Here I’m going to switch to the {tidybayes} prediction functions instead, in part because these multinomial models with the categorical() family are a lot more complex (though {marginaleffects} can handle them nicely), but mostly because in the actual paper I’m working on with real conjoint data, our MCMC results were generated with raw Stan code through rstan, and {marginaleffects} doesn’t support raw Stan models.\nCheck out this guide for the differences between {tidybayes}’s three general prediction functions: predicted_draws(), epred_draws(), and linpred_draws().\n\n\nAdditionally, we now actually have 4,000 draws in 3 categories (option 1, option 2, and option 3), so we actually have 12,000 sets of coefficients (!). To take advantage of the full posterior distribution of these coefficients, we can calculate shares within each set of draws within each of the three categories, resulting in a distribution of shares rather than single values.\n\ndraws_df &lt;- example_product_mix %&gt;% \n  add_linpred_draws(model_minivans_categorical_brms, value = \"utility\", re_formula = NA)\n\nshares_df &lt;- draws_df %&gt;% \n  # Look at each set of predicted utilities within each draw within each of the\n  # three outcomes\n  group_by(.draw, .category) %&gt;% \n  mutate(share = exp(utility) / sum(exp(utility))) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    mix_type = paste(seat, cargo, eng, price, sep = \" \"),\n    mix_type = fct_reorder(mix_type, share)\n  )\n\nWe can summarize this huge dataset of posterior shares to get medians and credible intervals, but we need to do one extra step first. Right now, we have three predictions for each mix type, one for each of the categories (i.e. option 1, option 2, and option 3.\n\nshares_df %&gt;% \n  group_by(mix_type, .category) %&gt;% \n  median_qi(share)\n## # A tibble: 18 × 8\n##    mix_type      .category  share .lower .upper .width .point .interval\n##    &lt;fct&gt;         &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 6 2ft elec 40 1         0.0161 0.0123 0.0208   0.95 median qi       \n##  2 6 2ft elec 40 2         0.0238 0.0183 0.0302   0.95 median qi       \n##  3 6 2ft elec 40 3         0.0216 0.0168 0.0275   0.95 median qi       \n##  4 7 2ft hyb 35  1         0.0513 0.0400 0.0647   0.95 median qi       \n##  5 7 2ft hyb 35  2         0.0485 0.0379 0.0605   0.95 median qi       \n##  6 7 2ft hyb 35  3         0.0532 0.0424 0.0660   0.95 median qi       \n##  7 7 3ft gas 40  1         0.0693 0.0543 0.0876   0.95 median qi       \n##  8 7 3ft gas 40  2         0.0891 0.0705 0.111    0.95 median qi       \n##  9 7 3ft gas 40  3         0.0775 0.0615 0.0967   0.95 median qi       \n## 10 7 2ft hyb 30  1         0.117  0.0976 0.139    0.95 median qi       \n## 11 7 2ft hyb 30  2         0.107  0.0891 0.128    0.95 median qi       \n## 12 7 2ft hyb 30  3         0.124  0.104  0.146    0.95 median qi       \n## 13 8 2ft gas 30  1         0.326  0.292  0.363    0.95 median qi       \n## 14 8 2ft gas 30  2         0.314  0.282  0.348    0.95 median qi       \n## 15 8 2ft gas 30  3         0.299  0.267  0.331    0.95 median qi       \n## 16 6 2ft gas 30  1         0.418  0.380  0.457    0.95 median qi       \n## 17 6 2ft gas 30  2         0.416  0.379  0.454    0.95 median qi       \n## 18 6 2ft gas 30  3         0.423  0.387  0.462    0.95 median qi\n\nSince those options were all randomized, we can lump them all together as a single choice. To do this we’ll take the average share across the three categories (this is also called “marginalizing”) within each posterior draw.\n\nshares_marginalized &lt;- shares_df %&gt;% \n  # Marginalize across categories within each draw\n  group_by(mix_type, .draw) %&gt;% \n  summarize(share = mean(share)) %&gt;% \n  ungroup()\n\nshares_marginalized %&gt;% \n  group_by(mix_type) %&gt;% \n  median_qi(share)\n## # A tibble: 6 × 7\n##   mix_type       share .lower .upper .width .point .interval\n##   &lt;fct&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 6 2ft elec 40 0.0206 0.0173 0.0242   0.95 median qi       \n## 2 7 2ft hyb 35  0.0512 0.0435 0.0600   0.95 median qi       \n## 3 7 3ft gas 40  0.0788 0.0673 0.0915   0.95 median qi       \n## 4 7 2ft hyb 30  0.116  0.103  0.131    0.95 median qi       \n## 5 8 2ft gas 30  0.313  0.291  0.337    0.95 median qi       \n## 6 6 2ft gas 30  0.419  0.394  0.446    0.95 median qi\n\nAnd we can plot them:\n\nshares_marginalized %&gt;% \n  ggplot(aes(x = share, y = mix_type)) +\n  stat_halfeye(fill = clrs[10], normalize = \"xy\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Predicted market share\", y = \"Hypothetical product mix\")\n\n\n\n\n\n\n\nThis is great because (1) it includes the uncertainty in the estimated shares, and (2) it lets us do neat Bayesian inference and say things like “there’s a 93% chance that in this market of 6 options, a $30,000 6-passenger gas minivan with 2 feet of storage would reach at least 40% market share”:\n\nshares_marginalized %&gt;% \n  filter(mix_type == \"6 2ft gas 30\") %&gt;% \n  summarize(prop_greater_40 = sum(share &gt;= 0.4) / n())\n## # A tibble: 1 × 1\n##   prop_greater_40\n##             &lt;dbl&gt;\n## 1           0.931\n\nshares_marginalized %&gt;% \n  filter(mix_type == \"6 2ft gas 30\") %&gt;% \n  ggplot(aes(x = share, y = mix_type)) +\n  stat_halfeye(aes(fill_ramp = after_stat(x &gt;= 0.4)), fill = clrs[10]) +\n  geom_vline(xintercept = 0.4) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_ramp_discrete(from = colorspace::lighten(clrs[10], 0.4), guide = \"none\") +\n  labs(x = \"Predicted market share\", y = NULL)"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-1",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-1",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nAs explained in the AMCEs section for the chocolate data, in the social sciences we’re less concerned about predicted market shares and more concerned about causal effects. Holding all other features constant, what is the effect of a $5,000 increase in price or moving from 2 feet → 3 feet of storage space on the probability (or favorability) of selecting a minivan?\nIn the chocolate example, we were able to use marginaleffects::avg_comparisons() with the Bayesian model and get categorical contrasts automatically. This was because we cheated and used a Poisson model, since those can secretly behave like multinomial models. For the frequentist {mlogit}-based model, we had to use base R’s predict() instead and then collapse the predictions into the different contrasts we were interested in using group_by() %&gt;% summarize(). We need to do the same thing here.\nFrequentist comparisons/contrasts\nTo help with the intuition behind this, since it’s more complex this time, we’ll create a data frame to show all the combinations of all the feature levels (3 seats × 2 cargos × 3 engines × 3 prices). There are 54 possible combinations:\n\nvan_all_combos &lt;- minivans %&gt;% \n  tidyr::expand(seat, cargo, eng, price)\nvan_all_combos\n## # A tibble: 54 × 4\n##    seat  cargo eng   price\n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;\n##  1 6     2ft   gas   30   \n##  2 6     2ft   gas   35   \n##  3 6     2ft   gas   40   \n##  4 6     2ft   hyb   30   \n##  5 6     2ft   hyb   35   \n##  6 6     2ft   hyb   40   \n##  7 6     2ft   elec  30   \n##  8 6     2ft   elec  35   \n##  9 6     2ft   elec  40   \n## 10 6     3ft   gas   30   \n## # ℹ 44 more rows\n\nWe want to calculate the probabilities for each of these combinations, regardless of whether the hypothetical minivan appeared as the first, second, or third option in the survey experiment.\nFortunately, {mlogit} already did most of the work for us! One of the slots in the model_minivans_mlogit object is named $probabilities, which contains the a matrix of predicted probabilities for each of the 3,000 choice sets (200 respondents × 15 sets of questions)\n\n# Number of choice sets\nnrow(model_minivans_mlogit$probabilities)\n## [1] 3000\n\n# Probability of selecting each alternative within each choice set\nhead(model_minivans_mlogit$probabilities)\n##         1      2       3\n## 1 0.15787 0.2076 0.63451\n## 2 0.68246 0.2583 0.05922\n## 3 0.67183 0.2881 0.04003\n## 4 0.05254 0.3054 0.64204\n## 5 0.64785 0.1153 0.23682\n## 6 0.10199 0.6218 0.27624\n\nWorking with data like this, though, is messy and untidy. Fortunately again, if we use model.frame(), {mlogit} will return a long data frame with probabilities for each of the alternatives within each of the choice sets, or 9,000 rows (200 respondents × 15 sets of questions × 3 options within each question). This is the original data we used for the model, only now with columns for the fitted values on the logit scale (the linpred column) and the probability scale (the probabilities column).\n\nmodel.frame(model_minivans_mlogit)\n## # A tibble: 9,000 × 8\n## # Index:    3000 (choice.id) x 3 (alt)\n## # Balanced: yes\n## # Nesting:  choice.id (resp.id)\n##    choice seat  cargo eng   price idx   probabilities linpred\n##    &lt;lgl&gt;  &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;idx&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n##  1 FALSE  6     2ft   gas   35    1:1          0.158   -0.914\n##  2 FALSE  8     3ft   hyb   30    1:2          0.208   -0.640\n##  3 TRUE   6     3ft   gas   30    1:3          0.635    0.477\n##  4 FALSE  6     2ft   gas   30    2:1          0.682    0    \n##  5 TRUE   7     3ft   gas   35    2:2          0.258   -0.971\n##  6 FALSE  6     2ft   elec  35    2:3          0.0592  -2.44 \n##  7 TRUE   8     3ft   gas   35    3:1          0.672   -0.742\n##  8 FALSE  7     3ft   elec  30    3:2          0.288   -1.59 \n##  9 FALSE  8     2ft   elec  40    3:3          0.0400  -3.56 \n## 10 TRUE   7     3ft   elec  40    4:1          0.0525  -3.31 \n## # ℹ 8,990 more rows\n\nThat’s neat, but it’s still a lot of data. Remember van_all_combos from earlier, with the 54 unique combinations of seat, cargo, engine, and price? We want to know the probabilities for each of those combinations.\nWe can find those with some grouping and summarizing, finding the average probability within each of the combinations:\n\nall_preds_mlogit &lt;- model.frame(model_minivans_mlogit) %&gt;% \n  group_by(seat, cargo, eng, price) %&gt;% \n  summarize(estimate = mean(probabilities))\nall_preds_mlogit\n## # A tibble: 54 × 5\n## # Groups:   seat, cargo, eng [18]\n##    seat  cargo eng   price estimate\n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt;\n##  1 6     2ft   gas   30      0.695 \n##  2 6     2ft   gas   35      0.491 \n##  3 6     2ft   gas   40      0.311 \n##  4 6     2ft   hyb   30      0.499 \n##  5 6     2ft   hyb   35      0.300 \n##  6 6     2ft   hyb   40      0.161 \n##  7 6     2ft   elec  30      0.348 \n##  8 6     2ft   elec  35      0.173 \n##  9 6     2ft   elec  40      0.0872\n## 10 6     3ft   gas   30      0.768 \n## # ℹ 44 more rows\n\nPerfect. The option with 6 seats, 2 feet of cargo space, a gas engine, at $30,000 had a 70% chance of being selected regardless of whether it was presented as option 1, 2, or 3.\nTo find the AMCE of specific features, we can collapse and average even further. For instance, suppose we’re interested in the AMCE of cargo space. We can first find the average predicted probability of selection with some grouping and summarizing, then calculate the difference between the two rows:\n\nmanual_cargo_example &lt;- all_preds_mlogit %&gt;% \n  group_by(cargo) %&gt;% \n  summarize(avg_pred = mean(estimate))\nmanual_cargo_example\n## # A tibble: 2 × 2\n##   cargo avg_pred\n##   &lt;fct&gt;    &lt;dbl&gt;\n## 1 2ft      0.292\n## 2 3ft      0.375\n\n# diff() way\ndiff(manual_cargo_example$avg_pred)\n## [1] 0.08326\n\n# Tidyverse way\nmanual_cargo_example %&gt;% \n  pivot_wider(names_from = cargo, values_from = avg_pred) %&gt;% \n  mutate(\n    term = \"3ft - 2ft\",\n    estimate = `3ft` - `2ft`\n  ) %&gt;% \n  select(term, estimate)\n## # A tibble: 1 × 2\n##   term      estimate\n##   &lt;chr&gt;        &lt;dbl&gt;\n## 1 3ft - 2ft   0.0833\n\nHolding all other features constant, the average probability (or average favorability, or average market share, or whatever we want to call it) of selecting a minivan with 2 feet of storage space is 0.292 (this is the average of the 27 predictions from all_preds_mlogit where cargo = 2ft); the average probability for a minivan with 3 feet of storage space is 0.375 (again, this is the average of the 27 predictions from all_preds_mlogit where cargo = 3ft). There’s an 8.3 percentage point difference between these groups. This is the causal effect or AMCE: switching from 2 feet to 3 feet increases minivan favorability by 8 percentage points on average.\nWe can make a big data frame with all the AMCEs we’re interested in. I’ve hidden the code here because it’s really repetitive.\n\nMake separate datasets of predictions and combine them in one data frameamce_minivan_seat_67_mlogit &lt;- all_preds_mlogit %&gt;% \n  group_by(seat) %&gt;% \n  summarize(avg_pred = mean(estimate)) %&gt;%\n  filter(seat %in% c(\"6\", \"7\")) %&gt;%\n  pivot_wider(names_from = seat, values_from = avg_pred) %&gt;% \n  mutate(\n    term = \"7 - 6\",\n    estimate = `7` - `6`,\n    variable = \"seat\"\n  )  %&gt;% \n  select(variable, term, estimate)\n\namce_minivan_seat_68_mlogit &lt;- all_preds_mlogit %&gt;% \n  group_by(seat) %&gt;% \n  summarize(avg_pred = mean(estimate)) %&gt;%\n  filter(seat %in% c(\"6\", \"8\")) %&gt;%\n  pivot_wider(names_from = seat, values_from = avg_pred) %&gt;% \n  mutate(\n    term = \"8 - 6\",\n    estimate = `8` - `6`,\n    variable = \"seat\"\n  )  %&gt;% \n  select(variable, term, estimate)\n\namce_minivan_cargo_mlogit &lt;- all_preds_mlogit %&gt;% \n  group_by(cargo) %&gt;% \n  summarize(avg_pred = mean(estimate)) %&gt;%\n  pivot_wider(names_from = cargo, values_from = avg_pred) %&gt;% \n  mutate(\n    term = \"3ft - 2ft\",\n    estimate = `3ft` - `2ft`,\n    variable = \"cargo\"\n  )  %&gt;% \n  select(variable, term, estimate)\n\namce_minivan_eng_gas_elec_mlogit &lt;- all_preds_mlogit %&gt;% \n  group_by(eng) %&gt;% \n  summarize(avg_pred = mean(estimate)) %&gt;%\n  filter(eng %in% c(\"gas\", \"elec\")) %&gt;%\n  pivot_wider(names_from = eng, values_from = avg_pred) %&gt;% \n  mutate(\n    term = \"elec - gas\",\n    estimate = elec - gas,\n    variable = \"eng\"\n  )  %&gt;% \n  select(variable, term, estimate)\n\namce_minivan_eng_gas_hyb_mlogit &lt;- all_preds_mlogit %&gt;% \n  group_by(eng) %&gt;% \n  summarize(avg_pred = mean(estimate)) %&gt;%\n  filter(eng %in% c(\"gas\", \"hyb\")) %&gt;%\n  pivot_wider(names_from = eng, values_from = avg_pred) %&gt;% \n  mutate(\n    term = \"hyb - gas\",\n    estimate = hyb - gas,\n    variable = \"eng\"\n  )  %&gt;% \n  select(variable, term, estimate)\n\namce_minivan_price_3035_mlogit &lt;- all_preds_mlogit %&gt;% \n  group_by(price) %&gt;% \n  summarize(avg_pred = mean(estimate)) %&gt;%\n  filter(price %in% c(\"30\", \"35\")) %&gt;%\n  pivot_wider(names_from = price, values_from = avg_pred) %&gt;% \n  mutate(\n    term = \"35 - 30\",\n    estimate = `35` - `30`,\n    variable = \"price\"\n  )  %&gt;% \n  select(variable, term, estimate)\n\namce_minivan_price_3040_mlogit &lt;- all_preds_mlogit %&gt;% \n  group_by(price) %&gt;% \n  summarize(avg_pred = mean(estimate)) %&gt;%\n  filter(price %in% c(\"30\", \"40\")) %&gt;%\n  pivot_wider(names_from = price, values_from = avg_pred) %&gt;% \n  mutate(\n    term = \"40 - 30\",\n    estimate = `40` - `30`,\n    variable = \"price\"\n  )  %&gt;% \n  select(variable, term, estimate)\n\namces_minivan_mlogit &lt;- bind_rows(\n  amce_minivan_seat_67_mlogit,\n  amce_minivan_seat_68_mlogit,\n  amce_minivan_cargo_mlogit,\n  amce_minivan_eng_gas_elec_mlogit,\n  amce_minivan_eng_gas_hyb_mlogit,\n  amce_minivan_price_3035_mlogit,\n  amce_minivan_price_3040_mlogit\n)\n\n\n\namces_minivan_mlogit\n## # A tibble: 7 × 3\n##   variable term       estimate\n##   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;\n## 1 seat     7 - 6       -0.0997\n## 2 seat     8 - 6       -0.0574\n## 3 cargo    3ft - 2ft    0.0833\n## 4 eng      elec - gas  -0.278 \n## 5 eng      hyb - gas   -0.161 \n## 6 price    35 - 30     -0.178 \n## 7 price    40 - 30     -0.309\n\nBayesian comparisons/contrasts\nUnlike the chocolate example, where the outcome variable was binary, we have to do similar grouping and summarizing and marginalizing shenanigans with the Bayesian minivan model here. We could theoretically work with things like marginaleffects::comparisons() or marginaleffects::slopes() to extract the AMCEs from the model, but as I’ll show below, there are some weird mathy things we have to deal with because of the multinomial outcome, and I think it’s beyond what {marginaleffects} is designed to easily do.\nSo instead we can use epred_draws() from {tidybayes} and calculate posterior predictions ourselves (see this guide for an overview of all of {tidybayes}’s different prediction functions).\nTo illustrate why predicting things with this multinomial model is so weird, we’ll first predict the probability that someone chooses a $30,000 6-seater electric van with 2 feet of storage space. For this combination of minivan characteristics, there’s a 66% chance that someone does not select it, shown as category 0. That means there’s a 33% chance that someone does select it. Because options 1, 2 and 3 were randomized, that 33% is split evenly across categories 1, 2, and 3 in the predictions here.\n\none_prediction &lt;- model_minivans_categorical_brms %&gt;% \n  epred_draws(newdata = data.frame(\n    seat = \"6\", cargo = \"2ft\", eng = \"elec\", price = \"30\", resp.id = 1)\n  )\n\none_prediction %&gt;% \n  group_by(.category) %&gt;% \n  median_qi(.epred)\n## # A tibble: 4 × 7\n##   .category .epred .lower .upper .width .point .interval\n##   &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 0          0.661 0.629   0.694   0.95 median qi       \n## 2 1          0.104 0.0843  0.128   0.95 median qi       \n## 3 2          0.112 0.0894  0.138   0.95 median qi       \n## 4 3          0.121 0.0990  0.147   0.95 median qi\n\nWe could add the predictions for categories 1, 2, and 3 together, but that would take a bit of extra data manipulation work. Instead, we can rely on the the fact that the prediction for category 0 is actually the inverse of the sum of categories 1+2+3, so we can instead just use 1 - .epred and only look at category 0. Even though the category column here says 0, it’s really the combined probability of choosing options 1, 2, or 3:\n\none_prediction %&gt;% \n  mutate(.epred = 1 - .epred) %&gt;%\n  filter(.category == 0) %&gt;% \n  median_qi(.epred)\n## # A tibble: 1 × 13\n##   seat  cargo eng   price resp.id  .row .category .epred .lower .upper .width .point .interval\n##   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 6     2ft   elec  30          1     1 0          0.339  0.306  0.371   0.95 median qi\n\nWith {mlogit}, we found AMCEs by essentially calculating marginal means for specific contrasts of predicted probabilities. We created a data frame of all 54 combinations of feature levels and then grouped and summarized that data frame as needed (e.g., the average of the 27 predictions for 2 feet of cargo space and the average of the 27 predictions for 3 feed of cargo space).\nWe can do the same thing with the {brms} model, but selecting only the 0 category and reversing the predicted value:\n\nnewdata_all_combos &lt;- minivans %&gt;% \n  tidyr::expand(seat, cargo, eng, price) %&gt;% \n  mutate(resp.id = 1)\n\nall_preds_brms &lt;- model_minivans_categorical_brms %&gt;% \n  epred_draws(newdata = newdata_all_combos) %&gt;% \n  filter(.category == 0) %&gt;% \n  mutate(.epred = 1 - .epred)\n\nTo make sure it worked, here are the posterior medians for all the different levels. It’s roughly the same as what we found with in all_preds_mlogit:\n\nall_preds_brms %&gt;% \n  group_by(seat, cargo, eng, price) %&gt;% \n  median_qi(.epred)\n## # A tibble: 54 × 10\n##    seat  cargo eng   price .epred .lower .upper .width .point .interval\n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 6     2ft   gas   30    0.682  0.651   0.713   0.95 median qi       \n##  2 6     2ft   gas   35    0.485  0.451   0.520   0.95 median qi       \n##  3 6     2ft   gas   40    0.305  0.275   0.338   0.95 median qi       \n##  4 6     2ft   hyb   30    0.502  0.466   0.537   0.95 median qi       \n##  5 6     2ft   hyb   35    0.306  0.276   0.338   0.95 median qi       \n##  6 6     2ft   hyb   40    0.171  0.150   0.194   0.95 median qi       \n##  7 6     2ft   elec  30    0.339  0.306   0.371   0.95 median qi       \n##  8 6     2ft   elec  35    0.183  0.161   0.207   0.95 median qi       \n##  9 6     2ft   elec  40    0.0952 0.0819  0.110   0.95 median qi       \n## 10 6     3ft   gas   30    0.769  0.743   0.795   0.95 median qi       \n## # ℹ 44 more rows\n\nTo pull out specific group-level averages, we can group and summarize. For example, here are the posterior median predictions for the two levels of cargo space:\n\nall_preds_brms %&gt;% \n  group_by(cargo) %&gt;% \n  median_qi(.epred)\n## # A tibble: 2 × 7\n##   cargo .epred .lower .upper .width .point .interval\n##   &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 2ft    0.256 0.0606  0.676   0.95 median qi       \n## 2 3ft    0.348 0.0905  0.763   0.95 median qi\n\nThe medians here are correct and basically what we found with {mlogit}, but the credible intervals are wildly off (5% to 75% favorability?!). If we plot this we can see why:\n\nall_preds_brms %&gt;% \n  ggplot(aes(x = .epred, y = cargo, fill = cargo)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = clrs[c(11, 7)], guide = \"none\") +\n  labs(x = \"Marginal means\", y = \"Cargo space\")\n\n\n\n\n\n\n\nhahahaha check out those mountain ranges. All those peaks come from combining the 27 different 2ft- and 3ft- posterior distributions for all the different combinations of other feature levels.\nTo get the actual marginal mean for cargo space, we need to marginalize out (or average out) all those other covariates. To do this, we need to group by the cargo column and the .draw column so that we find the average within each set of MCMC draws. To help with the intuition, look how many rows are in each of these groups of cargo and .draw—there are 27 different estimates for each of the 4,000 draws for 2 feet and 27 different estimates for each of the 4,000 draws for 3 feet. We want to collapse (or marginalize) those 27 rows into just one average.\n\nall_preds_brms %&gt;% \n  group_by(cargo, .draw) %&gt;% \n  summarize(nrows = n())\n## # A tibble: 8,000 × 3\n## # Groups:   cargo [2]\n##    cargo .draw nrows\n##    &lt;fct&gt; &lt;int&gt; &lt;int&gt;\n##  1 2ft       1    27\n##  2 2ft       2    27\n##  3 2ft       3    27\n##  4 2ft       4    27\n##  5 2ft       5    27\n##  6 2ft       6    27\n##  7 2ft       7    27\n##  8 2ft       8    27\n##  9 2ft       9    27\n## 10 2ft      10    27\n## # ℹ 7,990 more rows\n\nTo do that, we can find the average predicted value in those groups, then work with that as our main estimand. Check out these marginalized-out posteriors now—the medians are the same as before, but the credible intervals make a lot more sense:\n\npreds_cargo_marginalized &lt;- all_preds_brms %&gt;% \n  # Marginalize out the other covariates\n  group_by(cargo, .draw) %&gt;%\n  summarize(avg = mean(.epred))\n\npreds_cargo_marginalized %&gt;% \n  group_by(cargo) %&gt;% \n  median_qi()\n## # A tibble: 2 × 7\n##   cargo   avg .lower .upper .width .point .interval\n##   &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 2ft   0.292  0.275  0.309   0.95 median qi       \n## 2 3ft   0.374  0.356  0.393   0.95 median qi\n\nWe can confirm that marginalizing out the other covariates worked by plotting it:\n\npreds_cargo_marginalized %&gt;% \n  ggplot(aes(x = avg, y = cargo, fill = cargo)) +\n  stat_halfeye() +\n  scale_x_continuous(labels = label_percent()) +\n  scale_fill_manual(values = clrs[c(11, 7)], guide = \"none\") +\n  labs(x = \"Marginal means\", y = \"Cargo space\")\n\n\n\n\n\n\n\nheck. yes.\nFinally, we’re actually most interested in the AMCE, or the difference between these two cargo sizes. The compare_levels() function from {tidybayes} can calculate this automatically:\n\npreds_cargo_marginalized %&gt;%\n  compare_levels(variable = avg, by = cargo, comparison = \"control\") %&gt;% \n  median_qi(avg)\n## # A tibble: 1 × 7\n##   cargo        avg .lower .upper .width .point .interval\n##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 3ft - 2ft 0.0830 0.0643  0.100   0.95 median qi\n\nThat’s it! The causal effect of moving from 2 feet → 3 feet of storage space, holding all other features constant, is 8 percentage points (with a 95% credible interval of 6.5 to 10 percentage points).\nWe can combine all these AMCEs into a huge data frame. The marginalization process + compare_levels() has to happen with one feature at a time, so we need to create several separate data frames:\n\n# I could probably do this with purrr::map() to reduce all this repetition, but\n# whatever, it works\namces_minivan_brms &lt;- bind_rows(\n  seat = all_preds_brms %&gt;% \n    group_by(seat, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = seat, comparison = \"control\") %&gt;% \n    rename(contrast = seat),\n  cargo = all_preds_brms %&gt;% \n    group_by(cargo, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = cargo, comparison = \"control\") %&gt;% \n    rename(contrast = cargo),\n  eng = all_preds_brms %&gt;% \n    group_by(eng, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = eng, comparison = \"control\") %&gt;% \n    rename(contrast = eng),\n  price = all_preds_brms %&gt;% \n    group_by(price, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = price, comparison = \"control\") %&gt;% \n    rename(contrast = price),\n  .id = \"term\"\n)\n\namces_minivan_brms %&gt;% \n  group_by(term, contrast) %&gt;% \n  median_qi(avg)\n## # A tibble: 7 × 8\n##   term  contrast       avg  .lower  .upper .width .point .interval\n##   &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 cargo 3ft - 2ft   0.0830  0.0643  0.100    0.95 median qi       \n## 2 eng   elec - gas -0.279  -0.300  -0.256    0.95 median qi       \n## 3 eng   hyb - gas  -0.161  -0.184  -0.138    0.95 median qi       \n## 4 price 35 - 30    -0.178  -0.201  -0.155    0.95 median qi       \n## 5 price 40 - 30    -0.309  -0.330  -0.287    0.95 median qi       \n## 6 seat  7 - 6      -0.0995 -0.121  -0.0785   0.95 median qi       \n## 7 seat  8 - 6      -0.0570 -0.0788 -0.0355   0.95 median qi\n\nPlots\nAgain, plotting these AMCEs so that there’s a reference category at 0 requires some extra data work, so I’ve hidden all that code for the sake of space.\n\nExtract variable labelsminivan_var_levels &lt;- tibble(\n  variable = c(\"seat\", \"cargo\", \"eng\", \"price\")\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- minivans[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Make a little lookup table for nicer feature labels\nminivan_var_lookup &lt;- tribble(\n  ~variable, ~variable_nice,\n  \"seat\",    \"Passengers\",\n  \"cargo\",   \"Cargo space\",\n  \"eng\",     \"Engine type\",\n  \"price\",   \"Price (thousands of $)\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))\n\n\n\nCombine full dataset of factor levels with model comparisons and make {mlogit} plotamces_minivan_mlogit_split &lt;- amces_minivan_mlogit %&gt;% \n  separate_wider_delim(\n    term,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  rename(term = variable)\n\nplot_data &lt;- minivan_var_levels %&gt;%\n  left_join(\n    amces_minivan_mlogit_split,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;% \n  # Make these zero\n  mutate(\n    across(\n      c(estimate),\n      # This is from when this worked with marginaleffects\n      # c(estimate, conf.low, conf.high),\n      ~ ifelse(is.na(.x), 0, .x)\n    )\n  ) %&gt;% \n  left_join(minivan_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np1 &lt;- ggplot(\n  plot_data,\n  aes(x = estimate, y = levels, color = variable_nice)\n) +\n  geom_vline(xintercept = 0) +\n  geom_point() +\n  scale_x_continuous(labels = label_pp) +\n  scale_color_manual(values = clrs[c(3, 7, 8, 9)], guide = \"none\") +\n  labs(\n    x = \"Percentage point change in\\nprobability of minivan selection\",\n    y = NULL,\n    title = \"Frequentist AMCEs from {mlogit}\"\n  ) +\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\")\n\n\n\nCombine full dataset of factor levels with marginalized posterior draws and make {brms} plotposterior_mfx_minivan_nested &lt;- amces_minivan_brms %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\nplot_data_minivan_bayes &lt;- minivan_var_levels %&gt;%\n  left_join(\n    posterior_mfx_minivan_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(avg = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(minivan_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.)))\n\np2 &lt;- ggplot(plot_data_minivan_bayes, aes(x = avg, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(normalize = \"groups\") +  # Make the heights of the distributions equal within each facet\n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  scale_fill_manual(values = clrs[c(3, 7, 8, 9)], guide = \"none\") +\n  labs(\n    x = \"Percentage point change in\\nprobability of minivan selection\",\n    y = NULL,\n    title = \"Posterior Bayesian AMCEs from {brms}\"\n  )\n\n\n\np1 + p2"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#the-setup-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "The setup",
    "text": "The setup\nWe’ve cheated a little and have already used multilevel structures in the Bayesian models for the chocolate experiment and the minivan experiment. This was because these datasets had a natural panel grouping structure inside them. {mlogit} can work with panel-indexed data frames (that’s the point of that strange dfidx() function). By creating respondent-specific intercepts like we did with the {brms} models, we helped account for some of the variation caused by respondent differences.\nBut we can do better than that and get far richer and more complex models and estimates and predictions. In addition to using respondent-specific intercepts, we can (1) include respondent-level characteristics as covariates, and (2) include respondent-specific slopes for the minivan characteristic.\nIn the minivan data, we have data on feature levels (seat, cargo, eng, price) and on individual characteristics (carpool). The carpool variable indicates if the respondent uses their vehicle for carpooling. This is measured at the respondent level and not the choice level (i.e. someone won’t stop being a carpooler during one set of choices and then resume being a carpooler for another set). We can visualize where these different columns are measured by returning to the hierarchical model diagram:\n\n\n\n\nMultilevel experimental structure, with minivan choices \\(\\{y_1, y_2, y_3\\}\\) nested in sets of questions in respondents, w ith variables measured at different levels\n\n\n\nWe can use hierarchical models (or multilevel models, or mixed effects models, or whatever you want to call them) to account for choice-level and respondent-level covariates and incorporate respondent-level heterogeneity and covariance into the model estimates.\n\n\nImage by Chelsea Parlett-Pelleriti"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#important-sidenote-on-notation",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#important-sidenote-on-notation",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Important sidenote on notation",
    "text": "Important sidenote on notation\nBut before looking at how to incorporate that carpool column into the model, we need to take a quick little detour into the world of notation. There’s no consistent way of writing out multilevel models,1 and accordingly, I thought it was impossible to run fully specified marketing-style hierarchical Bayesian models with {brms}—all because of notation!\n1 These are not the only approaches—section 12.5 in Gelman and Hill (2007) is called “Five ways to write the same model,” and they don’t include the offset notation as one of their five!There are a couple general ways I’ve seen group-level random effects written out in formal model notation: one with complete random β terms and one with random offsets from a global β term.\n\n\n\n\n\n\n{brms} / {lme4} syntax\n\n\n\nFor the best overview of how to use {brms} and {lme4} with different random group-level intercept and slope specifications, check out this summary table by Ben Bolker.\n\n\nRandom intercepts\nIf you want group-specific intercept terms, you can use a formula like this:\nbf(y ~ x + (1 | group))\nIn formal mathy terms, we can write this group-specific intercept as a complete coefficient: \\(\\beta_{0_j}\\). Each group \\(j\\) gets its own intercept coefficient. Nice and straightforward.\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_1 X_{i_j} & \\text{Linear model of within-group variation } \\\\\n\\beta_{0_j} &\\sim \\mathcal{N}(\\beta_0, \\sigma_0) & \\text{Random group-specific intercepts}\n\\end{aligned}\n\\]\nHowever, I actually like to think of these random effects in a slightly different way, where each group intercept is actually a combination of a global average (\\(\\beta_0\\)) and a group-specific offset from that average (\\(b_{0_j}\\)), like this:\n\\[\n\\beta_{0_j} = \\beta_0 + b_{0_j}\n\\]\nThat offset is assumed to be normally distributed with a mean of 0 (\\(\\mathcal{N}(0, \\sigma_0)\\)):\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= (b_{0_j} + \\beta_0) + \\beta_1 X_{i_j} & \\text{Linear model of within-group variation } \\\\\nb_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0) & \\text{Random group-specific offsets from global intercept}\n\\end{aligned}\n\\]\nI prefer this offset notation because it aligns with the output of {brms}, which reports population-level coefficients (i.e. the global average \\(\\beta_0\\)) along with group-specific offsets from that average (i.e. \\(b_{0_j}\\)), which you can access with ranef(model_name).\nRandom slopes\nIf you want group-specific intercepts and slopes, you can use a formula like this:\nbf(y ~ x + (1 + x | group))\nThe same dual syntax applies when using random slopes too. We can either use whole group-specific \\(\\beta_{n_j}\\) terms, or use offsets (\\(b_{n_j}\\)) from a global average slope (\\(\\beta_n\\)). When working with random slopes, the math notation gets a little fancier because the random intercept and slope terms are actually correlated and move together across groups. The \\(\\beta\\) terms come from a multivariate (or joint) normal distribution with shared covariance.\nWith the complete β approach, we’re estimating the joint distribution of \\(\\begin{pmatrix} \\beta_{0_j} \\\\ \\beta_{1_j} \\end{pmatrix}\\):\n\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j} X_{i_j} & \\text{Linear model of within-group variation } \\\\\n\\left(\n  \\begin{array}{c}\n  \\beta_{0_j} \\\\\n  \\beta_{1_j}\n  \\end{array}\n\\right)\n&\\sim \\operatorname{MV}\\, \\mathcal{N}\n\\left(\n  \\left(\n    \\begin{array}{c}\n    \\beta_0 \\\\\n    \\beta_1 \\\\\n    \\end{array}\n  \\right)\n  , \\,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\\n     \\dots & \\sigma^2_{\\beta_1}\n  \\end{array}\n\\right)\n\\right) & \\text{Random group-specific slopes and intercepts}\n\\end{aligned}\n\\]\n\nWith the offset approach, we’re estimating the joint distribution of the offsets from the global intercept and slope, or \\(\\begin{pmatrix} b_{0_j} \\\\ b_{1_j} \\end{pmatrix}\\):\n\n\\[\n\\begin{aligned}\nY_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) & \\text{Outcome for individual}_i \\text{ within group}_j \\\\\n\\mu_{i_j} &= (b_{0_j} + \\beta_0) + (b_{1_j} + \\beta_1) X_{i_j} & \\text{Linear model of within-group variation } \\\\\n\\left(\n  \\begin{array}{c}\n  b_{0_j} \\\\\n  b_{1_j}\n  \\end{array}\n\\right)\n&\\sim \\operatorname{MV}\\, \\mathcal{N}\n\\left(\n  \\left(\n    \\begin{array}{c}\n    0 \\\\\n    0 \\\\\n    \\end{array}\n  \\right)\n  , \\,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\\n     \\dots & \\sigma^2_{\\beta_1}\n  \\end{array}\n\\right)\n\\right) & \\text{Random group-specific offsets from global intercept and slope}\n\\end{aligned}\n\\]\n\nSummary table\nAnd here’s a helpful little table summarizing these two types of notation (mostly for future me).\n\n\n\n\n\n\n\n\n\n\nFormula syntax\nFull \\(\\beta\\) notation\nOffset notation\n\n\n\nRandom intercept\ny ~ x + (1 | group)\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                                                                                                                                                                                                                                                                                                                                                  \\begin{aligned}                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                     Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                       \\mu_{i_j} &= \\beta_{0_j} + \\beta_1 X_{i_j} \\\\                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                      \\beta_{0_j} &\\sim \\mathcal{N}(\\beta_0, \\sigma_0)                                                                                                                                                                                                                                              \n                                                                                                                                       \\end{aligned}                                                                                                                                                                                                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                                                        \\]\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\begin{aligned}\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\mu_{i_j} &= (b_{0_j} + \\beta_0) + \\beta_1 X_{i_j} \\\\                                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            b_{0_j} &\\sim \\mathcal{N}(0, \\sigma_0)                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\end{aligned}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\]\n\n\nRandom intercept + slope\ny ~ x + (1 + x | group)\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                                                                                                                                                                                                                                                                                                                                                  \\begin{aligned}                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                     Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                     \\mu_{i_j} &= \\beta_{0_j} + \\beta_{1_j} X_{i_j} \\\\                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                                           \\left(                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                       \\begin{array}{c}                                                                                                                                                                                                                                                             \n                                                                                                                                                                                                                                                                                                                        \\beta_{0_j} \\\\                                                                                                                                                                                                                                                              \n                                                                                                                                                                                                                                                                                                                         \\beta_{1_j}                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                         \\end{array}                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                          \\right)                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                           &\\sim \\operatorname{MV}\\, \\mathcal{N}                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                           \\left(                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                        \\begin{array}{c}                                                                                                                                                                                                                                                            \n                                                                                                                                                                                                                                                                                                                           \\beta_0 \\\\                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                           \\beta_1 \\\\                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                          \\end{array}                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                           \\right)                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                         , \\, \\Sigma                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                         \\right) \\\\                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                        \\Sigma &\\sim                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                           \\left(                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                      \\begin{array}{cc}                                                                                                                                                                                                                                                             \n                                                                                                                                                                                                                                                                                       \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\                                                                                                                                                                                                                          \n                                                                                                                                                                                                                                                                                                                   \\dots & \\sigma^2_{\\beta_1}                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                         \\end{array}                                                                                                                                                                                                                                                                \n                                                                                                                                                                                                                                                                                                                          \\right)                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                      \\end{aligned}                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                                                        \\]\n\\[                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\begin{aligned}\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Y_{i_j} &\\sim \\mathcal{N}(\\mu_{i_j}, \\sigma_y) \\\\                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\mu_{i_j} &= (b_{0_j} + \\beta_0) + (b_{1_j} + \\beta_1) X_{i_j} \\\\                                                                                                                                                                                                                                                                          \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\begin{array}{c}                                                                                                                                                                                                                                                                                                  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           b_{0_j} \\\\                                                                                                                                                                                                                                                                                                     \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             b_{1_j}                                                                                                                                                                                                                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\end{array}                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\right)                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             &\\sim \\operatorname{MV}\\, \\mathcal{N}                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\left(                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\begin{array}{c}                                                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0 \\\\                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0 \\\\                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\end{array}                                                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\right)                                                                                                                                                                                                                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           , \\, \\Sigma                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\right) \\\\                                                                                                                                                                                                                                                                                                      \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\Sigma &\\sim                                                                                                                                                                                                                                                                                                     \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\left(                                                                                                                                                                                                                                                                                                        \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\begin{array}{cc}                                                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\sigma^2_{\\beta_0} & \\rho_{\\beta_0, \\beta_1}\\, \\sigma_{\\beta_0} \\sigma_{\\beta_1} \\\\                                                                                                                                                                                                                                                               \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\dots & \\sigma^2_{\\beta_1}                                                                                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\end{array}                                                                                                                                                                                                                                                                                                    \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \\right)                                                                                                                                                                                                                                                                                                       \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\end{aligned}                                                                                                                                                                                                                                                                                                     \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\]"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#translating-from-marketing-style-stan-notation-to-brms-syntax",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#translating-from-marketing-style-stan-notation-to-brms-syntax",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Translating from marketing-style Stan notation to {brms} syntax",
    "text": "Translating from marketing-style Stan notation to {brms} syntax\nIn Chapman and Feit (2019) and in all the marketing papers I’ve seen that use hierarchical Bayesian models—and even one I coauthored! (Chaudhry, Dotson, and Heiss 2021) (see the appendix)—they define their models using notation like this:\n\\[\n\\begin{aligned}\n\\text{Choice} &\\sim \\operatorname{Multinomial\\ logit}(\\beta X) & \\text{where } X = \\text{feature levels} \\\\\n\\beta &\\sim \\operatorname{Multivariate} \\mathcal{N}(\\gamma Z, \\Sigma) & \\text{where } Z = \\text{individual characteristics}\n\\end{aligned}\n\\]\nFor the longest time this threw me off because it’s slightly different from the two different notations we just reviewed (full βs vs. offsets from global β), and I figured that specifying a model like this with {brms} was impossible. The main reason for my confusion is that there are two different datasets involved in this model, and {brms} can only really work with one dataset.\nIn raw Stan (like in this tutorial on conjoint hierarchical Bayes models, or in this example of a different conjoint hierarchical model), you’d typically work with two different datasets or matrices: one \\(X\\) with feature levels and one \\(Z\\) with respondent-level characteristics. (This is actually the recommended way to write hierarchical models in raw Stan!).\nHere’s what separate \\(X\\) and \\(Z\\) matrices would look like with the minivan data—X contains the full data without respondent-level covariates like carpool and it has 9,000 rows; Z contains only respondent-level characteristics like carpool and it only has 200 rows (one per respondent).\n\nX &lt;- minivans %&gt;% select(-carpool)\nX\n## # A tibble: 9,000 × 8\n##    resp.id  ques   alt seat  cargo eng   price choice\n##      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n##  1       1     1     1 6     2ft   gas   35         0\n##  2       1     1     2 8     3ft   hyb   30         0\n##  3       1     1     3 6     3ft   gas   30         1\n##  4       1     2     1 6     2ft   gas   30         0\n##  5       1     2     2 7     3ft   gas   35         1\n##  6       1     2     3 6     2ft   elec  35         0\n##  7       1     3     1 8     3ft   gas   35         1\n##  8       1     3     2 7     3ft   elec  30         0\n##  9       1     3     3 8     2ft   elec  40         0\n## 10       1     4     1 7     3ft   elec  40         1\n## # ℹ 8,990 more rows\n\nZ &lt;- minivans %&gt;% \n  # Only keep the first row of each respondent\n  group_by(resp.id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% \n  # Only keep the respondent-level columns\n  select(resp.id, carpool)\nZ\n## # A tibble: 200 × 2\n##    resp.id carpool\n##      &lt;dbl&gt; &lt;fct&gt;  \n##  1       1 yes    \n##  2       2 no     \n##  3       3 no     \n##  4       4 no     \n##  5       5 yes    \n##  6       6 no     \n##  7       7 no     \n##  8       8 yes    \n##  9       9 no     \n## 10      10 no     \n## # ℹ 190 more rows\n\nX and Z are then passed to Stan as separate matrices and used at different places in the model fitting process. Here’s what that looks like in pseudo-Stan code. The matrix of individual characteristics Z is matrix-multiplied with a bunch of estimated \\(\\gamma\\) coefficients (Gamma here) to generate individual-specific \\(\\beta\\) coefficients (Beta here). The matrix of choices X is then matrix-multiplied with the individual-specific \\(\\beta\\) coefficients to generate predicted outcomes (Y here).\nfor (r in 1:n_respondents) {\n  // All the individual-specific slopes and intercepts\n  Beta[,r] ~ multi_normal(Gamma * Z[,r], ...);\n\n  // All the question-level outcomes, using individual-specific slopes and intercepts\n  for (s in 1:n_questions) {\n     Y[r,s] ~ categorical_logit( X [r,s] * Beta[,r]);\n  }\n}\nThat’s a neat way of working with multilevel models, but it’s different from how I’ve always worked with them (and it requires working with raw Stan). As seen throughout this post, I’m a fan of {brms}’s formula-style syntax for specifying multilevel models, but {brms} can only work with one dataset at a time—you can’t pass it both X and Z like you’d do with raw Stan. So I (naively) figured that this went beyond {brms}’s abilities and was only possible with raw Stan.\nHowever, if we use {brms}’s special formula syntax, we can actually specify an identical model with only one dataset (again, see this for a fantastic overview of the syntax).\nFirst, let’s look at the marketing-style syntax again:\n\\[\n\\begin{aligned}\n\\text{Choice} &\\sim \\operatorname{Multinomial\\ logit}(\\beta X) & \\text{where } X = \\text{feature levels} \\\\\n\\beta &\\sim \\operatorname{Multivariate} \\mathcal{N}(\\gamma Z, \\Sigma) & \\text{where } Z = \\text{individual characteristics}\n\\end{aligned}\n\\]\nThis is actually just a kind of really compact notation. That second line with the \\(\\beta \\sim \\operatorname{Multivariate}\\, \\mathcal{N}(\\cdot)\\) distribution is a shorthand version of the full-β syntax from earlier. To illustrate this, let’s expand this out to a more complete formal definition of the model. Instead of using \\(X\\) to stand in for all the feature levels and \\(Z\\) for all the individual characteristics, we’ll expand those to include all the covariates we’re using. And instead of calling the distribution “Multinomial logit” we’ll call it “Categorical” so it aligns with Stan. It’ll make for a really massive formula, but it shows what’s really going on.\n\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ \\beta_{0_j} + \\beta_{1_j} \\text{Seat[7]}_{i_j} + \\beta_{2_j} \\text{Seat[8]}_{i_j} + \\beta_{3_j} \\text{Cargo[3ft]}_{i_j} + \\\\\n&\\ \\beta_{4_j} \\text{Engine[hyb]}_{i_j} + \\beta_{5_j} \\text{Engine[elec]}_{i_j} + \\\\\n&\\ \\beta_{6_j} \\text{Price[35k]}_{i_j} + \\beta_{7_j} \\text{Price[40k]}_{i_j} \\\\[20pt]  \n&\\ \\textbf{Respondent-specific slopes} \\\\\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\beta_{0_j} \\\\\n      &\\beta_{1_j} \\\\\n      &\\beta_{2_j} \\\\\n      &\\beta_{3_j} \\\\\n      &\\beta_{4_j} \\\\\n      &\\beta_{5_j} \\\\\n      &\\beta_{6_j} \\\\\n      &\\beta_{7_j}\n    \\end{aligned}\n  \\end{array}\n\\right) \\sim&\\ \\operatorname{Multivariate}\\ \\mathcal{N} \\left[\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma^{\\beta_{0}}_{0} + \\gamma^{\\beta_{0}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{1}}_{0} + \\gamma^{\\beta_{1}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{3}}_{0} + \\gamma^{\\beta_{3}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{4}}_{0} + \\gamma^{\\beta_{4}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{5}}_{0} + \\gamma^{\\beta_{5}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{6}}_{0} + \\gamma^{\\beta_{6}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{7}}_{0} + \\gamma^{\\beta_{7}}_{1}\\text{Carpool}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cccccccc}\n     \\sigma^2_{\\beta_{0j}} & \\rho_{\\beta_{0j}\\beta_{1j}} & \\rho_{\\beta_{0j}\\beta_{2j}} & \\rho_{\\beta_{0j}\\beta_{3j}} & \\rho_{\\beta_{0j}\\beta_{4j}} & \\rho_{\\beta_{0j}\\beta_{5j}} & \\rho_{\\beta_{0j}\\beta_{6j}} & \\rho_{\\beta_{0j}\\beta_{7j}} \\\\\n     \\dots & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} & \\rho_{\\beta_{1j}\\beta_{5j}} & \\rho_{\\beta_{1j}\\beta_{6j}} & \\rho_{\\beta_{1j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} & \\rho_{\\beta_{2j}\\beta_{5j}} & \\rho_{\\beta_{2j}\\beta_{6j}} & \\rho_{\\beta_{2j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} & \\rho_{\\beta_{3j}\\beta_{5j}} & \\rho_{\\beta_{3j}\\beta_{6j}} & \\rho_{\\beta_{3j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{4j}} & \\rho_{\\beta_{4j}\\beta_{5j}} & \\rho_{\\beta_{4j}\\beta_{6j}} & \\rho_{\\beta_{4j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{5j}} & \\rho_{\\beta_{5j}\\beta_{6j}} & \\rho_{\\beta_{5j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{6j}} & \\rho_{\\beta_{6j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{7j}}\n  \\end{array}\n\\right)\n\\right]\n\\end{aligned}\n\\]\n\n \nImportantly, pay attention to where the choice-level and respondent-level variables show up in this expanded version. All the choice-level variables have respondent-specific \\(\\beta\\) coefficients, while the respondent-level variable (carpool) is down in that massive multivariate normal matrix with its own \\(\\gamma\\) coefficients, helping determine the respondent-specific \\(\\beta\\) coefficients. That’s great and exactly what we want, and we can do that with raw Stan, but raw Stan is no fun.\nWe can create this exact same model structure with {brms} like this:\nbf(choice_alt ~\n  # Choice-level predictors that are nested within respondents...\n  (seat + cargo + eng + price) *\n  # ...interacted with all respondent-level predictors...\n  (carpool) +\n  # ... with random respondent-specific slopes for the\n  # nested choice-level predictors\n  (1 + seat + cargo + eng + price | resp.id))\nWe can confirm that this worked by using the miraculous {equatiomatic} package, which automatically converts model objects into LaTeX code. {equatiomatic} doesn’t work with {brms} models, but it does work with frequentist {lme4} models, so we can fit a throwaway frequentist model with this syntax (it won’t actually converge and it’ll give a warning, but that’s fine—we don’t actually care about this model) and then feed it to equatiomatic::extract_eq() to see what it looks like in formal notation.\n(This is actually how I figured out the correct combination of interactions and random slopes—I kept trying different combinations that I thought were right until the math matched the huge full model above, with the \\(\\beta\\) and \\(\\gamma\\) terms in the right places.)\n\nlibrary(lme4)\nlibrary(equatiomatic)\n\nmodel_throwaway &lt;- lmer(\n  choice ~ (seat + cargo + eng + price) * (carpool) +\n    (1 + seat + cargo + eng + price | resp.id),\n  data = minivans\n)\n\nprint(extract_eq(model_throwaway))\n\\[\n\\begin{aligned}\n  \\operatorname{choice}_{i}  &\\sim N \\left(\\mu, \\sigma^2 \\right) \\\\\n    \\mu &=\\alpha_{j[i]} + \\beta_{1j[i]}(\\operatorname{seat}_{\\operatorname{7}}) + \\beta_{2j[i]}(\\operatorname{seat}_{\\operatorname{8}}) + \\beta_{3j[i]}(\\operatorname{cargo}_{\\operatorname{3ft}}) + \\beta_{4j[i]}(\\operatorname{eng}_{\\operatorname{hyb}}) + \\beta_{5j[i]}(\\operatorname{eng}_{\\operatorname{elec}}) + \\beta_{6j[i]}(\\operatorname{price}_{\\operatorname{35}}) + \\beta_{7j[i]}(\\operatorname{price}_{\\operatorname{40}}) \\\\    \n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\alpha_{j} \\\\\n      &\\beta_{1j} \\\\\n      &\\beta_{2j} \\\\\n      &\\beta_{3j} \\\\\n      &\\beta_{4j} \\\\\n      &\\beta_{5j} \\\\\n      &\\beta_{6j} \\\\\n      &\\beta_{7j}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma_{0}^{\\alpha} + \\gamma_{1}^{\\alpha}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{1}}_{0} + \\gamma^{\\beta_{1}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{3}}_{0} + \\gamma^{\\beta_{3}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{4}}_{0} + \\gamma^{\\beta_{4}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{5}}_{0} + \\gamma^{\\beta_{5}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{6}}_{0} + \\gamma^{\\beta_{6}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}}) \\\\\n      &\\gamma^{\\beta_{7}}_{0} + \\gamma^{\\beta_{7}}_{1}(\\operatorname{carpool}_{\\operatorname{yes}})\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cccccccc}\n     \\sigma^2_{\\alpha_{j}} & \\rho_{\\alpha_{j}\\beta_{1j}} & \\rho_{\\alpha_{j}\\beta_{2j}} & \\rho_{\\alpha_{j}\\beta_{3j}} & \\rho_{\\alpha_{j}\\beta_{4j}} & \\rho_{\\alpha_{j}\\beta_{5j}} & \\rho_{\\alpha_{j}\\beta_{6j}} & \\rho_{\\alpha_{j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{1j}\\alpha_{j}} & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} & \\rho_{\\beta_{1j}\\beta_{5j}} & \\rho_{\\beta_{1j}\\beta_{6j}} & \\rho_{\\beta_{1j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{2j}\\alpha_{j}} & \\rho_{\\beta_{2j}\\beta_{1j}} & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} & \\rho_{\\beta_{2j}\\beta_{5j}} & \\rho_{\\beta_{2j}\\beta_{6j}} & \\rho_{\\beta_{2j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{3j}\\alpha_{j}} & \\rho_{\\beta_{3j}\\beta_{1j}} & \\rho_{\\beta_{3j}\\beta_{2j}} & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} & \\rho_{\\beta_{3j}\\beta_{5j}} & \\rho_{\\beta_{3j}\\beta_{6j}} & \\rho_{\\beta_{3j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{4j}\\alpha_{j}} & \\rho_{\\beta_{4j}\\beta_{1j}} & \\rho_{\\beta_{4j}\\beta_{2j}} & \\rho_{\\beta_{4j}\\beta_{3j}} & \\sigma^2_{\\beta_{4j}} & \\rho_{\\beta_{4j}\\beta_{5j}} & \\rho_{\\beta_{4j}\\beta_{6j}} & \\rho_{\\beta_{4j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{5j}\\alpha_{j}} & \\rho_{\\beta_{5j}\\beta_{1j}} & \\rho_{\\beta_{5j}\\beta_{2j}} & \\rho_{\\beta_{5j}\\beta_{3j}} & \\rho_{\\beta_{5j}\\beta_{4j}} & \\sigma^2_{\\beta_{5j}} & \\rho_{\\beta_{5j}\\beta_{6j}} & \\rho_{\\beta_{5j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{6j}\\alpha_{j}} & \\rho_{\\beta_{6j}\\beta_{1j}} & \\rho_{\\beta_{6j}\\beta_{2j}} & \\rho_{\\beta_{6j}\\beta_{3j}} & \\rho_{\\beta_{6j}\\beta_{4j}} & \\rho_{\\beta_{6j}\\beta_{5j}} & \\sigma^2_{\\beta_{6j}} & \\rho_{\\beta_{6j}\\beta_{7j}} \\\\\n     \\rho_{\\beta_{7j}\\alpha_{j}} & \\rho_{\\beta_{7j}\\beta_{1j}} & \\rho_{\\beta_{7j}\\beta_{2j}} & \\rho_{\\beta_{7j}\\beta_{3j}} & \\rho_{\\beta_{7j}\\beta_{4j}} & \\rho_{\\beta_{7j}\\beta_{5j}} & \\rho_{\\beta_{7j}\\beta_{6j}} & \\sigma^2_{\\beta_{7j}}\n  \\end{array}\n\\right)\n\\right)\n    \\text{, for resp.id j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\nDifferent variations of group-level interactions\nThis syntax is a special R shortcut for interacting carpool with each of the feature variables:\n# Short way\n(seat + cargo + eng + price) * (carpool)\n\n# This expands to this\nseat*carpool + cargo*carpool + eng*carpool + price*carpool\nIf we had other respondent-level columns like age (age) and education (ed), the shortcut syntax is really helpful:\n# Short way\n(seat + cargo + eng + price) * (carpool + age + ed)\n\n# This expands to this\nseat*carpool + cargo*carpool + eng*carpool + price*carpool +\nseat*age + cargo*age + eng*age + price*age +\nseat*ed + cargo*ed + eng*ed + price*ed\nWe don’t necessarily need to fully interact everything. For instance, if we have theoretical reasons to think that carpool status is associated with seat count preferences, but not other features, we can only interact seat and carpool:\nbf(choice ~ \n    (seat * carpool) + cargo + eng + price + \n  (1 + seat + cargo + eng + price | resp.id))\n\n\n\n\n\n\nModel running times\n\n\n\nThe more individual-level interactions you add, the longer it will take for the model to run. As we’ll see below, interacting carpool with the four feature levels takes ≈30 minutes to fit. As you add more individual-level interactions, the running time blows up.\nIn the replication code for Jensen et al. (2021), where they model a ton of individual variables, they say it takes several days to run. Our models in Chaudhry, Dotson, and Heiss (2021) take hours and hours to run."
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#mlogit-model-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#mlogit-model-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "\nmlogit model",
    "text": "mlogit model\n{mlogit} can estimate hierarchical models with something like this:\n\n# Define the random parameters\nmodel_mlogit_rpar &lt;- rep(\"n\", length = length(model_minivans_mlogit$coef))\nnames(model_mlogit_rpar) &lt;- names(model_minivans_mlogit$coef)\n\n# This means these random terms are all normally distributed\nmodel_mlogit_rpar\n#&gt; seat7    seat8 cargo3ft   enghyb  engelec  price35  price40 \n#&gt;   \"n\"      \"n\"      \"n\"      \"n\"      \"n\"      \"n\"      \"n\" \n\n# Run the model with carpool as an individual-level covariate\nmodel_mlogit_hierarchical &lt;- mlogit(\n  choice ~ 0 + seat + eng + cargo + price | carpool,\n  data = minivans_idx,\n  panel = TRUE, rpar = model_mlogit_rpar, correlation = TRUE\n)\n\n# Show the results\nsummary(model_mlogit_hierarchical)\n\nHowever, I’m not a frequentist and I’m already not a huge fan of extracting the predictions and AMCEs from these {mlogit} models. Running and interpreting and working with the results of that object is left as an exercise for the reader :). (See p. 381 in Chapman and Feit (2019) for a worked example of how to do it.)"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#finally-the-full-brms-model",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#finally-the-full-brms-model",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Finally, the full {brms} model",
    "text": "Finally, the full {brms} model\nThis is a really complex model with a ton of moving parts, but it’s also incredibly powerful. It lets us account for individual-specific differences across each of the minivan features. For instance, whether an individual carpools probably influences their preferences for the number of seats, and maybe cargo space, but probably doesn’t influence their preferences for engine type. If we had other individual-level characteristics, we could also let those influence feature preferences. Like, the number of kids an individual has probably influences seat count preferences; the individual’s income probably influences their price preferences; and so on.\nLet’s define the model more formally again, this time with priors for the parameters we’ll be estimating:\n\n\\[\n\\begin{aligned}\n&\\ \\textbf{Multinomial probability of selection of choice}_i \\textbf{ in respondent}_j \\\\\n\\text{Choice}_{i_j} \\sim&\\ \\operatorname{Categorical}(\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\}) \\\\[10pt]\n&\\ \\textbf{Model for probability of each option} \\\\\n\\{\\mu_{1,i_j}, \\mu_{2,i_j}, \\mu_{3,i_j}\\} =&\\ \\beta_{0_j} + \\beta_{1_j} \\text{Seat[7]}_{i_j} + \\beta_{2_j} \\text{Seat[8]}_{i_j} + \\beta_{3_j} \\text{Cargo[3ft]}_{i_j} + \\\\\n&\\ \\beta_{4_j} \\text{Engine[hyb]}_{i_j} + \\beta_{5_j} \\text{Engine[elec]}_{i_j} + \\\\\n&\\ \\beta_{6_j} \\text{Price[35k]}_{i_j} + \\beta_{7_j} \\text{Price[40k]}_{i_j} \\\\[20pt]  \n&\\ \\textbf{Respondent-specific slopes} \\\\\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\beta_{0_j} \\\\\n      &\\beta_{1_j} \\\\\n      &\\beta_{2_j} \\\\\n      &\\beta_{3_j} \\\\\n      &\\beta_{4_j} \\\\\n      &\\beta_{5_j} \\\\\n      &\\beta_{6_j} \\\\\n      &\\beta_{7_j}\n    \\end{aligned}\n  \\end{array}\n\\right) \\sim&\\ \\operatorname{Multivariate}\\ \\mathcal{N} \\left[\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\gamma^{\\beta_{0}}_{0} + \\gamma^{\\beta_{0}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{1}}_{0} + \\gamma^{\\beta_{1}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{3}}_{0} + \\gamma^{\\beta_{3}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{4}}_{0} + \\gamma^{\\beta_{4}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{5}}_{0} + \\gamma^{\\beta_{5}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{6}}_{0} + \\gamma^{\\beta_{6}}_{1}\\text{Carpool} \\\\\n      &\\gamma^{\\beta_{7}}_{0} + \\gamma^{\\beta_{7}}_{1}\\text{Carpool}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cccccccc}\n     \\sigma^2_{\\beta_{0j}} & \\rho_{\\beta_{0j}\\beta_{1j}} & \\rho_{\\beta_{0j}\\beta_{2j}} & \\rho_{\\beta_{0j}\\beta_{3j}} & \\rho_{\\beta_{0j}\\beta_{4j}} & \\rho_{\\beta_{0j}\\beta_{5j}} & \\rho_{\\beta_{0j}\\beta_{6j}} & \\rho_{\\beta_{0j}\\beta_{7j}} \\\\\n     \\dots & \\sigma^2_{\\beta_{1j}} & \\rho_{\\beta_{1j}\\beta_{2j}} & \\rho_{\\beta_{1j}\\beta_{3j}} & \\rho_{\\beta_{1j}\\beta_{4j}} & \\rho_{\\beta_{1j}\\beta_{5j}} & \\rho_{\\beta_{1j}\\beta_{6j}} & \\rho_{\\beta_{1j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\sigma^2_{\\beta_{2j}} & \\rho_{\\beta_{2j}\\beta_{3j}} & \\rho_{\\beta_{2j}\\beta_{4j}} & \\rho_{\\beta_{2j}\\beta_{5j}} & \\rho_{\\beta_{2j}\\beta_{6j}} & \\rho_{\\beta_{2j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{3j}} & \\rho_{\\beta_{3j}\\beta_{4j}} & \\rho_{\\beta_{3j}\\beta_{5j}} & \\rho_{\\beta_{3j}\\beta_{6j}} & \\rho_{\\beta_{3j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{4j}} & \\rho_{\\beta_{4j}\\beta_{5j}} & \\rho_{\\beta_{4j}\\beta_{6j}} & \\rho_{\\beta_{4j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{5j}} & \\rho_{\\beta_{5j}\\beta_{6j}} & \\rho_{\\beta_{5j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{6j}} & \\rho_{\\beta_{6j}\\beta_{7j}} \\\\\n     \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\dots & \\sigma^2_{\\beta_{7j}}\n  \\end{array}\n\\right)\n\\right] \\\\[10pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_{0 \\dots 7} \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ [\\text{Prior for choice-level coefficients}] \\\\\n\\gamma^{\\beta_{0 \\dots 7}}_0 \\sim&\\ \\mathcal{N} (0, 3) \\qquad\\qquad\\ [\\text{Prior for individual-level coefficients}] \\\\\n\\sigma_{\\beta_{0 \\dots 7}} \\sim&\\ \\operatorname{Exponential}(1) \\qquad [\\text{Prior for between-respondent intercept and slope variability}] \\\\\n\\rho \\sim&\\ \\operatorname{LKJ}(1) \\qquad\\qquad [\\text{Prior for correlation between random slopes and intercepts}]\n\\end{aligned}\n\\]\n\n \nHere it is in code form. There are a couple new things here in the Stan settings. First, we’re going to create 4 MCMC chains with 4,000 draws rather than 2,000—there are so many parameters to be estimated that we need to let the simulation run longer. Second, we’ve modified the adapt_delta setting to 0.99. Conceptually, this adjusts the size of the steps the MCMC algorithm takes as it traverses the posterior space for each parameter—higher numbers make the steps smaller and more granular. This slows down the MCMC simulation, but it also helps avoid divergent transitions (or failed out-of-bounds draws).\nOn my 2021 M1 MacBook Pro, running through cmdstanr with 2 CPU cores per chain, it took about 30 minutes to fit. If you’re following along with this post, start running this and go get some lunch or go for a walk or something.\n\n\n\n\n\n\nPre-run model\n\n\n\nAlternatively, you can download an .rds file of this completed. This brm() code load the .rds file automatically instead of rerunning the model as long as you put it in a folder named “models” in your working directory. This code uses the {osfr} package to download the .rds file from OSF automatically and places it where it needs to go:\n\nlibrary(osfr)  # Interact with OSF via R\n\n# Make a \"models\" folder if it doesn't exist already\nif (!file.exists(\"models\")) { dir.create(\"models\") }\n\n# Download model_minivans_mega_mlm_brms.rds from OSF\nosf_retrieve_file(\"https://osf.io/zp6eh\") |&gt;\n  osf_download(path = \"models\", conflicts = \"overwrite\", progress = TRUE)\n\n\n\n\nmodel_minivans_mega_mlm_brms &lt;- brm(\n  bf(choice_alt ~\n    # Choice-level predictors that are nested within respondents...\n    (seat + cargo + eng + price) *\n    # ...interacted with all respondent-level predictors...\n    (carpool) +\n    # ... with random respondent-specific slopes for the\n    # nested choice-level predictors\n    (1 + seat + cargo + eng + price | ID | resp.id)),\n  data = minivans_choice_alt,\n  family = categorical(refcat = \"0\"),\n  prior = c(\n    prior(normal(0, 3), class = b, dpar = mu1),\n    prior(normal(0, 3), class = b, dpar = mu2),\n    prior(normal(0, 3), class = b, dpar = mu3),\n    prior(exponential(1), class = sd, dpar = mu1),\n    prior(exponential(1), class = sd, dpar = mu2),\n    prior(exponential(1), class = sd, dpar = mu3),\n    prior(lkj(1), class = cor)\n  ),\n  chains = 4, cores = 4, warmup = 1000, iter = 5000, seed = 1234,\n  backend = \"cmdstanr\", threads = threading(2), # refresh = 0,\n  control = list(adapt_delta = 0.9),\n  file = \"models/model_minivans_mega_mlm_brms\"\n)\n\nThis model is incredibly rich. We just estimated more than 5,000 parameters (!!!)—we have three sets of coefficients for each of the three options, and those are all interacted with carpool, plus we have individual-specific offsets to each of those coefficients, plus all the \\(\\rho\\) terms in that massive correlation matrix.\n\nlength(get_variables(model_minivans_mega_mlm_brms))\n## [1] 5159\n\nSince we’re dealing with interaction terms, these raw log odds coefficients are even less helpful on their own. It’s nearly impossible to interpret these coefficients in any meaningful way—there’s no point in even trying to combine each of the individual parts of each effect (random parts, interaction parts, etc.). The only way we’ll be able to interpret these things is by making predictions.\n\nminivans_mega_marginalized &lt;- model_minivans_mega_mlm_brms %&gt;% \n  gather_draws(`^b_.*$`, regex = TRUE) %&gt;% \n  # Each variable name has \"mu1\", \"mu2\", etc. built in, like \"b_mu1_seat6\". This\n  # splits the .variable column into two parts based on a regular expression,\n  # creating one column for the mu part (\"b_mu1_\") and one for the rest of the\n  # variable name (\"seat6\")\n  separate_wider_regex(\n    .variable,\n    patterns = c(mu = \"b_mu\\\\d_\", .variable = \".*\")\n  ) %&gt;% \n  # Find the average of the three mu estimates for each variable within each\n  # draw, or marginalize across the three options, since they're randomized\n  group_by(.variable, .draw) %&gt;% \n  summarize(.value = mean(.value)) \n\nminivans_mega_marginalized %&gt;% \n  group_by(.variable) %&gt;% \n  median_qi(.value)\n## # A tibble: 16 × 7\n##    .variable            .value  .lower  .upper .width .point .interval\n##    &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 cargo3ft             0.405   0.288   0.522    0.95 median qi       \n##  2 cargo3ft:carpoolyes  0.163  -0.0493  0.377    0.95 median qi       \n##  3 carpoolyes          -0.722  -1.01   -0.434    0.95 median qi       \n##  4 engelec             -1.59   -1.77   -1.42     0.95 median qi       \n##  5 engelec:carpoolyes   0.0859 -0.213   0.380    0.95 median qi       \n##  6 enghyb              -0.775  -0.911  -0.638    0.95 median qi       \n##  7 enghyb:carpoolyes   -0.0550 -0.307   0.197    0.95 median qi       \n##  8 Intercept           -0.125  -0.285   0.0303   0.95 median qi       \n##  9 price35             -0.813  -0.948  -0.675    0.95 median qi       \n## 10 price35:carpoolyes  -0.163  -0.408   0.0825   0.95 median qi       \n## 11 price40             -1.58   -1.74   -1.43     0.95 median qi       \n## 12 price40:carpoolyes  -0.246  -0.526   0.0307   0.95 median qi       \n## 13 seat7               -0.880  -1.03   -0.736    0.95 median qi       \n## 14 seat7:carpoolyes     1.14    0.875   1.40     0.95 median qi       \n## 15 seat8               -0.646  -0.796  -0.501    0.95 median qi       \n## 16 seat8:carpoolyes     1.13    0.857   1.40     0.95 median qi"
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#predictions-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "Predictions",
    "text": "Predictions\nIn the non-hierarchical model earlier, we made marketing-style predictions by thinking of a product mix and figuring out the predicted utility and market share of each product in that mix. We can do the same thing here, but now we can incorporate individual-level characteristics too.\nHere’s our example product mix again, but this time we’ll repeat it twice—once with carpool set to “yes” and once with it set to “no”. This will let us see the predicted market share for each mix of products for a market of only carpoolers and only non-carpoolers.\n\nexample_product_mix &lt;- tribble(\n  ~seat, ~cargo, ~eng, ~price,\n  \"7\", \"2ft\", \"hyb\", \"30\",\n  \"6\", \"2ft\", \"gas\", \"30\",\n  \"8\", \"2ft\", \"gas\", \"30\",\n  \"7\", \"3ft\", \"gas\", \"40\",\n  \"6\", \"2ft\", \"elec\", \"40\",\n  \"7\", \"2ft\", \"hyb\", \"35\"\n)\n\nproduct_mix_carpool &lt;- bind_rows(\n  mutate(example_product_mix, carpool = \"yes\"),\n  mutate(example_product_mix, carpool = \"no\")\n) %&gt;% \n  mutate(across(everything(), factor)) %&gt;% \n  mutate(eng = factor(eng, levels = levels(minivans$eng)))\n\nWe can now go through the same process from earlier where we get logit-scale predictions for this smaller dataset and find the shares inside each draw + category (options 1, 2, and 3) + carpool status group.\n\ndraws_df &lt;- product_mix_carpool %&gt;% \n  add_linpred_draws(model_minivans_mega_mlm_brms, value = \"utility\", re_formula = NA)\n\nshares_df &lt;- draws_df %&gt;% \n  # Look at each set of predicted utilities within each draw within each of the\n  # three outcomes across both levels of carpooling\n  group_by(.draw, .category, carpool) %&gt;% \n  mutate(share = exp(utility) / sum(exp(utility))) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    mix_type = paste(seat, cargo, eng, price, sep = \" \"),\n    mix_type = fct_reorder(mix_type, share)\n  )\n\nThis new dataset contains 576,000 (!!) rows: 6 products × 2 carpool types × 3 \\(\\mu\\)-specific sets of coefficients × 16,000 MCMC draws. We can summarize this to get posterior medians and credible intervals, making sure to find the average share across the three outcomes (choice 1, 2, and 3), or marginalizing across the outcome.\n\nshares_df %&gt;% \n  # Marginalize across the three outcomes within each draw\n  group_by(mix_type, carpool, .draw) %&gt;% \n  summarize(share = mean(share)) %&gt;% \n  median_qi(share)\n## # A tibble: 12 × 8\n##    mix_type      carpool   share  .lower .upper .width .point .interval\n##    &lt;fct&gt;         &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 6 2ft elec 40 no      0.0218  0.0172  0.0272   0.95 median qi       \n##  2 6 2ft elec 40 yes     0.00948 0.00648 0.0137   0.95 median qi       \n##  3 7 2ft hyb 35  no      0.0433  0.0352  0.0530   0.95 median qi       \n##  4 7 2ft hyb 35  yes     0.0573  0.0421  0.0766   0.95 median qi       \n##  5 7 3ft gas 40  no      0.0653  0.0531  0.0799   0.95 median qi       \n##  6 7 3ft gas 40  yes     0.0976  0.0722  0.130    0.95 median qi       \n##  7 7 2ft hyb 30  no      0.0971  0.0825  0.114    0.95 median qi       \n##  8 7 2ft hyb 30  yes     0.148   0.118   0.183    0.95 median qi       \n##  9 8 2ft gas 30  no      0.265   0.239   0.293    0.95 median qi       \n## 10 8 2ft gas 30  yes     0.424   0.367   0.480    0.95 median qi       \n## 11 6 2ft gas 30  no      0.506   0.472   0.540    0.95 median qi       \n## 12 6 2ft gas 30  yes     0.261   0.224   0.303    0.95 median qi\n\nAnd we can plot them:\n\nshares_df %&gt;% \n  mutate(carpool = case_match(carpool, \"no\" ~ \"Non-carpoolers\", \"yes\" ~ \"Carpoolers\")) %&gt;% \n  # Marginalize across the three outcomes within each draw\n  group_by(mix_type, carpool, .draw) %&gt;% \n  summarize(share = mean(share)) %&gt;% \n  ggplot(aes(x = share, y = mix_type, slab_alpha = carpool)) +\n  stat_halfeye(normalize = \"groups\", fill = clrs[10]) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_slab_alpha_discrete(\n    range = c(1, 0.4),\n    guide = \"none\"\n  ) +\n  facet_wrap(vars(carpool)) +\n  labs(x = \"Predicted market share\", y = \"Hypothetical product mix\")\n\n\n\n\n\n\n\nThis is so cool! In general, the market share for these six hypothetical products is roughly the same across carpoolers and non-carpoolers, with one obvious exception—among non-carpoolers, the $30,000 8-passenger gas minivan with 2 feet of space has 26% of the market, while among carpoolers it has 42%. Individuals who carpool apparently really care about the number of passengers their vehicle can carry."
  },
  {
    "objectID": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-2",
    "href": "blog/2023/08/12/conjoint-multilevel-multinomial-guide/index.html#amces-2",
    "title": "The ultimate practical guide to multilevel multinomial conjoint analysis with R",
    "section": "AMCEs",
    "text": "AMCEs\nTo find the average marginal component effects (AMCEs), or the causal effect of moving one of these features to another value, holding all other variables constant, we can go through the same process as before. We’ll calculate the predicted probabilities of choosing option 0, 1, 2, and 3 across a full grid of all the combinations of feature levels and carpool status. We’ll then filter those predictions to only look at option 0 and reverse the predicted probabilities. Again, that feels weird, but it’s a neat little trick—if there’s a 33% chance that someone will select a specific combination of features, that would imply a 66% chance of not selecting it and an 11% chance of selecting it when it appears in option 1, option 2, and option 3. Rather than adding the probabilities within those three options together, we can do 100% − 66% to get the same 33% value, only it’s automatically combined.\nEarlier we had 54 combinations—now we have 108 (54 × 2). We’ll set resp.id to one that’s not in the dataset (201) so that these effects all deal with a generic hypothetical respondent (we could also do some fancy “integrating out” work and find population-level averages; see here for more about that).\n\nnewdata_all_combos_carpool &lt;- minivans %&gt;% \n  tidyr::expand(seat, cargo, eng, price, carpool) %&gt;% \n  mutate(resp.id = 201)\nnewdata_all_combos_carpool\n## # A tibble: 108 × 6\n##    seat  cargo eng   price carpool resp.id\n##    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;\n##  1 6     2ft   gas   30    no          201\n##  2 6     2ft   gas   30    yes         201\n##  3 6     2ft   gas   35    no          201\n##  4 6     2ft   gas   35    yes         201\n##  5 6     2ft   gas   40    no          201\n##  6 6     2ft   gas   40    yes         201\n##  7 6     2ft   hyb   30    no          201\n##  8 6     2ft   hyb   30    yes         201\n##  9 6     2ft   hyb   35    no          201\n## 10 6     2ft   hyb   35    yes         201\n## # ℹ 98 more rows\n\nNext we can plug this grid into the model, filter to only keep option 0, and reverse the predictions:\n\nall_preds_brms_carpool &lt;- model_minivans_mega_mlm_brms %&gt;% \n  epred_draws(\n    newdata = newdata_all_combos_carpool,\n    re_formula = NULL, allow_new_levels = TRUE\n  ) %&gt;% \n  filter(.category == 0) %&gt;% \n  mutate(.epred = 1 - .epred)\n\nThis thing has 1.7 million rows in it, so we need to group and summarize to do anything useful with it. We also need to marginalize across all the other covariates when grouping (i.e. if we want the estimates for passenger seat count across carpool status, we need to average out all the other covariates).\nTo test that this worked, here are the posterior marginal means for seat count:\n\npreds_seat_carpool_marginalized &lt;- all_preds_brms_carpool %&gt;% \n  # Marginalize out the other covariates in each draw\n  group_by(seat, carpool, .draw) %&gt;% \n  summarize(avg = mean(.epred))\n\npreds_seat_carpool_marginalized %&gt;% \n  group_by(seat, carpool) %&gt;% \n  median_qi(avg)\n## # A tibble: 6 × 8\n##   seat  carpool   avg .lower .upper .width .point .interval\n##   &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 6     no      0.425  0.378  0.485   0.95 median qi       \n## 2 6     yes     0.287  0.246  0.339   0.95 median qi       \n## 3 7     no      0.263  0.205  0.335   0.95 median qi       \n## 4 7     yes     0.335  0.267  0.418   0.95 median qi       \n## 5 8     no      0.305  0.226  0.406   0.95 median qi       \n## 6 8     yes     0.379  0.288  0.487   0.95 median qi\n\nThose credible intervals all look reasonable (i.e. not ranging from 5% to 80% or whatever), but it’s hard to see any trends from just this table. Let’s plot it:\n\npreds_seat_carpool_marginalized %&gt;% \n  ggplot(aes(x = avg, y = seat)) +\n  stat_halfeye(aes(slab_alpha = carpool), fill = clrs[3]) + \n  scale_x_continuous(labels = label_percent()) +\n  scale_slab_alpha_discrete(\n    range = c(0.4, 1),\n    labels = c(\"Non-carpoolers\", \"Carpoolers\"),\n    guide = guide_legend(\n      reverse = TRUE, override.aes = list(fill = \"grey10\"), \n      keywidth = 0.8, keyheight = 0.8\n    )\n  ) +\n  labs(\n    x = \"Marginal means\",\n    y = NULL,\n    slab_alpha = NULL\n  ) +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = 0)\n  )\n\n\n\n\n\n\n\nNeat! The average posterior predicted probability of choosing six seats is substantially higher for carpoolers than for non-carpoolers, while the probability for seven and eight seats is bigger for carpoolers.\nWe’re most interested in the AMCE though, and not the marginal means, so we’ll use compare_levels() to find the carpool-specific differences between the effect of moving from 6 → 7 and 6 → seats:\n\namces_seat_carpool &lt;- preds_seat_carpool_marginalized %&gt;% \n  group_by(carpool) %&gt;% \n  compare_levels(variable = avg, by = seat, comparison = \"control\") \n\namces_seat_carpool %&gt;% \n  median_qi(avg)\n## # A tibble: 4 × 8\n##   carpool seat      avg  .lower  .upper .width .point .interval\n##   &lt;fct&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 no      7 - 6 -0.163  -0.232  -0.0887   0.95 median qi       \n## 2 no      8 - 6 -0.121  -0.217  -0.0108   0.95 median qi       \n## 3 yes     7 - 6  0.0475 -0.0254  0.127    0.95 median qi       \n## 4 yes     8 - 6  0.0909 -0.0108  0.204    0.95 median qi\n\nAmong carpoolers, the causal effect of moving from 6 → 7 passengers, holding all other features constant, is a 5ish percentage point increase in the probability of selecting the vehicle. The effect is bigger (9ish percentage points) when moving from 6 → 8.\nAmong non-carpoolers, the causal effect is reversed. Moving from 6 → 7 passengers causes a 16 percentage point decrease in the probability of selection, while moving from 6 → 8 causes a 12 percentage point decrease, holding all other features constant.\nThese effects are “significant” and have a 90–97% probability of being greater than zero for carpoolers and 98–99% probability of being less than zero for the non-carpoolers.\n\n# Calculate probability of direction\namces_seat_carpool %&gt;% \n  group_by(seat, carpool) %&gt;% \n  summarize(p_gt_0 = sum(avg &gt; 0) / n()) %&gt;% \n  mutate(p_lt_0 = 1 - p_gt_0)\n## # A tibble: 4 × 4\n## # Groups:   seat [2]\n##   seat  carpool   p_gt_0 p_lt_0\n##   &lt;chr&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n## 1 7 - 6 no      0.000625 0.999 \n## 2 7 - 6 yes     0.912    0.0883\n## 3 8 - 6 no      0.0186   0.981 \n## 4 8 - 6 yes     0.961    0.0387\n\n\namces_seat_carpool %&gt;% \n  ggplot(aes(x = avg, y = seat)) +\n  stat_halfeye(aes(slab_alpha = carpool), fill = clrs[3]) +\n  geom_vline(xintercept = 0) +\n  scale_x_continuous(labels = label_pp) +\n  scale_slab_alpha_discrete(\n    range = c(0.4, 1),\n    labels = c(\"Non-carpoolers\", \"Carpoolers\"),\n    guide = guide_legend(\n      reverse = TRUE, override.aes = list(fill = \"grey10\"), \n      keywidth = 0.8, keyheight = 0.8\n    )\n  ) +\n  labs(\n    x = \"Percentage point change in probability of minivan selection\",\n    y = NULL,\n    slab_alpha = NULL\n  ) +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = 0)\n  )\n\n\n\n\n\n\n\nHere are all the AMCEs across carpool status:\n\namces_minivan_carpool &lt;- bind_rows(\n  seat = all_preds_brms_carpool %&gt;% \n    group_by(seat, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = seat, comparison = \"control\") %&gt;% \n    rename(contrast = seat),\n  cargo = all_preds_brms_carpool %&gt;% \n    group_by(cargo, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = cargo, comparison = \"control\") %&gt;% \n    rename(contrast = cargo),\n  eng = all_preds_brms_carpool %&gt;% \n    group_by(eng, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = eng, comparison = \"control\") %&gt;% \n    rename(contrast = eng),\n  price = all_preds_brms_carpool %&gt;% \n    group_by(price, carpool, .draw) %&gt;%\n    summarize(avg = mean(.epred)) %&gt;% \n    compare_levels(variable = avg, by = price, comparison = \"control\") %&gt;% \n    rename(contrast = price),\n  .id = \"term\"\n)\n\namces_minivan_carpool %&gt;% \n  group_by(term, carpool, contrast) %&gt;% \n  median_qi(avg)\n## # A tibble: 14 × 9\n##    term  carpool contrast       avg  .lower  .upper .width .point .interval\n##    &lt;chr&gt; &lt;fct&gt;   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n##  1 cargo no      3ft - 2ft   0.0750  0.0351  0.117    0.95 median qi       \n##  2 cargo yes     3ft - 2ft   0.102   0.0557  0.151    0.95 median qi       \n##  3 eng   no      elec - gas -0.287  -0.397  -0.142    0.95 median qi       \n##  4 eng   no      hyb - gas  -0.160  -0.209  -0.107    0.95 median qi       \n##  5 eng   yes     elec - gas -0.271  -0.388  -0.120    0.95 median qi       \n##  6 eng   yes     hyb - gas  -0.166  -0.223  -0.104    0.95 median qi       \n##  7 price no      35 - 30    -0.167  -0.225  -0.109    0.95 median qi       \n##  8 price no      40 - 30    -0.295  -0.355  -0.231    0.95 median qi       \n##  9 price yes     35 - 30    -0.203  -0.270  -0.134    0.95 median qi       \n## 10 price yes     40 - 30    -0.342  -0.410  -0.272    0.95 median qi       \n## 11 seat  no      7 - 6      -0.163  -0.232  -0.0887   0.95 median qi       \n## 12 seat  no      8 - 6      -0.121  -0.217  -0.0108   0.95 median qi       \n## 13 seat  yes     7 - 6       0.0475 -0.0254  0.127    0.95 median qi       \n## 14 seat  yes     8 - 6       0.0909 -0.0108  0.204    0.95 median qi\n\nAnd finally, here’s a polisci-style plot of all these AMCEs, which is so so neat. An individual’s carpooling behavior interacts with seat count (increasing the seat count causes carpoolers to select the minivan more often), and it also interacts a bit with cargo space (increasing the cargo space makes both types of individuals more likely to select the minivan, but moreso for carpoolers) and also with price (increasing the price makes both types of individuals less likely to select the minivan, but moreso for carpoolers). Switching from gas → hybrid and gas → electric has a negative effect on both types of consumers, and there’s no carpooling-based difference.\n\nExtract variable labelsminivan_var_levels &lt;- tibble(\n  variable = c(\"seat\", \"cargo\", \"eng\", \"price\")\n) %&gt;% \n  mutate(levels = map(variable, ~{\n    x &lt;- minivans[[.x]]\n    if (is.numeric(x)) {\n      \"\"\n    } else if (is.factor(x)) {\n      levels(x)\n    } else {\n      sort(unique(x))\n    }\n  })) %&gt;% \n  unnest(levels) %&gt;% \n  mutate(term = paste0(variable, levels))\n\n# Make a little lookup table for nicer feature labels\nminivan_var_lookup &lt;- tribble(\n  ~variable, ~variable_nice,\n  \"seat\",    \"Passengers\",\n  \"cargo\",   \"Cargo space\",\n  \"eng\",     \"Engine type\",\n  \"price\",   \"Price (thousands of $)\"\n) %&gt;% \n  mutate(variable_nice = fct_inorder(variable_nice))\n\n\n\nCombine full dataset of factor levels with marginalized posterior draws and make plotposterior_amces_minivan_carpool_nested &lt;- amces_minivan_carpool %&gt;% \n  separate_wider_delim(\n    contrast,\n    delim = \" - \", \n    names = c(\"variable_level\", \"reference_level\")\n  ) %&gt;% \n  group_by(term, variable_level) %&gt;% \n  nest()\n\nplot_data_minivan_carpool &lt;- minivan_var_levels %&gt;%\n  left_join(\n    posterior_amces_minivan_carpool_nested,\n    by = join_by(variable == term, levels == variable_level)\n  ) %&gt;%\n  mutate(data = map_if(data, is.null, ~ tibble(avg = 0))) %&gt;% \n  unnest(data) %&gt;% \n  left_join(minivan_var_lookup, by = join_by(variable)) %&gt;% \n  mutate(across(c(levels, variable_nice), ~fct_inorder(.))) %&gt;% \n  # Make the missing carpool values be \"yes\" for the reference category\n  mutate(carpool = replace_na(carpool, \"yes\"))\n\nplot_data_minivan_carpool %&gt;% \n  ggplot(aes(x = avg, y = levels, fill = variable_nice)) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(aes(slab_alpha = carpool), normalize = \"groups\") + \n  facet_col(facets = \"variable_nice\", scales = \"free_y\", space = \"free\") +\n  scale_x_continuous(labels = label_pp) +\n  scale_slab_alpha_discrete(\n    range = c(0.4, 1),\n    labels = c(\"Non-carpoolers\", \"Carpoolers\"),\n    guide = guide_legend(\n      reverse = TRUE, override.aes = list(fill = \"grey10\"), \n      keywidth = 0.8, keyheight = 0.8\n    )\n  ) +\n  scale_fill_manual(values = clrs[c(3, 7, 8, 9)], guide = \"none\") +\n  labs(\n    x = \"Percentage point change in probability of minivan selection\",\n    y = NULL,\n    title = \"AMCEs across respondent carpool status\",\n    slab_alpha = NULL\n  ) +\n  theme(\n    legend.position = \"top\",\n    legend.justification = \"left\",\n    legend.margin = margin(l = -7, t = 0)\n  )"
  },
  {
    "objectID": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html",
    "href": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html",
    "title": "Guide to understanding the intuition behind the Dirichlet distribution",
    "section": "",
    "text": "I’ve been finishing up a project that uses ordered Beta regression (Kubinec 2022), a neat combination of Beta regression and ordered logistic regression that you can use for modeling continuous outcomes that are bounded on either side (in my project, we’re modeling a variable that can only be between 1 and 32, for instance). It’s possible to use something like zero-one-inflated Beta regression for outcomes like this, but that kind of model requires a lot more complexity and computing power (i.e. you need separate simultaneous models to predict if the outcome is (a) zero-or-one vs. not, if it is (b) zero vs. one, and if it is (c) somewhere in between). The magic of ordered Beta regression is that it handles the extremes (i.e. exactly zero and exactly one) using ordered-logit-style cutpoints, so you only have to specify one model + parameters for where those cutpoints fall. See here for a quick overview of it all—it’s a really neat kind of model.\nWhile writing up the formal math for the model, digging through the ordbetareg() documentation and reading Kubinec (2022)’s paper, I came across a prior distribution that I’ve always been afraid of and have never really fully understood: the Dirichlet distribution.\nBy default, ordbetareg() uses a \\(\\operatorname{Dirichlet}(1, 1, 1)\\) prior distribution for the cutpoints or probabilities/proportions for its three submodels (i.e. exactly 0, somewhere between 0 and 1, and exactly 1).\nBut I have no clue what this distribution looks or feels like or even how it works! Wikipedia has a long and detailed page about it, but it’s exceptionally mathy and beyond my math skills.\nSo in this post, my goal is to explore the Dirichlet distribution and wrap my head around how it works as mathlessly as possible. This is all focused on intuition.\nBut first, we need to load some libraries and make some helper functions:\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(GGally)\nlibrary(ggtern)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(tidybayes)\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Manrope\ntheme_nice &lt;- function() {\n  theme_minimal(base_family = \"Manrope\") +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(family = \"Manrope Extrabold\", face = \"plain\", size = rel(1.35)),\n      plot.subtitle = element_text(family = \"Manrope Medium\", face = \"plain\", size = rel(1.2)),\n      axis.title = element_text(family = \"Manrope SemiBold\", face = \"plain\", size = rel(1)),\n      axis.title.x = element_text(hjust = 0),\n      axis.title.y = element_text(hjust = 1),\n      axis.text = element_text(family = \"Manrope Light\", face = \"plain\", size = rel(0.8)),\n      strip.text = element_text(\n        family = \"Manrope\", face = \"bold\",\n        size = rel(1), hjust = 0\n      ),\n      strip.background = element_rect(fill = \"grey90\", color = NA)\n    )\n}\n\ntheme_nice_dist &lt;- function() {\n  theme_nice() +\n    theme(\n      panel.grid = element_blank(),\n      panel.spacing.x = unit(10, units = \"pt\"),\n      axis.ticks.x = element_line(linewidth = 0.25),\n      axis.text.y = element_blank()\n    )\n}\n\ntheme_set(theme_nice())\n\nggplot2::update_geom_defaults(\"label\", list(family = \"Manrope SemiBold\", fontface = \"plain\"))\nggplot2::update_geom_defaults(\"text\", list(family = \"Manrope SemiBold\", fontface = \"plain\"))\nMCRN colors, via Reddit\nTo help with the intuition behind distributions, I’ll talk about “gravities” of correlated variables throughout this post. Accordingly we’ll use a color scheme from the book series/TV show The Expanse, where gravity is a central element (and accurately depicted!). We’ll use the official colors from the Martian Congressional Republic Navy (MCRN) since they look neat.\nclrs &lt;- c(\n  \"#FFBE00\",  # MCRN yellow\n  \"#B92F0A\",  # MCRN red\n  \"#792A26\",  # MCRN maroon\n  \"#54191B\",  # MCRN brown\n  \"#242424\",  # MCRN dark gray\n  \"#2660ae\"   # Blue from MCR flag\n)"
  },
  {
    "objectID": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html#one-beta-distribution-with-two-shapes",
    "href": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html#one-beta-distribution-with-two-shapes",
    "title": "Guide to understanding the intuition behind the Dirichlet distribution",
    "section": "One Beta distribution with two shapes",
    "text": "One Beta distribution with two shapes\nTo understand these parameters, we need to quickly talk about the Beta distribution’s shape parameters. The Beta distribution uses two parameters \\(\\alpha\\) and \\(\\beta\\) (or shape1 and shape2 in R’s d/p/q/rbeta() functions) to define the shape of a distribution that is bounded between 0 and 1. (See here for a longer description of Beta parameters, including how to use a scale/location parameterization instead of these shapes.)\nThese shape parameters are used to create a ratio that defines the mean or central gravity of the distribution, like so:\n\\[\n\\frac{\\alpha}{\\alpha + \\beta} \\quad \\text{or} \\quad \\frac{\\texttt{shape1}}{\\texttt{shape1} + \\texttt{shape2}}\n\\]\nTo quickly illustrate, if \\(\\alpha\\) (or shape1) is 3 and \\(\\beta\\) (or shape2) is 7, the distribution would have a mean of 0.3:\n\\[\n\\frac{3}{3 + 7} = \\frac{3}{10} = 0.3\n\\]\nWe can confirm this with a graph—most of the values are asymmetrically clustered around 0.3:\n\nggplot() +\n  stat_function(\n    geom = \"area\", \n    fun = \\(x) dbeta(x, shape1 = 3, shape2 = 7), \n    n = 1000, fill = clrs[1]) +\n  labs(x = \"Probability or proportion\", y = NULL, title = \"Beta(3, 7)\") +\n  theme_nice_dist()"
  },
  {
    "objectID": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html#lots-of-correlated-beta-distributions-with-lots-of-shapes",
    "href": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html#lots-of-correlated-beta-distributions-with-lots-of-shapes",
    "title": "Guide to understanding the intuition behind the Dirichlet distribution",
    "section": "Lots of correlated Beta distributions with lots of shapes",
    "text": "Lots of correlated Beta distributions with lots of shapes\nThe Dirichlet distribution is just like the Beta distribution, but for multiple variables at the same time. Its parameters are also called “shapes,” just like the regular Beta family parameters, but instead of specifying just shape 1 and shape 2, we use a vector of shapes called \\(\\alpha\\).\nTo illustrate, we’ll first generate some random numbers from a Dirichlet distribution with \\(\\alpha = (3, 7)\\). We get a matrix with two columns, one for each variable in the distribution. Notice how the first variable is small, roughly around 0.3, while the second column is larger around 0.7. Notice also how each row sums to 1.\n\nwithr::with_seed(1234, {\n  brms::rdirichlet(n = 10, alpha = c(3, 7)) |&gt; \n    data.frame() |&gt; \n    set_names(1:2) |&gt; \n    mutate(total = `1` + `2`)\n})\n##          1      2 total\n## 1  0.12540 0.8746     1\n## 2  0.24772 0.7523     1\n## 3  0.33258 0.6674     1\n## 4  0.18422 0.8158     1\n## 5  0.28542 0.7146     1\n## 6  0.25251 0.7475     1\n## 7  0.38559 0.6144     1\n## 8  0.23298 0.7670     1\n## 9  0.21676 0.7832     1\n## 10 0.07216 0.9278     1\n\nThe reason that’s the case is because Dirichlet is just a fancy multivariate Beta distribution. The shape parameters work the same way. And in the case of a 2-parameter Dirichlet distribution, it is identical to a regular old Beta distribution. We just get two forms of it—the Beta distribution and its inverse:\n\\[\n\\operatorname{Dirichlet}(3, 7) = \\Bigl[\\operatorname{Beta}(3, 7),\\ \\operatorname{Beta}(7, 3)\\Bigr]\n\\]\nThe more general way of thinking about these shapes now is to use something like this formula—the mean for each variable is its value in the \\(\\alpha\\) vector divided by the sum of all the values in the the \\(\\alpha\\) vector:\n\\[\n\\textbf{E}(\\alpha_n) = \\frac{\\alpha_n}{\\sum{\\alpha}}\n\\]\nHere’s what that looks like with the \\(\\operatorname{Dirichlet}(3, 7)\\) distribution:\n\\[\n\\begin{align}\n\\textbf{E}(\\alpha_1) &= \\frac{\\alpha_1}{\\sum{\\alpha}} = \\frac{3}{3 + 7} = \\frac{3}{10} = 0.3 \\\\[8pt]\n\\textbf{E}(\\alpha_2) &= \\frac{\\alpha_2}{\\sum{\\alpha}} = \\frac{7}{3 + 7} = \\frac{7}{10} = 0.7\n\\end{align}\n\\]\nWe can confirm it with a graph too (with rdirichlet() for now instead of ddirichlet() because it’s a little weird to work with). The first variable is the same as \\(\\operatorname{Beta}(3, 7)\\) and has an average of 0.3, while the second column is the same as \\(\\operatorname{Beta}(7, 3)\\) with an average of 0.7.\n\nwithr::with_seed(1234, {\n  brms::rdirichlet(n = 1e5, alpha = c(3, 7)) |&gt; \n    data.frame() |&gt; \n    set_names(paste0(\"α&lt;sub&gt;\", 1:2, \"&lt;/sub&gt;\")) |&gt; \n    pivot_longer(everything()) |&gt; \n    ggplot(aes(x = value, fill = name)) +\n    geom_density(bounds = c(0, 1), color = NA) +\n    scale_fill_manual(values = clrs[c(1, 2)], guide = \"none\") +\n    labs(x = \"Probability or proportion\", y = NULL, title = \"Dirichlet(3, 7)\") +\n    facet_wrap(vars(name)) +\n    theme_nice_dist() +\n    theme(strip.text = element_markdown())\n})\n\n\n\n\n\n\n\nWith the general intuition that Dirichlet is just fancy Beta, watch what happens as we increase the number of elements in \\(\\alpha\\):\n\n# Three columns\nwithr::with_seed(1234, {\n  brms::rdirichlet(n = 3, alpha = c(3, 7, 2)) |&gt; \n    data.frame() |&gt; \n    set_names(1:3) |&gt; \n    mutate(total = `1` + `2` + `3`)\n})\n##        1      2       3 total\n## 1 0.1102 0.5501 0.33970     1\n## 2 0.2580 0.6707 0.07126     1\n## 3 0.3374 0.5568 0.10582     1\n\n# Six columns\nwithr::with_seed(1234, {\n  brms::rdirichlet(n = 3, alpha = c(3, 7, 2, 2, 9, 1)) |&gt; \n    data.frame() |&gt; \n    set_names(1:6) |&gt; \n    mutate(total = `1` + `2` + `3` + `4` + `5` + `6`)\n})\n##         1      2       3        4      5        6 total\n## 1 0.05581 0.2786 0.17205 0.007128 0.4778 0.008593     1\n## 2 0.13464 0.3500 0.03718 0.070399 0.3811 0.026703     1\n## 3 0.13486 0.2226 0.04230 0.126377 0.4646 0.009275     1\n\nThe values in these columns—both with the three-element \\(\\alpha = (3, 7, 2)\\) and with the six-element \\(\\alpha = (3, 7, 2, 2, 9, 1)\\)—all sum to 1.\nThe same \\(\\alpha_n / \\sum{\\alpha}\\) logic also applies here for determining the mean for each of these columns. We just need to work with more than two shapes. For \\(\\alpha = (3, 7, 2)\\), here’s what the means should be:\n\\[\n\\begin{align}\n\\textbf{E}(\\alpha_1) &= \\frac{\\alpha_1}{\\sum{\\alpha}} = \\frac{3}{3 + 7 + 2} = \\frac{3}{12} = 0.25 \\\\[8pt]\n\\textbf{E}(\\alpha_2) &= \\frac{\\alpha_2}{\\sum{\\alpha}} = \\frac{7}{3 + 7 + 2} = \\frac{7}{12} = 0.5833 \\\\[8pt]\n\\textbf{E}(\\alpha_3) &= \\frac{\\alpha_3}{\\sum{\\alpha}} = \\frac{2}{3 + 7 + 2} = \\frac{2}{12} = 0.1667\n\\end{align}\n\\]\nWe can confirm this with code too:\n\nwithr::with_seed(1234, {\n  brms::rdirichlet(n = 1e5, alpha = c(3, 7, 2)) |&gt; \n    data.frame() |&gt; \n    set_names(1:3) |&gt; \n    summarize(across(everything(), ~ mean(.x)))\n})\n##        1      2      3\n## 1 0.2495 0.5837 0.1667\n\nNeat!\nFor fun, we can plot these three distributions too:\n\nplot_dirichlet_3_7_2 &lt;- withr::with_seed(1234, {\n  brms::rdirichlet(n = 1e5, alpha = c(3, 7, 2)) |&gt; \n    data.frame() |&gt; \n    set_names(paste0(\"α&lt;sub&gt;\", 1:3, \"&lt;/sub&gt;\")) |&gt; \n    pivot_longer(everything()) |&gt; \n    ggplot(aes(x = value, fill = name)) +\n    geom_density(bounds = c(0, 1), color = NA) +\n    scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n    scale_fill_manual(values = clrs[c(1, 2, 4)], guide = \"none\") +\n    labs(x = \"Probability or proportion\", y = NULL, title = \"Dirichlet(3, 7, 2)\") +\n    facet_wrap(vars(name), scales = \"free_y\") +\n    theme_nice_dist() +\n    theme(strip.text = element_markdown())\n})\nplot_dirichlet_3_7_2\n\n\n\n\n\n\n\nAnd just to confirm for sure, here are the individual Beta-based parameterizations of each column:\n\nCode for combining separate dbeta()-based plots with the Dirichlet componentsp1 &lt;- ggplot() +\n  stat_function(\n    geom = \"area\", fun = \\(x) dbeta(x, 3, (7 + 2)), n = 1000,\n    fill = clrs[1]\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(x = \"Probability or proportion\", y = NULL, subtitle = \"Columns as Beta distributions\") +\n  facet_wrap(vars(\"α&lt;sub&gt;1&lt;/sub&gt;: Beta(3, 9 &lt;span style='font-size:7pt'&gt;(7 + 2)&lt;/span&gt;)\")) +\n  theme_nice_dist() +\n  theme(strip.text = element_markdown())\n\np2 &lt;- ggplot() +\n  stat_function(\n    geom = \"area\", fun = \\(x) dbeta(x, 7, (3 + 2)), n = 1000,\n    fill = clrs[2]\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(vars(\"α&lt;sub&gt;2&lt;/sub&gt;: Beta(7, 5 &lt;span style='font-size:7pt'&gt;(3 + 2)&lt;/span&gt;)\")) +\n  theme_nice_dist() +\n  theme(strip.text = element_markdown())\n\np3 &lt;- ggplot() +\n  stat_function(\n    geom = \"area\", fun = \\(x) dbeta(x, 2, (3 + 7)), n = 1000,\n    fill = clrs[4]\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(vars(\"α&lt;sub&gt;3&lt;/sub&gt;: Beta(2, 10 &lt;span style='font-size:7pt'&gt;(3 + 7)&lt;/span&gt;)\")) +\n  theme_nice_dist() +\n  theme(strip.text = element_markdown())\n\n(plot_dirichlet_3_7_2 + labs(x = NULL, subtitle = \"Distribution of 10,000 random draws\")) / \n  (p1 | p2 | p3)\n\n\n\n\n\n\n\nAnd that’s basically it! Dirichlet distributions are just fancy multivariate Beta distributions with multiple shapes and multiple columns"
  },
  {
    "objectID": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html#one-column-with-a-strong-pull",
    "href": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html#one-column-with-a-strong-pull",
    "title": "Guide to understanding the intuition behind the Dirichlet distribution",
    "section": "One column with a strong pull",
    "text": "One column with a strong pull\nFor this example, we’ll work with a distribution with one large shape value:\n\\[\n\\operatorname{Dirichlet}(5, 1, 14)\n\\]\nBefore generating any data or visualizing this distribution, let’s figure out the averages first to help with the intuition. The distribution has 3 \\(\\alpha\\) parameters, so it’ll create three different variables with probabilities that sum to one. Since Dirichlet distributions are just fancy Beta distributions, we can find the means (or central gravities) for each of the variables by combining the shapes with \\(\\alpha_n / \\sum \\alpha\\):\n\\[\n\\begin{align}\n\\textbf{E}(\\alpha_1) &= \\frac{\\alpha_1}{\\sum{\\alpha}} = \\frac{5}{5 + 1 + 14} = \\frac{5}{20} = 0.25 \\\\[8pt]\n\\textbf{E}(\\alpha_2) &= \\frac{\\alpha_2}{\\sum{\\alpha}} = \\frac{1}{5 + 1 + 14} = \\frac{1}{20} = 0.05 \\\\[8pt]\n\\textbf{E}(\\alpha_3) &= \\frac{\\alpha_3}{\\sum{\\alpha}} = \\frac{14}{5 + 1 + 14} = \\frac{14}{20} = 0.7\n\\end{align}\n\\]\nThe first column should be around 0.25, the second around 0.05, and the third around 0.7, and the third column should have the strongest gravity of the three. Let’s see if that’s the case. Here are 10 random rows from \\(\\operatorname{Dirichlet}(5, 1, 14)\\)—they seem to generally follow the expected pattern of low-medium, small, and big values:\n\nwithr::with_seed(1234, {\n  brms::rdirichlet(n = 10, alpha = c(5, 1, 14)) |&gt; \n    data.frame() |&gt; \n    set_names(1:3)\n})\n##          1        2      3\n## 1  0.19939 0.047312 0.7533\n## 2  0.25775 0.069934 0.6723\n## 3  0.27495 0.022074 0.7030\n## 4  0.15651 0.026037 0.8175\n## 5  0.23192 0.046199 0.7219\n## 6  0.11164 0.004884 0.8835\n## 7  0.43754 0.038405 0.5241\n## 8  0.14935 0.009720 0.8409\n## 9  0.17226 0.024471 0.8033\n## 10 0.09886 0.030168 0.8710\n\nIf we generate a bunch of rows and find the column averages, we’ll get the expected averages that we calculated by hand earlier: 0.25, 0.05, and 0.7:\n\nwithr::with_seed(1234, {\n  brms::rdirichlet(n = 1e5, alpha = c(5, 1, 14)) |&gt; \n    data.frame() |&gt; \n    set_names(1:3) |&gt; \n    summarize(across(everything(), ~ mean(.x)))\n})\n##      1      2      3\n## 1 0.25 0.0501 0.6999\n\nFor fun, we can plot these three individual distributions and their Beta equivalents:\n\nCode for showing the rdirichlet() and dbeta() plots simultaneouslyplot_dirichlet_5_1_14 &lt;- withr::with_seed(1234, {\n  brms::rdirichlet(n = 1e5, alpha = c(5, 1, 14)) |&gt;\n    data.frame() |&gt;\n    set_names(paste0(\"α&lt;sub&gt;\", 1:3, \"&lt;/sub&gt;\")) |&gt;\n    pivot_longer(everything()) |&gt;\n    ggplot(aes(x = value, fill = name)) +\n    geom_density(bounds = c(0, 1), color = NA) +\n    scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n    scale_fill_manual(values = clrs[c(1, 2, 4)], guide = \"none\") +\n    labs(\n      x = NULL, y = NULL, title = \"Dirichlet(5, 1, 14)\",\n      subtitle = \"Columns as Beta distributions\"\n    ) +\n    facet_wrap(vars(name), scales = \"free_y\") +\n    theme_nice_dist() +\n    theme(strip.text = element_markdown())\n})\n\np1 &lt;- ggplot() +\n  stat_function(\n    geom = \"area\", fun = \\(x) dbeta(x, 5, (1 + 14)), n = 1000,\n    fill = clrs[1]\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(\n    x = \"Probability or proportion\", y = NULL,\n    subtitle = \"Columns as Beta distributions\"\n  ) +\n  facet_wrap(vars(\"α&lt;sub&gt;1&lt;/sub&gt;: Beta(5, 15 &lt;span style='font-size:7pt'&gt;(1 + 14)&lt;/span&gt;)\")) +\n  theme_nice_dist() +\n  theme(strip.text = element_markdown())\n\np2 &lt;- ggplot() +\n  stat_function(\n    geom = \"area\", fun = \\(x) dbeta(x, 1, (5 + 14)), n = 1000,\n    fill = clrs[2]\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(vars(\"α&lt;sub&gt;2&lt;/sub&gt;: Beta(1, 19 &lt;span style='font-size:7pt'&gt;(5 + 14)&lt;/span&gt;)\")) +\n  theme_nice_dist() +\n  theme(strip.text = element_markdown())\n\np3 &lt;- ggplot() +\n  stat_function(\n    geom = \"area\", fun = \\(x) dbeta(x, 14, (5 + 1)), n = 1000,\n    fill = clrs[4]\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(vars(\"Beta(14, [5+1])\")) +\n  facet_wrap(vars(\"α&lt;sub&gt;3&lt;/sub&gt;: Beta(14, 6 &lt;span style='font-size:7pt'&gt;(5 + 1)&lt;/span&gt;)\")) +\n  theme_nice_dist() +\n  theme(strip.text = element_markdown())\n\nplot_dirichlet_5_1_14 / \n  (p1 | p2 | p3)\n\n\n\n\n\n\n\nThis shows us that \\(\\alpha_3\\) has the highest probability of having big values, but it doesn’t show much about the relationships between the three columns. A scatterplot matrix or ternary plot can help with this. Because \\(\\alpha_3\\) values are large so often, \\(\\alpha_1\\) and \\(\\alpha_2\\) necessarily need to be small. Notice how all the points are clustered very strongly in the bottom right corner of the triangle—it is very rare to encounter any low values of \\(\\alpha_3\\), so the other two columns are always tiny.\n\nCode for creating these ternary plots with {ggtern}# First triangle: random points\nwithr::with_seed(1234, {\n  draws_5_1_14 &lt;- brms::rdirichlet(n = 1e5, alpha = c(5, 1, 14)) |&gt; \n    data.frame() |&gt; \n    set_names(c(\"x\", \"y\", \"z\"))\n})\n\ntern1 &lt;- draws_5_1_14 |&gt; \n  ggtern(aes(x = x, y = y, z = z)) +\n  geom_point(size = 0.2, alpha = 0.1, color = clrs[5]) +\n  scale_L_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;1&lt;/sub&gt;\") +\n  scale_T_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;2&lt;/sub&gt;\") +\n  scale_R_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;3&lt;/sub&gt;\") +\n  theme(\n    tern.axis.title.L = element_markdown(face = \"bold\", color = clrs[1], size = rel(1.2)),\n    tern.axis.title.T = element_markdown(face = \"bold\", color = clrs[2], size = rel(1.2)),\n    tern.axis.title.R = element_markdown(face = \"bold\", color = clrs[4], size = rel(1.2))\n  )\n\n# Second triangle: actual densities\n# Create a sequence of values for x, y, and z\nvalues &lt;- seq(0, 1, by = 0.005)\n\n# Generate all possible combinations of x, y, and z that sum to 1\ndf &lt;- expand.grid(x = values, y = values, z = values) |&gt;\n  filter(x + y + z == 1) |&gt; \n  rowwise() |&gt;\n  mutate(density = pmap_dbl(\n    list(x, y, z),\n    ~ brms::ddirichlet(\n      as.numeric(c(..1, ..2, ..3)),\n      alpha = c(5, 1, 14)\n    )\n  )) |&gt; \n  filter(!is.nan(density))\n\ntern2 &lt;- ggtern(data = df, aes(x = x, y = y, z = z)) +\n  geom_point(aes(color = density)) +\n  scale_color_gradientn(\n    colors = clrs[5:1], \n    values = scales::rescale(x = c(0, 1, 20, 25, 70), from = c(0, 70)),\n    guide = \"none\"\n  ) +\n  scale_L_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;1&lt;/sub&gt;\") +\n  scale_T_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;2&lt;/sub&gt;\") +\n  scale_R_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;3&lt;/sub&gt;\") +\n  theme(\n    tern.axis.title.L = element_markdown(face = \"bold\", color = clrs[1], size = rel(1.2)),\n    tern.axis.title.T = element_markdown(face = \"bold\", color = clrs[2], size = rel(1.2)),\n    tern.axis.title.R = element_markdown(face = \"bold\", color = clrs[4], size = rel(1.2))\n  )\n\nggtern::grid.arrange(tern1, tern2, ncol = 2)"
  },
  {
    "objectID": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html#uniform-distribution-across-columns",
    "href": "blog/2023/09/18/understanding-dirichlet-beta-intuition/index.html#uniform-distribution-across-columns",
    "title": "Guide to understanding the intuition behind the Dirichlet distribution",
    "section": "Uniform distribution across columns",
    "text": "Uniform distribution across columns\nWhat happens if all the \\(\\alpha\\) values are 1, like this?\n\\[\n\\operatorname{Dirichlet}(1, 1, 1)\n\\]\nWhat kind of distributions will these three columns have, and how will they be related to each other?\nAgain, for the sake of illustration, we’ll first manually find each column’s average by using the \\(\\alpha_n / \\sum \\alpha\\) approach:\n\\[\n\\begin{align}\n\\textbf{E}(\\alpha_1) &= \\frac{\\alpha_1}{\\sum{\\alpha}} = \\frac{1}{1 + 1 + 1} = \\frac{1}{3} = 0.333 \\\\[8pt]\n\\textbf{E}(\\alpha_2) &= \\frac{\\alpha_2}{\\sum{\\alpha}} = \\frac{1}{1 + 1 + 1} = \\frac{1}{3} = 0.333 \\\\[8pt]\n\\textbf{E}(\\alpha_3) &= \\frac{\\alpha_3}{\\sum{\\alpha}} = \\frac{1}{1 + 1 + 1} = \\frac{1}{3} = 0.333\n\\end{align}\n\\]\nEach column has an equally-likely probability, and since there are 3 columns, each column has an average/central gravity of 0.33. But overall, this is actually a uniform Dirichlet distribution. Check out these 10 random rows from \\(\\operatorname{Dirichlet}(1, 1, 1)\\)—they’re all over the place! Some are nearly 0%, some are 95%, some are 30%, some are 70%. None of the columns really have any specific gravity, so their values are free to be whatever.\n\nwithr::with_seed(1234, {\n  brms::rdirichlet(n = 10, alpha = c(1, 1, 1)) |&gt; \n    data.frame() |&gt; \n    set_names(1:3)\n})\n##           1       2       3\n## 1  0.005967 0.03518 0.95885\n## 2  0.262315 0.05167 0.68601\n## 3  0.537214 0.40944 0.05334\n## 4  0.215604 0.39335 0.39105\n## 5  0.564366 0.30827 0.12736\n## 6  0.064393 0.17315 0.76245\n## 7  0.387843 0.47867 0.13349\n## 8  0.077467 0.54107 0.38146\n## 9  0.105293 0.71618 0.17852\n## 10 0.326226 0.49972 0.17405\n\nOnce again, we can verify this with column averages:\n\nwithr::with_seed(1234, {\n  brms::rdirichlet(n = 1e5, alpha = c(1, 1, 1)) |&gt; \n    data.frame() |&gt; \n    set_names(1:3) |&gt; \n    summarize(across(everything(), ~ mean(.x)))\n})\n##        1     2      3\n## 1 0.3339 0.332 0.3341\n\nWhen I think of a uniform distribution, however, I think of something flat like runif() or \\(\\operatorname{Beta}(1, 1)\\) where all values of x are equally likely:\n\nwithr::with_seed(1234, {\n  p1 &lt;- tibble(x = runif(10000, min = 0, max = 100)) |&gt; \n    ggplot(aes(x = x)) +\n    geom_histogram(binwidth = 10, boundary = 0, color = \"white\", fill = clrs[3]) +\n    labs(y = NULL, x = NULL) +\n    facet_wrap(vars(\"Uniform(0, 100)\")) +\n    theme_nice_dist()\n  \n  p2 &lt;- tibble(x = rbeta(10000, shape1 = 1, shape2 = 1)) |&gt; \n    ggplot(aes(x = x)) +\n    geom_histogram(binwidth = 0.1, boundary = 0, color = \"white\", fill = clrs[6]) +\n    labs(y = NULL, x = NULL) +\n    facet_wrap(vars(\"Beta(1, 1)\")) +\n    theme_nice_dist()\n  \n  p1 | p2\n})\n\n\n\n\n\n\n\nThe individual components of a uniform Dirichlet distribution, however, don’t look anything like this! Instead, they’re triangles:\n\nCode for showing the rdirichlet() and dbeta() plots simultaneouslyplot_dirichlet_1_1_1 &lt;- withr::with_seed(1234, {\n  brms::rdirichlet(n = 10000, alpha = c(1, 1, 1)) |&gt; \n    data.frame() |&gt; \n    set_names(paste0(\"α&lt;sub&gt;\", 1:3, \"&lt;/sub&gt;\")) |&gt; \n    pivot_longer(everything()) |&gt; \n    ggplot(aes(x = value, fill = name)) +\n    geom_density(bounds = c(0, 1), color = NA) +\n    scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n    scale_fill_manual(values = clrs[c(1, 2, 4)], guide = \"none\") +\n    labs(\n      x = NULL, y = NULL, \n      title = \"Dirichlet(1, 1, 1)\",\n      subtitle = \"Distribution of 10,000 random draws\") +\n    facet_wrap(vars(name), scales = \"free_y\") +\n    theme_nice_dist() +\n    theme(strip.text = element_markdown())\n})\n\np1 &lt;- ggplot() +\n  stat_function(\n    geom = \"area\", fun = \\(x) dbeta(x, 1, (1 + 1)), n = 1000,\n    fill = clrs[1]\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(\n    x = \"Probability or proportion\", y = NULL, \n    subtitle = \"Columns as Beta distributions\"\n  ) +\n  facet_wrap(vars(\"α&lt;sub&gt;1&lt;/sub&gt;: Beta(1, 2 &lt;span style='font-size:7pt'&gt;(1 + 1)&lt;/span&gt;)\")) +\n  theme_nice_dist() +\n  theme(strip.text = element_markdown())\n\np2 &lt;- ggplot() +\n  stat_function(\n    geom = \"area\", fun = \\(x) dbeta(x, 1, (1 + 1)), n = 1000,\n    fill = clrs[2]\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(vars(\"α&lt;sub&gt;2&lt;/sub&gt;: Beta(1, 2 &lt;span style='font-size:7pt'&gt;(1 + 1)&lt;/span&gt;)\")) +\n  theme_nice_dist() +\n  theme(strip.text = element_markdown())\n\np3 &lt;- ggplot() +\n  stat_function(\n    geom = \"area\", fun = \\(x) dbeta(x, 1, (1 + 1)), n = 1000,\n    fill = clrs[4]\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(x = NULL, y = NULL) +\n  facet_wrap(vars(\"α&lt;sub&gt;3&lt;/sub&gt;: Beta(1, 2 &lt;span style='font-size:7pt'&gt;(1 + 1)&lt;/span&gt;)\")) +\n  theme_nice_dist() +\n  theme(strip.text = element_markdown())\n\nplot_dirichlet_1_1_1 / \n  (p1 | p2 | p3)\n\n\n\n\n\n\n\nThis is because the three columns are linked together. If all three are perfectly equally likely, they’ll have a probability of 33%. If one is a little higher than 33%, the other two will need to be smaller than 33% to fit in the 100% constraint. If one is really big, the other two will need to be small (e.g. if one is 70%, the other two combined need to be 30%).\nThe flat shape that you’d expect from a uniform distribution is actually visible in a ternary plot where we can look at the joint distribution of all three variables at the same time. There’s no gravity at all here—each point is equally likely. Notice how all the rdirichlet() points are scattered evenly throughout the left triangle, and the ddirichlet() density gradient in the right triangle is a single color, representing one constant value. It truly is a uniform distribution.\n\nCode for creating these ternary plots with {ggtern}# First triangle: random points\nwithr::with_seed(1234, {\n  draws_1_1_1 &lt;- brms::rdirichlet(n = 1e5, alpha = c(1, 1, 1)) |&gt; \n    data.frame() |&gt; \n    set_names(c(\"x\", \"y\", \"z\"))\n})\n\ntern1 &lt;- draws_1_1_1 |&gt; \n  ggtern(aes(x = x, y = y, z = z)) +\n  geom_point(size = 0.05, alpha = 0.05, color = clrs[5]) +\n  scale_L_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;1&lt;/sub&gt;\") +\n  scale_T_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;2&lt;/sub&gt;\") +\n  scale_R_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;3&lt;/sub&gt;\") +\n  theme(\n    tern.axis.title.L = element_markdown(face = \"bold\", color = clrs[1], size = rel(1.2)),\n    tern.axis.title.T = element_markdown(face = \"bold\", color = clrs[2], size = rel(1.2)),\n    tern.axis.title.R = element_markdown(face = \"bold\", color = clrs[4], size = rel(1.2))\n  )\n\n# Second triangle: actual densities\n# Create a sequence of values for x, y, and z\nvalues &lt;- seq(0, 1, by = 0.005)\n\n# Generate all possible combinations of x, y, and z that sum to 1\ndf &lt;- expand.grid(x = values, y = values, z = values) |&gt;\n  filter(x + y + z == 1) |&gt; \n  rowwise() |&gt;\n  mutate(density = pmap_dbl(\n    list(x, y, z),\n    ~ brms::ddirichlet(\n      as.numeric(c(..1, ..2, ..3)),\n      alpha = c(1, 1, 1)\n    )\n  )) |&gt; \n  filter(!is.nan(density))\n\ntern2 &lt;- ggtern(data = df, aes(x = x, y = y, z = z)) +\n  geom_point(aes(color = density)) +\n  scale_color_gradientn(colors = clrs[5:1], guide = \"none\") +\n  scale_L_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;1&lt;/sub&gt;\") +\n  scale_T_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;2&lt;/sub&gt;\") +\n  scale_R_continuous(breaks = 0:5 / 5, labels = 0:5 / 5, name = \"α&lt;sub&gt;3&lt;/sub&gt;\") +\n  theme(\n    tern.axis.title.L = element_markdown(face = \"bold\", color = clrs[1], size = rel(1.2)),\n    tern.axis.title.T = element_markdown(face = \"bold\", color = clrs[2], size = rel(1.2)),\n    tern.axis.title.R = element_markdown(face = \"bold\", color = clrs[4], size = rel(1.2))\n  )\n\nggtern::grid.arrange(tern1, tern2, ncol = 2)"
  },
  {
    "objectID": "blog/2024/01/12/diy-api-plumber-quarto-ojs/index.html",
    "href": "blog/2024/01/12/diy-api-plumber-quarto-ojs/index.html",
    "title": "DIY API with Make and {plumber}",
    "section": "",
    "text": "Complete tutorial and code\n\n\n\nSee the full tutorial here. You can also see the tutorial’s code here and the code for the final API here.\n\n\nFor years, I’ve tracked all sorts of data about myself (and my family) through Google Forms, Airtable, and devices like Fitbits to keep track of all sorts of things: personal goals, progress of research projects, current health status, books read, and so on.\nIt’s nice to have all this data, but it’s hard to use it all immediately. I often look at it at the end of the year, or every few months, or whatever, but having an instant snapshot is helpful too. That’s why people invented data dashboards, after all.\nI like R a lot, and R has the ability to make dashboards, like with {flexdashboard} and Shiny. I’ve made several dashboards for tracking things like health and reading and research—I even have a blog post about making one with {flexdashboard}!\nBut I’ve always run into issues with getting live data. With {flexdashboard}, you can make it grab the most recent version of the data you’re interested in when you knit the document, but then to update the graphs and tables in the document, you have to re-knit it. With Shiny, there are ways to dynamically grab the latest data, but then you have to run a whole Shiny server, and that’s hard and costs money and it’s slow—it can sometimes take a few minutes to reanimate a hibernating Shiny app!\nHowever, nowadays it’s possible to use Observable JS chunks in Quarto that automatically grab live data from the internet and display it, like this:\n\n\nShow the OJS code\n```{ojs}\n//| echo: fenced\n//| code-fold: true\n//| code-summary: \"Show the OJS code\"\nd3 = require('d3')\n\nviewof year_to_show = Inputs.radio([\"2023\", \"2024\"], {value: \"2023\", label: \"Year to show\"})\n\nbooks = await d3.json(\n  \"https://api.andrewheiss.com/books_simple?year=\" + year_to_show\n)\n\nbook_noun = (books.count[0] === 1 ? \" book read\" : \" books read\")\n\nPlot.plot({\n  title: books.count[0] + book_noun + \" in \" + year_to_show,\n  y: {\n    label: \"Books read\",\n    grid: false,\n    percent: false\n  },\n  x: {\n    label: \"Month\",\n    domain: books.monthly_count.map(d =&gt; d.read_month_fct),\n  },\n  marks: [\n    Plot.ruleY([0]),\n    Plot.axisX({label: null, ticks: null}),\n    Plot.axisY({label: null, ticks: null}),\n\n    Plot.barY(books.monthly_count, {\n      x: \"read_month_fct\", \n      y: \"count\", \n      fill: \"#f3752f\",\n      tip: {\n        format: {\n          x: true,\n          y: true\n        }\n      }\n    })\n  ]\n})\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now that Quarto supports dashboards, you can create entire Shiny-like dashboards that can load and display data without needing a Shiny server. Like this one!\nThe trickiest part of all this, though, is getting data from all around the internet (Google Sheets, Airtable databases, RSS feeds, Fitbit, etc.) into an easily accessible, clean, and usable format that you can feed into things like Observable plots or R.\nFortunately there’s a good (and really neat!) solution for this! You can use the {plumber} R package to create your own API that you can use to grab and clean data from all around the internet. And to simplify life, you can use other services like Make.com to deal with the hard work of regularly checking in on different parts of the internet (checking RSS feeds, reading Google/Airtable data, logging into services like Fitbit).\nIn the end, you can have a server like api.yourname.com and access JSON, CSV, or .rds data like api.yourname.com/books?start_date=2024-01-01. You can then use that in an R file, in a Python script, or in a Quarto document with Observable JS. It’s magical!\nTo explain and illustrate this whole process, I started out writing a blog post, but it got long and complex, so I wrote a literal book instead.\nAccess it here for a full tutorial.\n\n\n\n\nOverview of the process\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{heiss2024,\n  author = {Heiss, Andrew},\n  title = {DIY {API} with {Make} and \\{Plumber\\}},\n  date = {2024-01-12},\n  url = {https://www.andrewheiss.com/blog/2024/01/12/diy-api-plumber-quarto-ojs/},\n  doi = {10.59350/pe5s8-e0f47},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHeiss, Andrew. 2024. “DIY API with Make and {Plumber}.”\nJanuary 12, 2024. https://doi.org/10.59350/pe5s8-e0f47."
  },
  {
    "objectID": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html",
    "href": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html",
    "title": "Visualizing {dplyr}’s mutate(), summarize(), group_by(), and ungroup() with animations",
    "section": "",
    "text": "I’ve used Garrick Aden-Buie’s tidyexplain animations since he first made them in 2018. They’re incredibly useful for teaching—being able to see which rows left_join() includes when merging two datasets, or which cells end up where when pivoting longer or pivoting wider is so valuable. Check them all out—they’re so fantastic:\nOne set of animations that I’ve always wished existed but doesn’t is how {dplyr}’s mutate(), summarize(), group_by(), and summarize() work. Unlike other more straightforward {dplyr} functions like filter() and select(), these mutating/summarizing/grouping functions often involve multiple behind-the-scenes steps that are hard to see. There’s even an official term for this kind of workflow: split/apply/combine.\nWhen I teach about group_by() |&gt; summarize(), I end up waving my arms around a lot to explain how group_by() puts rows into smaller, invisible datasets behind the scenes. This works, I guess, but I still find that it can be hard for people to conceptualize. It gets even trickier when explaining how {dplyr} keeps some grouping structures intact after summarizing and what exactly ungroup() does.\nSo, I finally buckled down and made my own tidyexplain-esque animations with Adobe Illustrator and After Effects.1\nIn this post, we’ll use these animations to explain each of these concepts and apply them to data from {palmerpenguins}. Let’s load some packages and data first:\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins |&gt; drop_na()"
  },
  {
    "objectID": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#adding-new-columns-with-mutate",
    "href": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#adding-new-columns-with-mutate",
    "title": "Visualizing {dplyr}’s mutate(), summarize(), group_by(), and ungroup() with animations",
    "section": "Adding new columns with mutate()\n",
    "text": "Adding new columns with mutate()\n\nThe mutate() function in {dplyr} adds new columns. It’s not destructive—all our existing data will still be there after you add new columns2\n2 Unless we use an existing column name inside mutate(), in which case that column will get replaced with the new one.\n\n\n \nBy default, mutate() sticks the new column on the far right of the dataset (scroll over to the right to see body_mass_kg here):\n\npenguins |&gt; \n  mutate(body_mass_kg = body_mass_g / 1000)\n## # A tibble: 333 × 9\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year body_mass_kg\n##    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;        &lt;dbl&gt;\n##  1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007         3.75\n##  2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007         3.8 \n##  3 Adelie  Torgersen           40.3          18                 195        3250 female  2007         3.25\n##  4 Adelie  Torgersen           36.7          19.3               193        3450 female  2007         3.45\n##  5 Adelie  Torgersen           39.3          20.6               190        3650 male    2007         3.65\n##  6 Adelie  Torgersen           38.9          17.8               181        3625 female  2007         3.62\n##  7 Adelie  Torgersen           39.2          19.6               195        4675 male    2007         4.68\n##  8 Adelie  Torgersen           41.1          17.6               182        3200 female  2007         3.2 \n##  9 Adelie  Torgersen           38.6          21.2               191        3800 male    2007         3.8 \n## 10 Adelie  Torgersen           34.6          21.1               198        4400 male    2007         4.4 \n## # ℹ 323 more rows\n\nWe can also control where the new column shows up with either the .before or .after argument:\n\npenguins |&gt; \n  mutate(\n    body_mass_kg = body_mass_g / 1000,\n    .after = island\n  )\n## # A tibble: 333 × 9\n##    species island    body_mass_kg bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n##    &lt;fct&gt;   &lt;fct&gt;            &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n##  1 Adelie  Torgersen         3.75           39.1          18.7               181        3750 male    2007\n##  2 Adelie  Torgersen         3.8            39.5          17.4               186        3800 female  2007\n##  3 Adelie  Torgersen         3.25           40.3          18                 195        3250 female  2007\n##  4 Adelie  Torgersen         3.45           36.7          19.3               193        3450 female  2007\n##  5 Adelie  Torgersen         3.65           39.3          20.6               190        3650 male    2007\n##  6 Adelie  Torgersen         3.62           38.9          17.8               181        3625 female  2007\n##  7 Adelie  Torgersen         4.68           39.2          19.6               195        4675 male    2007\n##  8 Adelie  Torgersen         3.2            41.1          17.6               182        3200 female  2007\n##  9 Adelie  Torgersen         3.8            38.6          21.2               191        3800 male    2007\n## 10 Adelie  Torgersen         4.4            34.6          21.1               198        4400 male    2007\n## # ℹ 323 more rows"
  },
  {
    "objectID": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#summarizing-with-summarize",
    "href": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#summarizing-with-summarize",
    "title": "Visualizing {dplyr}’s mutate(), summarize(), group_by(), and ungroup() with animations",
    "section": "Summarizing with summarize()\n",
    "text": "Summarizing with summarize()\n\nThe summarize() function, on the other hand, is destructive. It collapses our dataset into a single value and throws away any columns that we don’t use when summarizing.\n\n\n\n \nAfter using summarize() on the penguins data, we only see three values in one row: average bill length, total penguin weight, and the number of penguins in the dataset. All other columns are gone.\n\npenguins |&gt; \n  summarize(\n    avg_bill_length = mean(bill_length_mm),\n    total_weight = sum(body_mass_g),\n    n_penguins = n()  # This returns the number of rows in the dataset\n  )\n## # A tibble: 1 × 3\n##   avg_bill_length total_weight n_penguins\n##             &lt;dbl&gt;        &lt;int&gt;      &lt;int&gt;\n## 1            44.0      1400950        333"
  },
  {
    "objectID": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#grouping-and-ungrouping-with-group_by-and-ungroup",
    "href": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#grouping-and-ungrouping-with-group_by-and-ungroup",
    "title": "Visualizing {dplyr}’s mutate(), summarize(), group_by(), and ungroup() with animations",
    "section": "Grouping and ungrouping with group_by() and ungroup()\n",
    "text": "Grouping and ungrouping with group_by() and ungroup()\n\nThe group_by() function splits a dataset into smaller subsets based on the values of columns that we specify. Importantly, this splitting happens behind the scenes—you don’t actually ever see the data split up into smaller datasets.3 To undo the grouping and bring all the rows back together, use ungroup().\n3 I like to imagine that the data is splitting into smaller groups, Minority Report-style, or like Tony Stark’s JARVIS-enabled HUD.\n\n\n \nImportantly, grouping doesn’t actually change the order of the rows in the dataset. If we use group_by() and look at your dataset, it’ll still be in the existing order. The only sign that the data is invisibly grouped is a little Groups: sex [2] note at the top of the output.\n\npenguins |&gt; \n  group_by(sex)\n## # A tibble: 333 × 8\n## # Groups:   sex [2]\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n##    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n##  1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n##  2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n##  3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n##  4 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n##  5 Adelie  Torgersen           39.3          20.6               190        3650 male    2007\n##  6 Adelie  Torgersen           38.9          17.8               181        3625 female  2007\n##  7 Adelie  Torgersen           39.2          19.6               195        4675 male    2007\n##  8 Adelie  Torgersen           41.1          17.6               182        3200 female  2007\n##  9 Adelie  Torgersen           38.6          21.2               191        3800 male    2007\n## 10 Adelie  Torgersen           34.6          21.1               198        4400 male    2007\n## # ℹ 323 more rows\n\nGrouping is fairly useless on its own, but it becomes really powerful when combined with mutate() or summarize()."
  },
  {
    "objectID": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#mutating-within-groups",
    "href": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#mutating-within-groups",
    "title": "Visualizing {dplyr}’s mutate(), summarize(), group_by(), and ungroup() with animations",
    "section": "Mutating within groups",
    "text": "Mutating within groups\nIf we use mutate() after grouping, new columns are added to each subset separately. In many cases, you won’t notice any difference between using mutate() on an ungrouped or grouped dataset—you’ll get the same values. For instance, if we use mutate(body_mass_kg = body_mass_g / 1000) on an ungrouped dataset, R will create a column for the whole dataset that divides body_mass_g by 1,000; if we use mutate(body_mass_kg = body_mass_g / 1000) on a grouped dataset, R will create a new column within each of the subsets. Both approaches will generate the same values.4\n4 Using mutate() on the grouped dataset will be a tiiiiiny bit slower because it’s actually running mutate() on each of the groups.This is actually important if we’re referencing other values within the group. In the example above, we created a new column y that subtracted the smallest value of x from each value of x. When running mutate(y = x - min(x)) on the ungrouped dataset, the smallest value of x is 1, so all the numbers decrease by 1. When running mutate(y = x * 2) on a grouped dataset, though, min(x) refers to the smallest value of x within each of the subsets. Check out this example here: the minimum values in groups A, B, and C are 1, 4, and 7 respectively, so in subset A we subtract 1 from all the values of x, in subset B we subtract 4 from all the values of x, and in subset C we subtract 7 from all the values of x. As a result, the new y column contains 0, 1, and 2 in each of the groups:\n\n\n\n \nPanel data (or time-series cross-sectional data, like the gapminder dataset) is good example of a situation where grouping and mutating is important. For example, we can use lag() to create a new column (lifeExp_previous) that shows the previous year’s life expectancy.5\n5 This is super common with models where you time-shifted variables, like predicting an outcome based on covariates in the previous year.\nlibrary(gapminder)\n\ngapminder_smaller &lt;- gapminder |&gt; \n  filter(year %in% c(1997, 2002, 2007))  # Only show a few years\n  \ngapminder_smaller |&gt; \n  mutate(lifeExp_previous = lag(lifeExp), .after = lifeExp)\n## # A tibble: 426 × 7\n##    country     continent  year lifeExp lifeExp_previous      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;            &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1997    41.8             NA   22227415      635.\n##  2 Afghanistan Asia       2002    42.1             41.8 25268405      727.\n##  3 Afghanistan Asia       2007    43.8             42.1 31889923      975.\n##  4 Albania     Europe     1997    73.0             43.8  3428038     3193.\n##  5 Albania     Europe     2002    75.7             73.0  3508512     4604.\n##  6 Albania     Europe     2007    76.4             75.7  3600523     5937.\n##  7 Algeria     Africa     1997    69.2             76.4 29072015     4797.\n##  8 Algeria     Africa     2002    71.0             69.2 31287142     5288.\n##  9 Algeria     Africa     2007    72.3             71.0 33333216     6223.\n## 10 Angola      Africa     1997    41.0             72.3  9875024     2277.\n## # ℹ 416 more rows\n\nAfghanistan in 1997 has a lagged life expectancy of NA, but that’s fine and to be expected—there’s no row for it to look at and copy the value (i.e. there’s no Afghanistan 1992 row). Afghanistan’s lagged life expectancy in 2002 is the same value as the actual life expectancy in 1997. Great, it worked!6\n6 Technically this isn’t a one-year lag; this is a five-year lag, since the data is spaced every 5 years.But look at Albania’s lagged life expectancy in 1997—it’s 43.84, which is actually Afghanistan’s 2007 life expectancy! Lagged values bleed across countries here.\nIf we group the data by country before lagging, though, the lagging happens within each of the subsets, so the first year of every country is missing (since there’s no previous year to look at). Now every country’s 1997 value is NA, since the new column was created separately in each of the smaller behind-the-scenes country-specific datasets:\n\ngapminder_smaller |&gt; \n  group_by(country) |&gt; \n  mutate(lifeExp_previous = lag(lifeExp), .after = lifeExp)\n## # A tibble: 426 × 7\n## # Groups:   country [142]\n##    country     continent  year lifeExp lifeExp_previous      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;            &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1997    41.8             NA   22227415      635.\n##  2 Afghanistan Asia       2002    42.1             41.8 25268405      727.\n##  3 Afghanistan Asia       2007    43.8             42.1 31889923      975.\n##  4 Albania     Europe     1997    73.0             NA    3428038     3193.\n##  5 Albania     Europe     2002    75.7             73.0  3508512     4604.\n##  6 Albania     Europe     2007    76.4             75.7  3600523     5937.\n##  7 Algeria     Africa     1997    69.2             NA   29072015     4797.\n##  8 Algeria     Africa     2002    71.0             69.2 31287142     5288.\n##  9 Algeria     Africa     2007    72.3             71.0 33333216     6223.\n## 10 Angola      Africa     1997    41.0             NA    9875024     2277.\n## # ℹ 416 more rows"
  },
  {
    "objectID": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#summarizing-groups-with-group_by-summarize",
    "href": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#summarizing-groups-with-group_by-summarize",
    "title": "Visualizing {dplyr}’s mutate(), summarize(), group_by(), and ungroup() with animations",
    "section": "Summarizing groups with group_by() |> summarize()\n",
    "text": "Summarizing groups with group_by() |&gt; summarize()\n\nWhile collapsing an entire dataset can be helpful for finding overall summary statistics (e.g. the average, minimum, and maximum values for columns you’re interested in), summarize() is better used with groups. If we use summarize() on a grouped dataset, each subset is collapsed into a single row. This will create different summary values, depending on the groups you use. In this example, grouping by cat1 gives us a summarized dataset with three rows (for a, b, and c):\n\n\n\n \nWhile here, if we group by cat2, we get a summarized dataset with two rows (for j and k):\n\n\n\n \nIf we use group_by() before summarizing the penguins data, we’ll get a column for the group, along with average bill length, total penguin weight, and the number of penguins in each group. As before, all other columns are gone.\nWe can see summarized values by species:\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(\n    avg_bill_length = mean(bill_length_mm),\n    total_weight = sum(body_mass_g),\n    n_penguins = n()  # This returns the number of rows in each group\n  )\n## # A tibble: 3 × 4\n##   species   avg_bill_length total_weight n_penguins\n##   &lt;fct&gt;               &lt;dbl&gt;        &lt;int&gt;      &lt;int&gt;\n## 1 Adelie               38.8       541100        146\n## 2 Chinstrap            48.8       253850         68\n## 3 Gentoo               47.6       606000        119\n\n…or by sex…\n\npenguins |&gt; \n  group_by(sex) |&gt; \n  summarize(\n    avg_bill_length = mean(bill_length_mm),\n    total_weight = sum(body_mass_g),\n    n_penguins = n()\n  )\n## # A tibble: 2 × 4\n##   sex    avg_bill_length total_weight n_penguins\n##   &lt;fct&gt;            &lt;dbl&gt;        &lt;int&gt;      &lt;int&gt;\n## 1 female            42.1       637275        165\n## 2 male              45.9       763675        168\n\n…or by any other column.\n\n\n\n\n\n\nGrouping by numeric columns\n\n\n\nOne common mistake is to feed a numeric columns into group_by(), like this:\n\npenguins |&gt; \n  group_by(flipper_length_mm) |&gt; \n  summarize(\n    avg_bill_length = mean(bill_length_mm),\n    total_weight = sum(body_mass_g),\n    n_penguins = n()\n  )\n## # A tibble: 54 × 4\n##    flipper_length_mm avg_bill_length total_weight n_penguins\n##                &lt;int&gt;           &lt;dbl&gt;        &lt;int&gt;      &lt;int&gt;\n##  1               172            37.9         3150          1\n##  2               174            37.8         3400          1\n##  3               176            40.2         3450          1\n##  4               178            39.0        13300          4\n##  5               180            39.8        14900          4\n##  6               181            41.5        24000          7\n##  7               182            39.6         9775          3\n##  8               183            39.2         6625          2\n##  9               184            37.9        25650          7\n## 10               185            38.0        31550          9\n## # ℹ 44 more rows\n\nThis technically calculates something, but it’s generally not what you’re looking for. R is making groups for each of the unique values of flipper length and then calculating summaries for those groups. There’s only one penguin with a flipper length of 172 mm; there are 7 with 181 mm. Grouping by a numeric variable can be useful if you want to create a histogram-like table of counts of unique values, but most of the time, you don’t want to do this."
  },
  {
    "objectID": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#summarizing-multiple-groups",
    "href": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#summarizing-multiple-groups",
    "title": "Visualizing {dplyr}’s mutate(), summarize(), group_by(), and ungroup() with animations",
    "section": "Summarizing multiple groups",
    "text": "Summarizing multiple groups\nWe can specify more than one group with group_by(), which will create behind-the-scenes datasets for each unique combination of values in the groups. Here, when group by both cat1 and cat2, we get six groups (a & j, a & k, b & j, b & k, c & j, c & k), which we can then use with mutate() or summarize():"
  },
  {
    "objectID": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#leftover-groupings-and-ungroup",
    "href": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#leftover-groupings-and-ungroup",
    "title": "Visualizing {dplyr}’s mutate(), summarize(), group_by(), and ungroup() with animations",
    "section": "Leftover groupings and ungroup()\n",
    "text": "Leftover groupings and ungroup()\n\nSome subtle and interesting things happen when summarizing with multiple groups, though, and they throw people off all the time.\nWhen you use summarize() on a grouped dataset, {dplyr} will automatically ungroup the last of the groups. This happens invisibly when you’re only grouping by one thing. For example, this has three rows, and no Groups: species[3] note at the top:\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(total = n())\n## # A tibble: 3 × 2\n##   species   total\n##   &lt;fct&gt;     &lt;int&gt;\n## 1 Adelie      146\n## 2 Chinstrap    68\n## 3 Gentoo      119\n\nWhen grouping by multiple things, {dplyr} will automatically ungroup the last of the groups (i.e. the right-most group), but keep everything else grouped. This has six rows and is grouped by species (hence the Groups: species [3]), and R gives you an extra message alerting you to the fact that it’s still grouped by something: `summarise()` has grouped output by 'species'.\n\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(total = n())\n## `summarise()` has grouped output by 'species'. You can override using the `.groups` argument.\n## # A tibble: 6 × 3\n## # Groups:   species [3]\n##   species   sex    total\n##   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n## 1 Adelie    female    73\n## 2 Adelie    male      73\n## 3 Chinstrap female    34\n## 4 Chinstrap male      34\n## 5 Gentoo    female    58\n## 6 Gentoo    male      61\n\nThe same thing happens in reverse if we switch species and sex. The results here are still grouped by sex:\n\npenguins |&gt; \n  group_by(sex, species) |&gt; \n  summarize(total = n())\n## `summarise()` has grouped output by 'sex'. You can override using the `.groups` argument.\n## # A tibble: 6 × 3\n## # Groups:   sex [2]\n##   sex    species   total\n##   &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt;\n## 1 female Adelie       73\n## 2 female Chinstrap    34\n## 3 female Gentoo       58\n## 4 male   Adelie       73\n## 5 male   Chinstrap    34\n## 6 male   Gentoo       61\n\nWe can use ungroup() to bring the data all the way back together and get rid of the groups:\n\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(total = n()) |&gt; \n  ungroup()\n## `summarise()` has grouped output by 'species'. You can override using the `.groups` argument.\n## # A tibble: 6 × 3\n##   species   sex    total\n##   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n## 1 Adelie    female    73\n## 2 Adelie    male      73\n## 3 Chinstrap female    34\n## 4 Chinstrap male      34\n## 5 Gentoo    female    58\n## 6 Gentoo    male      61\n\nAlternatively, summarize has a .groups argument that you can use to control what happens to the groups after you summarize. By default, it uses .groups = \"drop_last\" and gets rid of the right-most group, but you can also drop all the groups (.groups = \"drop\") and keep all the groups (.groups = \"keep\"). See? No groups!\n\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(total = n(), .groups = \"drop\")\n## # A tibble: 6 × 3\n##   species   sex    total\n##   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n## 1 Adelie    female    73\n## 2 Adelie    male      73\n## 3 Chinstrap female    34\n## 4 Chinstrap male      34\n## 5 Gentoo    female    58\n## 6 Gentoo    male      61\n\n\n\n\n\n\n\nExperimental different way of grouping and summarizing\n\n\n\nWith newer versions of {dplyr} there’s a new experimental way to specify groups when summarizing, borrowed from {data.table}. Rather than specify groups in an explicit group_by() function, you can do it inside summarize() with the .by argument:\n\npenguins |&gt; \n  summarize(total = n(), .by = c(species, sex))\n## # A tibble: 6 × 3\n##   species   sex    total\n##   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n## 1 Adelie    male      73\n## 2 Adelie    female    73\n## 3 Gentoo    female    58\n## 4 Gentoo    male      61\n## 5 Chinstrap female    34\n## 6 Chinstrap male      34\n\nThis automatically ungroups everything when it’s done, so you don’t have any leftover groupings."
  },
  {
    "objectID": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#why-care-about-leftover-groups",
    "href": "blog/2024/04/04/group_by-summarize-ungroup-animations/index.html#why-care-about-leftover-groups",
    "title": "Visualizing {dplyr}’s mutate(), summarize(), group_by(), and ungroup() with animations",
    "section": "Why care about leftover groups?",
    "text": "Why care about leftover groups?\nLots of the time, you don’t actually need to worry about leftover groupings. If you’re plotting or modeling or doing other stuff with the data, those functions will ignore the groups and work on the whole dataset. For example, I do stuff like calculating and plotting group summaries all the time—plot_data here is still grouped by species after summarizing, but ggplot() doesn’t care:\n\nplot_data &lt;- penguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(total = n())\n## `summarise()` has grouped output by 'species'. You can override using the `.groups` argument.\n\n# plot_data is grouped by sex, but that doesn't matter here\nggplot(plot_data, aes(x = species, y = total, fill = species)) +\n  geom_col() + \n  guides(fill = \"none\") +\n  facet_wrap(vars(sex))\n\n\n\n\n\n\n\nLeftover groups are very important when you use things like mutate() on the summarized dataset.\nLike here, we’ll create a proportion column based on total / sum(total). Because we only grouped by one thing, there are no leftover groupings, so the prop column adds up to 100%:\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(total = n()) |&gt; \n  mutate(prop = total / sum(total))\n## # A tibble: 3 × 3\n##   species   total  prop\n##   &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n## 1 Adelie      146 0.438\n## 2 Chinstrap    68 0.204\n## 3 Gentoo      119 0.357\n\nNext, we’ll group by two things, which creates behind-the-scenes datasets for all the six combinations of species and sex. When {dplyr} is done, it ungroups the sex group, but leaves the dataset grouped by species. The prop column no longer adds up to 100%; it adds to 300%. That’s because it calculated total/sum(total) within each species group (so 50% of Adélies are female, 50% are male, etc.)\n\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(total = n()) |&gt; \n  mutate(prop = total / sum(total))\n## `summarise()` has grouped output by 'species'. You can override using the `.groups` argument.\n## # A tibble: 6 × 4\n## # Groups:   species [3]\n##   species   sex    total  prop\n##   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n## 1 Adelie    female    73 0.5  \n## 2 Adelie    male      73 0.5  \n## 3 Chinstrap female    34 0.5  \n## 4 Chinstrap male      34 0.5  \n## 5 Gentoo    female    58 0.487\n## 6 Gentoo    male      61 0.513\n\nIf we reverse the grouping order so that sex comes first, {dplyr} will automatically stop grouping by species and keep the dataset grouped by sex. That means mutate() will work within each sex group, so the prop column here adds to 200%. 44% of female penguins are Adélies, 21% of female penguins are Chinstraps, and 35% of female penguins are Gentoos, and so on.\n\npenguins |&gt; \n  group_by(sex, species) |&gt; \n  summarize(total = n()) |&gt; \n  mutate(prop = total / sum(total))\n## `summarise()` has grouped output by 'sex'. You can override using the `.groups` argument.\n## # A tibble: 6 × 4\n## # Groups:   sex [2]\n##   sex    species   total  prop\n##   &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n## 1 female Adelie       73 0.442\n## 2 female Chinstrap    34 0.206\n## 3 female Gentoo       58 0.352\n## 4 male   Adelie       73 0.435\n## 5 male   Chinstrap    34 0.202\n## 6 male   Gentoo       61 0.363\n\nIf we explicitly ungroup before calculating the proportion,7 then mutate() will work on the whole dataset instead of sex- or species-specific groups. Here, 22% of all penguins are female Adélies, 10% are female Chinstraps, etc.\n7 Or use the .groups argument or .by argument in summarize()\npenguins |&gt; \n  group_by(sex, species) |&gt; \n  summarize(total = n()) |&gt; \n  ungroup() |&gt; \n  mutate(prop = total / sum(total))\n## `summarise()` has grouped output by 'sex'. You can override using the `.groups` argument.\n## # A tibble: 6 × 4\n##   sex    species   total  prop\n##   &lt;fct&gt;  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n## 1 female Adelie       73 0.219\n## 2 female Chinstrap    34 0.102\n## 3 female Gentoo       58 0.174\n## 4 male   Adelie       73 0.219\n## 5 male   Chinstrap    34 0.102\n## 6 male   Gentoo       61 0.183\n\nWe don’t have to rely on {dplyr}’s automatic ungroup-the-last-grouping feature and we can add our own grouping explicitly later. Like here, {dplyr} stops grouping by sex, which means that the prop column would add to 300%, showing the proportion of sexes within each species. But if we throw in a group_by(sex) before mutate(), it’ll put everything in two behind-the-scenes datasets (male and female) and calculate the proportion of species within each sex. The resulting dataset is still grouped by sex, since mutate() doesn’t drop any groups like summarize():\n\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(total = n()) |&gt; \n  group_by(sex) |&gt;\n  mutate(prop = total / sum(total))\n## `summarise()` has grouped output by 'species'. You can override using the `.groups` argument.\n## # A tibble: 6 × 4\n## # Groups:   sex [2]\n##   species   sex    total  prop\n##   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;\n## 1 Adelie    female    73 0.442\n## 2 Adelie    male      73 0.435\n## 3 Chinstrap female    34 0.206\n## 4 Chinstrap male      34 0.202\n## 5 Gentoo    female    58 0.352\n## 6 Gentoo    male      61 0.363"
  },
  {
    "objectID": "blog/2024/05/08/coastline-to-border-proportions/index.html",
    "href": "blog/2024/05/08/coastline-to-border-proportions/index.html",
    "title": "Calculating the proportion of US state borders that are coastlines",
    "section": "",
    "text": "A few days ago, my wife, a bunch of my kids, and I were huddled around a big wall map of the United States, joking about the relative unimportance of Rhode Island, the smallest state in the US. It’s one of the states I never ever think about:\n…and it’s just so small.\nAmid the joking, my wife came to Rhode Island’s defense by declaring that even though it’s so small, it has one of the highest proportions of coastline to land borders. We all gave it a metaphorical gold star for being so maritime-y and moved on with our days.\nBut as I thought about it later, I got curious about how much of Rhode Island’s border really is coastline and how that proportion compares to other states. New England in general has lots of inlets and islands; North Carolina has the complex Outer Banks; Louisiana has the Mississippi Delta; Michigan is split into two parts and surrounded by the Great Lakes; Florida is Florida. Lots of other states have lots of coastline.\nUsing R, the extremely powerful {sf} package for working with geospatial data, and some high quality public domain geographic data, we can find the actual answers about coastline proportions. Spoilers: Rhode Island does really well, as expected.\nBut doing this is a lot more complicated than you might think, both for technical reasons and for philosophical reasons.\nLet’s explore this data and make some pretty maps!\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(patchwork)\nlibrary(gt)\n\nclrs &lt;- rcartocolor::carto_pal(12, \"Prism\")\nclr_ocean &lt;- colorspace::lighten(\"#88CCEE\", 0.7)\n\n# Custom ggplot theme to make pretty plots\n# Get the font at https://fonts.google.com/specimen/Overpass\ntheme_map &lt;- function() {\n  theme_void(base_family = \"Overpass Light\") +\n    theme(\n      plot.title = element_text(family = \"Overpass\", face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(family = \"Overpass\", face = \"plain\", hjust = 0.5)\n    )\n}\n\nupdate_geom_defaults(\"text\", list(family = \"Overpass\"))"
  },
  {
    "objectID": "blog/2024/05/08/coastline-to-border-proportions/index.html#use-us-census-data-on-states-and-coastlines",
    "href": "blog/2024/05/08/coastline-to-border-proportions/index.html#use-us-census-data-on-states-and-coastlines",
    "title": "Calculating the proportion of US state borders that are coastlines",
    "section": "Use US Census data on states and coastlines",
    "text": "Use US Census data on states and coastlines\nAt first glance, calculating the proportion of coastline borders in states feels fairly straightforward. We take state boundaries, find where they intersect with coastline boundaries, extract those overlapping sections, and voilá—we’re done.\nThe US Census Bureau even has shapefiles ready to use, like US state borders and the national coastline for 2023, and the {tigris} package makes it really easy to load that data directly into R.\nFirst, let’s grab US state data at medium resolution (1:5 million) and calculate the length of each state’s border. To make calculations easier, we’ll change the projection to Albers, which measures distances in meters instead of the default NAD 83 decimal degree system.\n\ncensus_states &lt;- states(\n  cb = TRUE, resolution = \"5m\", year = 2023,\n  progress_bar = FALSE, keep_zipped_shapefile = TRUE\n) |&gt;\n  st_transform(crs = st_crs(\"ESRI:102003\")) |&gt;  # Albers\n  mutate(\n    border_length = st_perimeter(geometry),\n    border_length_miles = units::set_units(border_length, \"miles\")\n  )\n\ncensus_states |&gt; \n  select(NAME, border_length, border_length_miles)\n## Simple feature collection with 56 features and 3 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -10430000 ymin: -1685000 xmax: 3408000 ymax: 5141000\n## Projected CRS: USA_Contiguous_Albers_Equal_Area_Conic\n## First 10 features:\n##            NAME border_length border_length_miles                       geometry\n## 1    New Mexico   2389194 [m]      1484.6 [miles] MULTIPOLYGON (((-1231344 -5...\n## 2   Puerto Rico    683648 [m]       424.8 [miles] MULTIPOLYGON (((3306526 -15...\n## 3         Texas   6218137 [m]      3863.8 [miles] MULTIPOLYGON (((-1e+06 -570...\n## 4      Kentucky   2097216 [m]      1303.1 [miles] MULTIPOLYGON (((584560 -886...\n## 5          Ohio   1614692 [m]      1003.3 [miles] MULTIPOLYGON (((1094061 536...\n## 6       Georgia   1950980 [m]      1212.3 [miles] MULTIPOLYGON (((939223 -230...\n## 7      Arkansas   2117255 [m]      1315.6 [miles] MULTIPOLYGON (((122656 -111...\n## 8        Oregon   2301152 [m]      1429.9 [miles] MULTIPOLYGON (((-2285910 94...\n## 9  Pennsylvania   1579842 [m]       981.7 [miles] MULTIPOLYGON (((1287712 486...\n## 10     Missouri   2357068 [m]      1464.6 [miles] MULTIPOLYGON (((19009 34499...\n\nNext we’ll grab coastline data and also convert it to the meter-based Albers projection:\n\ncensus_coastline &lt;- coastline(\n  year = 2023, progress_bar = FALSE, keep_zipped_shapefile = TRUE\n) |&gt;\n  st_transform(crs = st_crs(\"ESRI:102003\"))\ncensus_coastline\n## Simple feature collection with 4236 features and 2 fields\n## Geometry type: LINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -10430000 ymin: -1685000 xmax: 3408000 ymax: 5140000\n## Projected CRS: USA_Contiguous_Albers_Equal_Area_Conic\n## First 10 features:\n##         NAME MTFCC                       geometry\n## 1  Atlántico L4150 LINESTRING (3232962 -158263...\n## 2  Atlántico L4150 LINESTRING (3282570 -157424...\n## 3  Atlántico L4150 LINESTRING (3278311 -157345...\n## 4  Atlántico L4150 LINESTRING (3283967 -157426...\n## 5  Atlántico L4150 LINESTRING (3282914 -157442...\n## 6  Atlántico L4150 LINESTRING (3282240 -157431...\n## 7  Atlántico L4150 LINESTRING (3280919 -157406...\n## 8  Atlántico L4150 LINESTRING (3276748 -157350...\n## 9  Atlántico L4150 LINESTRING (3276791 -157363...\n## 10 Atlántico L4150 LINESTRING (3286366 -157464...\n\nThis coastline data isn’t state-based—instead, each row represents a segment of the US coastline. Here’s what it looks like when plotted:\n\nggplot() + \n  geom_sf(data = census_coastline, linewidth = 0.1) +\n  labs(title = \"US Census coastline data\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +\n  theme_map()\n\n\n\n\n\n\n\nTo find where the two maps intersect, we can use st_intersection(), and then we can calculate the length of each combined segment with st_length():\n\ncensus_combined &lt;- census_states |&gt; \n  st_intersection(census_coastline) |&gt; \n  mutate(coastline_length = st_length(geometry))\n  \ncensus_combined |&gt; \n  select(NAME, border_length, coastline_length, geometry)\n## Simple feature collection with 1276 features and 3 fields\n## Geometry type: GEOMETRY\n## Dimension:     XY\n## Bounding box:  xmin: -10430000 ymin: -1685000 xmax: 3408000 ymax: 5140000\n## Projected CRS: USA_Contiguous_Albers_Equal_Area_Conic\n## First 10 features:\n##            NAME border_length coastline_length                       geometry\n## 2   Puerto Rico    683648 [m]      119.450 [m] LINESTRING (3282570 -157424...\n## 2.1 Puerto Rico    683648 [m]       12.122 [m] LINESTRING (3282914 -157442...\n## 2.2 Puerto Rico    683648 [m]      722.088 [m] LINESTRING (3282240 -157431...\n## 2.3 Puerto Rico    683648 [m]        5.332 [m] LINESTRING (3280919 -157406...\n## 2.4 Puerto Rico    683648 [m]     2014.901 [m] LINESTRING (3279028 -157391...\n## 2.5 Puerto Rico    683648 [m]     2446.737 [m] LINESTRING (3081263 -163791...\n## 2.6 Puerto Rico    683648 [m]    12209.219 [m] MULTILINESTRING ((3306976 -...\n## 2.7 Puerto Rico    683648 [m]     2804.085 [m] MULTILINESTRING ((3178034 -...\n## 2.8 Puerto Rico    683648 [m]     7978.399 [m] MULTILINESTRING ((3230856 -...\n## 2.9 Puerto Rico    683648 [m]     6004.139 [m] MULTILINESTRING ((3172336 -...\n\nSome of these segments are thousands of miles; some are only a few miles. We can do some grouping and summarizing to collapse these into single values for each state. border_length is a state-level variable, not a border-segment-level variable, so it’s repeated in each of the rows of the combined dataset, so we only need to keep one of the values—here I keep the max, but min would work (since they’re the same).\n\ncoastline_length_by_state &lt;- census_combined |&gt; \n  st_drop_geometry() |&gt;  # Stop worrying about geographic stuff\n  group_by(NAME) |&gt; \n  summarize(\n    total_coastline_length = sum(coastline_length),\n    total_perimeter = max(border_length),\n    prop_coastline = as.numeric(total_coastline_length / total_perimeter)\n  )\ncoastline_length_by_state\n## # A tibble: 35 × 4\n##    NAME                                         total_coastline_length total_perimeter prop_coastline\n##    &lt;chr&gt;                                                           [m]             [m]          &lt;dbl&gt;\n##  1 Alabama                                                     192355.        1907670.         0.101 \n##  2 Alaska                                                    19223761.       29671399.         0.648 \n##  3 American Samoa                                              175827.         172059.         1.02  \n##  4 California                                                 1129738.        4191420.         0.270 \n##  5 Commonwealth of the Northern Mariana Islands                261865.         327378.         0.800 \n##  6 Connecticut                                                  77127.         574613.         0.134 \n##  7 Delaware                                                    110370.         433573.         0.255 \n##  8 Florida                                                    1842297.        3795469.         0.485 \n##  9 Georgia                                                     114280.        1950980.         0.0586\n## 10 Guam                                                        120734.         133668.         0.903 \n## # ℹ 25 more rows\n\nAt first glance, this looks fine. We have each state’s total border length, coastline length, and a proportion. Neat.\nBut some of these proportions look wrong, like Hawaiʻi:\n\ncoastline_length_by_state |&gt; \n  filter(NAME %in% c(\"Hawaii\", \"North Carolina\"))\n## # A tibble: 2 × 4\n##   NAME           total_coastline_length total_perimeter prop_coastline\n##   &lt;chr&gt;                             [m]             [m]          &lt;dbl&gt;\n## 1 Hawaii                        712716.        1413312.         0.504 \n## 2 North Carolina                248590.        3498277.         0.0711\n\nAccording to this, only 50% of Hawaiʻi’s borders are on the coast. That’s obviously wrong—that state shares no borders with other states and it’s in the middle of the Pacific Ocean.\nIf we plot the two datasets, we can see what’s going on. First, the resolutions of the state data and coastline data don’t quite match. Check out Oʻahu here—the purple coastline crosses some bays and misses some of the landmass:\n\n# Extract the state boundaries\ncensus_hi &lt;- census_states |&gt; filter(NAME == \"Hawaii\")\n\n# Extract the bounding box around the state so we can zoom in on the coastline map\nbbox_hi &lt;- census_hi |&gt; st_transform(st_crs(\"EPSG:4269\")) |&gt; st_bbox()\n\nggplot() +\n  geom_sf(data = census_hi, linewidth = 0.1, fill = clrs[5]) +\n  geom_sf(data = census_coastline, aes(color = \"US Census coastline data\")) +\n  annotate(geom = \"text\", x = I(0.425), y = I(0.89), label = \"Oʻahu\") +\n  annotate(\n    geom = \"rect\", \n    xmin = I(0.35), xmax = I(0.5), ymin = I(0.63), ymax = I(0.85), \n    color = clrs[12], fill = NA, linetype = \"21\"\n  ) +\n  scale_color_manual(values = c(clrs[11])) +\n  labs(title = \"Hawaiʻi\", color = NULL)  +\n  coord_sf(\n    xlim = bbox_hi[c(1, 3)],\n    ylim = bbox_hi[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nThis mismatch between datasets is even more obvious if we look at a state like North Carolina. The actual landmass in the Outer Banks along the east coast is complex, with all sorts of inlets and a big long barrier island, but the Census simplifies it down substantially (with good reason(!) as we’ll see later):\n\ncensus_nc &lt;- census_states |&gt; filter(NAME == \"North Carolina\")\nbbox_nc &lt;- census_nc |&gt; st_transform(st_crs(\"EPSG:4269\")) |&gt; st_bbox()\n\nggplot() +\n  geom_sf(data = census_nc, linewidth = 0.1, fill = clrs[6]) +\n  geom_sf(data = census_coastline, aes(color = \"US Census coastline data\")) +\n  annotate(\n    geom = \"rect\", \n    xmin = I(0.57), xmax = I(0.97), ymin = I(0.005), ymax = I(0.995), \n    color = clrs[12], fill = NA, linetype = \"21\"\n  ) +\n  scale_color_manual(values = c(clrs[11])) +\n  labs(title = \"North Carolina\", color = NULL)  +\n  coord_sf(\n    xlim = bbox_nc[c(1, 3)],\n    ylim = bbox_nc[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nThis mismatch in borders and coastlines matter a lot for calculations. We’re using st_intersection() to find where the two maps overlap. If the two maps are misaligned, we can’t identify the correct overlaps, which means we can’t identify the full coastal border. Remember how we calculated that only 50% of Hawaiʻi’s borders are coastlines? If we plot the coastal borders of Hawaiʻi, we can see why:\n\nggplot() +\n  geom_sf(data = census_combined, linewidth = 0.2) +\n  labs(title = \"Incomplete coastline overlaps in Hawaiʻi\") +\n  coord_sf(\n    xlim = bbox_hi[c(1, 3)],\n    ylim = bbox_hi[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nThere are a couple issues here. The dots scattered along the borders are a sign that there’s some misalignment between the two maps. The borders for the states sometimes cross the coastline borders in a single point instead of following the coast exactly. But the even bigger issue is that these coastlines look sketched out—there are so many major gaps that roughly 50% of the borders are missing. That’s again because of misalignment with the two maps. The borders don’t overlap exactly, so st_intersection() can’t pick them up.\n\nSo what do we do? We could try finding different coastline data that matches the same resolution as the Census data. The National Oceanic and Atmospheric Administration (NOAA) and the US Geological Survey (USGS) each have their own shoreline datasets, and we could download those and load them into R and hope that they align with one of the Census’s state maps. But they don’t.\nSo we give up."
  },
  {
    "objectID": "blog/2024/05/08/coastline-to-border-proportions/index.html#use-the-lack-of-land-as-the-coastline",
    "href": "blog/2024/05/08/coastline-to-border-proportions/index.html#use-the-lack-of-land-as-the-coastline",
    "title": "Calculating the proportion of US state borders that are coastlines",
    "section": "Use the lack of land as the coastline",
    "text": "Use the lack of land as the coastline\nJust kidding. There’s a better solution.\nWe already kind of have coastline data embedded in the state map data. For states that border an ocean or lake, anywhere the blue of the water touches the land is technically a coastline. Take Hawaiʻi, for instance, where the whole state border is the coastline:\n\nggplot() +\n  geom_sf(data = census_hi, linewidth = 0.1, fill = clrs[5]) +\n  labs(title = \"The islands of Hawaiʻi\") +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +  # NAD83\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean), \n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0))\n  )\n\n\n\n\n\n\n\nWhen looking at states with interior borders (like the other 49 states), though, the state map data has no way to distinguish which of those borders touch the ocean or touch other states. The east coast of North Carolina touches the ocean, but the northern, southern, and western borders do not. We need to somehow figure out which borders don’t touch other states.\n\nggplot() +\n  geom_sf(data = census_nc, linewidth = 0.1, fill = clrs[6]) +\n  annotate(geom = \"text\", x = I(0.6), y = I(0.89), label = \"↑ Virginia ↑\") +\n  annotate(geom = \"text\", x = I(0.58), y = I(0.3), label = \"↓ South Carolina ↓\", angle = 313) +\n  annotate(geom = \"text\", x = I(0.14), y = I(0.5), label = \"↓ Georgia\") +\n  annotate(geom = \"text\", x = I(0.27), y = I(0.64), label = \"← Tennessee\") +\n  labs(title = \"The “island” of North Carolina\") +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +  # NAD83\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean), \n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0))\n  )\n\n\n\n\n\n\n\nWe do have data about Virginia, South Carolina, Tennessee, and Georgia, though, so there is a way to know that those borders aren’t ocean borders.\nOne way to determine which borders are interior and which ones are coastal is to create a big unified shape of the United States and then use the shape of North Carolina as a kind of cookie cutter. We can assume that any exposed edges are coasts.\nFirst, we’ll make our big unified country shape:\n\ncensus_us_giant &lt;- census_states |&gt; \n  st_union() |&gt; \n  st_transform(crs = st_crs(\"ESRI:102003\"))\n\nggplot() + \n  geom_sf(data = census_us_giant, linewidth = 0, fill = clrs[1]) +\n  labs(title = \"One big US-shaped shape\") +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean), \n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0))\n  )\n\n\n\n\n\n\n\nBefore trying this with North Carolina, we’ll test the cookie cutter selection with Hawaiʻi, since we know that 100% of its borders are coastlines. If we use the Hawaiʻi shape to take a chunk out of the overall US shape, we get…\n\nhi_ocean_border_census &lt;- census_hi |&gt; \n  st_difference(census_us_giant)\n\nggplot() +\n  geom_sf(data = hi_ocean_border_census) +\n  labs(title = \"lol nothing\") +\n  theme_map() +\n  theme(\n    panel.border = element_rect(color = \"black\", fill = clr_ocean),\n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0))\n  )\n\n\n\n\n\n\n\n…nothing.\nThe state cookie cutter was too perfect and selected right up to the edge of the overall country, leaving nothing.\nTo fix this, we can expand the state shape just a tiiiiiiny bit. We can use st_buffer() to add a tiny amount of distance all around the shape—since we’re using the Albers projection, we’re working in meters, so let’s add just 1 millimeter around the border before finding the difference:\n\nhi_ocean_border_census &lt;- census_hi |&gt; \n  st_buffer(dist = 0.001) |&gt; \n  st_difference(census_us_giant)\n\nggplot() +\n  geom_sf(data = hi_ocean_border_census, linewidth = 0.2) +\n  labs(title = \"Hawaiʻi’s coastal borders extracted from US shape\") +\n  coord_sf(\n    xlim = bbox_hi[c(1, 3)],\n    ylim = bbox_hi[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nPerfect.\nWell, almost perfect. We’ve identified the coastal borders, but adding the buffer actually distorts things when we calculate the length of the border.\nLet’s find the coast-to-border proportion for Hawaiʻi, which should be 100%. First we’ll find the perimeter of the state:\n\nhi_border_perimeter &lt;- census_hi |&gt; \n  st_perimeter()\nhi_border_perimeter\n## 1413312 [m]\nunits::set_units(hi_border_perimeter, \"miles\")\n## 878.2 [miles]\n\nHawaiʻi’s border is 1,413,312 meters, or 878 miles. Sounds reasonable.\nNext we’ll find the perimeter of the coastline:\n\nhi_ocean_border_perimeter &lt;- hi_ocean_border_census |&gt; \n  st_perimeter()\nhi_ocean_border_perimeter\n## 2826623 [m]\nunits::set_units(hi_ocean_border_perimeter, \"miles\")\n## 1756 [miles]\n\nHrm. Hawaiʻi’s coastline is 2,826,623 meters, or 1,756 miles. That’s actually exactly twice the correct border length:\n\nas.numeric(hi_ocean_border_perimeter / hi_border_perimeter)\n## [1] 2\n\nThis happened because of the 1 mm buffer that we added around the state shape. Adding the buffer transformed the line into a polygon—a super tiny 1 mm-narrow polygon, but a polygon nonetheless. That means the border technically has a top and sides and a bottom. When calculating the length or perimeter of the border, we’re double counting because we’re getting both the top and the bottom of the hyper-thin polygon.\nTo better illustrate what’s going on, let’s add a three kilometer buffer around the borders.\n\nhi_ocean_border_census_huge_buffer &lt;- census_hi |&gt; \n  st_buffer(dist = 3000) |&gt; \n  st_difference(census_us_giant)\n\nggplot() +\n  geom_sf(\n    data = hi_ocean_border_census_huge_buffer, \n    linewidth = 0.2, fill = colorspace::lighten(clrs[5], 0.5)\n  ) +\n  labs(title = \"Hawaiʻi’s borders with a 3 km buffer\") +\n  coord_sf(\n    xlim = bbox_hi[c(1, 3)],\n    ylim = bbox_hi[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nCalculating the perimeter of these borders will add the inside ring and the outside ring, effectively doubling the distance.\nIt’s less obvious that this doubling is happening when we add just 1 mm, but it is, and it’s leading to incorrect calculations. Fortunately it’s easy to adjust—we can halve the ocean border distance. Coastal borders comprise 100% of Hawaiʻi’s state borders, as expected:\n\nas.numeric((hi_ocean_border_perimeter / 2) / hi_border_perimeter) |&gt; \n  scales::label_percent()()\n## [1] \"100%\"\n\n\nLet’s do the same thing with North Carolina:\n\nnc_ocean_border_census &lt;- census_nc |&gt; \n  st_buffer(dist = 0.001) |&gt; \n  st_difference(census_us_giant)\n\nggplot() +\n  geom_sf(data = nc_ocean_border_census, linewidth = 0.2) +\n  labs(title = \"North Carolina’s coastal borders extracted from US shape\") +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +\n  theme_map()\n\n\n\n\n\n\n\nPerfect! Those are all the state borders that don’t touch other states. That’s the coastline.\nWe can visualize this better by plotting the larger country shape, the North Carolina shape, and the coastal border:\n\nggplot() +\n  geom_sf(data = census_us_giant, fill = clrs[1], alpha = 0.4) +\n  geom_sf(data = census_nc, linewidth = 0.1, fill = clrs[6]) +\n  geom_sf(\n    data = nc_ocean_border_census, linewidth = 0.4, \n    aes(color = \"Coastal border extracted from US shape\"),\n    key_glyph = draw_key_path\n  ) +\n  annotate(geom = \"text\", x = I(0.6), y = I(0.89), label = \"↑ Virginia ↑\") +\n  annotate(geom = \"text\", x = I(0.58), y = I(0.3), label = \"↓ South Carolina ↓\", angle = 313) +\n  annotate(geom = \"text\", x = I(0.14), y = I(0.5), label = \"↓ Georgia\") +\n  annotate(geom = \"text\", x = I(0.27), y = I(0.64), label = \"← Tennessee\") +\n  scale_color_manual(values = c(clrs[8])) +\n  labs(title = \"North Carolina and its coastal border\", color = NULL) +\n  coord_sf(\n    xlim = bbox_nc[c(1, 3)],\n    ylim = bbox_nc[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean),\n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0)),\n    legend.key.height = unit(0.5, \"lines\"),\n    legend.key = element_rect(fill = NA, color = NA),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\nAnd we can find the proportion of the state’s borders that are coastline:\n\nnc_border_perimeter &lt;- census_nc |&gt; \n  st_perimeter()\nnc_border_perimeter\n## 3498277 [m]\nunits::set_units(nc_border_perimeter, \"miles\")\n## 2174 [miles]\n\nnc_ocean_border_perimeter &lt;- nc_ocean_border_census |&gt; \n  st_perimeter()\nnc_ocean_border_perimeter / 2\n## 1930852 [m]\nunits::set_units(nc_ocean_border_perimeter / 2, \"miles\")\n## 1200 [miles]\n\nas.numeric((nc_ocean_border_perimeter / 2) / nc_border_perimeter) |&gt; \n  scales::label_percent()()\n## [1] \"55%\"\n\nThis approach is great. We don’t need to worry about making sure the coastline map matches the resolution of the state map, since we’re using just one map. Everything aligns perfectly automatically.\n\nBUT there’s one more wrinkle to worry about. This approach gets trickier with states that touch other countries, like Washington, where the western border touches the ocean, the northern border touches Canada, and the eastern and southern borders touch other states. The state data knows about Idaho and Oregon, but it doesn’t know that Canada is there. Washington looks like it has an exterior, ocean-facing western border—Canada is replaced with an ocean.\n\ncensus_wa &lt;- census_states |&gt; filter(NAME == \"Washington\")\nbbox_wa &lt;- census_wa |&gt; st_transform(st_crs(\"EPSG:4269\")) |&gt; st_bbox()\n\nwa_ocean_border_census &lt;- census_wa |&gt; \n  st_buffer(dist = 0.001) |&gt; \n  st_difference(census_us_giant)\n\nggplot() +\n  geom_sf(data = census_us_giant, fill = clrs[1], alpha = 0.4) +\n  geom_sf(data = census_wa, linewidth = 0.1, fill = clrs[3]) +\n  geom_sf(\n    data = wa_ocean_border_census, linewidth = 0.4, \n    aes(color = \"Coastal border extracted from US shape\"),\n    key_glyph = draw_key_path\n  ) +\n  annotate(geom = \"text\", x = I(0.64), y = I(0.91), label = \"↑ Canada (British Columbia) ↑\") +\n  annotate(geom = \"text\", x = I(0.54), y = I(0.18), label = \"↓ Oregon ↓\") +\n  annotate(geom = \"text\", x = I(0.87), y = I(0.5), label = \"Idaho →\") +\n  scale_color_manual(values = c(clrs[8])) +\n  labs(title = \"Washington and the fake Canadian ocean\", color = NULL) +\n  coord_sf(\n    xlim = bbox_wa[c(1, 3)],\n    ylim = bbox_wa[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean),\n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0)),\n    legend.key.height = unit(0.5, \"lines\"),\n    legend.key = element_rect(fill = NA, color = NA),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\nThe big US-shaped polygon isn’t big enough for identifying land borders in other countries. In order to identify all the ocean-facing state borders, we need data about Canada and Mexico, and the US Census doesn’t provide that.\nSo we give up."
  },
  {
    "objectID": "blog/2024/05/08/coastline-to-border-proportions/index.html#use-data-from-natural-earth",
    "href": "blog/2024/05/08/coastline-to-border-proportions/index.html#use-data-from-natural-earth",
    "title": "Calculating the proportion of US state borders that are coastlines",
    "section": "Use data from Natural Earth",
    "text": "Use data from Natural Earth\nJust kidding. There’s a better solution.\nThe Census doesn’t have data on other countries, but other data sources do! The incredible Natural Earth project has a ton of shapefiles for the entire world, both physical (i.e. rivers, coastlines, etc.) and cultural, like borders for countries (what it calls Admin-0) and states/provinces (what it calls Admin-1). It offers three levels of resolution: 1:10 million (high resolution), 1:50 million (medium resolution), and 1:110 million (low resolution).\nPlus, like {tigris}, the {rnaturalearth} package makes it really easy to load that data directly into R.\nLet’s follow the same process for Hawaiʻi, North Carolina, and Washington using Natural Earth data instead of Census data.\nFirst, we’ll grab high resolution (1:10 million) maps from the Admin-1 (states and provinces) data:\n\nif (!file.exists(\"ne_data/ne_10m_admin_1_states_provinces_lakes.shp\")) {\n  ne_download(\n    type = \"admin_1_states_provinces_lakes\", \n    scale = 10,\n    destdir = \"ne_data\",\n    load = FALSE\n  )\n} \n\nstates_provinces &lt;- ne_load(\n  type = \"admin_1_states_provinces_lakes\", \n  scale = 10,\n  destdir = \"ne_data\"\n)\n\nWe only need to work with three countries, not the entire world, so we’ll filter it down to just North America before joining everything into a single shape with st_union():\n\nna_giant &lt;- states_provinces |&gt; \n  filter(admin %in% c(\"United States of America\", \"Canada\", \"Mexico\")) |&gt; \n  st_union() |&gt; \n  st_transform(crs = st_crs(\"ESRI:102003\"))  # Convert to Albers for meters\n\nggplot() + \n  geom_sf(data = na_giant, linewidth = 0, fill = clrs[10]) +\n  labs(title = \"One big North America-shaped shape\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean), \n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0))\n  )\n\n\n\n\n\n\n\n(That’s so pretty.)\nNext, we’ll extract all the US states from the Natural Earth data, since we want this big North American shape to match the states exactly:\n\nne_states &lt;- states_provinces |&gt; \n  filter(admin == \"United States of America\") |&gt; \n  st_transform(crs = st_crs(\"ESRI:102003\"))\n\nggplot() + \n  geom_sf(data = na_giant, linewidth = 0, fill = clrs[10]) +\n  geom_sf(data = ne_states, linewidth = 0.05, fill = clrs[8], color = \"white\") +\n  labs(title = \"The United States overlaid on North America\") +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean), \n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0))\n  )\n\n\n\n\n\n\n\nNow we’ll go through the same process as before, using state shapes as cookie cutters from the larger North American shape and extracting non-interior borders.\nHawaiʻi\nHere are Hawaiʻi’s extracted borders—because we’re working with high resolution data, we get a lot more detail, including all the Northwestern Hawaiian Islands:\n\nne_hi &lt;- ne_states |&gt; filter(name == \"Hawaii\")\nbbox_ne_hi &lt;- ne_hi |&gt; st_transform(st_crs(\"EPSG:4269\")) |&gt; st_bbox()\n\nhi_ocean_border_ne &lt;- ne_hi |&gt; \n  st_buffer(dist = 0.001) |&gt; \n  st_difference(na_giant)\n\nggplot() +\n  geom_sf(data = hi_ocean_border_ne, linewidth = 0.2) +\n  labs(title = \"Hawaiʻi’s coastal borders extracted from North America shape\") +\n  coord_sf(\n    xlim = bbox_ne_hi[c(1, 3)],\n    ylim = bbox_ne_hi[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map()\n\n\n\n\n\n\n\nAs before, we can calculate the perimeter of the state, the perimeter of the coastline, and find the proportion. It’s 100%, as expected:\n\n# State border\nhi_border_perimeter_ne &lt;- ne_hi |&gt; \n  st_perimeter()\nhi_border_perimeter_ne\n## 1458906 [m]\nunits::set_units(hi_border_perimeter_ne, \"miles\")\n## 906.5 [miles]\n\n# Coastal border\nhi_ocean_border_perimeter_ne &lt;- hi_ocean_border_ne |&gt; \n  st_perimeter()\nhi_ocean_border_perimeter_ne / 2\n## 1458906 [m]\nunits::set_units(hi_ocean_border_perimeter_ne / 2, \"miles\")\n## 906.5 [miles]\n\n# Proportion\nas.numeric((hi_ocean_border_perimeter_ne / 2) / hi_border_perimeter_ne) |&gt; \n  scales::label_percent()()\n## [1] \"100%\"\n\nNorth Carolina\nThe same process works for North Carolina\n\nne_nc &lt;- ne_states |&gt; filter(name == \"North Carolina\")\nbbox_ne_nc &lt;- ne_nc |&gt; st_transform(st_crs(\"EPSG:4269\")) |&gt; st_bbox()\n\nnc_ocean_border_ne &lt;- ne_nc |&gt; \n  st_buffer(dist = 0.001) |&gt; \n  st_difference(na_giant)\n\nggplot() +\n  geom_sf(data = nc_ocean_border_ne, linewidth = 0.2) +\n  labs(title = \"North Carolina’s coastal borders\\nextracted from North America shape\") +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +\n  theme_map()\n\n\n\n\n\n\n\nHere’s that extracted coastline with the rest of the state:\n\nggplot() +\n  geom_sf(data = na_giant, fill = clrs[10], alpha = 0.4) +\n  geom_sf(data = ne_nc, linewidth = 0.1, fill = clrs[6]) +\n  geom_sf(\n    data = nc_ocean_border_ne, linewidth = 0.4, \n    aes(color = \"Coastal border extracted from North America shape\"),\n    key_glyph = draw_key_path\n  ) +\n  annotate(geom = \"text\", x = I(0.6), y = I(0.89), label = \"↑ Virginia ↑\") +\n  annotate(geom = \"text\", x = I(0.58), y = I(0.3), label = \"↓ South Carolina ↓\", angle = 313) +\n  annotate(geom = \"text\", x = I(0.14), y = I(0.5), label = \"↓ Georgia\") +\n  annotate(geom = \"text\", x = I(0.27), y = I(0.64), label = \"← Tennessee\") +\n  scale_color_manual(values = c(clrs[8])) +\n  labs(title = \"The actual North Carolina coastline\", color = NULL) +\n  coord_sf(\n    xlim = bbox_ne_nc[c(1, 3)],\n    ylim = bbox_ne_nc[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean),\n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0)),\n    legend.key.height = unit(0.5, \"lines\"),\n    legend.key = element_rect(fill = NA, color = NA),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\nNow we can calculate proportion of coastline:\n\n# State border\nnc_border_perimeter_ne &lt;- ne_nc |&gt; \n  st_perimeter()\nnc_border_perimeter_ne\n## 4095093 [m]\nunits::set_units(nc_border_perimeter_ne, \"miles\")\n## 2545 [miles]\n\n# Coastal border\nnc_ocean_border_perimeter_ne &lt;- nc_ocean_border_ne |&gt; \n  st_perimeter()\nnc_ocean_border_perimeter_ne / 2\n## 2596482 [m]\nunits::set_units(nc_ocean_border_perimeter_ne / 2, \"miles\")\n## 1613 [miles]\n\n# Proportion\nas.numeric((nc_ocean_border_perimeter_ne / 2) / nc_border_perimeter_ne) |&gt; \n  scales::label_percent()()\n## [1] \"63%\"\n\nSharp-eyed readers will notice something odd, though! Using Census maps at 1:5 million resolution, we found that 55% of North Carolina’s borders were coastline. Using Natural Earth maps at 1:10 million resolution, it becomes 63%. I’ll talk more about that discrepancy later in this post (spoiler: it’s because of the new-to-me Coastline Paradox). The detail in the two maps is different, so the amount of landmass visible along the coastline is different, yielding different perimeters and distances.\nWashington\nWhen we were using Census data, we couldn’t use this process with Washington because Canada was invisible and was being treated as an ocean. Now that we have data from Natural Earth, Canada can be accounted for and we’ll get the correct ocean-facing border.\n\nne_wa &lt;- ne_states |&gt; filter(name == \"Washington\")\nbbox_ne_wa &lt;- ne_wa |&gt; st_transform(st_crs(\"EPSG:4269\")) |&gt; st_bbox()\n\nwa_ocean_border_ne &lt;- ne_wa |&gt; \n  st_buffer(dist = 0.001) |&gt; \n  st_difference(na_giant)\n\nggplot() +\n  geom_sf(data = wa_ocean_border_ne, linewidth = 0.2) +\n  labs(title = \"Washington’s coastal borders\\nextracted from North America shape\") +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +\n  theme_map()\n\n\n\n\n\n\n\nHere’s that coastline in context:\n\nggplot() +\n  geom_sf(data = na_giant, fill = clrs[10], alpha = 0.4) +\n  geom_sf(data = ne_wa, linewidth = 0.1, fill = clrs[3]) +\n  geom_sf(\n    data = wa_ocean_border_ne, linewidth = 0.4, \n    aes(color = \"Coastal border extracted from North America shape\"),\n    key_glyph = draw_key_path\n  ) +\n  annotate(geom = \"text\", x = I(0.64), y = I(0.91), label = \"↑ Canada (British Columbia) ↑\") +\n  annotate(geom = \"text\", x = I(0.54), y = I(0.18), label = \"↓ Oregon ↓\") +\n  annotate(geom = \"text\", x = I(0.87), y = I(0.5), label = \"Idaho →\") +\n  scale_color_manual(values = c(clrs[8])) +\n  labs(title = \"The actual Washington coastline\", color = NULL) +\n  coord_sf(\n    xlim = bbox_ne_wa[c(1, 3)],\n    ylim = bbox_ne_wa[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean),\n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0)),\n    legend.key.height = unit(0.5, \"lines\"),\n    legend.key = element_rect(fill = NA, color = NA),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\nSince the northern state border is no longer seen as a coastal border, can calculate the correct proportion of coastline:\n\n# State border\nwa_border_perimeter_ne &lt;- ne_wa |&gt; \n  st_perimeter()\nwa_border_perimeter_ne\n## 3913863 [m]\nunits::set_units(wa_border_perimeter_ne, \"miles\")\n## 2432 [miles]\n\n# Coastal border\nwa_ocean_border_perimeter_ne &lt;- wa_ocean_border_ne |&gt; \n  st_perimeter()\nwa_ocean_border_perimeter_ne / 2\n## 2591575 [m]\nunits::set_units(wa_ocean_border_perimeter_ne / 2, \"miles\")\n## 1610 [miles]\n\n# Proportion\nas.numeric((wa_ocean_border_perimeter_ne / 2) / wa_border_perimeter_ne) |&gt; \n  scales::label_percent()()\n## [1] \"66%\"\n\nAll states\nNow that we know that the Natural Earth approach works, let’s apply it to all the states. We’ll nest the geographic data into a list column with a cell for each state, then extract the borders for each state\n\nne_coastline &lt;- ne_states |&gt;\n  group_by(name) |&gt; \n  nest() |&gt; \n  mutate(ocean_only = map(data, ~ {\n    .x |&gt; \n      st_buffer(dist = 0.001) |&gt; \n      st_difference(na_giant)\n  })) |&gt; \n  unnest(ocean_only) |&gt; \n  ungroup() |&gt; \n  # This special column somehow lost its specialness\n  st_set_geometry(\"geometry\") \n\n\nggplot() +\n  geom_sf(data = na_giant, linewidth = 0, fill = clrs[10], alpha = 0.5) +\n  geom_sf(\n    data = ne_coastline, linewidth = 0.2, \n    aes(color = \"Coastal border extracted from North America shape\"),\n    key_glyph = draw_key_path\n  ) +\n  scale_color_manual(values = c(clrs[8])) +\n  labs(title = \"All US coastal borders\", color = NULL) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean),\n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0)),\n    legend.key.height = unit(0.5, \"lines\"),\n    legend.key = element_rect(fill = NA, color = NA),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\nThat looks great!\nFinally, let’s put this in a table. We’ll calculate the perimeter of all the states, then join that data to the coastal data, and then calculate the proportion of coastline.\n\nne_state_border_lengths &lt;- ne_states |&gt; \n  mutate(border_length = st_perimeter(geometry)) |&gt; \n  st_drop_geometry() |&gt; \n  select(name, border_length)\n\ncoastline_length_by_state_ne &lt;- ne_coastline |&gt; \n  mutate(coastline_length = st_perimeter(geometry) / 2) |&gt; \n  st_drop_geometry() |&gt; \n  left_join(ne_state_border_lengths, by = join_by(name)) |&gt; \n  mutate(prop_coastline = as.numeric(coastline_length / border_length)) |&gt; \n  mutate(rank = rank(-prop_coastline)) |&gt; \n  mutate(\n    across(c(border_length, coastline_length), \n      list(miles = ~units::set_units(., \"miles\")))\n  ) |&gt; \n  select(\n    name, rank, prop_coastline, \n    starts_with(\"border_length\"), starts_with(\"coastline_length\")\n  )\n\nThis is fantastic! Hawaiʻi has the highest proportion of coastline, for obvious reasons, and an astounding 95% of Alaska’s borders touch the ocean, even though a huge chunk of the state shares a land border with Canada—there are just so many islands and inlets.\nIn the contiguous United States, island-y states like Florida, Michigan, and Louisiana have the highest proportion of coastline. Despite their small size, the coastal New England states like New Jersey, Massachusetts, and Rhode Island have a lot of coastline relative to the rest of their borders. Check out Rhode Island at #10—my wife’s offhand observation was pretty accurate! (These columns are sortable.)\n\nCodecoastline_length_by_state_ne |&gt; \n  select(-border_length, -coastline_length) |&gt; \n  mutate(across(c(border_length_miles, coastline_length_miles), ~as.numeric(.))) |&gt; \n  arrange(desc(prop_coastline)) |&gt; \n  gt() |&gt; \n  cols_label(\n    name ~ \"State\", \n    rank ~ \"Rank\",\n    prop_coastline ~ \"% coastline\",\n    border_length_miles ~ \"Border length (miles)\",\n    coastline_length_miles ~ \"Coastline length (miles)\"\n  ) |&gt; \n  fmt_percent(\n    columns = prop_coastline,\n    decimals = 1\n  ) |&gt; \n  fmt_number(\n    columns = ends_with(\"miles\"),\n    decimals = 0\n  ) |&gt; \n  opt_interactive(page_size_default = 16, use_compact_mode = TRUE, use_highlight = TRUE) |&gt; \n  opt_table_font(\"Libre Franklin\")\n\n\n\n\n\n\n\n\nHighlighting coastal states\nThere are 31 states (technically 30 + Washington, D.C.) that touch the ocean or one of the Great Lakes somewhere. For bonus fun, we can highlight them on a map to show what we’re working with. For the sake of plotting, and to show off one really neat feature in {tigris}, we’ll shift Alaska and Hawaiʻi around so that they’re where Mexico would be.\n\nne_states_to_plot &lt;- ne_states |&gt; \n  mutate(is_coastal = name %in% ne_coastline$name) |&gt; \n  tigris::shift_geometry()\n\nggplot() + \n  geom_sf(\n    data = ne_states_to_plot, aes(fill = is_coastal),\n    linewidth = 0.05, color = \"grey70\"\n  ) +\n  scale_fill_manual(values = c(\"white\", clrs[8]), guide = \"none\") +\n  labs(title = \"US states with a coastline\") +\n  theme_map()\n\n\n\n\n\n\n\nThat’s cool, but we can make it fancier. Right now we’ve filled all the coastal states with red, but it would be neat to fill them with different colors. We could use aes(fill = name), but that would give us 31 different indistinguishable colors.\nAnother approach would be to choose a smaller number of colors (like 5) and assign them across the coastal states. But how we allocate these colors is a little tricky. If we assign them randomly, or based on alphabetical order, or some other system, we’ll get bordering states that share the same color, which looks bad aesthetically (and reduces the map’s readability and usability). For example, here we assign the colors 1–5 to the coastal states alphabetically, and we end up with blobs like Wisconsin + Michigan + Indiana, Pennsylvania + New Jersey + Delaware, Virginia + Maryland, and North + South Carolina. That’s not great.\n\nne_states_to_plot_colors_alphabetic &lt;- ne_states |&gt; \n  mutate(is_coastal = name %in% ne_coastline$name) |&gt; \n  arrange(name, is_coastal) |&gt; \n  group_by(is_coastal) |&gt; \n  mutate(coastal_colors = case_when(\n    is_coastal ~ rep(1:5, length.out = n()),\n     .default = NA\n  )) |&gt; \n  ungroup() |&gt; \n  tigris::shift_geometry()\n\nggplot() + \n  geom_sf(\n    data = ne_states_to_plot_colors_alphabetic, aes(fill = factor(coastal_colors)),\n    linewidth = 0.05, color = \"grey70\"\n  ) +\n  scale_fill_manual(\n    values = clrs[c(1, 6, 2, 7, 8)], na.value = \"grey95\", guide = \"none\"\n  ) +\n  labs(title = \"US states with a coastline\") +\n  theme_map()\n\n\n\n\n\n\n\nThere are ways to color all US states with four or five unique colors, but we don’t really want to do that here because we’re ignoring all the non-coastal states and want a balanced set of colors.\nSo we can use a neat approach where we pretend this map is a circle. We’ll start with some state and assign it color 1. We’ll then move along the exterior of the circle clockwise and assign the next state to color 2, then the next state color 3, and so on until we loop all the way around the circle back to the first state. That should give us noncontiguous colors around the whole border.\nWe’ll omit Alaska and Hawaiʻi for this part, since we can assign them any arbitrary colors later. We’ll first find the center of each state, then find the northernmost state as the starting point, and then calculate the angle of every other state in relation to the starting state. We can then sort the data by the angle and assign the colors 1–5 in order.\n\n# Calculate the center of each coastal state in the lower 48\ncoastal_states_centers &lt;- ne_states |&gt; \n  filter(name %in% ne_coastline$name) |&gt; \n  filter(!(name %in% c(\"Alaska\", \"Hawaii\"))) |&gt; \n  mutate(centroid = st_centroid(geometry)) |&gt; \n  mutate(centroid_coords = st_coordinates(centroid)) |&gt;\n  mutate(\n    longitude = centroid_coords[, \"X\"],\n    latitude = centroid_coords[, \"Y\"]\n  ) |&gt; \n  select(-centroid_coords)\n\n# Find the northernmost state (Washington) to use as a reference point\nnorthest_state &lt;- coastal_states_centers |&gt; slice_max(order_by = latitude)\n\n# Calculate the angle from the northernmost state to the center of all other\n# states, then arrange by angle so that the states are all sorted clockwise\ncoastal_states_angles &lt;- coastal_states_centers |&gt; \n  mutate(\n    angle_rad = atan2(\n      longitude - northest_state$longitude, \n      latitude - northest_state$latitude\n    ),\n    # Not really necessary, but I can't think in radians\n    angle_deg = angle_rad * (180 / pi)\n  ) |&gt; \n  arrange(angle_rad) |&gt; \n  mutate(color_group = rep(1:5, length.out = n()))\n\nThis is really cool. California and Oregon are both directly south of Washington, so they are at a nearly −180° (or −1π rad) angle from that state. The circle then jumps over to New England (since Maine is at a 90° or π/2 rad angle from Washington) before coming back to the Midwest, then looping down along the east and south coasts, ending with Texas.\n\ncoastal_states_angles |&gt; \n  st_drop_geometry() |&gt; \n  select(name, angle_rad, angle_deg, color_group)\n##                    name angle_rad angle_deg color_group\n## 1            California    -2.960   -169.57           1\n## 2                Oregon    -2.861   -163.93           2\n## 3            Washington     0.000      0.00           3\n## 4                 Maine     1.613     92.44           4\n## 5         New Hampshire     1.676     96.01           5\n## 6         Massachusetts     1.716     98.33           1\n## 7              New York     1.728     98.99           2\n## 8          Rhode Island     1.730     99.10           3\n## 9           Connecticut     1.741     99.76           4\n## 10            Minnesota     1.751    100.35           5\n## 11             Michigan     1.766    101.19           1\n## 12            Wisconsin     1.795    102.86           2\n## 13           New Jersey     1.798    103.02           3\n## 14         Pennsylvania     1.810    103.70           4\n## 15             Delaware     1.839    105.36           5\n## 16             Maryland     1.851    106.07           1\n## 17 District of Columbia     1.858    106.44           2\n## 18                 Ohio     1.883    107.88           3\n## 19             Virginia     1.917    109.86           4\n## 20              Indiana     1.940    111.18           5\n## 21             Illinois     1.976    113.22           1\n## 22       North Carolina     1.977    113.26           2\n## 23       South Carolina     2.040    116.89           3\n## 24              Georgia     2.112    121.01           4\n## 25              Alabama     2.168    124.20           5\n## 26              Florida     2.186    125.25           1\n## 27          Mississippi     2.223    127.37           2\n## 28            Louisiana     2.317    132.77           3\n## 29                Texas     2.493    142.82           4\n\nNow that we have better color assignments, we can make a nicer map with unique colors for each state:\n\ncoastal_state_colors &lt;- coastal_states_angles |&gt; \n  st_drop_geometry() |&gt;  # No need for geography stuff \n  select(name, color_group) |&gt; \n  # Add Alaska and Hawaii back; the last color assigned was 4, so continue with 5\n  add_row(name = \"Alaska\", color_group = 5) |&gt; \n  add_row(name = \"Hawaii\", color_group = 1)\n\nne_states_to_plot_colors &lt;- ne_states |&gt; \n  left_join(coastal_state_colors, by = join_by(name)) |&gt; \n  tigris::shift_geometry()\n\nggplot() + \n  geom_sf(\n    data = ne_states_to_plot_colors, aes(fill = factor(color_group)),\n    linewidth = 0.05, color = \"grey70\"\n  ) +\n  scale_fill_manual(\n    values = clrs[c(1, 6, 2, 7, 8)], na.value = \"grey95\", guide = \"none\"\n  ) +\n  labs(title = \"US states with a coastline\") +\n  theme_map()"
  },
  {
    "objectID": "blog/2024/05/08/coastline-to-border-proportions/index.html#the-coastline-paradox-and-map-resolutions",
    "href": "blog/2024/05/08/coastline-to-border-proportions/index.html#the-coastline-paradox-and-map-resolutions",
    "title": "Calculating the proportion of US state borders that are coastlines",
    "section": "The Coastline Paradox and map resolutions",
    "text": "The Coastline Paradox and map resolutions\nCalculating these coastline lengths has been a tricky (but fun!) journey so far, involving all sorts of GIS shenanigans to extract borders from overlapping shapefiles. So far we’ve really only dealt with technical challenges, like finding the right kinds of shapefiles.\nBut in the course of writing this post and figuring all of this out, I stumbled on a deeper, more philosophical challenge: the Coastline Paradox. Weirdly, coastlines don’t have well-defined lengths. The measured length of a coastline depends on how detailed the map is, and more detail isn’t actually better.\nHypothetical coastline at different resolutions\nTo illustrate this, let’s invent a fake coastline with lots of jagged edges. If we take a boat a few miles offshore and sail from one end to the other in a straight line, it’s 500 miles long. If we walk from one end to the other and walk as close as possible to the water, however, it’s 14,000 miles long. That’s a huge discrepancy.\n\nCodewithr::with_seed(12345, {\n  example_coastline &lt;- tibble(\n    x = seq(0, 10, length.out = 500), \n    y = rnorm(500, mean = 0, sd = 0.5)\n  ) |&gt; \n    as.matrix() |&gt; \n    st_linestring()\n})\n\ntotal_distance &lt;- example_coastline |&gt; st_length() * 50\n\nggplot() + \n  geom_sf(data = example_coastline, linewidth = 0.35, color = \"grey60\") +\n  annotate(geom = \"errorbar\", xmin = 0, xmax = 10, y = 2.3) +\n  annotate(\n    geom = \"text\", x = 5, y = 2.6, \n    label = \"500 miles\", \n    family = \"Overpass ExtraBold\"\n  ) +\n  annotate(\n    geom = \"text\", x = 5, y = 2, \n    label = \"Straight distance\"\n  ) +\n  annotate(geom = \"errorbar\", xmin = 0, xmax = 10, y = -2.3) +\n  annotate(\n    geom = \"text\", x = 5, y = -2, \n    label = paste0(scales::label_comma()(total_distance), \" miles\"), \n    family = \"Overpass ExtraBold\"\n  ) +\n  annotate(\n    geom = \"text\", x = 5, y = -2.6, \n    label = \"Total distance following complete path\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n14,000 miles feels excessive, and no map includes that much detail. Maps have different resolutions that define how much detail is included, scaling 10 kilomters in real life to 1 centimeter in the map, or 500 km to 1 cm, or 110 km to 1 cm, and so on. We need to adjust the resolution of our fictional coastline and simplify and smooth out the line.\nHowever, the border length changes dramatically the more we simplify:\n\nCodelibrary(smoothr)\n## \n## Attaching package: 'smoothr'\n## The following object is masked from 'package:stats':\n## \n##     smooth\n\nexample_coastlines &lt;- tibble(\n  tolerance = c(0.2, 0.5, 1.3, 5)\n) |&gt; \n  mutate(full_coast = list(example_coastline)) |&gt; \n  mutate(simplified_coast = map2(\n    full_coast, tolerance, ~st_simplify(.x, dTolerance = .y)\n  )) |&gt; \n  mutate(simplified_coast = map(simplified_coast, ~smooth(.x, method = \"ksmooth\", smoothness = 0.6))) |&gt;\n  mutate(simplified_coast = st_sfc(simplified_coast)) |&gt; \n  mutate(distance = st_length(simplified_coast) * 50) |&gt; \n  mutate(plot = pmap(list(distance, full_coast, simplified_coast), ~{\n    ggplot() + \n      geom_sf(data = ..2, linewidth = 0.15, color = \"grey70\") +\n      geom_sf(data = ..3, linewidth = 0.6, color = clrs[2]) +\n      labs(title = paste0(scales::label_comma()(..1), \" miles\")) +\n      theme_map()\n  }))\n\nwrap_plots(example_coastlines$plot, ncol = 2)\n\n\n\n\n\n\n\nSo which one of those is right? How long is the actual coastline? Is it 14,000 miles? 5,000 miles? 2,500 miles? 1,000 miles? 500 miles? There’s no right answer.\nThe length shrinks as we decrease the resolution, and the length grows as we increase the resolution. Coastlines behave almost like fractals—as you zoom in more and more and more, the length can increase almost infinitely.1 Imagine that we want to be hyper precise when measuring the coastline and we include any boulders or trees or driftwood poking into the water from the shore. Then imagine that we include all the little bumps and imperfections in the boulders and all the edges of the shattered wood in the broken logs. The coastline will become enormously long.\n1 Not actually infinitely, since there is a finite amount of land, but still.Dubai and artificial coastlines\nThis isn’t just hypothetical. In the early 2000s, in an effort to increase the coastline available for construction, Dubai started building a series of artificial islands in the shape of palm trees, and one in the shape of the world.\n\n\nIsland developments in Dubai in 2010, by Wikipedia user Lencer\n\nHow do we count how much coastline Dubai added with these islands? Do we only count the outer circles of Palm Jumeirah, Palm Deira, and Palm Jebel Ali? The island designers cut canals through those outer circles for boat access—do we count the border for each of those segments? If so, it more than doubles the coastline. Do we count the palm fronds within each of the circles? We probably should! That then adds another huge chunk of coastline. What if the island designers added hundreds of little bays into each of the fronds, creating the illusion of separate leaves? That would exponentially increase the coastline length. Each of those tiny islands in The World archipelago should probably get added to the coastline too.\nAt the extreme, Dubai could build a single actual fractal-shaped island and become a mathematical island—there would be so much coastline that the proportion of coastline to land borders would basically be 100%."
  },
  {
    "objectID": "blog/2024/05/08/coastline-to-border-proportions/index.html#use-lower-resolution-natural-earth-coastlines",
    "href": "blog/2024/05/08/coastline-to-border-proportions/index.html#use-lower-resolution-natural-earth-coastlines",
    "title": "Calculating the proportion of US state borders that are coastlines",
    "section": "Use lower resolution Natural Earth coastlines",
    "text": "Use lower resolution Natural Earth coastlines\nEarlier, we used high resolution (1:10 million) data from Natural Earth to calculate the proportion of coastline for each state. But this might have been too ambitious. We found that 95% of Alaska’s borders touch the ocean, despite its huge land border with Canada. Because the map data had such high resolution, we picked up lots of details in each of the Aleutian Islands, as well as every little inlet and bay on the mainland.\nWhat happens if we use low resolution data instead, like Natural Earth’s 1:110 million maps? How much does that influence the rankings of the proportion of coastline?\nLet’s grab the 1:110 million data from Natural Earth and try it out.\nBut first, a warning. We won’t be able to get exact answers with this data. The key to extracting coastlines from states earlier was to use data for all of North America. Natural Earth provided state/province-level data for the United States, Mexico, and Canada, and we merged all those administrative units into one big giant landmass, which guaranteed that the individual Natural Earth state shapes matched the overall landmass perfectly. This won’t work with other Natural Earth resolutions. At 1:50 million, Natural Earth does not include Mexican states; at 1:110 million, Natural Earth only includes the United States. We can’t make a consistent unified North American landmass at these resolutions. We can get close—we can get country-level data at 1:110 million and create a merged North American shape, but it won’t match the state borders exactly.\nNevertheless, we’ll try.\n\n# Country-level data\nif (!file.exists(\"ne_data/ne_110m_admin_0_countries_lakes.shp\")) {\n  ne_download(\n    type = \"admin_0_countries_lakes\", \n    scale = 110,\n    destdir = \"ne_data\",\n    load = FALSE\n  )\n}\n\nworld_110 &lt;- ne_load(\n  type = \"admin_0_countries_lakes\", \n  scale = 110,\n  destdir = \"ne_data\"\n)\n\n# Make a unified North America shape\nna_giant_110 &lt;- world_110 |&gt; \n  filter(NAME %in% c(\"United States of America\", \"Canada\", \"Mexico\")) |&gt; \n  st_transform(crs = st_crs(\"ESRI:102003\")) |&gt; \n  st_union()\n\n# State-level data\nif (!file.exists(\"ne_data/ne_110m_admin_1_states_provinces_lakes.shp\")) {\n  ne_download(\n    type = \"admin_1_states_provinces_lakes\", \n    scale = 110,\n    destdir = \"ne_data\",\n    load = FALSE\n  )\n}\n\nstates_110 &lt;- ne_load(\n  type = \"admin_1_states_provinces_lakes\", \n  scale = 110,\n  destdir = \"ne_data\"\n) |&gt; \n  filter(admin == \"United States of America\") |&gt; \n  st_transform(crs = st_crs(\"ESRI:102003\"))\n\n\n# Rename these for consistency with the 110 versions\nstates_10 &lt;- ne_states\nna_giant_10 &lt;- na_giant\n\np1 &lt;- ggplot() + \n  geom_sf(data = na_giant_10, linewidth = 0, fill = clrs[10]) +\n  geom_sf(data = states_10, linewidth = 0.05, fill = clrs[8], color = \"white\") +\n  labs(title = \"High resolution\", subtitle = \"1:10 million\") +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean), \n    plot.subtitle = element_text(margin = margin(6.5/2, 0, 6.5, 0))\n  )\n\np2 &lt;- ggplot() + \n  geom_sf(data = na_giant_110, linewidth = 0, fill = clrs[10]) +\n  geom_sf(data = states_110, linewidth = 0.05, fill = clrs[8], color = \"white\") +\n  labs(title = \"Low resolution\", subtitle = \"1:110 million\") +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean), \n    plot.subtitle = element_text(margin = margin(6.5/2, 0, 6.5, 0))\n  )\n\np1 | p2\n\n\n\n\n\n\n\nWe can already see some sizable differences in the detail in the border. The low resolution version drops most of the Aleutian Islands, removes North Carolina’s Outer Banks, and greatly simplifies Washington’s Puget Sound.\nLet’s zoom in on Hawaiʻi for fun. The high resolution version looks great; the low resolution one looks like it was drawn in Minecraft.\n\nhi_110 &lt;- states_110 |&gt; filter(name == \"Hawaii\")\nhi_10 &lt;- states_10 |&gt; filter(name == \"Hawaii\")\n\nbbox_hi_110 &lt;- hi_110 |&gt; st_transform(st_crs(\"EPSG:4269\")) |&gt; st_bbox()\n\np1 &lt;- ggplot() +\n  geom_sf(data = hi_10, linewidth = 0.1, fill = clrs[5]) +\n  labs(title = \"High resolution\", subtitle = \"1:10 million\") +\n  coord_sf(\n    xlim = bbox_hi_110[c(1, 3)],\n    ylim = bbox_hi_110[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean), \n    plot.subtitle = element_text(margin = margin(6.5/2, 0, 6.5, 0))\n  )\n\np2 &lt;- ggplot() +\n  geom_sf(data = hi_110, linewidth = 0.1, fill = clrs[5]) +\n  labs(title = \"Low resolution\", subtitle = \"1:110 million\") +\n  coord_sf(\n    xlim = bbox_hi_110[c(1, 3)],\n    ylim = bbox_hi_110[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) + \n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean), \n    plot.subtitle = element_text(margin = margin(6.5/2, 0, 6.5, 0))\n  )\n\np1 | p2\n\n\n\n\n\n\n\nWe can go through the same process as before, extracting the coastal borders from the larger North American shape, then finding the coast-to-border proportion:\n\n# Extract the Hawaiian borders from the low resolution North America shape\nhi_ocean_border_110 &lt;- hi_110 |&gt; \n  st_buffer(dist = 0.001) |&gt; \n  st_difference(na_giant_110)\n\n# State border\nhi_border_perimeter_110 &lt;- hi_110 |&gt; \n  st_perimeter()\nhi_border_perimeter_110\n## 1078669 [m]\nunits::set_units(hi_border_perimeter_110, \"miles\")\n## 670.3 [miles]\n\n# Coastal border\nhi_ocean_border_perimeter_110 &lt;- hi_ocean_border_110 |&gt; \n  st_perimeter()\nhi_ocean_border_perimeter_110 / 2\n## 1078669 [m]\nunits::set_units(hi_ocean_border_perimeter_110 / 2, \"miles\")\n## 670.3 [miles]\n\n# Proportion\nas.numeric((hi_ocean_border_perimeter_110 / 2) / hi_border_perimeter_110) |&gt; \n  scales::label_percent()()\n## [1] \"100%\"\n\nThe simplified borders reduced the coastline by 200+ miles, but we still have the correct proportion.\n\nunits::set_units(hi_border_perimeter - hi_border_perimeter_110, \"miles\")\n## 207.9 [miles]\n\nWe can do the same thing to all the states. But we actually run into serious issues here. The shapefile for the state borders doesn’t precisely match the shapefile for the country borders, so the states don’t perfectly overlap with the large North American shape. There are small (and some large) gaps in the coastal borders. To try to account for this, we’ll add a 100 meter buffer around all the individual state borders (instead of 1 mm like before), but even then, there are some gaps.\n\nne_states_ocean_borders_110 &lt;- states_110 |&gt;\n  group_by(name) |&gt; \n  nest() |&gt; \n  mutate(ocean_only = map(data, ~ {\n    .x |&gt; \n      st_buffer(dist = 100) |&gt;\n      st_difference(na_giant_110)\n  }))\n\nne_coastline_110 &lt;- ne_states_ocean_borders_110 |&gt; \n  unnest(ocean_only) |&gt; \n  ungroup() |&gt; \n  st_set_geometry(\"geometry\")\n\nFor instance, here’s Washington, which has a big gap in the southwest border:\n\nwa_110 &lt;- states_110 |&gt; filter(name == \"Washington\")\nwa_10 &lt;- states_10 |&gt; filter(name == \"Washington\")\n\nbbox_wa_110 &lt;- wa_110 |&gt; st_transform(st_crs(\"EPSG:4269\")) |&gt; st_bbox()\n\nggplot() +\n  geom_sf(data = na_giant_110, fill = clrs[10], alpha = 0.4) +\n  geom_sf(\n    data = filter(states_110, name == \"Washington\"), \n    linewidth = 0.1, fill = clrs[3]\n  ) +\n  geom_sf(\n    data = filter(ne_coastline_110, name == \"Washington\"), linewidth = 0.8, \n    aes(color = \"Coastal border extracted from low resolution North America shape\"),\n    key_glyph = draw_key_path\n  ) +\n  annotate(geom = \"text\", x = I(0.64), y = I(0.91), label = \"↑ Canada (British Columbia) ↑\") +\n  annotate(geom = \"text\", x = I(0.54), y = I(0.18), label = \"↓ Oregon ↓\") +\n  annotate(geom = \"text\", x = I(0.87), y = I(0.5), label = \"Idaho →\") +\n  scale_color_manual(values = c(clrs[8])) +\n  labs(title = \"Low resolution Washington coastline\", color = NULL) +\n  coord_sf(\n    xlim = bbox_wa_110[c(1, 3)],\n    ylim = bbox_wa_110[c(2, 4)],\n    crs = st_crs(\"EPSG:4269\")\n  ) +\n  theme_map() +\n  theme(\n    panel.background = element_rect(fill = clr_ocean),\n    plot.title = element_text(margin = margin(6.5, 0, 6.5, 0)),\n    legend.key.height = unit(0.5, \"lines\"),\n    legend.key = element_rect(fill = NA, color = NA),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\nIf Natural Earth had 1:110 million-resolution data for US states, Mexican states, and Canadian provinces, all would be well. Alas.\nSo with the caveat that there are sizable gaps in the coastal borders, let’s look at the proportions:\n\n# Find all low-resolution state perimeters\nne_state_border_lengths_110 &lt;- states_110 |&gt; \n  mutate(border_length = st_perimeter(geometry)) |&gt; \n  st_drop_geometry() |&gt; \n  select(name, border_length)\n\n# Calculate coastline perimeters and ratio of coast to border\ncoastline_length_by_state_ne_110 &lt;- ne_coastline_110 |&gt; \n  mutate(coastline_length = st_perimeter(geometry) / 2) |&gt; \n  st_drop_geometry() |&gt; \n  left_join(ne_state_border_lengths_110, by = join_by(name)) |&gt; \n  # Cheat a little because, with the 100 meter buffer, Hawaii's coastline is\n  # slightly larger than its border\n  mutate(border_length = case_when(\n    name == \"Hawaii\" ~ coastline_length,\n    .default = border_length\n  )) |&gt; \n  mutate(prop_coastline = as.numeric(coastline_length / border_length)) |&gt; \n  mutate(rank = rank(-prop_coastline)) |&gt; \n  mutate(\n    across(c(border_length, coastline_length), \n      list(miles = ~units::set_units(., \"miles\")))\n  ) |&gt; \n  select(\n    name, rank, prop_coastline, \n    starts_with(\"border_length\"), starts_with(\"coastline_length\")\n  )\n\nThe proportions and rankings changed a bit here. Alaska is now only 83% coastline instead of 95%. Michigan and Florida are still the top coastal states from the contiguous United States, but the New England states dropped substantially. Since the complex inlets and islands in New England were smoothed over, California rose from #14 to #6. (These columns are sortable.)\n\nCodecoastline_length_by_state_ne_110 |&gt; \n  select(-border_length, -coastline_length) |&gt; \n  mutate(across(c(border_length_miles, coastline_length_miles), ~as.numeric(.))) |&gt; \n  arrange(desc(prop_coastline)) |&gt; \n  gt() |&gt; \n  cols_label(\n    name ~ \"State\", \n    rank ~ \"Rank\",\n    prop_coastline ~ \"% coastline\",\n    border_length_miles ~ \"Border length (miles)\",\n    coastline_length_miles ~ \"Coastline length (miles)\"\n  ) |&gt; \n  fmt_percent(\n    columns = prop_coastline,\n    decimals = 1\n  ) |&gt; \n  fmt_number(\n    columns = ends_with(\"miles\"),\n    decimals = 0\n  ) |&gt; \n  opt_interactive(page_size_default = 16, use_compact_mode = TRUE, use_highlight = TRUE) |&gt; \n  opt_table_font(\"Libre Franklin\")\n\n\n\n\n\n\n\n\n\nWe’ll make one more table to make it easier to compare the rankings and proportions for the different resolutions.2 (These columns are sortable, by the way.)\n2 Wikipedia has a similar table with different rankings based on different maps and projections and resolutions, though they don’t rank the coast-to-border proportion like we’ve been doing.\nCodetbl_10 &lt;- coastline_length_by_state_ne |&gt; \n  select(\n    name, rank_10 = rank, prop_coastline_10 = prop_coastline,\n    border_length_10 = border_length_miles, coastline_length_10 = coastline_length_miles\n  )\n\ntbl_110 &lt;- coastline_length_by_state_ne_110 |&gt; \n  select(\n    name, rank_110 = rank, prop_coastline_110 = prop_coastline,\n    border_length_110 = border_length_miles, coastline_length_110 = coastline_length_miles\n  )\n\ntbl_10 |&gt; \n  left_join(tbl_110, by = join_by(name)) |&gt; \n  mutate(across(contains(\"length\"), ~as.numeric(.))) |&gt; \n  arrange(rank_10) |&gt; \n  gt() |&gt; \n  cols_label(\n    name ~ \"State\", \n    rank_10 ~ \"Rank\",\n    rank_110 ~ \"Rank\",\n    prop_coastline_10 ~ \"% coastline\",\n    prop_coastline_110 ~ \"% coastline\",\n    border_length_10 ~ \"Border length (miles)\",\n    border_length_110 ~ \"Border length (miles)\",\n    coastline_length_10 ~ \"Coastline length (miles)\",\n    coastline_length_110 ~ \"Coastline length (miles)\"\n  ) |&gt; \n  sub_missing() |&gt; \n  tab_spanner(\n    label = \"High resolution (1:10 million)\",\n    columns = ends_with(\"_10\")\n  ) |&gt; \n  tab_spanner(\n    label = \"Low resolution (1:110 million)\",\n    columns = ends_with(\"_110\")\n  ) |&gt; \n  fmt_percent(\n    columns = starts_with(\"prop_\"),\n    decimals = 1\n  ) |&gt; \n  fmt_number(\n    columns = contains(\"length\"),\n    decimals = 0\n  ) |&gt; \n  opt_interactive(use_pagination = FALSE, use_compact_mode = TRUE, use_highlight = TRUE) |&gt; \n  opt_table_font(\"Libre Franklin\")"
  },
  {
    "objectID": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html",
    "href": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html",
    "title": "Guide to generating and rendering computational markdown content programmatically with Quarto",
    "section": "",
    "text": "This year, I’ve helped build the Idaho Secretary of State’s office’s election results website for both the primary and general elections. Working with election data is a complex process, with each precinct reporting results to their parent counties, which all use different systems and software and candidate identifiers. Those county results then go into a central state-level database that state officials have access to for analysis and reporting.\nIn 2024, Idaho used a Quarto website to present the results for each statewide, congressional, and legislative contest (the URL is results.voteidaho.gov, though the Quarto website part probably won’t live there forever):\nI may write something up someday about the process of building the website, depending on NDAs and security and copyright arrangements and whatnot. There are some neat technical details involved in the whole process, like complex static {targets} branching, remote {targets} storage, and replicating the structure of the real, live results database with a local DuckDB database for testing things without connecting to the live database. The short, sanitized version is that it uses two {targets} pipelines:\nThe two pipelines run independently of each other every few minutes, and thanks to the magic of {targets}, if there are no updates (i.e. if there’s a lull in the reporting on election night), nothing needs to rebuild. It’s really neat.\nThere’s one Quarto/R Markdown trick that I used extensively when building the site: it’s possible to use R to automatically generate Quarto markdown before the entire document runs, allowing you to create parameterized templates for repeated elements.\nEach race uses a tabset to show panels for (1) a table and (2) an interactive map of the results, and the reporting status for all the counties involved in the race is included in a callout block.\nThe markdown for each of these race results sections looks something like this:\nFor pages where there’s only one race, like the presidential election and the state’s constitutional amendment election, it’s trivial enough to just copy/paste that general template and replace the corresponding R code. But for the state-level legislative page, there are dozens of races. Repeating and modifying all that markdown 100+ times would be miserable. So instead, we programmatically generate the markdown for each race before the site is rendered so that Quarto thinks it’s working with hand-typed markdown.\nGenerating big chunks of markdown like this is a really cool approach with all sorts of applications (generate sections of a website; generate panel tabsets; generate presentation slides; etc.), but it’s a little unwieldly at first. So in this post, I’ll (1) show why this is trickier than just using regular R chunks with results=\"asis\", (2) present a detailed step-by-step explanation of how to pre-render generated computational chunks, (3) provide a shorter, simpler, less-annotated example, and (4) give a more complex, less-annotated example."
  },
  {
    "objectID": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#why-not-just-use-resultsasis",
    "href": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#why-not-just-use-resultsasis",
    "title": "Guide to generating and rendering computational markdown content programmatically with Quarto",
    "section": "Why not just use results=\"asis\"?",
    "text": "Why not just use results=\"asis\"?\nIt’s easy to use R/Python chunks to generate HTML or LaTeX or Typst or markdown and have that output appear in the rendered document—this is essentially what table-making packages like {tinytable}, {gt}, and {kableExtra} all do. To illustrate this, let’s load {gapminder} data:\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(gapminder)\n\ngapminder_2007 &lt;- gapminder |&gt; filter(year == 2007)\n\nWe can create a markdown list of all the continents in the dataset. Here I do it with paste0(), but I could also use a nicer wrapper like {pander}, which includes all sorts of functions for generating markdown.\nThis correctly makes a list, but it doesn’t get rendered like a list—it’s displayed as code chunk output:\n\ncontinents &lt;- gapminder_2007 |&gt; \n  distinct(continent) |&gt; \n  pull(continent)\n\ncat(paste0(\"- \", continents, collapse = \"\\n\"))\n\n- Asia\n- Europe\n- Africa\n- Americas\n- Oceania\n\n\nWe can tell Quarto to treat the output of that chunk as raw markdown instead by setting the results=\"asis\" chunk option:\n\n```{r}\n#| results: asis\ncat(paste0(\"- \", continents, collapse = \"\\n\"))\n```\n\nAsia\nEurope\nAfrica\nAmericas\nOceania\n\n\nThat’s great and normal and I use this approach all the time for generating non-computational markdown.\nWhere this doesn’t work is when you have R chunks that need to be computed. To show this, let’s make a list showing π rounded to different digits using inline code chunks. We can manually type it like this:\n- `r round(pi, 1)`\n- `r round(pi, 2)`\n- `r round(pi, 3)`\n- `r round(pi, 4)`\n- `r round(pi, 5)`\n…which renders like this:\n\n3.1\n3.14\n3.142\n3.1416\n3.1416\n\nBut that’s a lot of typing. So let’s generate it automatically. I’ll do this in a data frame, just because I like working that way, but you could also use standalone vectors (or even—gasp—a loop!):\n\npi_stuff &lt;- tibble(digits = 1:5) |&gt; \n  mutate(list_element = paste0(\"- `r round(pi, \", digits, \")`\"))\n\n# Everything is in the list_element column:\npi_stuff\n## # A tibble: 5 × 2\n##   digits list_element      \n##    &lt;int&gt; &lt;chr&gt;             \n## 1      1 - `r round(pi, 1)`\n## 2      2 - `r round(pi, 2)`\n## 3      3 - `r round(pi, 3)`\n## 4      4 - `r round(pi, 4)`\n## 5      5 - `r round(pi, 5)`\n\nWe can put that column in a results=\"asis\" chunk…\n\n```{r}\n#| results: asis\ncat(paste0(pi_stuff$list_element, collapse = \"\\n\"))\n```\n\nr round(pi, 1)\nr round(pi, 2)\nr round(pi, 3)\nr round(pi, 4)\nr round(pi, 5)\n\n\n…and it renders correctly as markdown, but it doesn’t run the inline chunks :(\nThis is because of an issue with ordering: Quarto renders the chunk with cat(paste0(...)) and then moves on to the next chunk in the document. It won’t render the R chunks that the pi_stuff$list_element object contains because they’re all nested inside the parent chunk, and Quarto’s rendering process has moved on by the time the newly generated R chunks appear.\nThe trick is to pre-render the chunks before they officially show up in the document.1 We can feed the collapsed pi_stuff$list_element object to knitr::knit() in an inline chunk, which makes Quarto render all the R chunks inside the chunk first, then place the output in the document to be rendered in the correct order like normal chunks:\n1 I haven’t found this formally documented anywhere—I stumbled across this approach in this gist from 2015.Here's some regular markdown text. Let's show a list of \ndifferently-rounded values of $\\pi$ for fun:\n\n`r knitr::knit(text = paste0(pi_stuff$list_element, collapse = \"\\n\"))`\n\nIsn't that neat?\nThat markdown will render to this:\n\n\n\n\n\n\nHere’s some regular markdown text. Let’s show a list of differently-rounded values of \\(\\pi\\) for fun:\n\n3.1\n3.14\n3.142\n3.1416\n3.1416\n\nIsn’t that neat?"
  },
  {
    "objectID": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#building-a-panel-tabset-with-an-inline-chunk",
    "href": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#building-a-panel-tabset-with-an-inline-chunk",
    "title": "Guide to generating and rendering computational markdown content programmatically with Quarto",
    "section": "Building a panel tabset with an inline chunk",
    "text": "Building a panel tabset with an inline chunk\nTechnically there was no need to use knitr::knit() in an inline chunk for that previous example. It would be easier to generate the text output within the pi_stuff data frame instead of in a bunch of inline chunks, and then show the results like normal with results=\"asis\"\n\n```{r}\n#| results: asis\npi_stuff_easier &lt;- tibble(digits = 1:5) |&gt; \n  mutate(list_element = paste0(\"- \", round(pi, digits)))\n\ncat(paste0(pi_stuff_easier$list_element, collapse = \"\\n\"))\n```\n\n3.1\n3.14\n3.142\n3.1416\n3.14159\n\n\nHowever, using knitr::knit(text = BLAH) in an inline chunk like this is a powerful trick that lets you do all sorts of more complex document generation automation. Let’s make a more complicated example with real data instead of a bunch of π rounding.\nFor this example, let’s make a panel tabset with a plot for each continent in the gapminder_2007 dataset we made earlier.\nFirst, we’ll make a list of plots. This can be done any number of ways—I like using group_by() |&gt; nest() and {purrr} functions like map(), but any way will work as long as you have a list of ggplot objects in the end.\n\ncontinents_plots &lt;- gapminder_2007 |&gt; \n  group_by(continent) |&gt; \n  nest() |&gt; \n  ungroup() |&gt; \n  # We could use map2(), but I like using pmap() just in case I need to\n  # expand it beyond 2 things\n  mutate(plot = pmap(\n    lst(data, continent), \n    \\(data, continent) {\n      plot_title &lt;- paste0(\"Health and wealth in \", continent)\n      ggplot(data, aes(x = gdpPercap, y = lifeExp)) +\n        geom_point() +\n        scale_x_log10(labels = scales::label_dollar(accuracy = 1)) +\n        labs(title = plot_title)\n    }))\n\ncontinents_plots\n## # A tibble: 5 × 3\n##   continent data              plot  \n##   &lt;fct&gt;     &lt;list&gt;            &lt;list&gt;\n## 1 Asia      &lt;tibble [33 × 5]&gt; &lt;gg&gt;  \n## 2 Europe    &lt;tibble [30 × 5]&gt; &lt;gg&gt;  \n## 3 Africa    &lt;tibble [52 × 5]&gt; &lt;gg&gt;  \n## 4 Americas  &lt;tibble [25 × 5]&gt; &lt;gg&gt;  \n## 5 Oceania   &lt;tibble [2 × 5]&gt;  &lt;gg&gt;\n\nThe plots are all in the plot column in continents_plots, which is a list of ggplot objects. Here’s one of them:\n\ncontinents_plots$plot[[3]]\n\n\n\n\n\n\n\nTo make a tabset with a panel for each continent, we need to write markdown like this:\n::: {.panel-tabset}\n### Continent 1\n\n```{r}\n#| label: panel-continent-a\n#| echo: false\ncontinents_plots$plot[[1]]\n```\n\n### Continent 2\n\n```{r}\n#| label: panel-continent-b\n#| echo: false\ncontinents_plots$plot[[2]]\n```\n\n### (…and so on…)\n\n:::\nWe could just copy/paste those continent sections over and over, but that’s tedious and not very dynamic. Instead, we can create a little markdown template for each panel and generate all these chunks. To do that, we’ll use {glue}, which is a lot nicer for building strings than using paste0(), since it uses Python-style string interpolation. glue::glue() replaces any text inside {}s with the corresponding variable value:\n\n# Some values\nanimals &lt;- \"cats\"\nnumber &lt;- 12\nlocation &lt;- \"house\"\n\n# Ugly paste() way\npaste0(\"There are \", number, \" blue \", animals, \" in the \", location)\n## [1] \"There are 12 blue cats in the house\"\n\n# Nice glue() way\nglue(\"There are {number} blue {animals} in the {location}\")\n## There are 12 blue cats in the house\n\nIf you need to use literal curly braces in the text, you can either double them or change the delimiters:\n\npkg_name &lt;- \"ggplot2\"\n\n# The curly braces disappear\nglue(\"The {pkg_name} package is delightful\")\n## The ggplot2 package is delightful\n\n# Double them to keep them\nglue(\"The {{{pkg_name}}} package is delightful\")\n## The {ggplot2} package is delightful\n\n# Or change the delimiter\nglue(\"The {&lt;&lt;pkg_name&gt;&gt;} package is delightful\", .open = \"&lt;&lt;\", .close = \"&gt;&gt;\")\n## The {ggplot2} package is delightful\n\nBeing able to change the delimiter is useful since we’ll need to generate chunks that start with ```{r}.\nWe can use glue() in a function that takes a continent name and a row number and generates a markdown tabset panel:\n\nbuild_panel &lt;- function(panel_title, plot_index) {\n  chunk_label &lt;- glue(\"panel-continent-{title}\", title = janitor::make_clean_names(panel_title))\n\n  output &lt;- glue(\"\n  ### &lt;&lt;panel_title&gt;&gt;\n\n  ```{r}\n  #| label: &lt;&lt;chunk_label&gt;&gt;\n  #| echo: false\n  continents_plots$plot[[&lt;&lt;plot_index&gt;&gt;]]\n  ```\", .open = \"&lt;&lt;\", .close = \"&gt;&gt;\")\n\n  output\n}\n\nFirst let’s make sure it works by itself:\n\nbuild_panel(\"Africa\", 3)\n\n### Africa\n\n```{r}\n#| label: panel-continent-africa\n#| echo: false\ncontinents_plots$plot[[3]]\n```\n\n\nYep!\nNow we can iterate through the data frame of all the continents (continents_plots) and make a column that contains the markdown panel text.\n\ncontinents_plots_with_text &lt;- continents_plots |&gt; \n  mutate(row = row_number()) |&gt; \n  mutate(markdown = pmap_chr(\n    lst(continent, row), \n    \\(continent, row) build_panel(panel_title = continent, plot_index = row)\n  ))\n\n\ncontinents_plots_with_text\n\n# A tibble: 5 × 5\n  continent data              plot     row markdown                                                                                                   \n  &lt;fct&gt;     &lt;list&gt;            &lt;list&gt; &lt;int&gt; &lt;chr&gt;                                                                                                      \n1 Asia      &lt;tibble [33 × 5]&gt; &lt;gg&gt;       1 \"### Asia\\n\\n```{r}\\n#| label: panel-continent-asia\\n#| echo: false\\ncontinents_plots$plot[[1]]\\n```\"      \n2 Europe    &lt;tibble [30 × 5]&gt; &lt;gg&gt;       2 \"### Europe\\n\\n```{r}\\n#| label: panel-continent-europe\\n#| echo: false\\ncontinents_plots$plot[[2]]\\n```\"  \n3 Africa    &lt;tibble [52 × 5]&gt; &lt;gg&gt;       3 \"### Africa\\n\\n```{r}\\n#| label: panel-continent-africa\\n#| echo: false\\ncontinents_plots$plot[[3]]\\n```\"  \n4 Americas  &lt;tibble [25 × 5]&gt; &lt;gg&gt;       4 \"### Americas\\n\\n```{r}\\n#| label: panel-continent-americas\\n#| echo: false\\ncontinents_plots$plot[[4]]\\n`…\n5 Oceania   &lt;tibble [2 × 5]&gt;  &lt;gg&gt;       5 \"### Oceania\\n\\n```{r}\\n#| label: panel-continent-oceania\\n#| echo: false\\ncontinents_plots$plot[[5]]\\n```\"\n\n\nCheck out that new markdown column—it has a third level heading with the continent name, followed by an R chunk that will display the corresponding plot object from continents_plots. Here’s what one panel looks like:\n\ncat(continents_plots_with_text$markdown[[1]])\n\n### Asia\n\n```{r}\n#| label: panel-continent-asia\n#| echo: false\ncontinents_plots$plot[[1]]\n```\n\n\nFinally, we need to concatenate that column into one big string and include it as an inline chunk inside Quarto’s syntax for tabsets:\nHealth and wealth are related in each continent.\n\n::: {.panel-tabset}\n\n`r knitr::knit(text = paste0(continents_plots_with_text$markdown, collapse = \"\\n\\n\"))`\n\n:::\n\nAutomatic tabset panels!\nHere’s what it looks like when rendered:\n\n\n\n\n\n\nHealth and wealth are related in each continent.\n\n\nAsia\nEurope\nAfrica\nAmericas\nOceania\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic tabset panels!\n\n\n\nPerfect! We successfully generated a bunch of R chunks, pre-rendered them with Quarto, and then rendered the rest of the document."
  },
  {
    "objectID": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#condensed-example-showing-the-evolution-of-a-ggplot-plot",
    "href": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#condensed-example-showing-the-evolution-of-a-ggplot-plot",
    "title": "Guide to generating and rendering computational markdown content programmatically with Quarto",
    "section": "Condensed example showing the evolution of a ggplot plot",
    "text": "Condensed example showing the evolution of a ggplot plot\nNow that we’ve walked through the general process in detail, we’ll look at a less didactic example. Suppose you want to show the step-by-step process of creating a ggplot plot in a tabset panel or in a Revealjs Quarto slideshow. You could manually copy/paste a bunch of markdown over and over (ew), or you could generate chunks and make Quarto make the panels or slides for you (yay).\nFirst we’ll make a list of plots. There’s actually a neat new package—{ggreveal}—that can create a list of intermediate plots automatically, but I’ll just do it manually here (though this process will work the same with {ggreveal}).\n\np1 &lt;- ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point()\np1_text &lt;- glue(\"\n  ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n    geom_point()\")\n\np2 &lt;- p1 + \n  scale_x_log10(labels = scales::dollar_format(accuracy = 1))\np2_text &lt;- glue(\"\n  {p1_text} +\n    scale_x_log10(labels = scales::dollar_format(accuracy = 1))\n\")\n\np3 &lt;- p2 + \n  scale_color_viridis_d(option = \"plasma\", end = 0.9)\np3_text &lt;- glue('\n  {p2_text} +\n    scale_color_viridis_d(option = \"plasma\", end = 0.9)\n')\n\np4 &lt;- p3 + \n  theme_minimal()\np4_text &lt;- glue('\n  {p3_text} +\n    theme_minimal()\n')\n\np5 &lt;- p4 + \n  labs(x = \"GDP per capita\", y = \"Life expectancy\", color = \"Continent\")\np5_text &lt;- glue('\n  {p4_text} +\n    labs(x = \"GDP per capita\", y = \"Life expectancy\", color = \"Continent\")\n')\n\nplot_list &lt;- list(p1, p2, p3, p4, p5)\nplot_text &lt;- list(p1_text, p2_text, p3_text, p4_text, p5_text)\n\nplot_list &lt;- tribble(\n  ~plot, ~code_text, ~description,\n  p1, p1_text, \"Start with the initial plot…\",\n  p2, p2_text, \"…use a logarithmic x-axis…\",\n  p3, p3_text, \"…change the color palette…\",\n  p4, p4_text, \"…change the theme…\",\n  p5, p5_text, \"…and change the default labels\"\n)\n\nWe want the overall tabset to look something like this:\n::: {.panel-tabset}\n\n### Step 1\n\nShort description\n\n```{r}\nplot_list[[1]]\n```\n\n```r\n# Code here\n```\n\n### Step 2\n\nShort description\n\n```{r}\nplot_list[[2]]\n```\n\n```r\n# Code here\n```\n\n### …and so on\n\n:::\n…so next we’ll generate markdown for each panel:\n\npanels &lt;- map_chr(\n  seq_len(nrow(plot_list)), \n  \\(i) {\n    glue(\"\n    ### Step &lt;&lt;i&gt;&gt;\n\n    `r plot_list$description[[&lt;&lt;i&gt;&gt;]]`\n\n    ```{r}\n    #| label: plot-panel-&lt;&lt;i&gt;&gt;\n    #| echo: false\n    plot_list$plot[[&lt;&lt;i&gt;&gt;]]\n    ```\n\n    ```r\n    `r plot_list$code_text[[&lt;&lt;i&gt;&gt;]]`\n    ```\", .open = \"&lt;&lt;\", .close = \"&gt;&gt;\")\n  }\n)\n\nFinally we’ll wrap Quarto’s special markdown syntax for tabsets around these panels and then include the combined text as an inline chunk in the document:\nLet's slowly build up the plot:\n\n::: {.panel-tabset}\n\n`r knitr::knit(text = paste0(panels, collapse = \"\\n\\n\"))`\n\n:::\n\n\n\n\n\n\nLet’s slowly build up the plot:\n\n\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\n\n\n\nStart with the initial plot…\n\n\n\n\n\n\n\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point()\n\n\n…use a logarithmic x-axis…\n\n\n\n\n\n\n\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10(labels = scales::dollar_format(accuracy = 1))\n\n\n…change the color palette…\n\n\n\n\n\n\n\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10(labels = scales::dollar_format(accuracy = 1)) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.9)\n\n\n…change the theme…\n\n\n\n\n\n\n\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10(labels = scales::dollar_format(accuracy = 1)) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.9) +\n  theme_minimal()\n\n\n…and change the default labels\n\n\n\n\n\n\n\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10(labels = scales::dollar_format(accuracy = 1)) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.9) +\n  theme_minimal() +\n  labs(x = \"GDP per capita\", y = \"Life expectancy\", color = \"Continent\")"
  },
  {
    "objectID": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#condensed-example-of-continent-level-mini-reports",
    "href": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#condensed-example-of-continent-level-mini-reports",
    "title": "Guide to generating and rendering computational markdown content programmatically with Quarto",
    "section": "Condensed example of continent-level mini reports",
    "text": "Condensed example of continent-level mini reports\nFinally, let’s look at one more example that’s similar to what I used for the Idaho election results website, making a sort of miniature report for each continent. This time, instead of creating a tabset panel for each continent, we’ll make a whole markdown section for each continent, with a tabset panel included in each.\nEach continent section will look something like this:\n### Continent name\n\n::: {.callout-note icon=\"false\" title=\"X countries\" collapse=\"true\"}\nComma-separated list of countries\n:::\n\n::: {.panel-tabset}\n#### Details\n\n```{r}\n#| label: table-summary-continent\n#| echo: false\n\n# A table showing average GDP per capita and average life expectancy\n```\n\n#### Plot\n\n```{r}\n#| label: plot-summary-continent\n#| echo: false\n\n# A plot showing the relationship between GDP per capita and life expectancy\n```\n:::\nAs before, we’ll translate this template into a glue() string, feed some data into it, and generate a bunch of R chunks.\n\n\n\n\n\n\nChild documents\n\n\n\nThis template is getting gnarly with so many moving parts and so many common {glue} delimiters like {}s and []s and &lt;&gt;s. An alternative approach is to put the template in a separate child Quarto document and then pre-render it with knitr::knit_child(), which behaves just like the inline knitr::knit(file = BLAH) approach we’ve been using. Quarto has an official example of how to do it here: example and code.\n\n\nFirst, we’ll make a data frame with all the different pieces we want to include, using the same group_by(continent) |&gt; nest() approach as before:\n\ncontinent_report_items &lt;- gapminder_2007 |&gt; \n  group_by(continent) |&gt; \n  nest() |&gt; \n  ungroup() |&gt; \n  mutate(country_list = map_chr(data, \\(x) knitr::combine_words(x$country))) |&gt; \n  mutate(n_countries = map_int(data, \\(x) nrow(x))) |&gt; \n  mutate(summary_details = map(data, \\(x) {\n    x |&gt; \n      summarize(\n        `Average GDP per capita` = mean(gdpPercap),\n        `Average life expectancy` = mean(lifeExp),\n        `Average population` = mean(pop)\n      ) |&gt; \n      pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\") |&gt; \n      mutate(Value = scales::comma_format()(Value))\n  })) |&gt; \n  mutate(plot = map(data, \\(x) {\n    x |&gt; \n      ggplot(aes(x = gdpPercap, y = lifeExp)) +\n        geom_point() +\n        scale_x_log10(labels = scales::label_dollar(accuracy = 1)) +\n        labs(title = glue(\"Health and wealth in {continent}\"))\n  }))\n\nNext we’ll make a function that generates the markdown output:\n\nbuild_continent_report &lt;- function(i) {\n  name_for_labels &lt;- janitor::make_clean_names(continent_report_items$continent[[i]])\n\n  # Quarto and RStudio and Positron all really struggle with syntax highlighting \n  # and parsing when there are multiple ```s inside a string, so we can make \n  # life easier by splitting the output into a few parts here, ensuring that \n  # there's a maximum of one set of triple backticks\n\n  output_first_part &lt;- glue('\n  ### `r continent_report_items$continent[[&lt;&lt;i&gt;&gt;]]`\n\n  ::: {.callout-note icon=\"false\" title=\"`r continent_report_items$n_countries[[&lt;&lt;i&gt;&gt;]]` countries\" collapse=\"true\"}\n  `r continent_report_items$country_list[[&lt;&lt;i&gt;&gt;]]`\n  :::', .open = \"&lt;&lt;\", .close = \"&gt;&gt;\")\n\n  output_panel_details &lt;- glue('\n  #### Details\n\n  ```{r}\n  #| label: table-summary-&lt;&lt;name_for_labels&gt;&gt;\n  #| echo: false\n\n  continent_report_items$summary_details[[&lt;&lt;i&gt;&gt;]] |&gt;\n    knitr::kable()\n  ```', .open = \"&lt;&lt;\", .close = \"&gt;&gt;\")\n\n  output_panel_plot &lt;- glue('\n  #### Plot\n\n  ```{r}\n  #| label: plot-summary-&lt;&lt;name_for_labels&gt;&gt;\n  #| echo: false\n\n  continent_report_items$plot[[&lt;&lt;i&gt;&gt;]]\n  ```', .open = \"&lt;&lt;\", .close = \"&gt;&gt;\")\n\n  # Combine all the pieces\n  output &lt;- glue('\n  {output_first_part}\n\n  ::: {{.panel-tabset}}\n  {output_panel_details}\n\n  {output_panel_plot}\n  :::\n  ')\n\n  output\n}\n\nFinally, we’ll loop through each row in continent_report_items and generate the markdown report using the template…\n\ncontinent_reports &lt;- map_chr(\n  seq_len(nrow(continent_report_items)), \n  \\(i) build_continent_report(i)\n)\n\n…and include all the generated markdown in an inline chunk:\n## Continent reports\n\nCheck out all these automatically generated continent reports!\n\n`r knitr::knit(text = paste0(continent_reports, collapse = \"\\n\\n\"))`\nThat single inline chunk automatically generates dozens of inline and block chunks of R code before the full document goes through Quarto, which means all this output gets included in the final rendered version:\n\n\n\n\n\n\nGenerated output\n\n\nContinent reports\nCheck out all these automatically generated continent reports!\nAsia\n\n\n\n\n\n\n33 countries\n\n\n\n\n\nAfghanistan, Bahrain, Bangladesh, Cambodia, China, Hong Kong, China, India, Indonesia, Iran, Iraq, Israel, Japan, Jordan, Korea, Dem. Rep., Korea, Rep., Kuwait, Lebanon, Malaysia, Mongolia, Myanmar, Nepal, Oman, Pakistan, Philippines, Saudi Arabia, Singapore, Sri Lanka, Syria, Taiwan, Thailand, Vietnam, West Bank and Gaza, and Yemen, Rep.\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n12,473\n\n\nAverage life expectancy\n71\n\n\nAverage population\n115,513,752\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEurope\n\n\n\n\n\n\n30 countries\n\n\n\n\n\nAlbania, Austria, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Czech Republic, Denmark, Finland, France, Germany, Greece, Hungary, Iceland, Ireland, Italy, Montenegro, Netherlands, Norway, Poland, Portugal, Romania, Serbia, Slovak Republic, Slovenia, Spain, Sweden, Switzerland, Turkey, and United Kingdom\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n25,054\n\n\nAverage life expectancy\n78\n\n\nAverage population\n19,536,618\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\n\n\n\n\n\n\n52 countries\n\n\n\n\n\nAlgeria, Angola, Benin, Botswana, Burkina Faso, Burundi, Cameroon, Central African Republic, Chad, Comoros, Congo, Dem. Rep., Congo, Rep., Cote d’Ivoire, Djibouti, Egypt, Equatorial Guinea, Eritrea, Ethiopia, Gabon, Gambia, Ghana, Guinea, Guinea-Bissau, Kenya, Lesotho, Liberia, Libya, Madagascar, Malawi, Mali, Mauritania, Mauritius, Morocco, Mozambique, Namibia, Niger, Nigeria, Reunion, Rwanda, Sao Tome and Principe, Senegal, Sierra Leone, Somalia, South Africa, Sudan, Swaziland, Tanzania, Togo, Tunisia, Uganda, Zambia, and Zimbabwe\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n3,089\n\n\nAverage life expectancy\n55\n\n\nAverage population\n17,875,763\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmericas\n\n\n\n\n\n\n25 countries\n\n\n\n\n\nArgentina, Bolivia, Brazil, Canada, Chile, Colombia, Costa Rica, Cuba, Dominican Republic, Ecuador, El Salvador, Guatemala, Haiti, Honduras, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, Puerto Rico, Trinidad and Tobago, United States, Uruguay, and Venezuela\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n11,003\n\n\nAverage life expectancy\n74\n\n\nAverage population\n35,954,847\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOceania\n\n\n\n\n\n\n2 countries\n\n\n\n\n\nAustralia and New Zealand\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n29,810\n\n\nAverage life expectancy\n81\n\n\nAverage population\n12,274,974\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n↑ that’s how we were able to generate race-specific output for 100+ individual contests for the 2024 Idaho elections. It looks messy at first, but it’s a billion times easier to work with than copying/pasting markdown text and manually modifying the code in the chunks."
  },
  {
    "objectID": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#continent-reports",
    "href": "blog/2024/11/04/render-generated-r-chunks-quarto/index.html#continent-reports",
    "title": "Guide to generating and rendering computational markdown content programmatically with Quarto",
    "section": "Continent reports",
    "text": "Continent reports\nCheck out all these automatically generated continent reports!\nAsia\n\n\n\n\n\n\n33 countries\n\n\n\n\n\nAfghanistan, Bahrain, Bangladesh, Cambodia, China, Hong Kong, China, India, Indonesia, Iran, Iraq, Israel, Japan, Jordan, Korea, Dem. Rep., Korea, Rep., Kuwait, Lebanon, Malaysia, Mongolia, Myanmar, Nepal, Oman, Pakistan, Philippines, Saudi Arabia, Singapore, Sri Lanka, Syria, Taiwan, Thailand, Vietnam, West Bank and Gaza, and Yemen, Rep.\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n12,473\n\n\nAverage life expectancy\n71\n\n\nAverage population\n115,513,752\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEurope\n\n\n\n\n\n\n30 countries\n\n\n\n\n\nAlbania, Austria, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Czech Republic, Denmark, Finland, France, Germany, Greece, Hungary, Iceland, Ireland, Italy, Montenegro, Netherlands, Norway, Poland, Portugal, Romania, Serbia, Slovak Republic, Slovenia, Spain, Sweden, Switzerland, Turkey, and United Kingdom\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n25,054\n\n\nAverage life expectancy\n78\n\n\nAverage population\n19,536,618\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfrica\n\n\n\n\n\n\n52 countries\n\n\n\n\n\nAlgeria, Angola, Benin, Botswana, Burkina Faso, Burundi, Cameroon, Central African Republic, Chad, Comoros, Congo, Dem. Rep., Congo, Rep., Cote d’Ivoire, Djibouti, Egypt, Equatorial Guinea, Eritrea, Ethiopia, Gabon, Gambia, Ghana, Guinea, Guinea-Bissau, Kenya, Lesotho, Liberia, Libya, Madagascar, Malawi, Mali, Mauritania, Mauritius, Morocco, Mozambique, Namibia, Niger, Nigeria, Reunion, Rwanda, Sao Tome and Principe, Senegal, Sierra Leone, Somalia, South Africa, Sudan, Swaziland, Tanzania, Togo, Tunisia, Uganda, Zambia, and Zimbabwe\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n3,089\n\n\nAverage life expectancy\n55\n\n\nAverage population\n17,875,763\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmericas\n\n\n\n\n\n\n25 countries\n\n\n\n\n\nArgentina, Bolivia, Brazil, Canada, Chile, Colombia, Costa Rica, Cuba, Dominican Republic, Ecuador, El Salvador, Guatemala, Haiti, Honduras, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, Puerto Rico, Trinidad and Tobago, United States, Uruguay, and Venezuela\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n11,003\n\n\nAverage life expectancy\n74\n\n\nAverage population\n35,954,847\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOceania\n\n\n\n\n\n\n2 countries\n\n\n\n\n\nAustralia and New Zealand\n\n\n\n\n\nDetails\nPlot\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\nAverage GDP per capita\n29,810\n\n\nAverage life expectancy\n81\n\n\nAverage population\n12,274,974"
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "",
    "text": "Last week I was making some final revisions to a paper where we used a neat conjoint experiment to test the effect of a bunch of different treatments on nonprofit donor preference.\nOne of the peer reviewers asked us to compare the characteristics of our experimental sample with the general population so that we could speak a little to the experiment’s generalizability. This is a super common thing to do with survey research, and one of the main reasons survey researchers include demographic questions in their surveys.\nThanks to the wonders of the R community—and thanks to publicly accessible data—I was able to grab nationally representative demographic data, clean it up and summarize it, run some statistical tests, and make a table to meet the reviewer’s request, all in like 45 minutes.\nIt was a magically quick and easy process, so I figured I’d make a guide about it so that the rest of the world (but mostly future me) can see how to do it."
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#nationally-representative-demographic-data",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#nationally-representative-demographic-data",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "Nationally representative demographic data",
    "text": "Nationally representative demographic data\nFinding nationally representative demographic data (in the US, at least) is pretty easy, and there are two common sources for it:\n\nThe US Census’s American Community Survey (ACS) is a rolling monthly survey of ≈3.5 million (!!!) US households that’s compiled into an annual dataset.\nThe US Census’s Current Population Survey (CPS) is a monthly survey of ≈100,000 US individuals. A more comprehensive annual version—the Annual Social and Economic Supplement (ASEC)—is published every March.\n\nThe two surveys serve different purposes, and the Census has an FAQ fact sheet explaining the difference between the ACS and CPS. Notably, the ACS only surveys households, and it uses a shorter 8-question survey, while the CPS tries to reach the entire civilian noninstitutionalized population and uses a longer, more detailed survey.\nResearchers use both surveys—I’ve used both in my own work. According to the Census, due to its detailed questionnaire and staff experience and regular frequency, the CPS ASEC is a “high quality source of information used to produce the official annual estimate of poverty, and estimates of a number of other socioeconomic and demographic characteristics”. The ACS also has demographic details and people use those instead too.\nI’m not entirely sure which one is best—to me they’re both great 🤷‍♂️. Smarter people than me know and care about the difference."
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#accessing-us-census-data",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#accessing-us-census-data",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "Accessing US Census data",
    "text": "Accessing US Census data\nGetting data from the Census is a surprisingly complex process! There are websites and R packages that make it easier though.\nACS\nFor the ACS, the {tidycensus} R package provides an interface to the Census’s API, and its documentation is great and thorough. Working with the results is tricky though, and involves a lot of pivoting and reshaping and combining variables. I have a whole notebook showing how I access the ACS and create a bunch of variables, with little notes reminding myself how I constructed everything:\n\n\n\n\n\nExplanation of how I calculated the proportion of households in a block group with a high school education\n\nCPS (and others!)\n{tidycensus} doesn’t provide Census API access to CPS data. Instead, IPUMS—a project housed at the University of Minnesota and supported by a consortium of other institutions and companies—provides easy access to all sorts of census and survey data, both through its website and through an API. It’s wild how much data they have. In addition to the CPS, they have the ACS, census microtata for 100+ countries, historical GIS shapefiles, time use surveys, and a ton of other things. It’s an incredible project."
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#getting-started",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#getting-started",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "Getting started",
    "text": "Getting started\n\n\n\n\n\n\nWho this guide is for\n\n\n\nHere’s what I assume you know:\n\nYou’re familiar with R and the tidyverse (particularly {dplyr} and {ggplot2}).\nYou’re familiar with {brms} for running Bayesian regression models and {tidybayes} and {ggdist} for manipulating and plotting posterior draws.\n\n\n\nIn this guide, we’ll use IPUMS to get CPS data (monthly and ASEC) with R. Because their data explorer website takes a little while to get used to, I’ll show a bunch of step-by-step screenshots of how to navigate it. It’s possible to access the IPUMS API with the {ipumsr} package, and I also show how to do that in this guide. But in order to use the API, you still need to know how to use the website—you have to find variable names and figure out which samples include which variables. So the screenshots below are still important even if you’re using the API.\nI’ll then show how to answer the question of whether a survey proportion is equivalent to a population proportion in a couple diffferent ways:\n\nFrequentist/classical proportion tests with null hypothesis significance testing, and\nBayesian proportion tests and inference based on regions of practical equivalence, or ROPEs\n\nBefore getting started, let’s load all the packages we need and create some helpful functions and variables.\nThe experimental survey data here comes from Chaudhry, Dotson, and Heiss (2024). Since we haven’t published it yet (though it’s close—it’s under review post R&R now!), the original data isn’t quite public yet. So I used the {synthpop} R package to create a synthetic version of part of our data that has the same relationships and distributions as the real results, but is all fake. You can see the R code I used for that process here.\nIf you want to follow along, you can download this synthetic data here:\n\n\n synthetic_data.rds: RDS version with all the variable attributes (i.e. factor levels and ordering) included\n\n synthetic_data.csv: Plain-text version (but without variable attributes)\n\nTo reflect the fact that this is all public, government-created data, I’m using the Public Sans font, an open source font developed as part of the General Services Administration’s USWDS (US Web Design System) for making accessible federal government websites. I’m also using the USWDS’s basic color palette, developed by 18F.\nLet’s get started!\n\nlibrary(tidyverse)   # {ggplot2}, {dplyr}, and friends\nlibrary(tinytable)   # Nice tables\nlibrary(brms)        # Best way to run Stan models\nlibrary(tidybayes)   # Manipulate Stan objects and draws\nlibrary(broom)       # Convert model objects to data frames\nlibrary(glue)        # Easier string construction\nlibrary(scales)      # Nicer labels\nlibrary(ggdist)      # Plot posterior distributions\nlibrary(ggforce)     # Extra ggplot things like facet_col()\nlibrary(patchwork)   # Combine ggplot plots\n\n# Load the synthetic survey results\nresults &lt;- readRDS(\"synthetic_data.rds\")\n\n# Use the cmdstanr backend for brms because it's faster and more modern than the\n# default rstan backend. You need to install the cmdstanr package first\n# (https://mc-stan.org/cmdstanr/) and then run cmdstanr::install_cmdstan() to\n# install cmdstan on your computer.\noptions(\n  mc.cores = 4,\n  brms.backend = \"cmdstanr\"\n)\n\n# Set some global Stan options\nCHAINS &lt;- 4\nITER &lt;- 2000\nWARMUP &lt;- 1000\nBAYES_SEED &lt;- 1234\n\n# Nice ggplot theme\ntheme_public &lt;- function() {\n  theme_minimal(base_family = \"Public Sans\") +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(family = \"Public Sans\", face = \"bold\", size = rel(1.25)),\n      plot.subtitle = element_text(family = \"Public Sans Light\", face = \"plain\"),\n      plot.caption = element_text(family = \"Public Sans Light\", face = \"plain\"),\n      axis.title = element_text(family = \"Public Sans Semibold\", size = rel(0.8)),\n      axis.title.x = element_text(hjust = 0),\n      axis.title.y = element_text(hjust = 1),\n      strip.text = element_text(\n        family = \"Public Sans Semibold\", face = \"plain\",\n        size = rel(0.8), hjust = 0\n      ),\n      strip.background = element_rect(fill = \"grey90\", color = NA),\n      legend.title = element_text(family = \"Public Sans Semibold\", size = rel(0.8)),\n      legend.text = element_text(size = rel(0.8)),\n      legend.position = \"bottom\",\n      legend.justification = \"left\",\n      legend.title.position = \"top\",\n      legend.margin = margin(l = 0, t = 0)\n    )\n}\n\ntheme_set(theme_public())\nupdate_geom_defaults(\"text\", list(family = \"Public Sans\"))\nupdate_geom_defaults(\"label\", list(family = \"Public Sans\"))\n\n# USWDS basic palette\n# https://designsystem.digital.gov/utilities/color/#basic-palette-2\nclrs &lt;- c(\n  \"#e52207\", # .bg-red\n  \"#e66f0e\", # .bg-orange\n  \"#ffbe2e\", # .bg-gold\n  \"#fee685\", # .bg-yellow\n  \"#538200\", # .bg-green\n  \"#04c585\", # .bg-mint\n  \"#009ec1\", # .bg-cyan\n  \"#0076d6\", # .bg-blue\n  \"#676cc8\", # .bg-indigo\n  \"#8168b3\", # .bg-violet\n  \"#d72d79\" # .bg-magenta\n)\n\n# Some functions for creating percentage point labels\nlabel_pp &lt;- label_number(\n  accuracy = 1, scale = 100, suffix = \" pp.\", style_negative = \"minus\"\n)\n\nlabel_pp_01 &lt;- label_number(\n  accuracy = 0.1, scale = 100, suffix = \" pp.\", style_negative = \"minus\"\n)"
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#getting-cps-data-from-the-ipums-website",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#getting-cps-data-from-the-ipums-website",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "Getting CPS data from the IPUMS website",
    "text": "Getting CPS data from the IPUMS website\nGo to the IPUMS CPS website and create an account if you don’t already have one.\nOnce you’re logged in, go the “Select Data” page, where IPUMS tells you to do two things:\n\n\nSelect samples, or specific verisons of different surveys\n\nSelect variables, or specific columns in different surveys\n\n\n\nInitial data extract page\n\nVisually it looks like you should select samples first, but I actually find it easier to poke around for different variables first, since not all variables are recorded in every sample.\nFinding variables\nSo first let’s look at a few variables to get a feel for the IPUMS data extract website. Click on the little “SEARCH 🔍” button in the “SELECT VARIABLES” section. We could do a bunch of fancy advanced search options, but for now, just search for “age”\n\n\nSearching for age-related variables\n\nThere are 200+(!) age-related variables in the CPS data:\n\n\nSearch results related to age\n\nThis big list shows some useful information already. The first variable in the search results is AGE, and it’s the one we care about. It gets recorded in every CPS survey: each monthly one and in the annual ASEC one. Not all the age variables do this—notice that WHYSS1 only appears in the annual ASEC.\nIf you click on AGE in the “Variable” column, you can see detailed information about the variable, like how it’s coded, a description, and its availability. For example, prior to 1976, age was only available in the annual ASEC; starting in January 1976, it became a monthly thing.\n\n\nAGE availability\n\nSince we know we want this variable, we can add it to our “Data Cart”. IPUMS ues a shopping metaphor for building a data extract—we can add different variables and samples to a cart and then check it out (for free) once we’ve found everything we’re looking for.\nClick on “Add to cart” to add it, then go back to the search page to look for more variables. You can also add variables to the cart without going to the variable details page—there’s a plus sign in the search results page next to each result that will add the variable for you.\n\n\nAdd AGE to cart\n\nNow that we have age, we need to hunt around for other variables we care about, like sex, marital status, voting history, and so on. To speed things up, you can search for their official variable names and add each one to the cart:\n\nSex = SEX\n\nMarital status = MARST\n\nEducation = EDUC\n\nDonating = VLDONATE\n\nVolunteering = VLSTATUS\n\nVolunteering Supplement weight = VLSUPPWT\n\nVoting = VOTED\n\nVoter Supplement weight = VOSUPPWT\n\n\nPay attention to the details!\nLooking at the details for these variables is helpful since they’re all categorical variables, unlike age. For instance, marital status has 9 different levels:\n\n\nNine different marital status (MARST) codes\n\nAlso, it’s important to check the variable details to check for availability. While basic demographic variables like age, sex, marital status, etc. are available in both the monthly surveys and in the annual ASEC, more specialized variables are not.\nVariables related to philanthropy and volunteering are only available in September (since they’re part of a special CPS Volunteer Supplement), and only in some years:\n\n\nVolunteer status (VLSTATUS) only available in September, and only in some years\n\nVariables related to voting are only available in November in even-numbered years (since they’re part the CPS’s Voting and Registration Supplement)\n\n\nVoting status (VOTED) only available in even-numbered years in November\n\nSelecting samples\nGreat! If you check the cart, you’ll see all the variables we added at the bottom, along with a bunch of other pre-selected columns:\n\n\nAll variables added to the cart, but no samples are selected\n\nWe can’t download any data yet, though. We’ve selected the variables—now we need to select the samples. Go back to the “Select data” page and click on “Select samples” (or click on the “Add more samples” button at the top of the data cart page).\nBy default, IPUMS will have a bunch of different samples pre-checked. In my case, it grabbed all annual ASEC surveys from 2010–2024, and all monthly surveys from 2021–2024.\n\n\nPre-selected ASEC samples\nPre-selected monthly samples\n\n\n\n\n\nPre-selected ASEC samples\n\n\n\n\n\nPre-selected monthly samples\n\n\n\n\nIncluding all these samples would be useful if we were doing some sort of analysis of CPS trends over time, comparing changes in age or education or volunteering or whatever. But that doesn’t matter here—all we want to know is what age (and everything else) looked like at the time the survey was administered. That means we really just need one year.\nHowever, we can’t just choose one sample. Things like demographics are availble in all annual and monthly samples, but volunteering is only available in September in specific years, and voting is only available in November in specific years.\nThis survey was administered in mid-2019, so we’ll choose samples that are as close to that as possible. Though demographics are available both monthly and annually, I like to use the annual versions because ASEC data is typically used to stand in for annual information—like if you were building a state-year panel dataset, you’d use ASEC data for each year. The ASEC occurs in March and actually overlaps with the monthly March data (IPUMS has a note about that), so the data technically is for March 2019, but whatever. We don’t have to be super precise here.\nWe’ll use the September 2019 sample for volunteering, even though that’s after the survey was administered. The next earliest volunteering data is the September 2017 sample, which is like 2 years before the survey. Things don’t line up precisely, but again, that’s fine.\nFinally, we’ll use the November 2018 sample for voting. That’s before the survey, but it’s the closest we can get—the next alternative is November 2020, which is a year after the survey. Once again, nothing lines up exactly, but it’s fine.\nIn summary, here are the variables we want and the samples we’ll get them from:\n\nAge (AGE): 2019 ASEC\nSex (SEX): 2019 ASEC\nMarital status (MARST): 2019 ASEC\nEducation (EDUC): 2019 ASEC\nDonating (VLDONATE): 2019-09 Monthly\nVolunteering (VLSTATUS): 2019-09 Monthly\nVolunteering Supplement weight (VLSUPPWT): 2019-09 Monthly\nVoting (VOTED): 2018-11 Monthly\nVoter Supplement weight (VOSUPPWT): 2018-11 Monthly\n\nSelect those three samples (2019 ASEC, September 2019, and November 2018) and click on “Submit sample selections” to add them to the cart.\n\n\nSpecific ASEC sample\nSpecific monthly samples\n\n\n\n\n\nMarch 2019 ASEC sample\n\n\n\n\n\nSeptember 2019 and November 2018 monthly samples\n\n\n\n\nThe cart should now have 9 variables and 3 samples. Conveniently, it has a little summary table showing which samples have which variables, where we can confirm that age, sex, marital status, and education are in all three, volunteering and donating are only in September 2019, and voting is only in November 2018.\n\n\nAll variables and samples selected and ready to go\n\nDownloading the data\nNow that we have all the variables and samples we care about in the data cart, we can create a data extract and download this stuff.\nClick on the “Create data extract” button at the top of the data cart page, which will take you to the official Extract Request page. There are a bunch of extra options here, and you can optionally add a description to the extract, but we’ll ignore all those. Click on the “Submit Abstract” button and wait for the IPUMS server to compile it all.\n\n\nExtract submission page\n\nOnce it’s ready, it’ll appear at your “My Data” page, which will have a list of all your past extracts.\nTo download the data, we actually need to download two things:\n\n\nThe data itself. Click on the big green “Download .DAT” button.\n\n\nButton for download a .dat version of the data\n\n\n\n\n\n\n\n\n\n.dat vs .dat.gz\n\n\n\nDepending on your browser, the downloaded file will either end in .dat or .dat.gz. If it ends in .gz, it’ll be compressed and zipped (the compressed version of this extract is 7.6 MB); if it ends in .dat, it’ll be uncompressed and huge (≈55 MB in this case). Chrome and Firefox will keep the compressed .gz version; Safari will automatically unzip it and throw away the .gz version, which is annoying.\nTry to keep the compressed version. You don’t even need to extract/unzip it—the {ipumsr} data loading functions will handle unzipping for you automatically behind the scenes.\n\n\n\n\nThe machine-readable XML codebook, or the DDI file. R uses to clean and relabel the raw data when you load it. If you click on the DDI link in the Codebook column, your browser will likely open a plain text XML file, which isn’t really what you want. Instead, right click on the DDI link and choose “Save file as…” or “Download file as…” or whatever your browser calls it. This will let you save the XML file to your computer.\n\n\nContext menu for the DDI codebook link\n\n\n\nThere are some other helpful links there too:\n\n\nIf you click on the R link, it’ll give you a barebones R script for loading the data. It’ll look something like this:\n# NOTE: To load data, you must download both the extract's data and the DDI\n# and also set the working directory to the folder with these files (or change the path below).\n\nif (!require(\"ipumsr\")) stop(\"Reading IPUMS data into R requires the ipumsr package. It can be installed using the following command: install.packages('ipumsr')\")\n\nddi &lt;- read_ipums_ddi(\"cps_00001.xml\")\ndata &lt;- read_ipums_micro(ddi)\n\nIf you click on the basic codebook, you’ll get a short plain text version of the codebook, which I find really helpful for remembering which variables show up where and how each variable is coded.\n\nMove the newly downloaded .dat and the .ddi files to the same folder somewhere on your computer (preferably in an RStudio Project or a Positron project/folder or wherever your R working directory is). I put mine in a folder named raw_data; you can put it wherever.\nWe’re finally ready to load this CPS data into R!"
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#more-reproducible-alternative-using-the-ipums-api",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#more-reproducible-alternative-using-the-ipums-api",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "More reproducible alternative: using the IPUMS API",
    "text": "More reproducible alternative: using the IPUMS API\nAlternatively, it’s possible to use the {ipumsr} package to access the IPUMS API directly and not need to manually download the data extract from the IPUMS website.\nThe {ipumsr} API functions essentially let you programmatically create and download a data extract cart. Unless you know the IPUMS CPS data really well, you’ll still likely need to hunt around the website for specific variables and their availabilities, so the whole previous section is still relevant.\nThe {ipumsr} vignette for working with the API is nice and complete—see that for full details.\nHere’s an abbreviated example of how to get the same data extract we collected manually from the website:\n\n\nGo to your IPUMS dashboard and create an API key. This needs to be stored as an environment variable named IPUMS_API_KEY. You can manually add this to your .Renviron file, or you can run this to make {ipumsr} do it for you:\nipumsr::set_ipums_api_key(\"BLAH\", save = TRUE)\n\n\nBuild a data extract with define_extract_micro(). This is the equivalent of adding stuff to your data cart on the IPUMS website. You’ll need to know two things:\n\nThe variable names, which you can find by searching the IPUMS website\n\nThe sample IDs, which you can find by running get_sample_info():\nlibrary(tidyverse)\nlibrary(ipumsr)\n\nall_cps_samples &lt;- get_sample_info(collection = \"cps\")\n\n# Find the names for 2019 samples\nall_cps_samples |&gt; \n  filter(str_detect(description, \"ASEC 2019\"))\n#&gt; # A tibble: 13 × 2\n#&gt;    name        description              \n#&gt;    &lt;chr&gt;       &lt;chr&gt;                    \n#&gt;  1 cps2019_01s IPUMS-CPS, January 2019  \n#&gt;  2 cps2019_02s IPUMS-CPS, February 2019 \n#&gt;  3 cps2019_03b IPUMS-CPS, March 2019    \n#&gt;  4 cps2019_04b IPUMS-CPS, April 2019    \n#&gt;  5 cps2019_05s IPUMS-CPS, May 2019      \n#&gt;  6 cps2019_06s IPUMS-CPS, June 2019     \n#&gt;  7 cps2019_03s IPUMS-CPS, ASEC 2019  \n#&gt;  ...\n\n\nThe code for creating the extract will look like this:\ncps_extract_definition &lt;- define_extract_micro(\n  collection = \"cps\",\n  description = \"API extract for blog post\",\n  samples = c(\n    \"cps2019_03s\",  # ASEC, March 2019\n    \"cps2019_09s\",  # CPS, September 2019\n    \"cps2018_11s\"   # CPS, November 2018\n  ),\n  variables = c(\n    \"AGE\", \"SEX\", \"MARST\", \"EDUC\",\n    \"VLDONATE\", \"VLSTATUS\", \"VLSUPPWT\",\n    \"VOTED\", \"VOSUPPWT\"\n  )\n)\nThis is identical to what we had in the cart on the the website: 9 variables and 3 samples:\ncps_extract_definition\n#&gt; Unsubmitted IPUMS CPS extract \n#&gt; Description: API extract for blog post\n#&gt; \n#&gt; Samples: (3 total) cps2019_03s, cps2019_09s, cps2018_11s\n#&gt; Variables: (9 total) AGE, SEX, MARST, EDUC, VLDONATE, VLSTATUS, VLSU...\n\n\nSubmit the request to the server to generate the extract. This is equivalent to checking out your data cart on the website.\ncps_extract &lt;- submit_extract(cps_extract_definition)\n#&gt; Successfully submitted IPUMS CPS extract number ZZZZ\n\n\nDownload the extract. The extract won’t be downloadable immediately—you need to wait for e-mail confirmation. Once it’s ready, you can download it with download_extract(), which will download both the .dat.gz data and the .xml codebook to your computer:\ncps_downloaded &lt;- download_extract(cps_extract, download_dir = \"raw_data\")\n#&gt;  |==================================================| 100%\n#&gt;  |==================================================| 100%\n#&gt; DDI codebook file saved to ~/blah/raw_data/cps_ZZZZ.xml\n#&gt; Data file saved to ~/blah/raw_data/cps_ZZZZ.dat.gz\n\n\nOnce it’s on your computer, you can load it with the standard {ipumsr} process shown in the next section.\ncps_data &lt;- read_ipums_micro(cps_downloaded)\n\n\n\n\n\n\n\n\nThe IPUMS API and literate programming\n\n\n\nIf you’re using a literate programming document Quarto or R Markdown, don’t include this API extraction process in your document. It will rerun every time you render your document and create a new IPUMS extract each time, which is excessive. It’s best to run this process in a separate R script or function (perhaps orchestrated with something like {targets}), and then load the DDI .xml and .dat data in the document."
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#loading-cps-data",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#loading-cps-data",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "Loading CPS data",
    "text": "Loading CPS data\nGetting this data into R is easy thanks to the {ipumsr} package. We feed the XML DDI codebook into read_ipums_ddi() and then feed that into read_ipums_micro()\n\nlibrary(ipumsr)\n\nddi &lt;- read_ipums_ddi(\"raw_data/cps_00001.xml\")\ncps_data &lt;- read_ipums_micro(ddi, data_file = \"raw_data/cps_00001.dat.gz\", verbose = FALSE)\n\nglimpse(cps_data)\n## Rows: 421,402\n## Columns: 21\n## $ YEAR     &lt;dbl&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018,…\n## $ SERIAL   &lt;dbl&gt; 1, 1, 3, 4, 4, 4, 4, 5, 5, 6, 6, 7, 7, 7, 8, 8, 9, 9, 10, 10, 11, 12, 13, 13, 14, 14, 14, 15, 15, 16, 17, 19, 20, 20, 21, 21, 22, 23, 23, 23, 23, 24, 25, 26, 26, 26, 26, 26, 28, 28, 28, 28, 29, 30, 30, 30, 32, 36, 36, 37, 37, 37, 37, 39, 39, 39, 39, 39, 40, 40, 41, 41, 41, 41, 42,…\n## $ MONTH    &lt;int+lbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1…\n## $ HWTFINL  &lt;dbl&gt; 1704, 1704, 1957, 1688, 1688, 1688, 1688, 2090, 2090, 1832, 1832, 1779, 1779, 1779, 1853, 1853, 2077, 2077, 1427, 1427, 1611, 2044, 1738, 1738, 1690, 1690, 1690, 3135, 3135, 2679, 2253, 1639, 1615, 1615, 1515, 1515, 2254, 1459, 1459, 1459, 1459, 1960, 1942, 1701, 1701, 1701, 1701,…\n## $ CPSID    &lt;dbl&gt; 2.017e+13, 2.017e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e…\n## $ ASECFLAG &lt;int+lbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ ASECWTH  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ PERNUM   &lt;dbl&gt; 1, 2, 1, 1, 2, 3, 4, 1, 2, 1, 2, 1, 2, 3, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 3, 4, 1, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 1, 2, 3, 1, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 1, 1, 1, 2, 1, 1, 2, 3, 1, 2, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4,…\n## $ WTFINL   &lt;dbl&gt; 1704, 1845, 1957, 1688, 2780, 2780, 2679, 2090, 2090, 1832, 2679, 1754, 1779, 2452, 1853, 1870, 2151, 2077, 2003, 1427, 1611, 2044, 1738, 1738, 1690, 2102, 2537, 3135, 4172, 2679, 2253, 1639, 1615, 2609, 1900, 1515, 2254, 1459, 1546, 1912, 1752, 1960, 1942, 1527, 1701, 2231, 2104,…\n## $ CPSIDP   &lt;dbl&gt; 2.017e+13, 2.017e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.018e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e+13, 2.017e…\n## $ CPSIDV   &lt;dbl&gt; 2.017e+14, 2.017e+14, 2.018e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.018e+14, 2.018e+14, 2.017e+14, 2.017e+14, 2.018e+14, 2.018e+14, 2.018e+14, 2.017e+14, 2.017e+14, 2.018e+14, 2.018e+14, 2.018e+14, 2.018e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.017e+14, 2.017e…\n## $ ASECWT   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ AGE      &lt;int+lbl&gt; 26, 26, 48, 53, 16, 16, 20, 22, 23, 57, 23, 61, 62, 39, 74, 49, 54, 52, 69, 76, 41, 56, 64, 62, 53, 13, 21, 28, 28, 40, 51, 78, 64, 40, 59, 36, 27, 35, 36, 5, 8, 76, 80, 36, 33, 3, 6, 11, 62, 80, 61, 61, 57, 24, 22, 26, 61, 24, 0, 37, 3, 5, 32, 33, 8, 9, 10, 15, 55, 61, 29, 29…\n## $ SEX      &lt;int+lbl&gt; 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2…\n## $ MARST    &lt;int+lbl&gt; 6, 6, 4, 4, 6, 6, 6, 6, 6, 4, 6, 1, 1, 6, 5, 6, 1, 1, 1, 1, 6, 4, 6, 6, 5, 9, 6, 1, 1, 6, 3, 5, 6, 6, 1, 1, 6, 1, 1, 9, 9, 5, 4, 1, 1, 9, 9, 9, 6, 5, 4, 3, 4, 6, 6, 6, 3, 6, 9, 3, 9, 9, 6, 6, 9, 9, 9, 6, 1, 1, 1, 1, 9, 9, 6, 4, 1, 1, 4, 1, 1, 9, 1, 1, 1, 1, 9, 9, 1, 1, 4, 1, 5…\n## $ EDUC     &lt;int+lbl&gt; 111, 123, 73, 81, 50, 50, 81, 81, 81, 111, 81, 81, 81, 92, 81, 81, 123, 111, 81, 60, 111, 73, 73, 73, 81, 1, 81, 81, 81, 91, 111, 60, 73, 73, 125, 81, 92, 124, 111, 1, 1, 60, 73, 123, 125, 1, 1, 1, 73, 30, 73, 73, 124, 81, 92, 73, 73, 73, 1, 20, 1, 1, 20, 73, 1, 1, 1, 30, 111,…\n## $ VLSTATUS &lt;int+lbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ VLDONATE &lt;int+lbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ VLSUPPWT &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ VOTED    &lt;int+lbl&gt; 98, 98, 2, 2, 99, 99, 2, 99, 99, 98, 98, 2, 2, 2, 1, 97, 2, 2, 1, 1, 2, 98, 2, 1, 2, 99, 1, 1, 2, 98, 98, 2, 98, 98, 98, 98, 2, 2, 2, 99, 99, 1, 2, 1, 1, 99, 99, 99, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 99, 98, 99, 99, 98, 2, 99, 99, 99, 99, 1, 1, 2, 2, 99, 99, 2, 98, 2, 2, 96, 2, 2,…\n## $ VOSUPPWT &lt;dbl&gt; 1704, 1845, 1957, 1688, 2780, 2780, 2679, 2090, 2090, 1832, 2679, 1754, 1779, 2452, 1853, 1870, 2151, 2077, 2003, 1427, 1611, 2044, 1738, 1738, 1690, 2102, 2537, 3135, 4172, 2679, 2253, 1639, 1615, 2609, 1900, 1515, 2254, 1459, 1546, 1912, 1752, 1960, 1942, 1527, 1701, 2231, 2104,…\n\nholy moly we have nearly half a million rows. That’s because we have three samples (2019 ASEC, September 2019, and November 2018) and they’re all stacked on top of each other in this data. We need to filter this huge data to extract the three samples. We’ll also remove rows with missing data.\n\ncps_demographics &lt;- cps_data |&gt;\n  # Only look at the 2019 ASEC data\n  filter(YEAR == 2019, MONTH == 03, ASECFLAG == 1) |&gt;\n  # Remove rows that are missing or are \"not in universe\"\n  mutate(\n    SEX = ifelse(SEX == 9, NA, SEX),\n    MARST = ifelse(MARST == 9, NA, MARST),\n    EDUC = ifelse(EDUC &lt; 1 | EDUC == 999, NA, EDUC)\n  )\n\ncps_volunteer &lt;- cps_data |&gt; \n  filter(YEAR == 2019, MONTH == 09) |&gt; \n  # Remove rows that are missing or are \"not in universe\"\n  mutate(across(c(VLSTATUS, VLDONATE), \\(x) ifelse(x == 99, NA, x)))\n\ncps_voting &lt;- cps_data |&gt; \n  filter(YEAR == 2018, MONTH == 11) |&gt;\n  # Remove rows that are missing or are \"not in universe\"\n  mutate(VOTED = ifelse(VOTED == 99, NA, VOTED))\n\nThese counts are more reasonable (but still huge!)\n\nnrow(cps_demographics)\n## [1] 180101\nnrow(cps_volunteer)\n## [1] 118557\nnrow(cps_voting)\n## [1] 122744"
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#summarizing-cps-data",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#summarizing-cps-data",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "Summarizing CPS data",
    "text": "Summarizing CPS data\nUltimately, our goal is to find the population-level average of a bunch of characteristics and see if our sample plausibly matches population averages.\nThings get a little tricky and loosey-goosey here. The different levels measured by the CPS don’t always match what’s in the survey. For example, the CPS measures sex and provides only 2 levels (1 = male; 2 = female); the experiment called this construct gender and included male, female,1 transgender, prefer not to say, and other.\n1 We should have called these “man” and “woman” since gender ≠ sex.To make the survey question reasonably match what the CPS is capturing, I find that it’s easiest to collapse both the survey data and the CPS data to simpler constructs. Before we collapse things, though, we need to look at one statistical issue: weighting.\nWeighting\nIn an effort to make the CPS nationally representative, every row is weighted—each individual does not represent the same number of persons in the population. The Census oversamples some subpopulations and shifts weights up and down to give individuals more or less statistical influence in the sample so that the survey results better approximate the characteristics of the general population. Any analysis we do with CPS data needs to take those weights into account.\nThe weights for ASEC variables are included in the ASECWT column; the weights for volunteering and voting variables are in the VLSUPPWT and VOSUPPWT columns\nIf we’re calculating basic averages, we can use weighted.mean() instead of mean(). Note the difference in average when we don’t weight!\n\ncps_demographics |&gt; \n  summarize(\n    avg_age_weighted = weighted.mean(AGE, w = ASECWT),  # BAD\n    avg_age_unweighted = mean(AGE)  # GOOD\n  )\n## # A tibble: 1 × 2\n##   avg_age_weighted avg_age_unweighted\n##              &lt;dbl&gt;              &lt;dbl&gt;\n## 1             38.8               37.3\n\nIf we’re doing stuff with models, we can use the weights argument:\n\n# BAD: Non-weighted intercept-only model\nlm(AGE ~ 1, data = cps_demographics) |&gt; \n  tidy(conf.int = TRUE)\n## # A tibble: 1 × 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)     37.3    0.0540      690.       0     37.2      37.4\n\n# GOOD: Weighted intercept-only model\nlm(AGE ~ 1, data = cps_demographics, weights = ASECWT) |&gt; \n  tidy(conf.int = TRUE)\n## # A tibble: 1 × 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)     38.8    0.0542      715.       0     38.7      38.9\n\nBase R only really has weighted.mean(). If we want other things, like a weighted variance, or weighted rank, or weighted table/crosstabs, we can use a bunch of different functions in the {Hmisc} package:\n\n# Some Hmisc::wtd.*() things:\ncps_demographics |&gt; \n  summarize(\n    avg_age = Hmisc::wtd.mean(AGE, weights = ASECWT),\n    var_age = Hmisc::wtd.var(AGE, weights = ASECWT),\n    sd_age = sqrt(var_age)\n  )\n## # A tibble: 1 × 3\n##   avg_age var_age sd_age\n##     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1    38.8    529.   23.0\n\nCalculating population-level proportions\nWe’ll collapse these population-level CPS values into binary versions of each question so that we can look at things like the proportion of women, the proportion of people who volunteer, and so on. We’ll also collapse age into a binary above/below the median age—this isn’t necessary, and we could totally work with numeric age instead of proportions, but in our anonymized survey data, our age column is an indicator representing being above/below 36 (the median age at the time of the survey).\nWe’ll do some basic summarizing with weighted.mean() and calculate all these national proportions, along with the weighted standard deviations (which will be important for the Bayesian analysis later in this post).\n\nnational_demographics &lt;- cps_demographics |&gt; \n  summarize(\n    # AGE is already numeric\n    age = weighted.mean(AGE &gt;= 36, ASECWT), \n    age_sd = sqrt(Hmisc::wtd.var(AGE &gt;= 36, weights = ASECWT)),\n\n    # 1 = Female\n    female = weighted.mean(SEX == 2, ASECWT),\n    female_sd = sqrt(Hmisc::wtd.var(SEX == 2, weights = ASECWT)),\n\n    # 1 = Married, spouse present\n    # 2 = Married, spouse absent\n    married = weighted.mean(MARST %in% 1:2, na.rm = TRUE),\n    married_sd = sqrt(Hmisc::wtd.var(MARST %in% 1:2, weights = ASECWT)),\n\n    # 111 = Bachelor's degree\n    college = weighted.mean(EDUC &gt;= 111, ASECWT, na.rm = TRUE),\n    college_sd = sqrt(Hmisc::wtd.var(EDUC &gt;= 111, weights = ASECWT))\n)\n\nnational_volunteer &lt;- cps_volunteer |&gt; \n  summarize(\n    # 1 = Volunteer\n    volunteering = weighted.mean(VLSTATUS == 1, VLSUPPWT, na.rm = TRUE),\n    volunteering_sd = sqrt(Hmisc::wtd.var(VLSTATUS == 1, weights = VLSUPPWT)),\n\n    # 2 = Yes, made a donation to charity in the past 12 months\n    donating = weighted.mean(VLDONATE == 2, VLSUPPWT, na.rm = TRUE),\n    donating_sd = sqrt(Hmisc::wtd.var(VLDONATE == 2, weights = VLSUPPWT))\n  )\n\nnational_voting &lt;- cps_voting |&gt;\n  summarize(\n    # 2 = Voted in the most recent November election\n    voting = weighted.mean(VOTED == 2, VOSUPPWT, na.rm = TRUE),\n    voting_sd = sqrt(Hmisc::wtd.var(VOTED == 2, weights = VOSUPPWT))\n  )\n\nI like to store these in a little one-row data frame so that it’s easy to access invidiual values:\n\nnational_values &lt;- bind_cols(\n  national_demographics, national_volunteer, national_voting\n)\nnational_values\n## # A tibble: 1 × 14\n##     age age_sd female female_sd married married_sd college college_sd volunteering volunteering_sd donating donating_sd voting voting_sd\n##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n## 1 0.530  0.499  0.510     0.500   0.410      0.492   0.258      0.437        0.300           0.458    0.474       0.499  0.534     0.499\n\n# Proportion of women\nnational_values$female\n## [1] 0.5097\n\n# Proportion that voted\nnational_values$voting\n## [1] 0.5344"
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#summarizing-sample-proportions",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#summarizing-sample-proportions",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "Summarizing sample proportions",
    "text": "Summarizing sample proportions\nWe’re almost done! All that’s left is testing whether the demographic characteristics of the survey experiment respondents reasonably match their corresponding population proportions.\nFirst, though, we need to make binary versions of the survey responses. To make life easier, we’ll use the same names as the CPS data:\n\nresults_to_test &lt;- results |&gt; \n  mutate(\n    age = age == \"More than median\",\n    female = gender == \"Female\",\n    married = marital_status == \"Married\",\n    college = education %in% c(\n      \"4 year degree\", \n      \"Graduate or professional degree\", \n      \"Doctorate\"\n    ),\n    volunteering = volunteer_frequency != \"Haven't volunteered in past 12 months\",\n    donating = donate_frequency == \"More than once a month, less than once a year\",\n    voting = voted == \"Yes\"\n  ) |&gt; \n  select(female, age, married, college, volunteering, donating, voting)\n\nglimpse(results_to_test)\n## Rows: 1,300\n## Columns: 7\n## $ female       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, …\n## $ age          &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, F…\n## $ married      &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRU…\n## $ college      &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, …\n## $ volunteering &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, TR…\n## $ donating     &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n## $ voting       &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE…"
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#testing-sample-vs.-population-proportions-frequentist-ly",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#testing-sample-vs.-population-proportions-frequentist-ly",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "Testing sample vs. population proportions frequentist-ly",
    "text": "Testing sample vs. population proportions frequentist-ly\nOne-sample proportion test for age\nAs a quick and easy check, we can run a one-sample proportion test to see if the proportion of a variable is significantly different from a null value. We can do this with prop.test(), which works a bunch of different ways—with matrices, with vectors, and with single values (see this blog post for some other examples of prop.test()).\nLet’s look at age first. 50.77% of people in the sample are older than 36; 53% of people in the population are older than 36:\n\n# Proportion of sample older than 36\nmean(results_to_test$age)\n## [1] 0.5077\n\n# CPS proportion older than 36\nnational_values$age\n## [1] 0.5303\n\nIs that an issue? Is the sample significantly younger than the rest of the country?\nFor this one-sample test, we need to feed prop.test() three things: (1) the number of “successes”, or the count of rows where the respondent is older than 36 (or where age is TRUE), (2) the number of rows in the sample, and (3) the null value, or the population-level CPS proportion:\n\nprop_test_freq_age &lt;- prop.test(\n  x = sum(results_to_test$age),  # Number of \"successes\" (rows where age == TRUE)\n  n = nrow(results_to_test),     # Sample size\n  p = national_values$age        # Population-level proportion from the CPS\n)\n\ntidy(prop_test_freq_age)\n## # A tibble: 1 × 8\n##   estimate statistic p.value parameter conf.low conf.high method                                               alternative\n##      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                                &lt;chr&gt;      \n## 1    0.508      2.59   0.108         1    0.480     0.535 1-sample proportions test with continuity correction two.sided\n\nFor fun, we can plot this too:\n\nCode for making this plot ↓prop_test_freq_age |&gt; \n  tidy() |&gt; \n  mutate(\n    prop_label = glue(\n      \"Sample proportion\\n{prop} [{low}, {high}]\",\n      prop = label_percent(accuracy = 0.01)(estimate),\n      low = label_percent(accuracy = 0.01)(conf.low),\n      high = label_percent(accuracy = 0.01)(conf.high)\n    )\n  ) |&gt; \n  ggplot(aes(x = estimate, y = \"Age\")) +\n  geom_vline(\n    xintercept = national_values$age, color = clrs[2]\n  ) +\n  annotate(\n    geom = \"label\", x = national_values$age, y = I(1.3), \n    label = glue(\n      \"CPS proportion\\n{x}\", \n      x = label_percent(accuracy = 0.01)(national_values$age)\n    ),\n    fill = clrs[2], color = \"white\", size = 8, size.unit = \"pt\"\n  ) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_label(aes(label = prop_label), nudge_y = -0.3, size = 8, size.unit = \"pt\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Proportion older than 36\", y = NULL)\n\n\n\n\n\n\nFigure 1: Proportion of survey respondents older than 36 compared to the national proportion\n\n\n\n\nBased on this, the 95% confidence interval for the proportion in the sample is 0.48–0.54, and the null/population value is 0.53, which fits safely in that confidence interval. The corresponding p-value is 0.108, which means that the sample proportion isn’t significantly different from the national proportion.\nWe can’t be certain that the sample doesn’t generally match the population, age-wise.\nThat’s a horribly convoluted sentence—welcome to the world of frequentist null hypothesis testing! Technically we can’t really say that the sample matches the population, and we can only say that we don’t know if it doesn’t match. Stay tuned for the Bayesian analysis section for a way to get an answer that we do care about. For now, as a kind of cheat-y shorthand, we can (semi-illegally) say that since the CPS proportion is in the sample confidence interval, there probably isn’t a significant difference between the two.\nOne-sample proportion test for volunteering\nLet’s do another one: volunteering.\n\nprop_test_freq_vol &lt;- prop.test(\n  x = sum(results_to_test$volunteering),\n  n = nrow(results_to_test),\n  p = national_values$volunteering\n)\n\ntidy(prop_test_freq_vol)\n## # A tibble: 1 × 8\n##   estimate statistic   p.value parameter conf.low conf.high method                                               alternative\n##      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                                &lt;chr&gt;      \n## 1    0.598      546. 8.79e-121         1    0.570     0.624 1-sample proportions test with continuity correction two.sided\n\n\nCode for making this plot ↓prop_test_freq_vol |&gt; \n  tidy() |&gt; \n  mutate(\n    prop_label = glue(\n      \"Sample proportion\\n{prop} [{low}, {high}]\",\n      prop = label_percent(accuracy = 0.01)(estimate),\n      low = label_percent(accuracy = 0.01)(conf.low),\n      high = label_percent(accuracy = 0.01)(conf.high)\n    )\n  ) |&gt; \n  ggplot(aes(x = estimate, y = \"Volunteering\")) +\n  geom_vline(\n    xintercept = national_values$volunteering, color = clrs[2]\n  ) +\n  annotate(\n    geom = \"label\", x = national_values$volunteering, y = I(1.3), \n    label = glue(\n      \"CPS proportion\\n{x}\", \n      x = label_percent(accuracy = 0.01)(national_values$volunteering)\n    ),\n    fill = clrs[2], color = \"white\", size = 8, size.unit = \"pt\"\n  ) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_label(aes(label = prop_label), nudge_y = -0.3, size = 8, size.unit = \"pt\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(x = \"Proportion that volunteers regularly\", y = NULL) +\n  expand_limits(x = c(0.26, 0.7))\n\n\n\n\n\n\nFigure 2: Proportion of survey respondents that volunteer compared to the national proportion\n\n\n\n\nPhew, this one is way off. 30% of the general population has volunteered in the last year; 60% of the sample has volunteered. The sample proportion is most definitely significantly different from the general population, and we can reject the null hypothesis that it’s the same.\nIn this case, that’s fine. In our experiment, we only wanted to test our different treatments on people who donate to charity on at least an annual basis, so we screened out respondents who hadn’t donated in the past year. Volunteer behavior and donation behavior are closely correlated, so we have way more volunteers in our sample.\nProportion tests and differences for everything all at once\nWe can repeat this one-sample proportion test for all the different characteristics we care about. Instead of repeating the same code over and over, we’ll make a little wrapper function for it. We’ll also calculate the difference between the sample and population proportions:\n\nprop_test_freq() wrapper function#' Perform a basic one-sample proportion test\n#'\n#' @param sample_column A numeric vector representing the sample data (0s and 1s).\n#' @param cps_prop A numeric value representing the proportion to compare against.\n#' @return A tibble containing the test results and the differences between the\n#'         sample estimate and the specified proportion.\n#' @examples\n#' sample_data &lt;- c(1, 0, 1, 1, 0, 1, 0, 1, 1, 0)\n#' cps_prop &lt;- 0.5\n#' prop_test_freq(sample_data, cps_prop)\nprop_test_freq &lt;- function(sample_column, cps_prop) {\n  n_yes &lt;- sum(sample_column)\n  n_total = length(sample_column)\n\n  sample_prop_test &lt;- prop.test(\n    x = n_yes,\n    n = n_total,\n    p = cps_prop\n  )\n\n  out_df &lt;- tidy(sample_prop_test) |&gt; \n    mutate(\n      diff = estimate - cps_prop,\n      diff_low = conf.low - cps_prop,\n      diff_high = conf.high - cps_prop\n    )\n\n  return(tibble(test = list(sample_prop_test), out_df))\n}\n\n\nWe’ll then create a little summary dataset and plug each row of it into our new prop_test_freq() function with the magic of purrr::map(), which will store the results from the hypothesis test in a list column named prop_test_results, which we’ll finally unnest so that we can access the results as columns:\n\nCreate sample_cps_props_freq and use prop_test_freq() on each rowsample_cps_props_freq &lt;- tribble(\n  ~category, ~variable, ~sample_value, ~national_value,\n  \"Demographics\", \"Age (% 36+)\", results_to_test$age, national_values$age,\n  \"Demographics\", \"Female (%)\", results_to_test$female, national_values$female,\n  \"Demographics\", \"Married (%)\", results_to_test$married, national_values$married,\n  \"Demographics\", \"Education (% BA+)\", results_to_test$college, national_values$college,\n  \"Philanthropy\", \"Donated in past year (%)\", results_to_test$donating, national_values$donating,\n  \"Philanthropy\", \"Volunteered in past year (%)\", results_to_test$volunteering, national_values$volunteering,\n  \"Voting\", \"Voted in last November election (%)\", results_to_test$voting, national_values$voting\n) |&gt; \n  mutate(prop_test_results = pmap(\n    list(sample_value, national_value), \\(x, y) prop_test_freq(x, y)\n  )) |&gt;\n  unnest_wider(prop_test_results)\n\n\nWe can use this new sample_cps_props_freq data frame for plotting:\n\nCode for making this plot ↓sample_cps_props_freq |&gt;\n  mutate(across(c(variable, category), \\(x) fct_inorder(x))) |&gt;\n  ggplot(aes(x = estimate, y = fct_rev(variable))) +\n  # It would be nicer to use geom_segment() to add vertical lines here, but it \n  # doesn't play nicely with categorical y-axis breaks, so we can cheat and use \n  # geom_point() instead with `shape = \"|\"` to use the | character\n  geom_point(aes(x = national_value, color = \"CPS proportion\"), shape = \"|\", size = 6) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high, color = \"Sample proportion\")) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_y_discrete(labels = label_wrap(30)) +\n  scale_color_manual(values = c(\"CPS proportion\" = clrs[2], \"Sample proportion\" = \"black\")) +\n  facet_col(vars(category), scales = \"free_y\", space = \"free\") +\n  labs(x = \"Average proportion\", y = NULL, color = NULL)\n\n\n\n\n\n\nFigure 3: Respondent demographic proportions compared to national proportions\n\n\n\n\nThat looks super neat and it’s helpful to visualize all these differences. In general, the sample looks like the population in terms of age, gender, marital status, and education, but the sample is way more socially and civically oriented than the rest of the country (again, by design).\nThe only issue with this plot is that it’s a little hard to read with the CPS proportion moving around in each variable. We can center it at 0 and look at differences from the CPS proportion:\n\nCode for making this plot ↓sample_cps_props_freq |&gt; \n  mutate(across(c(variable, category), \\(x) fct_inorder(x))) |&gt; \n  ggplot(aes(x = diff, y = fct_rev(variable))) +\n  geom_vline(aes(xintercept = 0), color = clrs[2]) +\n  geom_pointrange(\n    aes(xmin = diff_low, xmax = diff_high, color = \"Sample proportion − CPS proportion\")\n  ) +\n  scale_x_continuous(labels = label_pp) +\n  scale_y_discrete(labels = label_wrap(30)) +\n  scale_color_manual(values = c(\"Sample proportion − CPS proportion\" = \"black\")) +\n  facet_col(vars(category), scales = \"free_y\", space = \"free\") +\n  labs(x = \"Difference in proportion from CPS\", y = NULL, color = NULL)\n\n\n\n\n\n\nFigure 4: Differences between demographic proportions and national proportions\n\n\n\n\nAnd we can also use sample_cps_props_freq to make a pretty table. Here’s a table with {tinytable} (though you could do this with any of R’s tablemaking packages, like {gt} or {kableExtra} or whatever)\n\nCode for making this table ↓notes &lt;- list(\n  \"*\" = \"Sample proportion significantly different from CPS (p &lt; 0.05)\",\n  \"a\" = list(i = 1:4, j = 1, text = \"Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS), March 2019\"),\n  \"b\" = list(i = 5:6, j = 1, text = \"Monthly CPS, September 2019\"),\n  \"c\" = list(i = 7, j = 1, text = \"Monthly CPS, November 2018\")\n)\n\nsample_cps_props_freq |&gt; \n  mutate(significant = ifelse(p.value &lt; 0.05, \"*\", \"\")) |&gt; \n  mutate(sample_nice = glue(\n    \"{estimate}{significant}&lt;br&gt;[{conf.low}, {conf.high}]\",\n    estimate = label_percent(accuracy = 0.1)(estimate),\n    conf.low = label_percent(accuracy = 0.1)(conf.low),\n    conf.high = label_percent(accuracy = 0.1)(conf.high)\n  )) |&gt; \n  mutate(diff_nice = glue(\n    \"{diff}{significant}&lt;br&gt;[{diff_low}, {diff_high}]\",\n    diff = label_pp_01(diff),\n    diff_low = label_number(accuracy = 0.1, scale = 100)(diff_low),\n    diff_high = label_number(accuracy = 0.1, scale = 100)(diff_high)\n  )) |&gt; \n  select(\n    Variable = variable, \n    National = national_value, \n    Sample = sample_nice,\n    `∆` = diff_nice) |&gt; \n  tt(width = c(0.3, 0.2, 0.3, 0.2), notes = notes) |&gt; \n  group_tt(i = sample_cps_props_freq$category) |&gt; \n  format_tt(j = 2, fn = label_percent(accuracy = 0.1)) |&gt; \n  style_tt(i = c(1, 6, 9), bold = TRUE, background = \"#e6e6e6\") |&gt; \n  style_tt(\n    bootstrap_class = \"table\",\n    bootstrap_css_rule = \".table tfoot { text-align: left; } .table { font-size: 0.85rem; }\"\n  ) |&gt; \n  style_tt(j = 1, align = \"l\") |&gt; \n  style_tt(j = 2:4, align = \"c\")\n\n\nTable 1: Sample characteristics compared to nationally representative Current Population Survey (CPS) estimates\n\n\n\n\n    \n\n      \n\nVariable\n                National\n                Sample\n                ∆\n              \n\n\n* Sample proportion significantly different from CPS (p &lt; 0.05)\n\na Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS), March 2019\n\nb Monthly CPS, September 2019\n\nc Monthly CPS, November 2018\n\n\n\nAge (% 36+)a\n\n                  53.0%\n                  50.8%[48.0%, 53.5%]\n                  −2.3 pp.[-5.0, 0.5]\n                \n\nFemale (%)a\n\n                  51.0%\n                  47.2%*[44.4%, 49.9%]\n                  −3.8 pp.*[-6.6, -1.1]\n                \n\nMarried (%)a\n\n                  41.0%\n                  40.6%[37.9%, 43.3%]\n                  −0.4 pp.[-3.1, 2.3]\n                \n\nEducation (% BA+)a\n\n                  25.8%\n                  28.5%*[26.1%, 31.1%]\n                  2.8 pp.*[0.4, 5.3]\n                \n\nDonated in past year (%)b\n\n                  47.4%\n                  56.9%*[54.2%, 59.6%]\n                  9.5 pp.*[6.8, 12.2]\n                \n\nVolunteered in past year (%)b\n\n                  30.0%\n                  59.8%*[57.0%, 62.4%]\n                  29.7 pp.*[27.0, 32.4]\n                \n\nVoted in last November election (%)c\n\n                  53.4%\n                  71.5%*[69.0%, 74.0%]\n                  18.1 pp.*[15.5, 20.5]"
  },
  {
    "objectID": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#testing-sample-vs.-population-proportions-bayesian-ly",
    "href": "blog/2025/01/27/ipums-cps-proportions-bayes/index.html#testing-sample-vs.-population-proportions-bayesian-ly",
    "title": "Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly",
    "section": "Testing sample vs. population proportions Bayesian-ly",
    "text": "Testing sample vs. population proportions Bayesian-ly\new null hypothesis significance testing\nThat’s all well and good, but I’m actually not 100% sure if that’s the right proportion test to use. As I mention here, every classical statistical test has a bunch of “flavors” for different situations and assumptions (e.g., do the two samples have equal or unequal variances? do we need to correct the sample size? make a continuity correction? is it tuesday?). There are all sorts of flowcharts you can follow to choose the right version.\nTechnically, the results from all those one-sample proportion tests above tell us the answer to this question:\n\nIn a hypothetical world where the difference between the sample and population proportions is 0 (or where the sample proportion is equal to the population proportion), what’s the probability that this one-time collection of data fits in that world—and if the probability is low, is there enough evidence (i.e. is the probability less than 0.05?) to confidently reject that hypothetical world of no difference?\n\nThat’s a mouthful and it’s a weird question. With age, there’s a 2.3 percentage point difference between the sample and the national proportions, with a p-value of 0.108. This means that there’s a 10.8% chance of seeing a 2.3 percentage point difference in a world where the difference is actually 0. That’s less than 5%, so we cannot confidently declare that there’s not not a difference. Or in other words, we can’t reject the possibility that we’re in the hypothetical null world.\nAs a kind of shorthand, we then handwavily concluded that the sample and population proportions are probably about the same, but technically that’s wrong. All we really concluded is that we don’t have enough evidence that the hypothetical world of no difference is wrong. It could be right; it could be wrong. Who knows.\nWith volunteering, there’s a 29.7 percentage point difference between the sample and the national proportions, with a tiny tiny p-value of 8.79 × 10−121. This means that there’s basicaly a 0% chance that we’d see that 29.7 percentage point difference in a world where the difference is actually 0. That makes it statistically significant—we have enough evidence to safely declare that we’re not in the hypothetical null world.\nI really really don’t like this logic of null hypothesis testing. It doesn’t really answer the question we want to know. Here’s what we’re really actually interested in:\n\nGiven this data, what’s the probability that there’s no difference between the sample and population proportions?\n\nWe can answer this question with Bayesian statistics and avoid all this null hypothesis stuff. And as an added bonus, we don’t need to think about which flavor of which context-specific statistical test we need to use. We can instead model the data-generating process more directly and then work with the simulated posterior distribution of that process.\nModeling proportions with a binomial distribution\nThe actual process for generating the age column (and all the other variables in the sample) involved asking each survey respondent their age. If someone is older than 36, it’s counted as a “success”; if they are younger than 36, it’s not a success. It’s a binary choice2 that is repeated across hundreds of other respondents (or “trials”). There’s some underlying probability for being older than 36 that corresponds to the proportion of people that select that answer.\n2 Kind of—technically they selected their actual age, but we can pretend that it was just a binary choice.This data-generating process involves a bunch of independent trials (or respondents) with some probability of success (or being older than 36), which makes it a binomial distribution.\nFormally, it has three parameters:\n\\[\ny \\sim \\operatorname{Binomial}(n, \\pi)\n\\]\n\n\n\\(y\\), or the number of successes (the number of people older than 36)\n\n\\(n\\), or the number of trials (the total number of respondents)\n\n\\(\\pi\\), or the probability of success (the probability that someone is older than 36)\n\nWe know \\(y\\) and \\(n\\) from our data:\n\n\n\\(y\\) = sum(results_to_test$age) = 660\n\n\\(n\\) = nrow(results_to_test) = 1300\n\nWe want to find out \\(\\pi\\) so we can find the difference between \\(\\pi\\) and the population proportion to see if it’s 0 or not.\nWe can estimate \\(\\pi\\) with a Bayesian beta-binomial regression model using {brms}. We’ll use a vague Beta(1, 1) prior, which is 50% with a wide range—see here for more about Beta distributions and priors. Note that this is an intercept-only model with no other covariates. That’s because we just want to know the underlying proportion of age—we’re not conditioning that proportion on anything else.\n\\[\n\\begin{aligned}\ny_{(n \\text{ age &gt; 36})} \\sim&\\ \\operatorname{Binomial}(n_{\\text{Total respondents}}, \\pi_{\\text{age &gt; 36}}) \\\\\n\\pi_{\\text{age &gt; 36}} =&\\ \\beta_0 \\\\[10pt]\n\\beta_0 =&\\ \\operatorname{Beta}(1, 1)\n\\end{aligned}\n\\]\n{brms} likes working with data frames, so we’ll put our \\(y\\) and \\(n\\) into a little one-row dataset and then use brm() to fit a model:\n\nage_binomial_df &lt;- tibble(\n  n_yes = sum(results_to_test$age),\n  n_total = nrow(results_to_test)\n)\n\n# lol tiny data\nage_binomial_df\n## # A tibble: 1 × 2\n##   n_yes n_total\n##   &lt;int&gt;   &lt;int&gt;\n## 1   660    1300\n\n\nmodel_age_binomial &lt;- brm(\n  bf(n_yes | trials(n_total) ~ 1),\n  data = age_binomial_df,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(1, 1), class = \"Intercept\", lb = 0, ub = 1)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0,\n  file = \"models/model_age_binomial\"\n)\n\nWorking with the posterior\nSince this is just a regression model, it behaves like any normal {brms} model. The coefficient for the intercept represents the estimated proportion of people older than 36 in the sample:\n\nsummary(model_age_binomial)\n##  Family: binomial \n##   Links: mu = identity \n## Formula: n_yes | trials(n_total) ~ 1 \n##    Data: age_binomial_df (Number of observations: 1) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Regression Coefficients:\n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.51      0.01     0.48     0.54 1.00     1537     1828\n## \n## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nSince we’re in The Land of Bayes, we can work with the full posterior and calculate estimands directly, like the posterior difference between the sample proportion and the national proportion:\n\nage_draws &lt;- model_age_binomial |&gt; \n  spread_draws(b_Intercept) |&gt; \n  mutate(diff = b_Intercept - national_values$age)\n\nAnd we can plot these estimands:\n\nCodep1 &lt;- ggplot(age_draws, aes(x = b_Intercept, y = \"Age (% 36+)\")) + \n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = national_values$age, color = clrs[2]) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Proportion older than 36\", y = NULL)\n\np2 &lt;- ggplot(age_draws, aes(x = diff, y = \"Age (% 36+)\")) + \n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = 0, color = clrs[2]) +\n  scale_x_continuous(labels = label_pp) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Difference from CPS\", y = NULL)\n\np1 / p2\n\n\n\n\n\n\nFigure 5: Posterior proportion of survey respondents older than 36 compared to the national proportion, and posterior differences between sample and national proportions (like Figure 1, but Bayesian)\n\n\n\n\nThe population value is in the sample posterior, which means that the posterior difference between the sample and population includes 0, but that doesn’t tell us much about how likely that is. We could calculate the probability that the difference isn’t equal to 0, but that’s relatively useless—zero is a single point, and the probability that the posterior is different from that one point is infinite.\nThe region of practical equivalence (ROPE)\nAnother approach is to think of a range of values around 0 that all have “practically no effect.” I like to think of this as a sort of “dead zone.” If the difference between the sample and the population is 0, we can safely conclude that there’s no difference between the two. If the measured difference were 0.3 percentage points, or −0.6 percentage points, or even 2 percentage points, I’d still feel pretty confident that that’s basically the same. Bayesians call this the region of practical equivalence, or ROPE (Kruschke 2010, 2015; Kruschke, Aguinis, and Joo 2012; Kruschke and Liddell 2018).\nThere are lots of ways to define this ROPE or dead zone. You can base it on experience with the phenomenon, or you can base it on data that you have. Kruschke and Liddell (2018) suggest looking at a tenth of the variable’s standard deviation above and below the main null value, or\n\\[\n[-0.1 \\times SD_y, 0.1 \\times SD_y]\n\\]\nTo illustrate, let’s find the ROPE for the proportion of people older than 36 with ±0.1 × standard deviation:\n\nage_rope &lt;- tibble(\n  avg_age = national_values$age,\n  sd_age = national_values$age_sd\n) |&gt; \n  mutate(\n    rope_low = -0.1 * sd_age,\n    rope_avg_low = avg_age + rope_low,\n    rope_high = 0.1 * sd_age,\n    rope_avg_high = avg_age + rope_high\n  )\nage_rope\n## # A tibble: 1 × 6\n##   avg_age sd_age rope_low rope_avg_low rope_high rope_avg_high\n##     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n## 1   0.530  0.499  -0.0499        0.480    0.0499         0.580\n\nFollowing this rule, we shouldn’t care about sample/population differences between ±4.99 percentage points. For all intents and purposes, any differences in that range—or any sample proportions between 53% ± 4.99 (or 48%–58%)—are equivalent.\nHere’s what the ROPE for age looks like, both for the full proportion and for the difference:\n\nCodep1 &lt;- ggplot(age_draws, aes(x = b_Intercept, y = \"Age (% 36+)\")) + \n  annotate(\n    geom = \"rect\", \n    xmin = age_rope$rope_avg_low, \n    xmax = age_rope$rope_avg_high, \n    ymin = 1, ymax = Inf, \n    fill = clrs[2], alpha = 0.2\n  ) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = age_rope$avg_age, color = clrs[2]) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Proportion older than 36\", y = NULL)\n\np2 &lt;- ggplot(age_draws, aes(x = diff, y = \"Age (% 36+)\")) + \n  annotate(\n    geom = \"rect\", \n    xmin = age_rope$rope_low, \n    xmax = age_rope$rope_high, \n    ymin = 1, ymax = Inf, \n    fill = clrs[2], alpha = 0.2\n  ) +\n  stat_halfeye(fill = clrs[5]) +\n  geom_vline(xintercept = 0, color = clrs[2]) +\n  scale_x_continuous(labels = label_pp) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Difference from CPS\", y = NULL)\n\np1 / p2\n\n\n\n\n\n\nFigure 6: Posterior proportion of survey respondents older than 36 compared to the national proportion, and posterior differences between sample and national proportions, overlaid on a region of practical equivalence (ROPE)\n\n\n\n\nA huge chunk of that posterior distribution is inside the ROPE dead zone. We can calculate the exact proportion:\n\nprop_in_rope &lt;- age_draws |&gt; \n  mutate(\n    is_below_rope = diff &lt; age_rope$rope_low,\n    is_above_rope = diff &gt; age_rope$rope_high\n  ) |&gt; \n  summarize(is_inside_rope = 1 - mean(is_below_rope | is_above_rope))\nprop_in_rope$is_inside_rope\n## [1] 0.9762\n\n97.6% of the posterior is inside the ROPE, which means that there’s a 97.6% probability that the sample matches the population.\nThat’s way cooler and way more interpretable than null hypothesis testing.\n\n\n\n\n\n\n\nBONUS: Faster, more automatic ROPE calculations\n\n\n\n\n\nWe just went through a lot of work ↑ up there to calculate the bounds of the ROPE and then find how much of the posterior is in it.\nThere’s a faster and easier way! The {bayestestR} package has a rope() function that can do it automatically.\n\nmodel_age_binomial |&gt; \n  bayestestR::rope(ci = 1)\n## # Proportion of samples inside the ROPE [-45.25, 45.25]:\n## \n## Parameter | inside ROPE\n## -----------------------\n## Intercept |    100.00 %\n\nIt doesn’t give exactly the same result as before because the automatic ROPE limits (range here) aren’t based on the weighted standard deviation, but we can define that range ourselves:\n\nmodel_age_binomial |&gt; \n  bayestestR::rope(\n    ci = 1, \n    range = c(age_rope$rope_avg_low, age_rope$rope_avg_high)\n  )\n## # Proportion of samples inside the ROPE [0.48, 0.58]:\n## \n## Parameter | inside ROPE\n## -----------------------\n## Intercept |     97.62 %\n\nbayestestR::rope() is incorporated in the {parameters} package too, so you can do this:\n\nmodel_age_binomial |&gt; \n  parameters::model_parameters(\n    test = \"rope\",\n    rope_ci = 1, \n    rope_range = c(age_rope$rope_avg_low, age_rope$rope_avg_high),\n    verbose = FALSE\n  )\n## # Fixed Effects\n## \n## Parameter   | Median |       95% CI | % in ROPE |  Rhat |     ESS\n## -----------------------------------------------------------------\n## (Intercept) |   0.51 | [0.48, 0.54] |    97.62% | 1.001 | 1528.00\n\n\n\n\n\n\n\n\n\n\nBONUS: How much of the posterior should we count?\n\n\n\n\n\nThere are big debates in the ROPE world about how much of the posterior we should look at when working with ROPEs. We just looked at the 100% of the sample posterior. Some people (Kruschke 2015; McElreath 2020) say to look instead at how much of the 95% (or 89%) highest density interval (HDI) falls within the ROPE.\nI have no strong preferences either way, and it’s fairly straightforward to calculate the proportion of the HDI in the ROPE. Here’s an example with a Richard McElreath-style 89% HDI:\n\n# Find the 89% HDI\nage_hdi &lt;- age_draws |&gt; \n  median_hdi(diff, .width = 0.89)\nage_hdi\n## # A tibble: 1 × 6\n##      diff  .lower   .upper .width .point .interval\n##     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n## 1 -0.0227 -0.0450 -0.00108   0.89 median hdi\n\n# Find how much of the the age HDI is in the ROPE\nage_draws |&gt; \n  filter(diff &gt;= age_hdi$.lower & diff &lt;= age_hdi$.upper) |&gt; \n  mutate(\n    is_below_rope = diff &lt; age_rope$rope_low,\n    is_above_rope = diff &gt; age_rope$rope_high\n  ) |&gt; \n  summarize(is_inside_rope = 1 - mean(is_below_rope | is_above_rope))\n## # A tibble: 1 × 1\n##   is_inside_rope\n##            &lt;dbl&gt;\n## 1              1\n\nOr with the more automatic bayestestR::rope():\n\nmodel_age_binomial |&gt; \n  bayestestR::rope(\n    ci = 0.89, \n    range = c(age_rope$rope_avg_low, age_rope$rope_avg_high)\n  )\n## # Proportion of samples inside the ROPE [0.48, 0.58]:\n## \n## Parameter | inside ROPE\n## -----------------------\n## Intercept |    100.00 %\n\nIn this case, 100% of the HDI is in the ROPE, so we can say that the age proportion is equivalent to the population proportion.\n\n\n\nBayesian proportion test for volunteering\nLet’s look at volunteer status by itself next, to keep things parallel with the frequentist section earlier.\nWe’ll make a little one-row dataset:\n\nvol_binomial_df &lt;- tibble(\n  n_yes = sum(results_to_test$volunteering),\n  n_total = nrow(results_to_test)\n)\nvol_binomial_df\n## # A tibble: 1 × 2\n##   n_yes n_total\n##   &lt;int&gt;   &lt;int&gt;\n## 1   777    1300\n\nThen we’ll use it to model the proportion/probability of volunteering:\n\nmodel_vol_binomial &lt;- brm(\n  bf(n_yes | trials(n_total) ~ 1),\n  data = vol_binomial_df,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(1, 1), class = \"Intercept\", lb = 0, ub = 1)),\n  chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n  refresh = 0,\n  file = \"models/model_vol_binomial\"\n)\n\n\nvol_draws &lt;- model_vol_binomial |&gt; \n  spread_draws(b_Intercept) |&gt; \n  mutate(diff = b_Intercept - national_values$volunteering)\n\nWe can calculate the ROPE for the proportion volunteering, or ±0.1 × SDvolunteering\n\nvol_rope &lt;- tibble(\n  avg_vol = national_values$volunteering,\n  sd_vol = national_values$volunteering_sd\n) |&gt; \n  mutate(\n    rope_low = -0.1 * sd_vol,\n    rope_avg_low = avg_vol + rope_low,\n    rope_high = 0.1 * sd_vol,\n    rope_avg_high = avg_vol + rope_high\n  )\nvol_rope\n## # A tibble: 1 × 6\n##   avg_vol sd_vol rope_low rope_avg_low rope_high rope_avg_high\n##     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n## 1   0.300  0.458  -0.0458        0.254    0.0458         0.346\n\nAnd finally we can visualize things and calculate how much of the posterior fits inside the ROPE:\n\nCodep1 &lt;- ggplot(vol_draws, aes(x = b_Intercept, y = \"Volunteered in past year (%)\")) + \n  annotate(\n    geom = \"rect\", \n    xmin = vol_rope$rope_avg_low, \n    xmax = vol_rope$rope_avg_high, \n    ymin = 1, ymax = Inf, \n    fill = clrs[2], alpha = 0.2\n  ) +\n  stat_halfeye(fill = clrs[8]) +\n  geom_vline(xintercept = vol_rope$avg_vol, color = clrs[2]) +\n  scale_x_continuous(labels = label_percent()) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Proportion\", y = NULL)\n\np2 &lt;- ggplot(vol_draws, aes(x = diff, y = \"Volunteered in past year (%)\")) + \n  annotate(\n    geom = \"rect\", \n    xmin = vol_rope$rope_low, \n    xmax = vol_rope$rope_high, \n    ymin = 1, ymax = Inf, \n    fill = clrs[2], alpha = 0.2\n  ) +\n  stat_halfeye(fill = clrs[8]) +\n  geom_vline(xintercept = 0, color = clrs[2]) +\n  scale_x_continuous(labels = label_pp) +\n  coord_cartesian(ylim = c(1.5, 1.5)) +\n  labs(x = \"Difference from CPS\", y = NULL)\n\np1 / p2\n\n\n\n\n\n\nFigure 7: Posterior proportion of survey respondents that volunteer to the national proportion, and posterior differences between sample and national proportions, overlaid on a region of practical equivalence (ROPE); like Figure 2, but Bayesian\n\n\n\n\nAs we saw with the frequentist version of this analysis, the proportion of people who volunteer is substantially different from the CPS—≈30 percentage points higher! The probability that the sample is equivalent to the CPS population is 0:\n\nvol_draws |&gt; \n  mutate(\n    is_below_rope = b_Intercept &lt; vol_rope$rope_avg_low,\n    is_above_rope = b_Intercept &gt; vol_rope$rope_avg_high\n  ) |&gt; \n  summarize(is_inside_rope = 1 - mean(is_below_rope | is_above_rope))\n## # A tibble: 1 × 1\n##   is_inside_rope\n##            &lt;dbl&gt;\n## 1              0\n\nPosterior proportions, differences, and ROPEs for everything all at once\nLike we did before with the frequentist approach, we’ll make a little wrapper function for this process. It’s a little more complex, and we’ll make it return a list of lots of things: the original {brms} model, a long data frame of all the MCMC draws, a data frame with the ROPE bounds, and the probability that the full posterior is in the ROPE.\n\nprop_test_bayes() wrapper function#' Perform a Bayesian proportion test using a beta-binomial model\n#'\n#' @param short_name A character string representing a short name for the model file.\n#' @param sample_column A numeric vector representing the sample data (0/1 or logical).\n#' @param cps_prop A numeric value representing the population proportion to compare against.\n#' @param cps_sd A numeric value representing the standard deviation of the population proportion.\n#' @return A list containing the model, MCMC draws, ROPE details, and the proportion of the posterior\n#'         inside the ROPE.\n#' @examples\n#' sample_data &lt;- c(1, 0, 1, 1, 0, 1, 0, 1, 1, 0)\n#' cps_prop &lt;- 0.5\n#' cps_sd &lt;- 0.1\n#' prop_test_bayes(\"example\", sample_data, cps_prop, cps_sd)\nprop_test_bayes &lt;- function(short_name, sample_column, cps_prop, cps_sd) {\n  # Little one-row data frame\n  df &lt;- tibble(\n    n_yes = sum(sample_column),\n    n_total = length(sample_column)\n  )\n\n  # Intercept-only beta-binomial model\n  model &lt;- brm(\n    bf(n_yes | trials(n_total) ~ 1),\n    data = df,\n    family = binomial(link = \"identity\"),\n    prior = c(prior(beta(1, 1), class = \"Intercept\", lb = 0, ub = 1)),\n    chains = CHAINS, warmup = WARMUP, iter = ITER, seed = BAYES_SEED,\n    refresh = 0,\n    file = glue(\"models/model_{short_name}_binomial\")\n  )\n\n  # ROPE details\n  rope &lt;- tibble(\n    cps_avg = cps_prop,\n    cps_sd = cps_sd\n  ) |&gt; \n    mutate(\n      rope_low = -0.1 * cps_sd,\n      rope_avg_low = cps_avg + rope_low,\n      rope_high = 0.1 * cps_sd,\n      rope_avg_high = cps_avg + rope_high\n    )\n\n  # MCMC draws of intercept and difference from population \n  draws &lt;- model |&gt; \n    spread_draws(b_Intercept) |&gt; \n    mutate(diff = b_Intercept - cps_prop)\n\n  # Proportion of posterior inside the ROPE\n  prop_in_rope &lt;- draws |&gt; \n    mutate(\n      is_below_rope_full = b_Intercept &lt; rope$rope_avg_low,\n      is_above_rope_full = b_Intercept &gt; rope$rope_avg_high\n    ) |&gt; \n    summarize(is_inside_rope_full = 1 - mean(is_below_rope_full | is_above_rope_full))\n\n  return(lst(model, draws, rope, prop_in_rope))\n}\n\n\nAnd as before, we’ll make a little data frame with each of the variables we want to compare.\n\nCreate sample_cps_props_bayes and use prop_test_bayes() on each rowsample_cps_props_bayes &lt;- tribble(\n  ~category, ~short_name, ~variable, ~sample_value, ~national_value, ~national_sd,\n  \"Demographics\", \"age\", \"Age (% 36+)\", results_to_test$age, national_values$age, national_values$age_sd,\n  \"Demographics\", \"female\", \"Female (%)\", results_to_test$female, national_values$female, national_values$female_sd,\n  \"Demographics\", \"married\", \"Married (%)\", results_to_test$married, national_values$married, national_values$married_sd,\n  \"Demographics\", \"college\", \"Education (% BA+)\", results_to_test$college, national_values$college, national_values$college_sd,\n  \"Philanthropy\", \"donating\", \"Donated in past year (%)\", results_to_test$donating, national_values$donating, national_values$donating_sd,\n  \"Philanthropy\", \"volunteering\", \"Volunteered in past year (%)\", results_to_test$volunteering, national_values$volunteering, national_values$volunteering_sd,\n  \"Voting\", \"voting\", \"Voted in last November election (%)\", results_to_test$voting, national_values$voting, national_values$voting_sd\n) |&gt;\n  mutate(prop_test_results = pmap(\n    list(short_name, sample_value, national_value, national_sd),\n    \\(short_name, sample_value, national_value, national_sd) prop_test_bayes(short_name, sample_value, national_value, national_sd)\n  ))\n\n\nWe’ll then extract the MCMC draws and ROPE details from the little data frame so that we can make some plots and tables.\n\nExtract draws and ROPE from sample_cps_props_bayes resultsrope_details &lt;- sample_cps_props_bayes |&gt;\n  mutate(prop_in_rope = map(prop_test_results, \\(x) x$prop_in_rope)) |&gt;\n  mutate(rope = map(prop_test_results, \\(x) x$rope)) |&gt;\n  mutate(sample_median = map_dbl(prop_test_results, \\(x) median(x$draws$b_Intercept))) |&gt;\n  unnest_wider(c(rope, prop_in_rope)) |&gt;\n  mutate(\n    variable = fct_rev(fct_inorder(variable)),\n    category = fct_inorder(category)\n  ) |&gt;\n  mutate(p_rope = case_when(\n    row_number() == 1 ~ glue(\"p(Sample = CPS) = {x}\", x = label_percent(accuracy = 0.1)(is_inside_rope_full)),\n    TRUE ~ glue(\"{x}\", x = label_percent(accuracy = 0.1)(is_inside_rope_full))\n  )) |&gt; \n  group_by(category) |&gt;\n  mutate(variable_numeric = as.numeric(fct_drop(variable)))\n\nsample_cps_props_draws &lt;- sample_cps_props_bayes |&gt;\n  mutate(draws = map(prop_test_results, \\(x) x$draws)) |&gt;\n  unnest(draws) |&gt;\n  mutate(\n    variable = fct_rev(fct_inorder(variable)),\n    category = fct_inorder(category)\n  )\n\n\nAnd finally, we can make a plot of all these fancy results!\n\nCode for making this plot ↓sample_cps_props_draws |&gt;\n  ggplot(aes(x = b_Intercept, y = variable)) +\n  geom_rect(\n    data = rope_details,\n    aes(\n      xmin = rope_avg_low, xmax = rope_avg_high,\n      ymin = variable_numeric,\n      ymax = variable_numeric + 0.9,\n      fill = \"ROPE (± 0.1 SD)\"\n    ),\n    alpha = 0.2, inherit.aes = FALSE\n  ) +\n  stat_halfeye(aes(slab_fill = category)) +\n  geom_segment(\n    data = rope_details,\n    aes(\n      x = national_value, xend = national_value,\n      y = variable_numeric, yend = variable_numeric + 0.9,\n      color = \"CPS proportion\"\n    ),\n    key_glyph = \"vline\"\n  ) +\n  geom_label(\n    data = rope_details, \n    aes(x = 0.9, y = variable, label = p_rope), \n    inherit.aes = FALSE,\n    hjust = 1, vjust = -0.1, size = 8, size.unit = \"pt\", label.size = 0, fill = \"grey95\"\n  ) +\n  guides(color = guide_legend(order = 1), fill = guide_legend(order = 2)) +\n  scale_x_continuous(labels = label_percent()) +\n  scale_y_discrete(labels = label_wrap(30)) +\n  scale_color_manual(values = c(clrs[2])) +\n  scale_fill_manual(values = c(clrs[2])) +\n  scale_fill_manual(values = clrs[c(5, 8, 11)], aesthetics = \"slab_fill\", guide = \"none\") +\n  labs(x = \"Proportion\", y = NULL, color = NULL, fill = NULL) +\n  facet_col(vars(category), scales = \"free_y\", space = \"free\") +\n  theme(\n    legend.key.width = unit(2, \"lines\"),\n    legend.key.height = unit(0.9, \"lines\"),\n    axis.text.y = element_text(vjust = 0)\n  )\n\n\n\n\n\n\nFigure 8: Respondent demographic proportions compared to national proportions, overlaid on variable-specific ROPEs; like Figure 3, but Bayesian\n\n\n\n\n\nCode for making this plot ↓sample_cps_props_draws |&gt;\n  ggplot(aes(x = diff, y = variable)) +\n  geom_rect(\n    data = rope_details,\n    aes(\n      xmin = rope_low, xmax = rope_high,\n      ymin = variable_numeric,\n      ymax = variable_numeric + 0.9,\n      fill = \"ROPE (± 0.1 SD)\"\n    ),\n    alpha = 0.2, inherit.aes = FALSE\n  ) +\n  geom_vline(aes(xintercept = 0, color = \"CPS proportion\")) +\n  stat_halfeye(aes(slab_fill = category)) +\n  geom_label(\n    data = rope_details, \n    aes(x = 0.4, y = variable, label = p_rope), \n    inherit.aes = FALSE,\n    hjust = 1, vjust = -0.1, size = 8, size.unit = \"pt\", label.size = 0, fill = \"grey95\"\n  ) +\n  guides(color = guide_legend(order = 1), fill = guide_legend(order = 2)) +\n  scale_x_continuous(labels = label_pp) +\n  scale_y_discrete(labels = label_wrap(30)) +\n  scale_color_manual(values = c(clrs[2])) +\n  scale_fill_manual(values = c(clrs[2])) +\n  scale_fill_manual(values = clrs[c(5, 8, 11)], aesthetics = \"slab_fill\", guide = \"none\") +\n  labs(x = \"Difference from CPS\", y = NULL, color = NULL, fill = NULL) +\n  facet_col(vars(category), scales = \"free_y\", space = \"free\") +\n  theme(\n    legend.key.width = unit(2, \"lines\"),\n    legend.key.height = unit(0.9, \"lines\"),\n    axis.text.y = element_text(vjust = 0)\n  )\n\n\n\n\n\n\nFigure 9: Differences between demographic proportions and national proportions, overlaid on variable-specific ROPEs; like Figure 4, but Bayesian\n\n\n\n\nAnd as before, we can make pretty tables with {tinytable}:\n\nCode for making this table ↓notes &lt;- list(\n  \"†\" = list(i = 0, j = 5, text = \"Proportion of the complete sample posterior that falls outside of the region of practical equivalence (ROPE) around the national proportion, or [−0.1 × SD, 0.1 × SD]. This essentially respresents the probabilty that the sample posterior is equivalent to the national proportion.\"),\n  \"a\" = list(i = 1:4, j = 1, text = \"Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS), March 2019\"),\n  \"b\" = list(i = 5:6, j = 1, text = \"Monthly CPS, September 2019\"),\n  \"c\" = list(i = 7, j = 1, text = \"Monthly CPS, November 2018\")\n)\n\ntbl_bayes &lt;- sample_cps_props_bayes |&gt;\n  mutate(prop_in_rope = map(prop_test_results, \\(x) x$prop_in_rope)) |&gt;\n  mutate(sample_median_ci = map(prop_test_results, \\(x) x$draws |&gt; median_qi(b_Intercept, diff))) |&gt; \n  unnest_wider(c(prop_in_rope, sample_median_ci))\n\ntbl_bayes |&gt; \n  mutate(sample_nice = glue(\n    \"{value}&lt;br&gt;[{lower}, {upper}]\",\n    value = label_percent(accuracy = 0.1)(b_Intercept),\n    lower = label_percent(accuracy = 0.1)(b_Intercept.lower),\n    upper = label_percent(accuracy = 0.1)(b_Intercept.upper)\n  )) |&gt; \n  mutate(diff_nice = glue::glue(\n    \"{diff}&lt;br&gt;[{lower}, {upper}]\",\n    diff = label_pp_01(diff),\n    lower = label_number(accuracy = 0.1, scale = 100)(diff.lower),\n    upper = label_number(accuracy = 0.1, scale = 100)(diff.upper)\n  )) |&gt; \n  mutate(p_in_rope = label_percent(accuracy = 0.1, scale = 100)(is_inside_rope_full)) |&gt; \n  select(\n    Variable = variable,\n    National = national_value,\n    Sample = sample_nice,\n    `∆` = diff_nice,\n    `p(Sample = CPS)` = p_in_rope\n  ) |&gt; \n  tt(width = c(0.3, 0.1, 0.2, 0.2, 0.2), notes = notes) |&gt; \n  group_tt(i = tbl_bayes$category) |&gt; \n  format_tt(j = 2, fn = label_percent(accuracy = 0.1)) |&gt; \n  style_tt(i = c(1, 6, 9), bold = TRUE, background = \"#e6e6e6\") |&gt; \n  style_tt(\n    bootstrap_class = \"table\",\n    bootstrap_css_rule = \".table tfoot { text-align: left; } .table { font-size: 0.85rem; }\"\n  ) |&gt; \n  style_tt(j = 1, align = \"l\") |&gt; \n  style_tt(j = 2:5, align = \"c\")\n\n\nTable 2: Sample characteristics compared to nationally representative Current Population Survey (CPS) estimates, with probability that the sample is equivalent to the national proportion; like Table 1, but Bayesian\n\n\n\n\n    \n\n      \n\nVariable\n                National\n                Sample\n                ∆\n                p(Sample = CPS)†\n\n              \n\n\n† Proportion of the complete sample posterior that falls outside of the region of practical equivalence (ROPE) around the national proportion, or [−0.1 × SD, 0.1 × SD]. This essentially respresents the probabilty that the sample posterior is equivalent to the national proportion.\n\na Annual Social and Economic Supplement (ASEC) of the Current Population Survey (CPS), March 2019\n\nb Monthly CPS, September 2019\n\nc Monthly CPS, November 2018\n\n\n\nAge (% 36+)a\n\n                  53.0%\n                  50.8%[48.1%, 53.5%]\n                  −2.3 pp.[-5.0, 0.5]\n                  97.6%\n                \n\nFemale (%)a\n\n                  51.0%\n                  47.2%[44.4%, 49.9%]\n                  −3.8 pp.[-6.5, -1.1]\n                  80.9%\n                \n\nMarried (%)a\n\n                  41.0%\n                  40.6%[37.9%, 43.3%]\n                  −0.4 pp.[-3.1, 2.3]\n                  99.9%\n                \n\nEducation (% BA+)a\n\n                  25.8%\n                  28.6%[26.0%, 31.1%]\n                  2.8 pp.[0.3, 5.3]\n                  89.8%\n                \n\nDonated in past year (%)b\n\n                  47.4%\n                  56.9%[54.3%, 59.7%]\n                  9.5 pp.[6.9, 12.3]\n                  0.1%\n                \n\nVolunteered in past year (%)b\n\n                  30.0%\n                  59.7%[57.1%, 62.4%]\n                  29.7 pp.[27.1, 32.3]\n                  0.0%\n                \n\nVoted in last November election (%)c\n\n                  53.4%\n                  71.5%[68.9%, 73.9%]\n                  18.0 pp.[15.5, 20.5]\n                  0.0%"
  },
  {
    "objectID": "blog/2025/02/13/natural-earth-crimea/index.html",
    "href": "blog/2025/02/13/natural-earth-crimea/index.html",
    "title": "How to move Crimea from Russia to Ukraine in maps with R",
    "section": "",
    "text": "The Natural Earth Project provides high quality public domain geographic data with all sorts of incredible detail, at three resolutions: high (1:10m), medium (1:50m), and low (1:110m). I use their data all the time in my own work and research, and the {rnaturalearth} package makes it really easy to get their data into R for immediate mapping. I mean, look at this!\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\n\n# Set some colors\nukr_blue &lt;- \"#0057b7\"  # Blue from the Ukrainian flag\nukr_yellow &lt;- \"#ffdd00\"  # Yellow from the Ukrainian flag\nrus_red &lt;- \"#d62718\"  # Red from the Russian flag\n\nclr_ocean &lt;- \"#d9f0ff\"\nclr_land &lt;- \"#facba6\"\n\n# CARTOColors Prism (https://carto.com/carto-colors/)\ncarto_prism = c(\n  \"#5F4690\", \"#1D6996\", \"#38A6A5\", \"#0F8554\", \"#73AF48\", \"#EDAD08\", \n  \"#E17C05\", \"#CC503E\", \"#94346E\", \"#6F4070\", \"#994E95\", \"#666666\"\n)\n\nne_countries(scale = 110) |&gt; \n  filter(admin != \"Antarctica\") |&gt; \n  ggplot() + \n  geom_sf(aes(fill = continent), color = \"white\", linewidth = 0.1) +\n  scale_fill_manual(values = carto_prism, guide = \"none\") +\n  coord_sf(crs = \"+proj=robin\") +\n  theme_void()\n\n\n\n\n\n\n\n\nMaps are intensely political things. There are dozens of disputes over maps and land and territories (e.g., Palestine, Western Sahara, Northern Cyprus, Taiwan, Kashmir, etc.), and many UN member states don’t recognize other UN member states (Israel isn’t recognized by many Arab states; Pakistan doesn’t recognize Armenia).\nThe Natural Earth Project’s official policy for disputed territories is to reflect on-the-ground de facto control over each piece of land1:\n1 OpenStreetMap does this too.\nNatural Earth Vector draws boundaries of sovereign states according to defacto status. We show who actually controls the situation on the ground. For instance, we show China and Taiwan as two separate states. But we show Palestine as part of Israel.\n\nThough they claim that this de facto policy “is rigorous and self consistent”, it gets them in trouble a lot. For instance, there are nearly two dozen issues on GitHub about Crimea, which is illegally occupied by Russia but de jure part of Ukraine. There are huge debates over the ethics of the de facto policy.\n\n\n\n\n\n\nTreating the Natural Earth de facto policy as a de facto policy\n\n\n\nI’m not weighing in on that policy here! I don’t super like it—it makes it really hard to map Palestine, for instance—but it is what it is. In this post I’m treating the de facto policy as the de facto situation of the data.\n\n\n\nNatural Earth’s solution for disputed territories is to offer different options to reflect country-specific de jure points of view. They offer pre-built high resolution shapefiles for 31 different points of views, so it’s possible to download data that reflect de jure boundaries for a bunch of different countries. Their other shapefiles all have columns like fclass_us, fclass_ua, and so on for doing… something?… with the point of view. I can’t figure out how these columns work beyond localization stuff (i.e. changing place names based on the point of view). The documentation doesn’t say much about how to actually use these different points of view, and pre-built medium and low resolution maps don’t exist yet.\nFor example, the US doesn’t de jure-ily recognize the Russian occupation of Crimea, so if we download the pre-built high resolution (10m) version of the world from the US point of view, we can see Crimea as part of Ukraine (we have to download this manually—rnaturalearth::ne_countries() doesn’t support POV files):\n\nworld_10_us &lt;- read_sf(\"ne_10m_admin_0_countries_usa/ne_10m_admin_0_countries_usa.shp\")\n\nworld_10_us |&gt; \n  filter(ADMIN == \"Ukraine\") |&gt; \n  ggplot() +\n  geom_sf(fill = ukr_blue) +\n  theme_void()\n\n\n\n\n\n\n\nUnfortunately, the pre-built point-of-view datasets only exist for the 10m high resolution data. If we want to show medium or low resolution maps, we’re stuck with the de facto version of the map, which means Crimea will be shown as part of Russia. Here’s the low resolution version of Ukraine, with Crimea in Russia:\n\nworld &lt;- ne_countries(scale = 110, type = \"map_units\")\n\nukraine &lt;- world |&gt; filter(admin == \"Ukraine\")\nrussia &lt;- world |&gt; filter(admin == \"Russia\")\n\nukraine_bbox &lt;- ukraine |&gt; \n  st_buffer(dist = 100000) |&gt;  # Add 100,000 meter buffer around the country \n  st_bbox()\n\nggplot() +\n  geom_sf(data = world, fill = clr_land) +\n  geom_sf(data = russia, fill = rus_red) + \n  geom_sf(data = ukraine, fill = ukr_blue, color = ukr_yellow, linewidth = 2) + \n  coord_sf(\n    xlim = c(ukraine_bbox[\"xmin\"], ukraine_bbox[\"xmax\"]), \n    ylim = c(ukraine_bbox[\"ymin\"], ukraine_bbox[\"ymax\"])\n  ) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = clr_ocean))"
  },
  {
    "objectID": "blog/2025/02/13/natural-earth-crimea/index.html#the-natural-earth-project",
    "href": "blog/2025/02/13/natural-earth-crimea/index.html#the-natural-earth-project",
    "title": "How to move Crimea from Russia to Ukraine in maps with R",
    "section": "",
    "text": "The Natural Earth Project provides high quality public domain geographic data with all sorts of incredible detail, at three resolutions: high (1:10m), medium (1:50m), and low (1:110m). I use their data all the time in my own work and research, and the {rnaturalearth} package makes it really easy to get their data into R for immediate mapping. I mean, look at this!\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rnaturalearth)\n\n# Set some colors\nukr_blue &lt;- \"#0057b7\"  # Blue from the Ukrainian flag\nukr_yellow &lt;- \"#ffdd00\"  # Yellow from the Ukrainian flag\nrus_red &lt;- \"#d62718\"  # Red from the Russian flag\n\nclr_ocean &lt;- \"#d9f0ff\"\nclr_land &lt;- \"#facba6\"\n\n# CARTOColors Prism (https://carto.com/carto-colors/)\ncarto_prism = c(\n  \"#5F4690\", \"#1D6996\", \"#38A6A5\", \"#0F8554\", \"#73AF48\", \"#EDAD08\", \n  \"#E17C05\", \"#CC503E\", \"#94346E\", \"#6F4070\", \"#994E95\", \"#666666\"\n)\n\nne_countries(scale = 110) |&gt; \n  filter(admin != \"Antarctica\") |&gt; \n  ggplot() + \n  geom_sf(aes(fill = continent), color = \"white\", linewidth = 0.1) +\n  scale_fill_manual(values = carto_prism, guide = \"none\") +\n  coord_sf(crs = \"+proj=robin\") +\n  theme_void()\n\n\n\n\n\n\n\n\nMaps are intensely political things. There are dozens of disputes over maps and land and territories (e.g., Palestine, Western Sahara, Northern Cyprus, Taiwan, Kashmir, etc.), and many UN member states don’t recognize other UN member states (Israel isn’t recognized by many Arab states; Pakistan doesn’t recognize Armenia).\nThe Natural Earth Project’s official policy for disputed territories is to reflect on-the-ground de facto control over each piece of land1:\n1 OpenStreetMap does this too.\nNatural Earth Vector draws boundaries of sovereign states according to defacto status. We show who actually controls the situation on the ground. For instance, we show China and Taiwan as two separate states. But we show Palestine as part of Israel.\n\nThough they claim that this de facto policy “is rigorous and self consistent”, it gets them in trouble a lot. For instance, there are nearly two dozen issues on GitHub about Crimea, which is illegally occupied by Russia but de jure part of Ukraine. There are huge debates over the ethics of the de facto policy.\n\n\n\n\n\n\nTreating the Natural Earth de facto policy as a de facto policy\n\n\n\nI’m not weighing in on that policy here! I don’t super like it—it makes it really hard to map Palestine, for instance—but it is what it is. In this post I’m treating the de facto policy as the de facto situation of the data.\n\n\n\nNatural Earth’s solution for disputed territories is to offer different options to reflect country-specific de jure points of view. They offer pre-built high resolution shapefiles for 31 different points of views, so it’s possible to download data that reflect de jure boundaries for a bunch of different countries. Their other shapefiles all have columns like fclass_us, fclass_ua, and so on for doing… something?… with the point of view. I can’t figure out how these columns work beyond localization stuff (i.e. changing place names based on the point of view). The documentation doesn’t say much about how to actually use these different points of view, and pre-built medium and low resolution maps don’t exist yet.\nFor example, the US doesn’t de jure-ily recognize the Russian occupation of Crimea, so if we download the pre-built high resolution (10m) version of the world from the US point of view, we can see Crimea as part of Ukraine (we have to download this manually—rnaturalearth::ne_countries() doesn’t support POV files):\n\nworld_10_us &lt;- read_sf(\"ne_10m_admin_0_countries_usa/ne_10m_admin_0_countries_usa.shp\")\n\nworld_10_us |&gt; \n  filter(ADMIN == \"Ukraine\") |&gt; \n  ggplot() +\n  geom_sf(fill = ukr_blue) +\n  theme_void()\n\n\n\n\n\n\n\nUnfortunately, the pre-built point-of-view datasets only exist for the 10m high resolution data. If we want to show medium or low resolution maps, we’re stuck with the de facto version of the map, which means Crimea will be shown as part of Russia. Here’s the low resolution version of Ukraine, with Crimea in Russia:\n\nworld &lt;- ne_countries(scale = 110, type = \"map_units\")\n\nukraine &lt;- world |&gt; filter(admin == \"Ukraine\")\nrussia &lt;- world |&gt; filter(admin == \"Russia\")\n\nukraine_bbox &lt;- ukraine |&gt; \n  st_buffer(dist = 100000) |&gt;  # Add 100,000 meter buffer around the country \n  st_bbox()\n\nggplot() +\n  geom_sf(data = world, fill = clr_land) +\n  geom_sf(data = russia, fill = rus_red) + \n  geom_sf(data = ukraine, fill = ukr_blue, color = ukr_yellow, linewidth = 2) + \n  coord_sf(\n    xlim = c(ukraine_bbox[\"xmin\"], ukraine_bbox[\"xmax\"]), \n    ylim = c(ukraine_bbox[\"ymin\"], ukraine_bbox[\"ymax\"])\n  ) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = clr_ocean))"
  },
  {
    "objectID": "blog/2025/02/13/natural-earth-crimea/index.html#relocating-crimea-manually-with-r-and-sf",
    "href": "blog/2025/02/13/natural-earth-crimea/index.html#relocating-crimea-manually-with-r-and-sf",
    "title": "How to move Crimea from Russia to Ukraine in maps with R",
    "section": "Relocating Crimea manually with R and {sf}",
    "text": "Relocating Crimea manually with R and {sf}\nNatural Earth’s recommendation is to “mashup our countries and disputed areas themes to match their particular political outlook”, so we’ll do that here. Though we won’t use any of the point-of-view themes or features because I have no idea how to get those to work.\nInstead we’ll manipulate the geometry data directly and move Crimea from the Russia shape to the Ukraine shape by extracting the Crimea POLYGON from Russia and merging it with Ukraine.\nThe actual geometric shapes for all the countries in world are MULTIPOLYGONs, or collections of POLYGON geometric objects. For instance, Russia is defined as a single MULTIPOLYGON:\n\nrussia |&gt; st_geometry()\n## Geometry set for 1 feature \n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -180 ymin: 41.15 xmax: 180 ymax: 81.25\n## Geodetic CRS:  WGS 84\n## MULTIPOLYGON (((178.7 71.1, 180 71.52, 180 70.8...\n\nWe can split MULTIPOLYGONs into their component POLYGONs with st_cast(). Russia consists of 14 different shapes:\n\nrussia_polygons &lt;- russia |&gt; \n  st_geometry() |&gt; \n  st_cast(\"POLYGON\")\n\nrussia_polygons\n## Geometry set for 14 features \n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -180 ymin: 41.15 xmax: 180 ymax: 81.25\n## Geodetic CRS:  WGS 84\n## First 5 geometries:\n## POLYGON ((178.7 71.1, 180 71.52, 180 70.83, 178...\n## POLYGON ((49.1 46.4, 48.65 45.81, 47.68 45.64, ...\n## POLYGON ((93.78 81.02, 95.94 81.25, 97.88 80.75...\n## POLYGON ((102.8 79.28, 105.4 78.71, 105.1 78.31...\n## POLYGON ((138.8 76.14, 141.5 76.09, 145.1 75.56...\n\nThe second one is the main Russia landmass:\n\nplot(russia_polygons[2])\n\n\n\n\n\n\n\nThe last one is the Crimean peninsula:\n\nplot(russia_polygons[14])\n\n\n\n\n\n\n\nIdentifying the Crimea POLYGON from a POINT\nThe only way I figured out what of these POLYGONs were was to plot them individually until I saw a recognizable shape. And if I use a different map (like the 50m or 10m resolution maps), there’s no guarantee that Russia will have 14 POLYGONs or that the 14th one will be Crimea. We need a more reliable way to find the Crimea shape.\nOne way to do this is to create a POINT object based somewhere in Crimea and do some geometric set math to identify which Russian POLYGON contains it. The point 45°N 34°E happens to be in the middle of Crimea:\n\ncrimea_point &lt;- st_sfc(st_point(c(34, 45)), crs = st_crs(world))\n\nggplot() +\n  geom_sf(data = world, fill = clr_land) +\n  geom_sf(data = russia, fill = rus_red) + \n  geom_sf(data = ukraine, fill = ukr_blue, color = ukr_yellow, linewidth = 2) + \n  geom_sf(data = crimea_point) +\n  coord_sf(\n    xlim = c(ukraine_bbox[\"xmin\"], ukraine_bbox[\"xmax\"]), \n    ylim = c(ukraine_bbox[\"ymin\"], ukraine_bbox[\"ymax\"])\n  ) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = clr_ocean))\n\n\n\n\n\n\n\nWe can use it with st_intersects() to identify the Russia POLYGON that contains it:\n\n# Extract the Russia MULTIPOLYGON and convert it to polygons\nrussia_polygons &lt;- world |&gt; \n  filter(admin == \"Russia\") |&gt; \n  st_geometry() |&gt; \n  st_cast(\"POLYGON\")\n\n# Extract the Russia polygon that has Crimea in it\ncrimea_polygon &lt;- russia_polygons |&gt;\n  keep(\\(x) st_intersects(x, crimea_point, sparse = FALSE))\n\n# This is the same as russia_polygons[14]\nplot(crimea_polygon)\n\n\n\n\n\n\n\nExtracting the Crimea POLYGON from Russia\nWe can then remove that polygon from Russia and recombine everything back into a MULTIPOLYGON. It works!\n\n# Remove Crimea from Russia\nnew_russia &lt;- russia_polygons |&gt;\n  discard(\\(x) any(st_equals(x, crimea_polygon, sparse = FALSE))) |&gt; \n  st_combine() |&gt; \n  st_cast(\"MULTIPOLYGON\")\n\nggplot() +\n  geom_sf(data = world, fill = clr_land) +\n  geom_sf(data = new_russia, fill = rus_red) + \n  geom_sf(data = ukraine, fill = ukr_blue, color = ukr_yellow, linewidth = 2) + \n  coord_sf(\n    xlim = c(ukraine_bbox[\"xmin\"], ukraine_bbox[\"xmax\"]), \n    ylim = c(ukraine_bbox[\"ymin\"], ukraine_bbox[\"ymax\"])\n  ) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = clr_ocean))\n\n\n\n\n\n\n\nAdding the Crimea POLYGON to Ukraine\nNext we need to merge crimea_polygon with Ukraine. We’ll convert Ukraine to its component POLYGONs, combine those with Crimea, and recombine everything back to a MULTIPOLYGON. It also works!\n\n# Extract the Ukraine MULTIPOLYGON and convert it to polygons\nukraine_polygons &lt;- world |&gt; \n  filter(admin == \"Ukraine\") |&gt; \n  st_geometry() |&gt; \n  st_cast(\"POLYGON\")\n\n# Add Crimea to Ukraine\nnew_ukraine &lt;- st_union(c(ukraine_polygons, crimea_polygon)) |&gt;\n  st_cast(\"MULTIPOLYGON\")\n\nggplot() +\n  geom_sf(data = world, fill = clr_land) +\n  geom_sf(data = new_russia, fill = rus_red) + \n  geom_sf(data = new_ukraine, fill = ukr_blue, color = ukr_yellow, linewidth = 2) + \n  coord_sf(\n    xlim = c(ukraine_bbox[\"xmin\"], ukraine_bbox[\"xmax\"]), \n    ylim = c(ukraine_bbox[\"ymin\"], ukraine_bbox[\"ymax\"])\n  ) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = clr_ocean))\n\n\n\n\n\n\n\nUpdating Russia and Ukraine in the full data\nThe last step is to modify the full world dataset and replace the existing geometry values for the two countries with the updated boundaries:\n\nworld_un &lt;- world |&gt;\n  mutate(geometry = case_when(\n    admin == \"Ukraine\" ~ new_ukraine,\n    admin == \"Russia\" ~ new_russia,\n    .default = geometry\n  ))\n\nNow that world_un has the corrected boundaries in it, it works like normal. Here’s a map of Eastern Europe, colored by mapcolor9 (a column that comes with Natural Earth data that lets you use 9 distinct colors to fill all countries without having bordering countries share colors). Crimea is in Ukraine now:\n\neastern_eu_bbox &lt;- ukraine |&gt; \n  st_buffer(dist = 700000) |&gt;  # Add 700,000 meter buffer around the country \n  st_bbox()\n\nggplot() +\n  geom_sf(data = world_un, aes(fill = factor(mapcolor9)), linewidth = 0.25, color = \"white\") +\n  scale_fill_manual(values = carto_prism, guide = \"none\") +\n  coord_sf(\n    xlim = c(eastern_eu_bbox[\"xmin\"], eastern_eu_bbox[\"xmax\"]), \n    ylim = c(eastern_eu_bbox[\"ymin\"], eastern_eu_bbox[\"ymax\"])\n  ) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = clr_ocean))\n\n\n\n\n\n\n\nThe whole game\nEverything above was fairly didactic, with illustrations at each intermediate step. Here’s the whole process all in one place:\n\nworld_110 &lt;- ne_countries(scale = 110, type = \"map_units\")\n\ncrimea_point_110 &lt;- st_sfc(st_point(c(34, 45)), crs = st_crs(world_110))\n\n# Extract the Russia MULTIPOLYGON and convert it to polygons\nrussia_polygons_110 &lt;- world_110 |&gt; \n  filter(admin == \"Russia\") |&gt; \n  st_geometry() |&gt; \n  st_cast(\"POLYGON\")\n\n# Extract the Russia polygon that has Crimea in it\ncrimea_polygon_110 &lt;- russia_polygons_110 |&gt;\n  keep(\\(x) st_intersects(x, crimea_point_110, sparse = FALSE))\n\n# Remove Crimea from Russia\nnew_russia_110 &lt;- russia_polygons_110 |&gt;\n  discard(\\(x) any(st_equals(x, crimea_polygon_110, sparse = FALSE))) |&gt; \n  st_combine() |&gt; \n  st_cast(\"MULTIPOLYGON\")\n\n# Extract the Ukraine MULTIPOLYGON and convert it to polygons\nukraine_polygons_110 &lt;- world_110 |&gt; \n  filter(admin == \"Ukraine\") |&gt; \n  st_geometry() |&gt; \n  st_cast(\"POLYGON\")\n\n# Add Crimea to Ukraine\nnew_ukraine_110 &lt;- st_union(c(ukraine_polygons_110, crimea_polygon_110)) |&gt;\n  st_cast(\"MULTIPOLYGON\")\n\nworld_un_110 &lt;- world_110 |&gt;\n  mutate(geometry = case_when(\n    admin == \"Ukraine\" ~ new_ukraine_110,\n    admin == \"Russia\" ~ new_russia_110,\n    .default = geometry\n  ))\n\nMoving Crimea with medium resolution (50m) data\nThis same approach works for other map resolutions too, like 50m:\n\nworld_50 &lt;- ne_countries(scale = 50, type = \"map_units\")\n\ncrimea_point_50 &lt;- st_sfc(st_point(c(34, 45)), crs = st_crs(world_50))\n\n# Extract the Russia MULTIPOLYGON and convert it to polygons\nrussia_polygons_50 &lt;- world_50 |&gt; \n  filter(admin == \"Russia\") |&gt; \n  st_geometry() |&gt; \n  st_cast(\"POLYGON\")\n\n# Extract the Russia polygon that has Crimea in it\ncrimea_polygon_50 &lt;- russia_polygons_50 |&gt;\n  keep(\\(x) st_intersects(x, crimea_point_50, sparse = FALSE))\n\n# Remove Crimea from Russia\nnew_russia_50 &lt;- russia_polygons_50 |&gt;\n  discard(\\(x) any(st_equals(x, crimea_polygon_50, sparse = FALSE))) |&gt; \n  st_combine() |&gt; \n  st_cast(\"MULTIPOLYGON\")\n\n# Extract the Ukraine MULTIPOLYGON and convert it to polygons\nukraine_polygons_50 &lt;- world_50 |&gt; \n  filter(admin == \"Ukraine\") |&gt; \n  st_geometry() |&gt; \n  st_cast(\"POLYGON\")\n\n# Add Crimea to Ukraine\nnew_ukraine_50 &lt;- st_union(c(ukraine_polygons_50, crimea_polygon_50)) |&gt;\n  st_cast(\"MULTIPOLYGON\")\n\nworld_un_50 &lt;- world_50 |&gt;\n  mutate(geometry = case_when(\n    admin == \"Ukraine\" ~ new_ukraine_50,\n    admin == \"Russia\" ~ new_russia_50,\n    .default = geometry\n  ))\n\nHere’s a higher quality map of Eastern Europe with Crimea in Ukraine:\n\nggplot() +\n  geom_sf(data = world_un_50, aes(fill = factor(mapcolor9)), linewidth = 0.25, color = \"white\") +\n  scale_fill_manual(values = carto_prism, guide = \"none\") +\n  coord_sf(\n    xlim = c(eastern_eu_bbox[\"xmin\"], eastern_eu_bbox[\"xmax\"]), \n    ylim = c(eastern_eu_bbox[\"ymin\"], eastern_eu_bbox[\"ymax\"])\n  ) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = clr_ocean))"
  },
  {
    "objectID": "blog/2025/02/13/natural-earth-crimea/index.html#using-the-adjusted-natural-earth-data-as-geojson-in-observable-js",
    "href": "blog/2025/02/13/natural-earth-crimea/index.html#using-the-adjusted-natural-earth-data-as-geojson-in-observable-js",
    "title": "How to move Crimea from Russia to Ukraine in maps with R",
    "section": "Using the adjusted Natural Earth data as GeoJSON in Observable JS",
    "text": "Using the adjusted Natural Earth data as GeoJSON in Observable JS\nThis updated shapefile works with Observable Plot too (see here for more about how to make nice maps with Observable), but requires one strange tweak because of weird behavior with the GeoJSON file format.\nBroken GeoJSON\nLet’s export the adjusted geographic data to GeoJSON:\n\n# Save as geojson for Observable Plot\nst_write(\n  obj = world_un, \n  dsn = \"ne_110m_admin_0_countries_un_BROKEN.geojson\", \n  driver = \"GeoJSON\",\n  quiet = TRUE,\n  delete_dsn = TRUE  # Overwrite the existing .geojson if there is one\n)\n\nAnd then load it with Observable JS:\n\nworld_broken = FileAttachment(\"ne_110m_admin_0_countries_un_BROKEN.geojson\").json()\n\nclr_ocean = \"#d9f0ff\"\nclr_land = \"#facba6\"\nukr_blue = \"#0057b7\"\nukr_yellow = \"#ffdd00\"\nrus_red = \"#d62718\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd then plot it:\n\nPlot.plot({\n  projection: \"equal-earth\",\n  marks: [\n    Plot.sphere({ fill: clr_ocean }),\n    Plot.geo(world_broken, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    }) \n  ]\n})\n\n\n\n\n\n\nlol what even. The new Ukraine shape seems to have broken boundaries that distort everything else in the map. Weirdly, Ukraine is filled with the ocean color while the rest of the globe—both the ocean and whatever countries didn’t have their borders erased—is the color of land.\nLet’s zoom in on just Ukraine:\n\nukraine = world_broken.features.find(d =&gt; d.properties.name === \"Ukraine\")\n\nPlot.plot({\n  projection: { \n    type: \"equal-earth\", \n    domain: ukraine, \n    inset: 50 \n  }, \n  width: 800, \n  marks: [\n    Plot.sphere({ fill: clr_ocean }),\n    Plot.geo(world_broken, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    }),\n    Plot.geo(ukraine, { fill: ukr_blue })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¯\\_(ツ)_/¯. Now Ukraine is the color of the ocean and the whole rest of the world is the dark blue of the Ukrainian flag. And it didn’t zoom in at all.\nGeoJSON and ↻ ↺ winding order ↻ ↺\nThis is a symptom of an issue with GeoJSON winding order. GeoJSON cares about the direction that country borders (and all LINESTRING elements) are drawn in. Exterior borders should be drawn counterclockwise; interior borders should be drawn clockwise. If a geographic shape doesn’t follow this winding order, bad things happen. Specifically:\n\na shape that represents a tiny speck of land becomes inflated to represent the whole globe minus that tiny speck of land, the map fills with a uniform color, the local projection explodes. (via @fil)\n\nThat’s exactly what’s happening here. Somehow the winding order is getting reversed when we combine Ukraine with Crimea. {sf} itself doesn’t care about winding order, so everything works fine within R; GeoJSON is picky about winding order, so things break.\nFixing it is tricky though! {sf} uses a bunch of different libraries behind the scenes to do its geographic calculations, including GEOS and S2, and they all have different approaches to polygon creation. Apparently GEOS goes clockwise by default while others go counterclockwise, or something. It should theoretically be possible to fix by adding st_sfc(check_ring_dir = TRUE) after making the new Ukraine shape:\n# It would be cool if this worked but it doesn't :(\nnew_ukraine &lt;- st_union(c(ukraine_polygons, crimea_polygon)) |&gt;\n  st_sfc(check_ring_dir = TRUE) |&gt; \n  st_cast(\"MULTIPOLYGON\")\nBut that doesn’t change anything (nor does it work for this person at GitHub).\nClean GeoJSON with correct winding order\nBUT there’s another solution. We can force {sf} to not use the S2 library (which it uses by default, I guess?), since S2 seems go in the wrong direction. If we turn off S2 with sf_use_s2(FALSE), make the new Ukraine shape, and then turn S2 back on with sf_use_s2(TRUE), things work!\n# Add Crimea to Ukraine\nsf_use_s2(FALSE)\nnew_ukraine_110 &lt;- st_union(c(ukraine_polygons_110, crimea_polygon_110)) |&gt;\n  st_cast(\"MULTIPOLYGON\")\nsf_use_s2(TRUE)\nHere’s the full process with the 110m map:\n\nworld_110 &lt;- ne_countries(scale = 110, type = \"map_units\")\n\ncrimea_point_110 &lt;- st_sfc(st_point(c(34, 45)), crs = st_crs(world_110))\n\n# Extract the Russia MULTIPOLYGON and convert it to polygons\nrussia_polygons_110 &lt;- world_110 |&gt; \n  filter(admin == \"Russia\") |&gt; \n  st_geometry() |&gt; \n  st_cast(\"POLYGON\")\n\n# Extract the Russia polygon that has Crimea in it\ncrimea_polygon_110 &lt;- russia_polygons_110 |&gt;\n  keep(\\(x) st_intersects(x, crimea_point_110, sparse = FALSE))\n\n# Extract the Ukraine MULTIPOLYGON and convert it to polygons\nukraine_polygons_110 &lt;- world_110 |&gt; \n  filter(admin == \"Ukraine\") |&gt; \n  st_geometry() |&gt; \n  st_cast(\"POLYGON\")\n\n# Add Crimea to Ukraine\nsf_use_s2(FALSE)\n## Spherical geometry (s2) switched off\nnew_ukraine_110 &lt;- st_union(c(ukraine_polygons_110, crimea_polygon_110)) |&gt;\n  st_cast(\"MULTIPOLYGON\")\n## although coordinates are longitude/latitude, st_union assumes that they are planar\nsf_use_s2(TRUE)\n## Spherical geometry (s2) switched on\n\n# Remove Crimea from Russia\nnew_russia_110 &lt;- russia_polygons_110 |&gt;\n  discard(\\(x) any(st_equals(x, crimea_polygon_110, sparse = FALSE))) |&gt; \n  st_combine() |&gt; \n  st_cast(\"MULTIPOLYGON\")\n\n# Add the modified Russia and Ukraine to the main data\nworld_un_110_fixed &lt;- world_110 |&gt;\n  mutate(geometry = case_when(\n    admin == \"Ukraine\" ~ new_ukraine_110,\n    admin == \"Russia\" ~ new_russia_110,\n    .default = geometry\n  ))\n\n# Save as GeoJSON\nst_write(\n  obj = world_un_110_fixed, \n  dsn = \"ne_110m_admin_0_countries_un.geojson\", \n  driver = \"GeoJSON\",\n  quiet = TRUE,\n  delete_dsn = TRUE\n)\n\nHere’s the new world map with the the correct Ukraine:\n\nworld_fixed = FileAttachment(\"ne_110m_admin_0_countries_un.geojson\").json()\n\nPlot.plot({\n  projection: \"equal-earth\",\n  marks: [\n    Plot.sphere({ fill: clr_ocean }),\n    Plot.geo(world_fixed, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    }) \n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can zoom in on Ukraine too:\n\nukraine_good = world_fixed.features.find(d =&gt; d.properties.name === \"Ukraine\")\nrussia = world_fixed.features.find(d =&gt; d.properties.name === \"Russia\")\n\nPlot.plot({\n  projection: { \n    type: \"equal-earth\", \n    domain: ukraine_good, \n    inset: 50 \n  }, \n  width: 800, \n  marks: [\n    Plot.sphere({ fill: clr_ocean }),\n    Plot.geo(world_fixed, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    }),\n    Plot.geo(russia, { fill: rus_red }),\n    Plot.geo(ukraine_good, { \n      fill: ukr_blue, \n      stroke: ukr_yellow, \n      strokeWidth: 3\n    })\n  ]\n})"
  },
  {
    "objectID": "blog/2025/02/13/natural-earth-crimea/index.html#alternative-data-sources",
    "href": "blog/2025/02/13/natural-earth-crimea/index.html#alternative-data-sources",
    "title": "How to move Crimea from Russia to Ukraine in maps with R",
    "section": "Alternative data sources",
    "text": "Alternative data sources\nNatural Earth isn’t the only source of geographic data online, and other sources use de jure borders instead of de facto borders, like these:\nGISCO\nThe European Commission’s Eurostat hosts the Geographic Information System of the Commission (GISCO), which provides GIS data for the EU. They offer global shapefiles that follow EU-based de jure borders. The {giscoR} package provides a nice frontend for getting that data into R at 5 different resolutions (1:60m, 1:20m, 1:10m, 1:3m, and super detailed 1:1m!). It does not come with additional metadata for each country, though (i.e. there are no regional divisions, population values, map colors, names in other languages, and so on), so it requires some extra cleaning work if you want those details. For example, can add region information with {countrycode}:\n\nlibrary(giscoR)\nlibrary(countrycode)\n\nworld_gisco &lt;- gisco_get_countries(\n  year = \"2024\",\n  epsg = \"4326\",\n  resolution = \"60\"\n) |&gt; \n  # Add World Bank regions\n  mutate(region = countrycode(ISO3_CODE, origin = \"iso3c\", destination = \"region\"))\n\nworld_gisco |&gt; \n  filter(NAME_ENGL != \"Antarctica\") |&gt; \n  ggplot() + \n  geom_sf(aes(fill = region), color = \"white\", linewidth = 0.1) +\n  scale_fill_manual(values = carto_prism, guide = \"none\") +\n  coord_sf(crs = \"+proj=robin\") +\n  theme_void()\n\n\n\n\n\n\n\nSince the EU doesn’t de jure-ily recognize the Russian occupation of Crimea, Crimea is in Ukraine:\n\nukraine_gisco &lt;- world_gisco |&gt; filter(NAME_ENGL == \"Ukraine\")\nrussia_gisco &lt;- world_gisco |&gt; filter(NAME_ENGL == \"Russian Federation\")\n\nukraine_gisco_bbox &lt;- ukraine_gisco |&gt; \n  st_buffer(dist = 100000) |&gt;  # Add 100,000 meter buffer around the country \n  st_bbox()\n\nggplot() +\n  geom_sf(data = world_gisco, fill = clr_land) +\n  geom_sf(data = russia_gisco, fill = rus_red) + \n  geom_sf(data = ukraine_gisco, fill = ukr_blue, color = ukr_yellow, linewidth = 2) + \n  coord_sf(\n    xlim = c(ukraine_gisco_bbox[\"xmin\"], ukraine_gisco_bbox[\"xmax\"]), \n    ylim = c(ukraine_gisco_bbox[\"ymin\"], ukraine_gisco_bbox[\"ymax\"])\n  ) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = clr_ocean))\n\n\n\n\n\n\n\nIt’s possible to use GISCO data with Observable too. We could load the data into R and clean it up there (like adding regions and other details) and the save it as GeoJSON, like we did with the Natural Earth data.\nOr we can grab the original raw GeoJSON from Eurostat directly. For example, here’s the raw 60M 2024 world map using the WGS84 (4326) projection that we grabbed with gisco_get_countries() earlier.\n\nworld_gisco = await FileAttachment(\"https://gisco-services.ec.europa.eu/distribution/v2/countries/geojson/CNTR_RG_60M_2024_4326.geojson\").json()\n\nukraine_gisco = world_gisco.features.find(d =&gt; d.properties.NAME_ENGL === \"Ukraine\")\nrussia_gisco = world_gisco.features.find(d =&gt; d.properties.NAME_ENGL === \"Russian Federation\")\n\nPlot.plot({\n  projection: { \n    type: \"equal-earth\", \n    domain: ukraine_gisco, \n    inset: 50 \n  }, \n  width: 800, \n  marks: [\n    Plot.sphere({ fill: clr_ocean }),\n    Plot.geo(world_gisco, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    }),\n    Plot.geo(russia_gisco, { fill: rus_red }),\n    Plot.geo(ukraine_gisco, { \n      fill: ukr_blue, \n      stroke: ukr_yellow, \n      strokeWidth: 3\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisionscarto\nThere are JSON-based map files created by @fil at Observable as part of the Visionscarto project.\nThey’re based on Natural Earth, but with some specific adjustments like adding Crimea to Ukraine, making Gaza a little bit bigger so that it doesn’t get dropped at lower resolutions like 110m, and using UN boundaries for Western Sahara.\nLike GISCO, though, these don’t have the additional columns that Natural Earth comes with (country names in a bunch of languages, region and continent designations, map coloring schemes, population and GDP estimates, etc.), and those would need to be added manually in R or Observable or whatever.\n\nimport {world110m} from \"@visionscarto/geo\"\n\ncountries110m = topojson.feature(world110m, world110m.objects.countries)\nukraine_visionscarto = countries110m.features.find(d =&gt; d.properties.name === \"Ukraine\")\nrussia_visionscarto = countries110m.features.find(d =&gt; d.properties.name === \"Russia\")\n\nPlot.plot({\n  projection: { \n    type: \"equal-earth\", \n    domain: ukraine_visionscarto, \n    inset: 50 \n  }, \n  width: 800, \n  marks: [\n    Plot.sphere({ fill: clr_ocean }),\n    Plot.geo(countries110m, {\n      stroke: \"black\",\n      strokeWidth: 0.5,\n      fill: clr_land\n    }),\n    Plot.geo(russia_visionscarto, { fill: rus_red }),\n    Plot.geo(ukraine_visionscarto, { \n      fill: ukr_blue, \n      stroke: ukr_yellow, \n      strokeWidth: 3\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLess automatic sources\nThere are other sources too, but they require manual downloading:\n\n\nThe geoBoundaries project, hosted by the William & Mary geoLab\n\nThe UN’s Country Boundaries of the World hosted at the UN’s Food and Agricultural Organization (FAO) site\nOpendatasoft’s copy of the World Food Programme’s World Administrative Boundaries shapefile"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2025",
    "text": "2025\n\n\n    \n    \n                  \n            February 19, 2025\n        \n        \n            How to use a histogram as a legend in {ggplot2}\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    gis\n                \n                \n                \n                    maps\n                \n                \n            \n            \n\n            Land isn't unemployed—people are. Here's how to use R, {ggplot2}, {sf}, and {patchwork} to create a histogram legend in a choropleth map to better see the distribution of values.\n            \n            \n            10.59350/gt0nr-wct91\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 13, 2025\n        \n        \n            How to move Crimea from Russia to Ukraine in maps with R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    ojs\n                \n                \n                \n                    observable plot\n                \n                \n                \n                    gis\n                \n                \n                \n                    maps\n                \n                \n            \n            \n\n            Natural Earth's de facto on-the-ground policy conflicts with de jure boundaries. Use {sf} and R to relocate parts of country shapes.\n            \n            \n            10.59350/28kp0-nbq92\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 10, 2025\n        \n        \n            Using USAID data to make fancy world maps with Observable Plot\n\n            \n            \n                \n                \n                    ojs\n                \n                \n                \n                    observable plot\n                \n                \n                \n                    gis\n                \n                \n                \n                    maps\n                \n                \n                \n                    usaid\n                \n                \n            \n            \n\n            Manipulate geographic data, change projections, get live data from a database, and make interactive plots with Observable JS\n            \n            \n            10.59350/c0aep-hp989\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 27, 2025\n        \n        \n            Guide to comparing sample and population proportions with CPS data, both classically and Bayesianly\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    us census\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    bayes\n                \n                \n                \n                    brms\n                \n                \n                \n                    surveys\n                \n                \n            \n            \n\n            Download CPS demographic data from IPUMS and use R and {brms} to calculate differences between sample and national proportions with Bayesian ROPE-based inference\n            \n            \n            10.59350/8ws3f-1fd56\n            \n        \n        \n        \n            \n\n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            December 4, 2024\n        \n        \n            Apple Music Wrapped with R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    music\n                \n                \n            \n            \n\n            Use R to parse Apple Music XML files and create your own Spotify Wrapped-like stats\n            \n            \n            10.59350/64kxj-xp130\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 4, 2024\n        \n        \n            Guide to generating and rendering computational markdown content programmatically with Quarto\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    quarto\n                \n                \n                \n                    markdown\n                \n                \n            \n            \n\n            Learn how to use `knitr::knit()` in inline chunks to correctly render auto-generated R and markdown content in Quarto documents\n            \n            \n            10.59350/pa44j-cc302\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 8, 2024\n        \n        \n            Fun with Positron\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    python\n                \n                \n                \n                    positron\n                \n                \n                \n                    data science\n                \n                \n            \n            \n\n            Combine the best of RStudio and Visual Studio Code in Posit's new Positron IDE\n            \n            \n            10.59350/zs7da-17c67\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 8, 2024\n        \n        \n            Calculating the proportion of US state borders that are coastlines\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    gis\n                \n                \n                \n                    maps\n                \n                \n            \n            \n\n            Measuring coastlines is hard and causes fractal paradoxes, but we can use R and {sf} to try!\n            \n            \n            10.59350/wzn5q-wxk02\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 3, 2024\n        \n        \n            Calculating birthday probabilities with R instead of math\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    simulations\n                \n                \n                \n                    probability\n                \n                \n            \n            \n\n            Probability math is hard. Use brute force simulation to find the probability that a household has a cluster of birthdays.\n            \n            \n            10.59350/r419r-zqj73\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 4, 2024\n        \n        \n            Visualizing {dplyr}'s mutate(), summarize(), group_by(), and ungroup() with animations\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    dplyr\n                \n                \n                \n                    animations\n                \n                \n            \n            \n\n            Visually explore how {dplyr}'s more complex core functions work together to wrangle data\n            \n            \n            10.59350/d2sz4-w4e25\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 21, 2024\n        \n        \n            Demystifying causal inference estimands: ATE, ATT, and ATU\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    causal inference\n                \n                \n                \n                    DAGs\n                \n                \n                \n                    inverse probability weighting\n                \n                \n            \n            \n\n            Explore why we care about the ATE, ATT, and ATU and figure out how to calculate them with observational data\n            \n            \n            10.59350/c9z3a-rcq16\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 12, 2024\n        \n        \n            DIY API with Make and {plumber}\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    quarto\n                \n                \n                \n                    observablejs\n                \n                \n                \n                    plumber\n                \n                \n                \n                    api\n                \n                \n                \n                    make\n                \n                \n                \n                    goals\n                \n                \n            \n            \n\n            Use Make and {plumber} to create your own API and show live data in Quarto with R and Observable JS\n            \n            \n            10.59350/pe5s8-e0f47\n            \n        \n        \n        \n            \n\n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "Blog",
    "section": "2023",
    "text": "2023\n\n\n    \n    \n                  \n            December 11, 2023\n        \n        \n            How to create separate bibliographies in a Quarto document\n\n            \n            \n                \n                \n                    quarto\n                \n                \n                \n                    pandoc\n                \n                \n                \n                    citations\n                \n                \n                \n                    markdown\n                \n                \n                \n                    acwri\n                \n                \n                \n                    writing\n                \n                \n            \n            \n\n            Use multibib and Quarto to create separate bibliographies for a document and an appendix\n            \n            \n            10.59350/5dvez-q6817\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 18, 2023\n        \n        \n            Guide to understanding the intuition behind the Dirichlet distribution\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    statistics\n                \n                \n                \n                    regression\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n            \n            \n\n            Learn about the Dirichlet distribution and explore how it's just a fancier version of the Beta distribution\n            \n            \n            10.59350/64j0k-26134\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 15, 2023\n        \n        \n            Manually generate predicted values for logistic regression with matrix multiplication in R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    statistics\n                \n                \n                \n                    regression\n                \n                \n            \n            \n\n            This is like basic stats stuff, but I can never remember how to do it—here's how to use matrix multiplication to replicate the results of `predict()`\n            \n            \n            10.59350/qba9a-b3561\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 12, 2023\n        \n        \n            The ultimate practical guide to multilevel multinomial conjoint analysis with R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    statistics\n                \n                \n                \n                    brms\n                \n                \n                \n                    stan\n                \n                \n            \n            \n\n            Learn how to use R, {brms}, {marginaleffects}, and {tidybayes} to analyze discrete choice conjoint data with fully specified hierarchical multilevel multinomial models\n            \n            \n            10.59350/2mz75-rrc46\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 28, 2023\n        \n        \n            How to fill maps with density gradients with R, {ggplot2}, and {sf}\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    gis\n                \n                \n                \n                    maps\n                \n                \n            \n            \n\n            Fix overplotted points on maps by creating bins or filled desntiy gradients using R, {ggplot2}, and {sf}\n            \n            \n            10.59350/bsctw-0a955\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 25, 2023\n        \n        \n            The ultimate practical guide to conjoint analysis with R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    statistics\n                \n                \n                \n                    brms\n                \n                \n                \n                    stan\n                \n                \n            \n            \n\n            Learn how to use R, {brms}, and {marginaleffects} to analyze conjoint data and find causal and descriptive quantities of interest, both frequentistly and Bayesianly\n            \n            \n            10.59350/xgwjy-dyj66\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 3, 2023\n        \n        \n            Road trip analysis! How to use and play with Google Location History in R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    gis\n                \n                \n                \n                    maps\n                \n                \n            \n            \n\n            Learn how to use R to load and clean and play with all the location history data Google keeps about you and look at some neat plots and tables about our 5,000-mile summer road trip along the way\n            \n            \n            10.59350/24rwv-k9n62\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 1, 2023\n        \n        \n            How to make fancy road trip maps with R and OpenStreetMap\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    gis\n                \n                \n                \n                    maps\n                \n                \n            \n            \n\n            Use R to get geocoded location and routing data from OpenStreetMap and explore our family's impending 5,000 mile road trip around the USA\n            \n            \n            10.59350/rgwda-0tv16\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 15, 2023\n        \n        \n            A guide to Bayesian proportion tests with R and {brms}\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    bayes\n                \n                \n                \n                    brms\n                \n                \n                \n                    stan\n                \n                \n                \n                    surveys\n                \n                \n                \n                    categorical data\n                \n                \n            \n            \n\n            Use R, Stan, and {brms} to calculate differences between categorical proportions in a principled Bayesian way\n            \n            \n            10.59350/kw2gj-kw740\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 26, 2023\n        \n        \n            Making Middle Earth maps with R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    gis\n                \n                \n                \n                    maps\n                \n                \n                \n                    nerdery\n                \n                \n            \n            \n\n            Explore Tolkien's Middle Earth with R-based GIS tools, including {ggplot2} and {sf}\n            \n            \n            10.59350/ccrtd-z3s22\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 21, 2023\n        \n        \n            How old was Aragorn in regular human years?\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    simulations\n                \n                \n                \n                    brms\n                \n                \n                \n                    nerdery\n                \n                \n            \n            \n\n            Use statistical simulation and a hidden table of Númenórean ages from Tolkien's unpublished works to convert Aragorn's Dúnedan years to actual human years\n            \n            \n            10.59350/e0855-b1171\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 9, 2023\n        \n        \n            One Simple Trick™ to create inline bibliography entries with Markdown and pandoc\n\n            \n            \n                \n                \n                    writing\n                \n                \n                \n                    markdown\n                \n                \n                \n                    citations\n                \n                \n                \n                    pandoc\n                \n                \n                \n                    zotero\n                \n                \n            \n            \n\n            By default, pandoc doesn't include full bibliographic references inline in documents, but with one tweak to a CSL file, you can create syllabus-like lists of citations with full references\n            \n            \n            10.59350/hwwgk-v9636\n            \n        \n        \n    \n    \n    \n                  \n            January 8, 2023\n        \n        \n            How to migrate from BibDesk to Zotero for pandoc-based writing\n\n            \n            \n                \n                \n                    writing\n                \n                \n                \n                    markdown\n                \n                \n                \n                    citations\n                \n                \n                \n                    pandoc\n                \n                \n                \n                    zotero\n                \n                \n            \n            \n\n            Tips, tricks, and rationale for converting from a single big BibTeX file to a Zotero database\n            \n            \n            10.59350/cwrq4-m7h10\n            \n        \n        \n        \n            \n\n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-3",
    "href": "blog/index.html#section-3",
    "title": "Blog",
    "section": "2022",
    "text": "2022\n\n\n    \n    \n                  \n            December 8, 2022\n        \n        \n            How to use natural and base 10 log scales in ggplot2\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    data visualization\n                \n                \n            \n            \n\n            Use the {scales} R package to automatically adjust and format x- and y-axis scales to use log base 10 and natural log values\n            \n            \n            10.59350/b4gjd-50c81\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 29, 2022\n        \n        \n            Marginal and conditional effects for GLMMs with {marginaleffects}\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    regression\n                \n                \n                \n                    statistics\n                \n                \n                \n                    bayes\n                \n                \n                \n                    brms\n                \n                \n                \n                    lognormal\n                \n                \n            \n            \n\n            Use the {marginaleffects} package to calculate tricky and nuanced marginal and conditional effects in generalized linear mixed models\n            \n            \n            10.59350/xwnfm-x1827\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 26, 2022\n        \n        \n            Visualizing the differences between Bayesian posterior predictions, linear predictions, and the expectation of posterior predictions\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    regression\n                \n                \n                \n                    bayes\n                \n                \n                \n                    brms\n                \n                \n                \n                    stan\n                \n                \n            \n            \n\n            A guide to different types of Bayesian posterior distributions and the nuances of posterior_predict, posterior_epred, and posterior_linpred\n            \n            \n            10.59350/xge39-emt86\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            June 23, 2022\n        \n        \n            Quick and easy ways to deal with long labels in ggplot2\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    data visualization\n                \n                \n            \n            \n\n            Explore different manual and automatic ways to rotate, dodge, recode, break up, and otherwise deal with long axis labels with ggplot2\n            \n            \n            10.59350/x7xtj-3dh31\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 20, 2022\n        \n        \n            Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    regression\n                \n                \n                \n                    statistics\n                \n                \n                \n                    data visualization\n                \n                \n                \n                    marginal effects\n                \n                \n            \n            \n\n            Define what marginal effects even are, and then explore the subtle differences between average marginal effects, marginal effects at the mean, and marginal effects at representative values with the marginaleffects and emmeans R packages\n            \n            \n            10.59350/40xaj-4e562\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            May 9, 2022\n        \n        \n            A guide to modeling outcomes that have lots of zeros with Bayesian hurdle lognormal and hurdle Gaussian regression models\n\n            \n\n            Create, manipulate, understand, analyze, interpret, and plot Bayesian hurdle regression models (and a custom hurdle Gaussian model!) using R, the tidyverse, emmeans, brms, and Stan\n            \n            \n            10.59350/ety2j-09566\n            \n        \n        \n        \n            \n\n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-4",
    "href": "blog/index.html#section-4",
    "title": "Blog",
    "section": "2021",
    "text": "2021\n\n\n    \n    \n                  \n            December 20, 2021\n        \n        \n            How to create a(n almost) fully Bayesian outcome model with inverse probability weights\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    regression\n                \n                \n                \n                    statistics\n                \n                \n                \n                    data visualization\n                \n                \n                \n                    causal inference\n                \n                \n                \n                    do calculus\n                \n                \n                \n                    DAGs\n                \n                \n                \n                    bayes\n                \n                \n                \n                    brms\n                \n                \n                \n                    stan\n                \n                \n            \n            \n\n            Use a posterior distribution of inverse probability weights in a Bayesian outcome model to conduct (nearly) fully Bayesian causal inference with R, brms, and Stan\n            \n            \n            10.59350/gyvjk-hrx68\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 18, 2021\n        \n        \n            How to use Bayesian propensity scores and inverse probability weights\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    regression\n                \n                \n                \n                    statistics\n                \n                \n                \n                    data visualization\n                \n                \n                \n                    causal inference\n                \n                \n                \n                    do calculus\n                \n                \n                \n                    DAGs\n                \n                \n                \n                    bayes\n                \n                \n                \n                    brms\n                \n                \n                \n                    stan\n                \n                \n            \n            \n\n            For mathematical and philosophical reasons, propensity scores and inverse probability weights don't work in Bayesian inference. But never fear! There's still a way to do it!\n            \n            \n            10.59350/nrwsd-3jz20\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 1, 2021\n        \n        \n            A guide to working with country-year panel data and Bayesian multilevel models\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    regression\n                \n                \n                \n                    statistics\n                \n                \n                \n                    data visualization\n                \n                \n                \n                    bayes\n                \n                \n                \n                    brms\n                \n                \n                \n                    stan\n                \n                \n            \n            \n\n            How to use multilevel models with R and brms to work with country-year panel data.\n            \n            \n            10.59350/t19jz-ds665\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 10, 2021\n        \n        \n            A guide to correctly calculating posterior predictions and average marginal effects with multilievel Bayesian models\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    regression\n                \n                \n                \n                    statistics\n                \n                \n                \n                    data visualization\n                \n                \n                \n                    bayes\n                \n                \n                \n                    brms\n                \n                \n                \n                    stan\n                \n                \n                \n                    beta\n                \n                \n            \n            \n\n            How to calculate grand means, conditional group means, and hypothetical group means of posterior predictions from multilevel brms models.\n            \n            \n            10.59350/wbn93-edb02\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            November 8, 2021\n        \n        \n            A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    regression\n                \n                \n                \n                    statistics\n                \n                \n                \n                    data visualization\n                \n                \n                \n                    bayes\n                \n                \n                \n                    beta\n                \n                \n            \n            \n\n            Everything you ever wanted to know about beta regression! Use R and brms to correctly model proportion data, and learn all about the beta distribution along the way.\n            \n            \n            10.59350/7p1a4-0tw75\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 7, 2021\n        \n        \n            Do-calculus adventures! Exploring the three rules of do-calculus in plain language and deriving the backdoor adjustment formula by hand\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    DAGs\n                \n                \n                \n                    causal inference\n                \n                \n                \n                    do calculus\n                \n                \n            \n            \n\n            Use R to explore the three rules of do-calculus in plain language and derive the backdoor adjustment formula by hand\n            \n            \n            10.59350/fqkhz-kq526\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 27, 2021\n        \n        \n            How to automatically convert TikZ images to SVG (with fonts!) from knitr\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    R markdown\n                \n                \n                \n                    tikz\n                \n                \n            \n            \n\n            Make knitr and R Markdown convert TikZ graphics to font-embedded SVG files when knitting to HTML\n            \n            \n            10.59350/6qevp-87j81\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 25, 2021\n        \n        \n            Exploring Pamela Jakiela's simple TWFE diagnostics with R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    regression\n                \n                \n                \n                    statistics\n                \n                \n                \n                    data visualization\n                \n                \n                \n                    econometrics\n                \n                \n                \n                    panel data\n                \n                \n            \n            \n\n            Use R to explore possible biases that come from differential treatment timing in two-way fixed effects (TWFE) regression models\n            \n            \n            10.59350/yrbym-m0y62\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 21, 2021\n        \n        \n            Exploring R² and regression variance with Euler/Venn diagrams\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    regression\n                \n                \n                \n                    statistics\n                \n                \n                \n                    data visualization\n                \n                \n            \n            \n\n            Use Venn diagrams to visualize the proportion of an outcome explained by a regression model\n            \n            \n            10.59350/t57vy-p5115\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            July 20, 2021\n        \n        \n            AFC Richmond / Ted Lasso cross stitch pattern\n\n            \n            \n                \n                \n                    art\n                \n                \n                \n                    cross stitch\n                \n                \n                \n                    pandemic boredom\n                \n                \n                \n                    ted lasso\n                \n                \n            \n            \n\n            Make your own Ted Lasso AFC Richmond crest cross stitch with a free pattern and an Illustrator template\n            \n            \n            10.59350/qvp0m-tr543\n            \n        \n        \n    \n    \n    \n                  \n            July 10, 2021\n        \n        \n            Hex sticker/logo cross stitch pattern\n\n            \n            \n                \n                \n                    art\n                \n                \n                \n                    cross stitch\n                \n                \n                \n                    pandemic boredom\n                \n                \n                \n                    data science\n                \n                \n            \n            \n\n            Make your own data science hex logo cross stitch with a free pattern and an Illustrator template\n            \n            \n            10.59350/ha58q-xzf62\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 26, 2021\n        \n        \n            Bayesian (cross stitch) sampler\n\n            \n            \n                \n                \n                    art\n                \n                \n                \n                    cross stitch\n                \n                \n                \n                    pandemic boredom\n                \n                \n                \n                    bayes\n                \n                \n            \n            \n\n            Make your own Bayesian cross stitch sampler with a free pattern of Bayes Theorem and the accompanying Illustrator template\n            \n            \n            10.59350/pdqkh-czk27\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 15, 2021\n        \n        \n            Marginal structural models for panel data with GEE and multilevel models\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    causal inference\n                \n                \n                \n                    DAGs\n                \n                \n                \n                    do calculus\n                \n                \n                \n                    inverse probability weighting\n                \n                \n            \n            \n\n            Use R to correctly close backdoor confounding in panel data with marginal structural models and inverse probability weights with both GEE and multilevel models\n            \n            \n            10.59350/yqs5b-36r77\n            \n        \n        \n        \n            \n\n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-5",
    "href": "blog/index.html#section-5",
    "title": "Blog",
    "section": "2020",
    "text": "2020\n\n\n    \n    \n                  \n            December 3, 2020\n        \n        \n            Generating inverse probability weights for marginal structural models with time-series cross-sectional panel data\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    causal inference\n                \n                \n                \n                    DAGs\n                \n                \n                \n                    do calculus\n                \n                \n                \n                    inverse probability weighting\n                \n                \n            \n            \n\n            Use R to close backdoor confounding in panel data with marginal structural models and inverse probability weights for both binary and continuous treatments\n            \n            \n            10.59350/48w1z-xen07\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            December 1, 2020\n        \n        \n            Generating inverse probability weights for both binary and continuous treatments\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    causal inference\n                \n                \n                \n                    DAGs\n                \n                \n                \n                    do calculus\n                \n                \n                \n                    inverse probability weighting\n                \n                \n            \n            \n\n            Use R to close backdoor confounding by generating and using inverse probability weights for both binary and continuous treatments\n            \n            \n            10.59350/1svkc-rkv91\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            March 12, 2020\n        \n        \n            Emergency online teaching resources\n\n            \n            \n                \n                \n                    teaching\n                \n                \n                \n                    COVID-19\n                \n                \n            \n            \n\n            List of resources to help teach online as universities rapidly shut down during the COVID-19 pandemic\n            \n            \n            10.59350/3bzxm-ecg70\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 25, 2020\n        \n        \n            Ways to close backdoors in DAGs\n\n            \n\n            Use regression, inverse probability weighting, and matching to close confounding backdoors and find causation in observational data\n            \n            \n            10.59350/z5y4e-jgk85\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 10, 2020\n        \n        \n            Automatically zip up subdirectories with Make\n\n            \n\n            Use a Makefile to automatically zip up all subdirectories in a given folder while also accounting for dependencies\n            \n            \n            10.59350/t1hra-4a041\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 1, 2020\n        \n        \n            Create a dynamic dashboard with R, flexdashboard, and Shiny\n\n            \n            \n                \n                \n                    R markdown\n                \n                \n                \n                    shiny\n                \n                \n                \n                    r\n                \n                \n                \n                    google sheets\n                \n                \n            \n            \n\n            Use R Markdown, flexdashboard, and Shiny to create a dashboard that automatically loads data from a Google Sheet\n            \n            \n            10.59350/8vtys-s0p68\n            \n        \n        \n        \n            \n\n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-6",
    "href": "blog/index.html#section-6",
    "title": "Blog",
    "section": "2019",
    "text": "2019\n\n\n    \n    \n                  \n            October 9, 2019\n        \n        \n            Convert Markdown to rich text (with syntax highlighting!) in any macOS app\n\n            \n            \n                \n                \n                    pandoc\n                \n                \n                \n                    macos\n                \n                \n            \n            \n\n            Create a macOS Automator service to convert Markdown to rich text from any app in macOS\n            \n            \n            10.59350/y07h7-cn637\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            February 16, 2019\n        \n        \n            Chidi's budget and utility: doing algebra and calculus with R and yacas\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    yacas\n                \n                \n                \n                    economics\n                \n                \n            \n            \n\n            Use algebra and calculus with R and yacas to find Chidi's optimal level of pizza and frozen yogurt consumption given his budget and utility function.\n            \n            \n            10.59350/ass44-ypk45\n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            January 29, 2019\n        \n        \n            Half a dozen frequentist and Bayesian ways to measure the difference in means in two groups\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    stan\n                \n                \n                \n                    bayes\n                \n                \n            \n            \n\n            Learn how to run standard t-tests, simulations, and Bayesian difference in means tests with R and Stan\n            \n            \n            10.59350/94eck-1gd29\n            \n        \n        \n        \n            \n\n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-7",
    "href": "blog/index.html#section-7",
    "title": "Blog",
    "section": "2018",
    "text": "2018\n\n\n    \n    \n                  \n            December 28, 2018\n        \n        \n            Tidy text, parts of speech, and unique words in the Qur'an\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    tidytext\n                \n                \n                \n                    arabic\n                \n                \n            \n            \n\n            Use R and parts-of-speech tagging to explore the Qur'an in Arabic\n            \n            \n        \n        \n    \n    \n    \n                  \n            December 26, 2018\n        \n        \n            Tidy text, parts of speech, and unique words in the Bible\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    tidytext\n                \n                \n            \n            \n\n            Use R and parts-of-speech tagging to explore the distinctive features of John\n            \n            \n        \n        \n    \n    \n    \n                  \n            December 17, 2018\n        \n        \n            The academic job search finally comes to an end\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    jobs\n                \n                \n            \n            \n\n            Explore 2.5 years of applying for academic jobs with fancy data visualization\n            \n            \n            10.59350/szk3h-v8674\n            \n        \n        \n    \n    \n    \n                  \n            December 5, 2018\n        \n        \n            How to test any hypothesis with the infer package\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    infer\n                \n                \n                \n                    hypothesis testing\n                \n                \n            \n            \n\n            Use the infer package in R to test any statistical hypothesis through simulation.\n            \n            \n        \n        \n    \n    \n    \n                  \n            July 30, 2018\n        \n        \n            Create a cheap, disposable supercomputer with R, DigitalOcean, and future\n\n            \n\n            Use the future R package to run computationally intensive R commands on a cluster of remote computers\n            \n            \n        \n        \n    \n    \n    \n                  \n            March 8, 2018\n        \n        \n            Show multiply imputed results in a side-by-side regression table with broom and huxtable\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    imputation\n                \n                \n                \n                    tidyverse\n                \n                \n                \n                    markdown\n                \n                \n            \n            \n\n            Extend broom's tidy() and glance() to work with lists of multiply imputed regression models\n            \n            \n        \n        \n    \n    \n    \n                  \n            March 7, 2018\n        \n        \n            Meld regression output from multiple imputations with tidyverse\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    imputation\n                \n                \n                \n                    tidyverse\n                \n                \n            \n            \n\n            Use tidyverse functions to correctly meld and pool multiply imputed model output.\n            \n            \n        \n        \n    \n    \n    \n                  \n            February 15, 2018\n        \n        \n            Fun with empirical and function-based derivatives in R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    economics\n                \n                \n            \n            \n\n            Use R to do things with derivatives, both with actual functions and with existing empirical data.\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-8",
    "href": "blog/index.html#section-8",
    "title": "Blog",
    "section": "2017",
    "text": "2017\n\n\n    \n    \n                  \n            September 27, 2017\n        \n        \n            Working with R, Cairo graphics, custom fonts, and ggplot\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    cairo\n                \n                \n                \n                    fonts\n                \n                \n            \n            \n\n            The Cairo graphics library makes it easy to embed custom fonts in PDFs and create high resolution PNGs (with either AGG or Cairo).\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            September 15, 2017\n        \n        \n            Create supply and demand economics curves with ggplot2\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    economics\n                \n                \n            \n            \n\n            Use ggplot to create economics-style, non-data-based conceptual graphs.\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 26, 2017\n        \n        \n            Quickly play with Polity IV and OECD data (and see the danger of US democracy)\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    dataviz\n                \n                \n            \n            \n\n            Use ggplot to reproducibly see how much trouble the Polity Project thinks the US is in.\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            August 10, 2017\n        \n        \n            Exploring Minard's 1812 plot with ggplot2\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    dataviz\n                \n                \n            \n            \n\n            Use ggplot to do fun and fancy things with Minard's famous plot of Napoleon's 1812 retreat from Russia.\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 27, 2017\n        \n        \n            Super basic practical guide to Docker and RStudio\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    docker\n                \n                \n            \n            \n\n            Use RStudio inside Docker containers for portable and reproducible development environments.\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-9",
    "href": "blog/index.html#section-9",
    "title": "Blog",
    "section": "2016",
    "text": "2016\n\n\n    \n    \n                  \n            December 8, 2016\n        \n        \n            Save base graphics as pseudo-objects in R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    ggplot\n                \n                \n                \n                    graphics\n                \n                \n            \n            \n\n            Use pryr to save a series of R commands as a kind of macro you can call repeatedly.\n            \n            \n        \n        \n    \n    \n    \n                  \n            April 25, 2016\n        \n        \n            Convert logistic regression standard errors to odds ratios with R\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    regression\n                \n                \n            \n            \n\n            Correctly transform logistic regression standard errors to odds ratios using R\n            \n            \n        \n        \n        \n            \n\n        \n    \n    \n    \n                  \n            April 3, 2016\n        \n        \n            Drone sightings in the US, visualized\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    dataviz\n                \n                \n                \n                    hrbrmstr-challenge\n                \n                \n            \n            \n\n            See where the FAA has reported hobbyist drone sightings from 2014–2016\n            \n            \n        \n        \n    \n    \n    \n                  \n            February 11, 2016\n        \n        \n            Fauxcasts: Use a podcast app to listen to audiobooks\n\n            \n            \n                \n                \n                    rss\n                \n                \n                \n                    podcasts\n                \n                \n                \n                    audiobooks\n                \n                \n            \n            \n\n            Create a temporary podcast feed of a CD-based audiobook and use a modern podcast app to listen to the book faster and better.\n            \n            \n        \n        \n    \n    \n    \n                  \n            February 10, 2016\n        \n        \n            Use LibreOffice Base as a GUI for an SQLite database in OS X\n\n            \n            \n                \n                \n                    osx\n                \n                \n                \n                    sqlite\n                \n                \n                \n                    databases\n                \n                \n            \n            \n\n            Connect LibreOffice to an SQLite database to take advantage of SQLite’s ubiquitousness and LibreOffice’s form-based GUI.\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-10",
    "href": "blog/index.html#section-10",
    "title": "Blog",
    "section": "2013",
    "text": "2013\n\n\n    \n    \n                  \n            May 28, 2013\n        \n        \n            Toggle the Bluetooth menu item with AppleScript\n\n            \n            \n                \n                \n                    osx\n                \n                \n                \n                    automation\n                \n                \n                \n                    AppleScript\n                \n                \n            \n            \n\n            The OS X Bluetooth menu item re-enables itself every time a device battery gets low. This simple application turns it back off.\n            \n            \n        \n        \n    \n    \n    \n                  \n            March 15, 2013\n        \n        \n            True side-by-side page numbers in InDesign\n\n            \n            \n                \n                \n                    arabic\n                \n                \n                \n                    graphic design\n                \n                \n            \n            \n\n            Automatically create side-by-side page numbers for parallel texts or spread numbers in InDesign.\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-11",
    "href": "blog/index.html#section-11",
    "title": "Blog",
    "section": "2012",
    "text": "2012\n\n\n    \n    \n                  \n            July 2, 2012\n        \n        \n            Gutenberg ipsum\n\n            \n            \n                \n                \n                    lorem ipusm\n                \n                \n                \n                    text\n                \n                \n                \n                    perl\n                \n                \n                \n                    sh\n                \n                \n            \n            \n\n            Stop using boring, boilerplate Lorem ipsum filler text and build your own random, semi-coherent text from Project Gutenberg books.\n            \n            \n        \n        \n    \n    \n    \n                  \n            April 17, 2012\n        \n        \n            Install R, RStudio, and R Commander in Windows and OS X\n\n            \n            \n                \n                \n                    statistics\n                \n                \n                \n                    r\n                \n                \n            \n            \n\n            R, RStudio, and R Commander are all powerful open source statistical tools, but they can be a little tricky to install. These instructions make it easy to get everything working right.\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-12",
    "href": "blog/index.html#section-12",
    "title": "Blog",
    "section": "2011",
    "text": "2011\n\n\n    \n    \n                  \n            June 25, 2011\n        \n        \n            World-Ready Composer not Perfect\n\n            \n            \n                \n                \n                    arabic\n                \n                \n                \n                    graphic design\n                \n                \n            \n            \n\n            Even though InDesign’s new World-Ready Composer is awesome, it is still buggy and struggles with a few Arabic fonts.\n            \n            \n        \n        \n    \n    \n    \n                  \n            June 24, 2011\n        \n        \n            Using Arabic in InDesign CS5 without InDesign ME\n\n            \n            \n                \n                \n                    arabic\n                \n                \n                \n                    graphic design\n                \n                \n            \n            \n\n            Use InDesign CS5‘s hidden World Ready Composer to typeset text in Arabic and other complex scripts.\n            \n            \n        \n        \n    \n    \n    \n                  \n            June 19, 2011\n        \n        \n            Fake CloudApp with Dropbox and Quicksilver\n\n            \n            \n                \n                \n                    dropbox\n                \n                \n                \n                    quicksilver\n                \n                \n                \n                    cloudapp\n                \n                \n                \n                    automator\n                \n                \n            \n            \n\n            Move files to your Dropbox public folder and generate a shareable URL instantly, à la CloudApp, with a combination of a few services and Quicksilver triggers.\n            \n            \n        \n        \n    \n    \n    \n                  \n            February 3, 2011\n        \n        \n            In Tahrir Square\n\n            \n            \n                \n                \n                    jan25\n                \n                \n                \n                    egypt\n                \n                \n                \n                    nancy\n                \n                \n            \n            \n\n            A poem in honor of the #Jan25 Tahrir protestors\n            \n            \n        \n        \n    \n    \n    \n                  \n            January 27, 2011\n        \n        \n            Mona Prince on #jan25 Egyptian Protests\n\n            \n            \n                \n                \n                    jan25\n                \n                \n                \n                    egypt\n                \n                \n            \n            \n\n            Mona Prince's personal account of the #jan25 Egyptian protests\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-13",
    "href": "blog/index.html#section-13",
    "title": "Blog",
    "section": "2010",
    "text": "2010\n\n\n    \n    \n                  \n            September 24, 2010\n        \n        \n            iOS 4, Multitasking, and Battery Life\n\n            \n            \n                \n                \n                    ipod\n                \n                \n                \n                    ios\n                \n                \n            \n            \n\n            The multitasking capabilities of iOS 4 seem to be draining my battery. Help me figure out how to stop it!\n            \n            \n        \n        \n    \n    \n    \n                  \n            February 28, 2010\n        \n        \n            Queen Rania at AUC\n\n            \n            \n                \n                \n                    egypt\n                \n                \n                \n                    jordan\n                \n                \n                \n                    politics\n                \n                \n                \n                    civic engagement\n                \n                \n                \n                    queen rania\n                \n                \n            \n            \n\n            On February 28, 2010 at the American University in Cairo, Jordan's Queen Rania gave a speech on the importance of civic engagement in the Arab world. While her idea that regular citizens need to be more involved in government, the hardhanded policies of Arab governments make it almost impossible for that to happen.\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-14",
    "href": "blog/index.html#section-14",
    "title": "Blog",
    "section": "2009",
    "text": "2009\n\n\n    \n    \n                  \n            September 23, 2009\n        \n        \n            flashbakectl released\n\n            \n            \n                \n                \n                    flashbake\n                \n                \n                \n                    git\n                \n                \n                \n                    writing\n                \n                \n            \n            \n\n            flashbakectl is a handy little script that starts and stops Flashbake by loading and unloading plist files.\n            \n            \n        \n        \n    \n    \n    \n                  \n            August 18, 2009\n        \n        \n            iTunes plugin for Flashbake\n\n            \n            \n                \n                \n                    flashbake\n                \n                \n                \n                    git\n                \n                \n                \n                    writing\n                \n                \n            \n            \n\n            Flashbake-iTunes is a plugin for Flashbake that allows you to include information for the current track in the periodic git commit message.\n            \n            \n        \n        \n    \n    \n    \n                  \n            August 1, 2009\n        \n        \n            Using Google Voice and Gizmo Project Together\n\n            \n            \n                \n                \n                    ata\n                \n                \n                \n                    gizmo\n                \n                \n                \n                    google voice\n                \n                \n                \n                    phone\n                \n                \n                \n                    voip\n                \n                \n            \n            \n\n            Description of how to make Google Voice and the Gizmo Project work together with an ATA so that you get a free phone number and almost free phone calls.\n            \n            \n        \n        \n    \n    \n    \n                  \n            July 30, 2009\n        \n        \n            Alexandria Train Crash\n\n            \n            \n                \n                \n                    alexandria\n                \n                \n                \n                    egypt\n                \n                \n                \n                    train wreck\n                \n                \n            \n            \n\n            Pictures and details of a minor train wreck in Alexandria, Egypt on July 30, 2009.\n            \n            \n        \n        \n    \n    \n    \n                  \n            July 29, 2009\n        \n        \n            Installing pdftk-php\n\n            \n            \n                \n                \n                    html\n                \n                \n                \n                    pdftk\n                \n                \n                \n                    pdftk-php\n                \n                \n                \n                    php\n                \n                \n                \n                    sh\n                \n                \n                \n                    sql\n                \n                \n                \n                    text\n                \n                \n            \n            \n\n            A detailed, updated tutorial on how to install, use, and customize pdftk-php.php, which combines the power of pdftk and PHP, allowing you to serve dynamic PDF forms from the web.\n            \n            \n        \n        \n    \n    \n    \n                  \n            July 28, 2009\n        \n        \n            On narrowing and redefining research\n\n            \n            \n                \n                \n                    bloggers\n                \n                \n                \n                    history\n                \n                \n                \n                    italians\n                \n                \n                \n                    mutamasirun\n                \n                \n                \n                    thesis\n                \n                \n            \n            \n\n            After a year of deliberation, I may have finally decided what to write about for my thesis.\n            \n            \n        \n        \n    \n    \n    \n                  \n            July 19, 2009\n        \n        \n            Import a Blogger Blog to InDesign with Perl\n\n            \n            \n                \n                \n                    blogger xml\n                \n                \n                \n                    indesign\n                \n                \n                \n                    indesign tagged text\n                \n                \n                \n                    perl\n                \n                \n            \n            \n\n            This Perl script lets you take a backed-up Blogger XML file and convert it to an InDesign Tagged Text file for book layout.\n            \n            \n        \n        \n    \n    \n    \n                  \n            June 19, 2009\n        \n        \n            pdftk-php Officially Released\n\n            \n\n            After almost two years, I've officially developed and released pdftk-php--a script that lets you inject form data into a PDF with PHP.\n            \n            \n        \n        \n    \n    \n    \n                  \n            May 14, 2009\n        \n        \n            A Tale of Three Taxis\n\n            \n            \n                \n                \n                    cairo\n                \n                \n                \n                    chaos\n                \n                \n                \n                    egypt\n                \n                \n                \n                    life abroad\n                \n                \n                \n                    taxis\n                \n                \n            \n            \n\n            Traffic in Cairo is horrible, especially when all the taxi drivers, the ubiquitous life-blood of the Egyptian streets, have a deathwish for you.\n            \n            \n        \n        \n    \n    \n    \n                  \n            May 1, 2009\n        \n        \n            Google Profile Business Cards\n\n            \n            \n                \n                \n                    cool\n                \n                \n                \n                    ego-googling\n                \n                \n                \n                    ego-surfing\n                \n                \n                \n                    online identity\n                \n                \n            \n            \n\n            I got 25 Google Profile business cards for free. Woot!\n            \n            \n        \n        \n    \n    \n    \n                  \n            April 26, 2009\n        \n        \n            Typing transliterated Arabic quickly\n\n            \n            \n                \n                \n                    arabic\n                \n                \n                \n                    automation\n                \n                \n                \n                    ijmes\n                \n                \n                \n                    texter\n                \n                \n                \n                    transliteration\n                \n                \n                \n                    typinator\n                \n                \n            \n            \n\n            Use text-replacement software to automate Arabic transliteration.\n            \n            \n        \n        \n    \n    \n    \n                  \n            March 18, 2009\n        \n        \n            Libya, obsolete paradigms, and Rip Van Winkle\n\n            \n            \n                \n                \n                    auc\n                \n                \n                \n                    faculty seminar\n                \n                \n                \n                    libya\n                \n                \n                \n                    middle east studies\n                \n                \n            \n            \n\n            Lisa Anderson's research of political systems in Libya reveals that standard Middle East Studies paradigms don't fully apply.\n            \n            \n        \n        \n    \n    \n    \n                  \n            March 15, 2009\n        \n        \n            Do not succumb to economic stupidity\n\n            \n            \n                \n                \n                    economic crisis\n                \n                \n                \n                    links\n                \n                \n                \n                    npr\n                \n                \n                \n                    podcasts\n                \n                \n            \n            \n\n            Listening to NPR's Planet Money economics podcast really does help to understand the current economic crisis.\n            \n            \n        \n        \n    \n    \n    \n                  \n            March 8, 2009\n        \n        \n            New site launched\n\n            \n\n            I finally converted my ancient, inefficient PHP-ish website to a mean, lean, WordPresss running CMS.\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-15",
    "href": "blog/index.html#section-15",
    "title": "Blog",
    "section": "2007",
    "text": "2007\n\n\n    \n    \n                  \n            October 6, 2007\n        \n        \n            Populating a LiveCycle PDF with PHP and MySQL\n\n            \n            \n                \n                \n                    mysql\n                \n                \n                \n                    pdftk\n                \n                \n                \n                    php\n                \n                \n                \n                    sh\n                \n                \n                \n                    sql\n                \n                \n                \n                    text\n                \n                \n            \n            \n\n            Tutorial explaining how to populate a LiveCycle PDF form using PHP and MySQL.\n            \n            \n        \n        \n    \n    \n    \n                  \n            September 17, 2007\n        \n        \n            Using Arabic in InDesign without InDesign ME\n\n            \n            \n                \n                \n                    arabic\n                \n                \n                \n                    graphic design\n                \n                \n            \n            \n\n            How to use the Glyphs panel in InDesign CS3 to insert Arabic text, despite the lack of support for Arabic.\n            \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I’m an assistant professor in the Department of Public Management and Policy at the Andrew Young School of Policy Studies at Georgia State University. I received a PhD in public policy and political science from Duke University’s Sanford School of Public Policy in 2017.\nI study how international NGOs work in authoritarian countries, and I received the 2016–2018 Emerging Scholar Dissertation Award from the International Society for Third Sector Research (ISTR). I do research in public administration and policy, nonprofit management, international relations, and comparative politics.\nI teach courses on program evaluation and causal inference, statistics and data science, data visualization, economics, and science communication. I’m also a certified RStudio instructor and a Posit Academy mentor and absolutely love teaching how to use R and the tidyverse."
  },
  {
    "objectID": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html",
    "href": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html",
    "title": "How to Interpret Statistical Models Using {marginaleffects} in R and Python",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html#important-links",
    "href": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html#important-links",
    "title": "How to Interpret Statistical Models Using {marginaleffects} in R and Python",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html#abstract",
    "href": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html#abstract",
    "title": "How to Interpret Statistical Models Using {marginaleffects} in R and Python",
    "section": "Abstract",
    "text": "Abstract\nThe parameters of a statistical model can sometimes be difficult to interpret substantively, especially when that model includes non-linear components, interactions, or transformations. Analysts who fit such complex models often seek to transform raw parameter estimates into quantities that are easier for domain experts and stakeholders to understand. This article presents a simple conceptual framework to describe a vast array of such quantities of interest, which are reported under imprecise and inconsistent terminology across disciplines: predictions, marginal predictions, marginal means, marginal effects, conditional effects, slopes, contrasts, risk ratios, etc. We introduce {marginaleffects}, a package for R and Python which offers a simple and powerful interface to compute all of those quantities, and to conduct (non-)linear hypothesis and equivalence tests on them. {marginaleffects} is lightweight; extensible; it works well in combination with other R and Python packages; and it supports over 100 classes of models, including Linear, Generalized Linear, Generalized Additive, Mixed Effects, Bayesian, and several machine learning models."
  },
  {
    "objectID": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html#important-figures",
    "href": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html#important-figures",
    "title": "How to Interpret Statistical Models Using {marginaleffects} in R and Python",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2: Predicted probability of impartiality by levels of equality and democracy\n\n\n\n\n\nFigure 4: Tangents to the prediction function at 25 and 50"
  },
  {
    "objectID": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html#citation",
    "href": "research/articles/arel-bundock-greifer-heiss-mfxplainer-2024/index.html#citation",
    "title": "How to Interpret Statistical Models Using {marginaleffects} in R and Python",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{Arel-BundockGreiferHeiss:2024,\n    title = {How to Interpret Statistical Models Using {marginaleffects} in {R} and {Python}},\n    author = {Vincent Arel-Bundock and Noah Greifer and Andrew Heiss},\n    doi = {10.18637/jss.v111.i09},\n    journal = {Journal of Statistical Software},\n    year = {2024},\n    volume = {111},\n    number = {9},\n    pages = {1–32}}"
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#important-links",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#important-links",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Important links",
    "text": "Important links\n\nPaper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository\nExperiment preregistration (research question #2)"
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#abstract",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#abstract",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Abstract",
    "text": "Abstract\nThe phenomenon of closing civic space has adversely impacted INGO funding. We argue that individual private donors can be important in sustaining the operations of INGOs working in repressive contexts. Individual donors do not use the same performance-based metrics as official aid donors. Rather, trust can be an important component of individual donor support for nonprofits working towards difficult goals. How does trust in charitable organizations influence individuals’ preferences to donate, especially when these groups face crackdown? Using a simulated market for philanthropic donations based on data from a nationally representative sample of individuals in the United States who regularly donate to charity, we find that trust in INGOs matters substantially in shaping donor preferences. Donor profiles with high levels of social trust are likely to donate to INGOs with friendly relationships with host governments. This support holds steady if INGOs face criticism or crackdown. In contrast, donor profiles with lower levels of social trust prefer to donate to organizations that do not face criticism or crackdown abroad. The global crackdown on NGOs may thus possibly sour NGOs’ least trusting individual donors. Our findings have practical implications for INGOs raising funds from individuals amid closing civic space."
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#important-figure",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#important-figure",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Important figure",
    "text": "Important figure\nFigure 4: Average predicted donation market shares across all personas, segmented by persona public affairs knowledge, political ideology, and social trust across different NGO–host government relationships\n\n\n\nFigure 4: Average predicted donation market shares across all personas, segmented by persona public affairs knowledge, political ideology, and social trust across different NGO–host government relationships"
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#data-and-code",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#data-and-code",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Data and code",
    "text": "Data and code\nThe project is reproducible with R code available at GitHub. Follow the instructions there to install all supporting files and R packages.\nThis project includes the following data files:\n\ndata/raw_data/final_data.rds: Original results from the Qualtrics survey. This is hosted at OSF because of its size. Running targets::tar_make(survey_results_file) will download the .rds file from OSF and place it in data/raw_data. The code for cleaning and processing this data is part of a separate project, “Why Donors Donate”.\ndata/derived_data/survey_results.csv: CSV version of the survey data.\ndata/derived_data/survey_results.yaml: YAML metadata describing the syntax of the survey data.\ndata/raw_data/posterior_draws/public_political_social_charity_demo.rds: Gamma (Γ) coefficients from our multilevel Bayesian model. This is hosted at OSF because of its size. Running targets::tar_make(gamma_draws_file) will download the .rds file from OSF and place it in data/raw_data/posterior_draws. The code for running this model is part of a separate project, “Why Donors Donate”.\ndata/raw_data/Market Simulator Version 01.xlsx: An interactive Excel version of the market simulator to help demonstrate the intuition behind all the moving parts of the simulation."
  },
  {
    "objectID": "research/articles/chaudhry-dotson-heiss-2021/index.html#citation",
    "href": "research/articles/chaudhry-dotson-heiss-2021/index.html#citation",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryDotsonHeiss:2021,\n    Author = {Suparna Chaudhry and Marc Dotson and Andrew Heiss},\n    Doi = {10.1111/1758-5899.12984},\n    Journal = {Global Policy},\n    Month = {7},\n    Number = {S5},\n    Pages = {45--58},\n    Title = {Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy},\n    Volume = {12},\n    Year = {2021}}"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-links",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-links",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#abstract",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#abstract",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Abstract",
    "text": "Abstract\nAn increasing number of countries have recently cracked down on non-governmental organizations (NGOs). Much of this crackdown is sanctioned by law and represents a bureaucratic form of repression that could indicate more severe human rights abuses in the future. This is especially the case for democracies, which unlike autocracies, may not aggressively attack civic space. We explore if crackdowns on NGOs predict broader human rights repression. Anti-NGO laws are among the most subtle means of repression and attract lesser domestic and international condemnation compared to the use of violence. Using original data on NGO repression, we test whether NGO crackdown is a predictor of political terror, and violations of physical integrity rights and civil liberties. We find that while de jure anti-NGO laws provide little information in predicting future repression, their patterns of implementation—or de facto civil society repression—predicts worsening respect for physical integrity rights and civil liberties."
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-figures",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-figures",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 6: Marginal effects of changing levels of civil society repression on the probability of specific levels of political terror and predicted latent human rights values\n\n\n\n\n\nFigure 7: The disconnect between Egypt’s de jure 2002 law and the widespread de facto repression of civil society a decade later"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#citation",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#citation",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryHeiss:2022,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Doi = {10.1080/14754835.2022.2030205},\n    Journal = {Journal of Human Rights},\n    Number = {2},\n    Pages = {123--140},\n    Title = {NGO Repression as a Predictor of Worsening Human Rights Abuses},\n    Volume = {21},\n    Year = {2022}}"
  },
  {
    "objectID": "research/articles/heiss-2012/index.html",
    "href": "research/articles/heiss-2012/index.html",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "",
    "text": "In the weeks since the January 25 revolution there have been dozens of explanations for Mubarak’s downfall. Political scientists, diplomats, economists, sociologists, and even international aid workers have proposed theories to explain the economic, political, historical, and social causes of the Egyptian revolution. Despite the plethora of interdisciplinary theories in the press and in academia, few—if any—have analyzed Mubarak’s resignation in the light of managerial dynamics and behavior.\nIn addition to a ripe political environment, horrible economic conditions, and a mobilized and angry population, Mubarak’s fall from power can be attributed to his failure as a public manager. This paper analyzes Mubarak’s managerial strategy throughout the course of his presidency by (1) reviewing the structure of the National Democratic Party (NDP) and Mubarak’s relationship with it; (2) analyzing the foundational principles and assumptions of his strategy‚ both as the leader of Egypt and as the chairman of the NDP; and (3) tracing the application of that strategy and determining its effectiveness in confronting “the management challenge” over a period of 30 years."
  },
  {
    "objectID": "research/articles/heiss-2012/index.html#abstract",
    "href": "research/articles/heiss-2012/index.html#abstract",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "",
    "text": "In the weeks since the January 25 revolution there have been dozens of explanations for Mubarak’s downfall. Political scientists, diplomats, economists, sociologists, and even international aid workers have proposed theories to explain the economic, political, historical, and social causes of the Egyptian revolution. Despite the plethora of interdisciplinary theories in the press and in academia, few—if any—have analyzed Mubarak’s resignation in the light of managerial dynamics and behavior.\nIn addition to a ripe political environment, horrible economic conditions, and a mobilized and angry population, Mubarak’s fall from power can be attributed to his failure as a public manager. This paper analyzes Mubarak’s managerial strategy throughout the course of his presidency by (1) reviewing the structure of the National Democratic Party (NDP) and Mubarak’s relationship with it; (2) analyzing the foundational principles and assumptions of his strategy‚ both as the leader of Egypt and as the chairman of the NDP; and (3) tracing the application of that strategy and determining its effectiveness in confronting “the management challenge” over a period of 30 years."
  },
  {
    "objectID": "research/articles/heiss-2012/index.html#figure",
    "href": "research/articles/heiss-2012/index.html#figure",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "Figure",
    "text": "Figure\nFigure 1: NDP Leadership Organizational Chart, 2010 (based on original chart by The Arabist)\n\n\n\nFigure 1: NDP Leadership Organizational Chart, 2010"
  },
  {
    "objectID": "research/articles/heiss-2012/index.html#citation",
    "href": "research/articles/heiss-2012/index.html#citation",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "Citation",
    "text": "Citation\n@article{Heiss:2012,\n    Author = {Andrew Heiss},\n    Issue = {Spring},\n    Journal = {Journal of Third World Studies},\n    Number = {1},\n    Pages = {155-171},\n    Title = {The Failed Management of a Dying Regime: {Hosni Mubarak}, {Egypt's National Democratic Party}, and the {January} 25 Revolution},\n    Volume = {28},\n    Year = {2012}}"
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html",
    "href": "research/articles/heiss-johnson-2016/index.html",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "",
    "text": "Recent scholarship works to open the “black box” of international non-governmental organizations (INGOs), explaining their activities without conventional assumptions of altruism and high-mindedness. We review Borders Among Activists by Sarah Stroup, The Opening up of International Organizations by Jonas Tallberg, et al., and Internal Affairs by Wendy Wong and pinpoint how each book makes important contributions to understanding the determinants of INGO activities. After developing an organizing framework that permits careful analysis of the internal, interactive, and institutional factors that influence policy outcomes, we demonstrate the merits of the framework by applying it to each book. We evaluate each work’s contributions to individual layers in the framework, locating each book within its intended scholarly context. We identify ties between books, examining how each work implicitly treats the other layers of our framework. The conclusion identifies a new research agenda for INGO studies and outlines promising avenues for future INGO scholarship."
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html#abstract",
    "href": "research/articles/heiss-johnson-2016/index.html#abstract",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "",
    "text": "Recent scholarship works to open the “black box” of international non-governmental organizations (INGOs), explaining their activities without conventional assumptions of altruism and high-mindedness. We review Borders Among Activists by Sarah Stroup, The Opening up of International Organizations by Jonas Tallberg, et al., and Internal Affairs by Wendy Wong and pinpoint how each book makes important contributions to understanding the determinants of INGO activities. After developing an organizing framework that permits careful analysis of the internal, interactive, and institutional factors that influence policy outcomes, we demonstrate the merits of the framework by applying it to each book. We evaluate each work’s contributions to individual layers in the framework, locating each book within its intended scholarly context. We identify ties between books, examining how each work implicitly treats the other layers of our framework. The conclusion identifies a new research agenda for INGO studies and outlines promising avenues for future INGO scholarship."
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html#important-figures",
    "href": "research/articles/heiss-johnson-2016/index.html#important-figures",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "Important figures",
    "text": "Important figures\nFigure 1 from the paper, showing a unified framework for understanding the types of influences on INGO behavior and policy outcomes. The triangular shape conveys the specificity of the layers: from narrow “micro” phenomena at the internal layer to much wider “macro” phenomena at the institutional layer.\n\n\n\nFigure 1: A Unified Framework for Analyzing INGO Behavior and Outputs"
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html#books-reviewed",
    "href": "research/articles/heiss-johnson-2016/index.html#books-reviewed",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "Books reviewed",
    "text": "Books reviewed\n\nSarah S. Stroup, Borders among Activists: International NGOs in the United States, Britain, and France (Ithaca, New York: Cornell University Press, 2012), doi: 10.7591/9780801464256.\nJonas Tallberg, Thomas Sommerer, Theresa Squatrito, and Christer Jönsson, The Opening Up of International Organizations: Transnational Access in Global Governance (Cambridge: Cambridge University Press, 2013), doi: 10.1017/cbo9781107325135.\nWendy H. Wong, Internal Affairs: How the Structure of NGOs Transforms Human Rights (Ithaca: Cornell University Press, 2012), doi: 10.7591/9780801466069."
  },
  {
    "objectID": "research/articles/heiss-johnson-2016/index.html#citation",
    "href": "research/articles/heiss-johnson-2016/index.html#citation",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{HeissJohnson:2016,\n    Author = {Andrew Heiss and Tana Johnson},\n    Doi = {10.1093/isr/viv014},\n    Journal = {International Studies Review},\n    Month = {9},\n    Number = {3},\n    Pages = {528--41},\n    Title = {Internal, Interactive, and Institutional Factors: Towards a Unified Theory of {INGO} Behavior},\n    Volume = {18},\n    Year = {2016}}"
  },
  {
    "objectID": "research/articles/heiss-kelley-2017a/index.html",
    "href": "research/articles/heiss-kelley-2017a/index.html",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "",
    "text": "Amid the academic and policy critiques of the United States 15-year push to eliminate human trafficking, the perspective of the nongovernmental organizations (NGOs) working with anti-trafficking advocacy and services has been largely ignored. This article presents the results of a global survey of nearly 500 anti-trafficking NGOs in working in 133 countries, and is the first NGO-focused survey of its kind. Based on the results of the survey, we provide an overview of the anti-trafficking NGO sector as a whole, detail the relationship between anti-trafficking NGOs and the US, and account for some of the variation in NGO opinions of US efforts. Notably, we find that NGOs are remarkably satisfied with US-led efforts—despite their acknowledged flaws—and that NGOs believe that American anti-TIP policies are important and, on balance, helpful. These results also provide a warning for the future of the United States’ anti-trafficking advocacy, suggesting that the US avoid politicizing its annual Trafficking in Persons Report."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017a/index.html#abstract",
    "href": "research/articles/heiss-kelley-2017a/index.html#abstract",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "",
    "text": "Amid the academic and policy critiques of the United States 15-year push to eliminate human trafficking, the perspective of the nongovernmental organizations (NGOs) working with anti-trafficking advocacy and services has been largely ignored. This article presents the results of a global survey of nearly 500 anti-trafficking NGOs in working in 133 countries, and is the first NGO-focused survey of its kind. Based on the results of the survey, we provide an overview of the anti-trafficking NGO sector as a whole, detail the relationship between anti-trafficking NGOs and the US, and account for some of the variation in NGO opinions of US efforts. Notably, we find that NGOs are remarkably satisfied with US-led efforts—despite their acknowledged flaws—and that NGOs believe that American anti-TIP policies are important and, on balance, helpful. These results also provide a warning for the future of the United States’ anti-trafficking advocacy, suggesting that the US avoid politicizing its annual Trafficking in Persons Report."
  },
  {
    "objectID": "research/articles/heiss-kelley-2017a/index.html#important-figures",
    "href": "research/articles/heiss-kelley-2017a/index.html#important-figures",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "Important figures",
    "text": "Important figures\nFigure 1: Countries where NGOs work, excluding NGOs working only in the US\n\n\n\nFigure 1: Countries where NGOs work, excluding NGOs working only in the US\n\n\nFigure 4: Embassies or foreign governments NGOs reported as active partners in the fight against human trafficking\n\n\n\nFigure 4: Embassies or foreign governments NGOs reported as active partners in the fight against human trafficking\n\n\nFigure 7: Average importance and positivity of US anti-TIP efforts across regions\n\n\n\nFigure 7: Average importance and positivity of US anti-TIP efforts across regions"
  },
  {
    "objectID": "research/articles/heiss-kelley-2017a/index.html#citation",
    "href": "research/articles/heiss-kelley-2017a/index.html#citation",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{HeissKelley:2017,\n  Author = {Andrew Heiss and Judith G. Kelley},\n  Doi = {10.1080/23322705.2016.1199241},\n  Journal = {Journal of Human Trafficking},\n  Number = {3},\n  Pages = {231--254},\n  Title = {From the Trenches: A Global Survey of Anti-{TIP} {NGOs} and their Views of {US} Efforts},\n  Volume = {3},\n  Year = {2017}}"
  },
  {
    "objectID": "research/articles/witesman-heiss-2016/index.html",
    "href": "research/articles/witesman-heiss-2016/index.html",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "",
    "text": "Collaboration and its promotion by funders continue to accelerate. Although research has identified significant transaction costs associated with collaboration, little empirical work has examined the broader, societal-level economic outcomes of a resource-sharing environment. Does an environment that encourages collaboration shift our focus toward certain types of social objectives and away from others? This paper uses agent-based Monte Carlo simulation to demonstrate that collaboration is particularly useful when resources are rare but a social objective is commonly held. However, collaboration can lead to bad outcomes when the objective is not commonly shared; in such cases, markets outperform collaborative arrangements. These findings suggest that encouraging a resource-sharing environment can lead to inefficiencies even worse than market failure. We also demonstrate that failure to account for transaction costs when prescribing collaboration can result in quantifiably lower outcome levels than expected."
  },
  {
    "objectID": "research/articles/witesman-heiss-2016/index.html#abstract",
    "href": "research/articles/witesman-heiss-2016/index.html#abstract",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "",
    "text": "Collaboration and its promotion by funders continue to accelerate. Although research has identified significant transaction costs associated with collaboration, little empirical work has examined the broader, societal-level economic outcomes of a resource-sharing environment. Does an environment that encourages collaboration shift our focus toward certain types of social objectives and away from others? This paper uses agent-based Monte Carlo simulation to demonstrate that collaboration is particularly useful when resources are rare but a social objective is commonly held. However, collaboration can lead to bad outcomes when the objective is not commonly shared; in such cases, markets outperform collaborative arrangements. These findings suggest that encouraging a resource-sharing environment can lead to inefficiencies even worse than market failure. We also demonstrate that failure to account for transaction costs when prescribing collaboration can result in quantifiably lower outcome levels than expected."
  },
  {
    "objectID": "research/articles/witesman-heiss-2016/index.html#figure",
    "href": "research/articles/witesman-heiss-2016/index.html#figure",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "Figure",
    "text": "Figure\nFigure 1: Simulation results\n\n\n\nFigure 1: Simulation results"
  },
  {
    "objectID": "research/articles/witesman-heiss-2016/index.html#citation",
    "href": "research/articles/witesman-heiss-2016/index.html#citation",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{WitesmanHeiss:2016,\n    Author = {Eva Witesman and Andrew Heiss},\n    Doi = {10.1007/s11266-016-9684-5},\n    Journal = {Voluntas: International Journal of Voluntary and Nonprofit Organizations},\n    Month = {8},\n    Number = {4},\n    Pages = {1500--1528},\n    Title = {Nonprofit Collaboration and the Resurrection of Market Failure: How a Resource-Sharing Environment Can Suppress Social Objectives},\n    Volume = {28},\n    Year = {2016}}"
  },
  {
    "objectID": "research/chapters/heiss-causal-inference-2021/index.html",
    "href": "research/chapters/heiss-causal-inference-2021/index.html",
    "title": "Causal Inference",
    "section": "",
    "text": "One of the most repeated phrases in any introductory statistics class is the warning that “correlation is not causation.” This chapter presents new non-statistical language for creating, measuring, and evaluating causal stories and relationships using observational (i.e. non-experimental) data. It introduces the concept of causal directed acyclic graphs (DAGs) that allow us to formally encode our understanding of causal stories. With well-crafted DAGs, one can use a set of rules called do-calculus to make specific adjustments to statistical models and isolate or identify causal relationships between variables of interest. Edges (or arrows) transmit associations between nodes. Following the fundamental problem of causal inference, answering causal questions without an experiment appears impossible. The chapter explores different methods of making adjustments using synthetic data. Instead of throwing away potentially useful data, one can use other methods to create matches that are less discrete and more informative."
  },
  {
    "objectID": "research/chapters/heiss-causal-inference-2021/index.html#abstract",
    "href": "research/chapters/heiss-causal-inference-2021/index.html#abstract",
    "title": "Causal Inference",
    "section": "",
    "text": "One of the most repeated phrases in any introductory statistics class is the warning that “correlation is not causation.” This chapter presents new non-statistical language for creating, measuring, and evaluating causal stories and relationships using observational (i.e. non-experimental) data. It introduces the concept of causal directed acyclic graphs (DAGs) that allow us to formally encode our understanding of causal stories. With well-crafted DAGs, one can use a set of rules called do-calculus to make specific adjustments to statistical models and isolate or identify causal relationships between variables of interest. Edges (or arrows) transmit associations between nodes. Following the fundamental problem of causal inference, answering causal questions without an experiment appears impossible. The chapter explores different methods of making adjustments using synthetic data. Instead of throwing away potentially useful data, one can use other methods to create matches that are less discrete and more informative."
  },
  {
    "objectID": "research/chapters/heiss-causal-inference-2021/index.html#important-figures",
    "href": "research/chapters/heiss-causal-inference-2021/index.html#important-figures",
    "title": "Causal Inference",
    "section": "Important figures",
    "text": "Important figures\nFigure 6: More complicated DAG showing the relationship between campaign spending and votes won in an election\n\n\n\nFigure 6: More complicated DAG showing the relationship between campaign spending and votes won in an election\n\n\nFigure 17: Adjustment set to identify the relationship between mosquito net use and malaria risk\n\n\n\nFigure 17: Adjustment set to identify the relationship between mosquito net use and malaria risk"
  },
  {
    "objectID": "research/chapters/heiss-causal-inference-2021/index.html#citation",
    "href": "research/chapters/heiss-causal-inference-2021/index.html#citation",
    "title": "Causal Inference",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@incollection{Heiss:2021,\n    address = {Boca Raton, Florida},\n    author = {Andrew Heiss},\n    booktitle = {R for Political Data Science: A Practical Guide},\n    chapter = {10},\n    doi = {10.1201/9781003010623-10},\n    editor = {Francisco Urdinez and Andr{\\'e}s Cruz},\n    pages = {235--274},\n    publisher = {{Chapman and Hall} / CRC},\n    title = {Causal Inference},\n    year = {2021}}"
  },
  {
    "objectID": "research/chapters/johnson-heiss-2018/index.html",
    "href": "research/chapters/johnson-heiss-2018/index.html",
    "title": "Liberal Institutionalism",
    "section": "",
    "text": "Liberal institutionalism presumes that domestic and international institutions play central roles in facilitating cooperation and peace between states. But currently, this influential approach to thinking and practice appears to be in jeopardy. The United Kingdom seeks to be the first state ever to withdraw from the European Union (EU). The United States threatens to renegotiate or leave several international arrangements that it has recently signed or long supported. Meanwhile, China hints that it would be happy to take on greater global leadership if the United States retreats from this traditional role."
  },
  {
    "objectID": "research/chapters/johnson-heiss-2018/index.html#abstract",
    "href": "research/chapters/johnson-heiss-2018/index.html#abstract",
    "title": "Liberal Institutionalism",
    "section": "",
    "text": "Liberal institutionalism presumes that domestic and international institutions play central roles in facilitating cooperation and peace between states. But currently, this influential approach to thinking and practice appears to be in jeopardy. The United Kingdom seeks to be the first state ever to withdraw from the European Union (EU). The United States threatens to renegotiate or leave several international arrangements that it has recently signed or long supported. Meanwhile, China hints that it would be happy to take on greater global leadership if the United States retreats from this traditional role."
  },
  {
    "objectID": "research/chapters/johnson-heiss-2018/index.html#citation",
    "href": "research/chapters/johnson-heiss-2018/index.html#citation",
    "title": "Liberal Institutionalism",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@incollection{JohnsonHeiss:2018,\n    Address = {London},\n    Author = {Tana Johnson and Andrew Heiss},\n    Booktitle = {International Organization and Global Governance},\n    Chapter = {8},\n    Doi = {10.4324/9781315301914},\n    Edition = {2},\n    Editor = {Thomas G. Weiss and Rorden Wilkinson},\n    Pages = {123--34},\n    Publisher = {Routledge},\n    Title = {Liberal Institutionalism},\n    Year = {2018}}"
  },
  {
    "objectID": "research/reviews/heiss-death-idealism/index.html",
    "href": "research/reviews/heiss-death-idealism/index.html",
    "title": "What Kills Idealism? Review of The Death of Idealism: Development and Anti-Politics in the Peace Corps, by Meghan Elizabeth Kallman",
    "section": "",
    "text": "Add to Zotero \n\n@article{Heiss:20211,\n    author = {Andrew Heiss},\n    doi = {10.1177/00943061211050046g},\n    journal = {Contemporary Sociology},\n    number = {6},\n    pages = {486--488},\n    titleaddon = {\\bibstring{reviewof} Meghan Elizabeth Kallman, \\mkbibemph{The Death of Idealism: Development and Anti-Politics in the Peace Corps}},\n    volume = {50},\n    year = {2021}}"
  },
  {
    "objectID": "research/reviews/heiss-death-idealism/index.html#citation",
    "href": "research/reviews/heiss-death-idealism/index.html#citation",
    "title": "What Kills Idealism? Review of The Death of Idealism: Development and Anti-Politics in the Peace Corps, by Meghan Elizabeth Kallman",
    "section": "",
    "text": "Add to Zotero \n\n@article{Heiss:20211,\n    author = {Andrew Heiss},\n    doi = {10.1177/00943061211050046g},\n    journal = {Contemporary Sociology},\n    number = {6},\n    pages = {486--488},\n    titleaddon = {\\bibstring{reviewof} Meghan Elizabeth Kallman, \\mkbibemph{The Death of Idealism: Development and Anti-Politics in the Peace Corps}},\n    volume = {50},\n    year = {2021}}"
  },
  {
    "objectID": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html",
    "href": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html",
    "title": "Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy",
    "section": "",
    "text": "GitHub repository\nExperiment preregistration (research question #1)"
  },
  {
    "objectID": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html#important-links",
    "href": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html#important-links",
    "title": "Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy",
    "section": "",
    "text": "GitHub repository\nExperiment preregistration (research question #1)"
  },
  {
    "objectID": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html#abstract",
    "href": "research/working-papers/chaudhry-dotson-heiss-why-donors-donate/index.html#abstract",
    "title": "Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy",
    "section": "Abstract",
    "text": "Abstract\nAs space for civil society has closed around the world, transnational NGOs have faced a crisis of funding. We explore how NGOs have shifted from traditionally Northern funding sources toward grassroots private philanthropic money. How do individual donors in the US feel about donating to legally besieged NGOs abroad? Do legal restrictions on NGOs influence donors’ decision to donate? We use a conjoint survey experiment to argue that domestic political environments of NGO host countries influence preferences of private donors and that legal crackdowns on NGOs serve as a heuristic of organizational deservingness."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "",
    "text": "Paper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#important-links",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#important-links",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "",
    "text": "Paper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#abstract",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#abstract",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "Abstract",
    "text": "Abstract\nForeign donors routinely use nongovernmental organizations (NGOs) to deliver foreign aid. However, states are increasingly relying on repressive legislation to crack down on NGOs within their borders. How have foreign aid donors responded to this legal crackdown on NGOs? Using original data from all countries that received aid from 1981–2012, we assess the impact of anti-NGO laws on total flows of official foreign aid, the nature of projects funded, and the channels used for distributing this aid. Overall, we find that donors scale back their operations in repressive countries. However, rather than completely withdraw, we find that donors redirect funds within restrictive countries by decreasing funds for politically sensitive issues, and channeling more aid through domestic rather than foreign NGOs. While our findings challenge existing notions of foreign aid running on “autopilot,” they also have worrying implications for Western donors and domestic NGOs working on contentious issues."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#figure",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#figure",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "Figure",
    "text": "Figure\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution.\n\n\n\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution."
  },
  {
    "objectID": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#bibtex-citation",
    "href": "research/working-papers/chaudhry-heiss-ngos-aid/index.html#bibtex-citation",
    "title": "Are Donors Really Responding? Analyzing the Impact of Global Restrictions on NGOs",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@unpublished{ChaudhryHeiss:2023,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Note = {Working paper},\n    Title = {Are Donors Really Responding? Analyzing the Impact of Global Restrictions on {NGO}s},\n    Year = {2023}}"
  },
  {
    "objectID": "research/working-papers/heiss-mission-money-environment/index.html",
    "href": "research/working-papers/heiss-mission-money-environment/index.html",
    "title": "‘We Changed Our Strategy… Without Losing Our Values, Vision and Mission’: Mission, Money, and the Practical Operating Environment for International NGOs",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-mission-money-environment/index.html#important-links",
    "href": "research/working-papers/heiss-mission-money-environment/index.html#important-links",
    "title": "‘We Changed Our Strategy… Without Losing Our Values, Vision and Mission’: Mission, Money, and the Practical Operating Environment for International NGOs",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-program-capture-ingos/index.html",
    "href": "research/working-papers/heiss-program-capture-ingos/index.html",
    "title": "‘Some State Officials Want Your Services’: International NGO Responses to Authoritarian Program Capture Regulations",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-program-capture-ingos/index.html#important-links",
    "href": "research/working-papers/heiss-program-capture-ingos/index.html#important-links",
    "title": "‘Some State Officials Want Your Services’: International NGO Responses to Authoritarian Program Capture Regulations",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-np-causal-inference/index.html",
    "href": "research/working-papers/heiss-ye-np-causal-inference/index.html",
    "title": "Clarifying Correlation and Causation: A Guide to Modern Quantitative Causal Inference in Nonprofit Studies",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-np-causal-inference/index.html#important-links",
    "href": "research/working-papers/heiss-ye-np-causal-inference/index.html#important-links",
    "title": "Clarifying Correlation and Causation: A Guide to Modern Quantitative Causal Inference in Nonprofit Studies",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "research/working-papers/heiss-ye-np-causal-inference/index.html#abstract",
    "href": "research/working-papers/heiss-ye-np-causal-inference/index.html#abstract",
    "title": "Clarifying Correlation and Causation: A Guide to Modern Quantitative Causal Inference in Nonprofit Studies",
    "section": "Abstract",
    "text": "Abstract\nDiscovering causal relationships and testing theoretical mechanisms is a core endeavor of social science. Randomized experiments have long served as a gold standard for making valid causal inferences, but most of the data social scientists work with is observational and non-experimental. However, with newer methodological developments in economics, political science, epidemiology, and other disciplines, an increasing number of studies in social science make causal claims with observational data. As a newer interdisciplinary field, however, nonprofit studies has lagged behind other disciplines in its use of observational causal inference. In this article, we present a hands-on introduction and guide to design-based observational causal inference methods. We first review and categorize all studies making causal claims in top nonprofit studies journals over the past decade to illustrate the field’s current of experimental and observational approaches to causal inference. We then introduce a framework for modeling and identifying causal processes using directed acyclic graphs (DAGs) and provide a walk-through of the assumptions and procedures for making inferences with a range of different methods, including matching, inverse probability weighting, difference-in-differences, regression discontinuity designs, and instrumental variables. We illustrate each approach with synthetic and empirical examples and provide sample R and Stata code for implementing these methods. We conclude by encouraging scholars and practitioners to make more careful and explicit causal claims in their observational empirical research, collectively developing and improving quantitative work in the broader field of nonprofit studies."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page contains the source code, links, and slides for various workshops, talks, lectures, and presentations I’ve given. Research presentations given at conferences are typically stored in their respective repositories on GitHub; one-off workshops and talks can all be found in a catch-all GitHub repository."
  },
  {
    "objectID": "talks/index.html#section",
    "href": "talks/index.html#section",
    "title": "Talks",
    "section": "2024",
    "text": "2024\n\n\n\n    \n    \n                  \n            November 13, 2024\n        \n        \n            Go beyond OLS! An introduction to Poisson, Beta, and zero-inflated Beta Bayesian distributional regression\n            Workshop presented at the Département de science politique at the Université de Montréal | Montréal, Québec\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Companion site and materials\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            April 25, 2024\n        \n        \n            Move beyond OLS! An introduction to Poisson, Beta, and zero-inflated Beta Bayesian distributional regression models\n            Workshop presented at the Department of Agronomy at Kansas State University | Manhattan, Kansas (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Companion site and materials\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            March 21, 2024\n        \n        \n            Be curious, not judgmental: Creating a welcoming learning environment through radical transparency and judgment-free curiosity\n            Workshop for Posit Academy's \"Mentors on Mentoring\" series\" | Online\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-1",
    "href": "talks/index.html#section-1",
    "title": "Talks",
    "section": "2022",
    "text": "2022\n\n\n\n    \n    \n                  \n            September 12, 2022\n        \n        \n            APIs and web scraping with R\n            Workshop for SEACEN's September 2022 online course on \"Data Analytics for Macroeconomic Surveillance\" | Kuala Lumpur, Malaysia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Companion site, slides, and materials\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            February 22, 2022\n        \n        \n            Putting everything out there as a researcher\n            Workshop for EPID 9100 (PhD/MS seminar), Department of Epidemiology & Biostatistics, University of Georgia | Athens, Georgia\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Handout page\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-2",
    "href": "talks/index.html#section-2",
    "title": "Talks",
    "section": "2021",
    "text": "2021\n\n\n\n    \n    \n                  \n            November 16, 2021\n        \n        \n            Introduction to the tidyverse + data visualization with ggplot2\n            Two sessions for SEACEN's November 2021 online course on \"Data Analytics for Macroeconomic Surveillance\" | Kuala Lumpur, Malaysia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Companion site, slides, and materials\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            August 17, 2021\n        \n        \n            Making documents, websites, and dashboards with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Companion site and slides\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            July 13, 2021\n        \n        \n            Introduction to Geographic Information Systems (GIS) data with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Companion site and slides\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            June 29, 2021\n        \n        \n            Graphic design for public administrators\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Companion site and slides\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            March 4, 2021\n        \n        \n            Universal documents and reproducibility\n            Workshop for EPID 9100 (PhD/MS seminar), Department of Epidemiology & Biostatistics, University of Georgia | Athens, Georgia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Companion site\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            February 5, 2021\n        \n        \n            What does the internet say about you?\n            Workshop for the Political Science Graduate Student Association, Department of Political Science, Georgia State University | Atlanta, Georgia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Handout page\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-3",
    "href": "talks/index.html#section-3",
    "title": "Talks",
    "section": "2020",
    "text": "2020\n\n\n\n    \n    \n                  \n            October 29, 2020\n        \n        \n            The US and the 2020 Election\n            Guest lecture for KIEN 2263 (English Academic and Professional Skills II), Centre for Language & Communication Studies, University of Turku | Turku, Finland (via Zoom)\n\n            \n            \n        \n    \n    \n    \n                  \n            October 23, 2020\n        \n        \n            Universal documents and reproducibility\n            \"Unpacking the Hidden Curriculum\": Graduate Student Professionalization Workshop, Department of Political Science, University of Utah | Salt Lake City, Utah (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Companion site\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            October 23, 2020\n        \n        \n            What does the internet say about you?\n            \"Unpacking the Hidden Curriculum\": Graduate Student Professionalization Workshop, Department of Political Science, University of Utah | Salt Lake City, Utah (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Handout page\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            September 17, 2020\n        \n        \n            Truth, beauty, and data: Why data visualization matters in research\n            Methods workshop in the Département de science politique, Université de Montréal | Montréal, Canada (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Handout page\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            August 21, 2020\n        \n        \n            Program evaluation and causal inference with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides and materials\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            July 20, 2020\n        \n        \n            Data visualization with R\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides and materials\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            July 16, 2020\n        \n        \n            What does the internet say about you?\n            Workshop for members of the International Society for Third-Sector Research (ISTR) | Online workshop via Zoom\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            July 9, 2020\n        \n        \n            Welcome to the tidyverse: Introduction to R and tidy data analysis\n            Workshop for the Georgia Policy Labs at the Andrew Young School of Policy Studies | Atlanta, Georgia (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides and materials\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            July 7, 2020\n        \n        \n            Truth, Beauty, and Data\n            Workshop on data visualization for employees of the Church of Jesus Christ of Latter-day Saints | Salt Lake City, Utah (via Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-4",
    "href": "talks/index.html#section-4",
    "title": "Talks",
    "section": "2019",
    "text": "2019\n\n\n\n    \n    \n                  \n            November 21, 2019\n        \n        \n            Why Donors Donate: Disentangling Organizational and Structural Heuristics for International Philanthropy\n            2019 Conference for the Association for Research on Nonprofit Organizations and Voluntary Action (ARNOVA) | San Diego, California\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            November 5, 2019\n        \n        \n            NGOs and authoritarianism\n            Guest lecture for International NGOs (PMAP 8201) | Atlanta, Georgia\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            October 18, 2019\n        \n        \n            Open source resources for teaching data public service-focused data science courses\n            2019 Conference for the Network of Schools of Public Policy, Affairs, and Administration (NASPAA) | Los Angeles, California\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Handout page\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            October 10, 2019\n        \n        \n            Data Science, Open Source, and Radical Transparency\n            Brown bag presentation to PhD students in the Andrew Young School of Policy Studies | Atlanta, Georgia\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            September 19, 2019\n        \n        \n            Curiosity, Transparency, and Failure\n            Guest talk to epidemiology PhD students at the University of Georgia | Athens, Georgia\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            May 14, 2019\n        \n        \n            \"Why won't this run again?!\": Making R analysis more reproducible\n            Utah County R Users Group | Provo, Utah\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Handout page\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-5",
    "href": "talks/index.html#section-5",
    "title": "Talks",
    "section": "2018",
    "text": "2018\n\n\n\n    \n    \n                  \n            September 19, 2018\n        \n        \n            A lightning quick introduction to data visualization\n            Utah Valley University Department of Biology Seminar | Orem, Utah\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Handout page\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            July 11, 2018\n        \n        \n            Introduction to data visualization with R\n            Utah R Users Group | Salt Lake City, Utah\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Resources and code examples\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Video\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            June 12, 2018\n        \n        \n            Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries\n            Interest Groups, International Organizations, and Global Problem-solving Capacity workshop | Stockholm, Sweden\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for paper\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            April 27, 2018\n        \n        \n            A lightning quick introduction to data visualization\n            West Coast Nonprofit Data Conference | Salt Lake City, Utah\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Handout page\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-6",
    "href": "talks/index.html#section-6",
    "title": "Talks",
    "section": "2016",
    "text": "2016\n\n\n\n    \n    \n                  \n            March 7, 2016\n        \n        \n            Telling Stories with Data\n            MPP workshop at the Sanford School of Public Policy, Duke University | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-7",
    "href": "talks/index.html#section-7",
    "title": "Talks",
    "section": "2015",
    "text": "2015\n\n\n\n    \n    \n                  \n            October 16, 2015\n        \n        \n            Enhancing basic statistical figures: Make pretty pictures like the New York Times\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            October 2, 2015\n        \n        \n            Graphic Design for Non-Designers\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            September 15, 2015\n        \n        \n            A Very Brief Overview of Topics in Data Visualization\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            April 17, 2015\n        \n        \n            Telling Stories with Data\n            PhD workshop at the Sanford School of Public Policy | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-8",
    "href": "talks/index.html#section-8",
    "title": "Talks",
    "section": "2014",
    "text": "2014\n\n\n\n    \n    \n                  \n            September 11, 2014\n        \n        \n            Data Visualization and Exploratory Data Analysis\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            September 2, 2014\n        \n        \n            Practically Perfect Professional Policy Presentations\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            January 22, 2014\n        \n        \n            Data Visualization and Exploratory Data Analysis\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#section-9",
    "href": "talks/index.html#section-9",
    "title": "Talks",
    "section": "2013",
    "text": "2013\n\n\n\n    \n    \n                  \n            September 12, 2013\n        \n        \n            Telling Stories with Data\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            September 12, 2013\n        \n        \n            Practically Perfect Professional Policy Presentations\n            MPP workshop at the Sanford School of Public Policy | Durham, North Carolina\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "uses/index.html",
    "href": "uses/index.html",
    "title": "What I use",
    "section": "",
    "text": "People often ask me what programs I use for my writing and design. In truth, my workflow tends to look like this or this, but here’s a more detailed list of all the interconnected programs I use.\nI try to keep this updated fairly regularly. As of February 19, 2025 this is what I’m using:"
  },
  {
    "objectID": "uses/index.html#writing",
    "href": "uses/index.html#writing",
    "title": "What I use",
    "section": "Writing",
    "text": "Writing\n\nI permanently ditched Word as a writing environment in 2008 after starting grad school. I do all my writing in pandoc-flavored Markdown (including e-mails and paper-and-pencil writing)—it’s incredibly intuitive, imminently readable, flexible, future proof, and lets me ignore formatting and focus on content.\nThe key to my writing workflow is the magical pandoc, which converts Markdown files into basically anything else. I use Quarto to convert Markdown to HTML, PDF (through LaTeX), Word, and any other output format.\nI do my academic writing in several different programs: for stats-heavy stuff, I use RStudio or Positron, and for prose-heavy stuff, I use iA Writer or Typora. I used to use Ulysses (and still think it’s a fantastic app!), but I found that I wasn’t using it as much in the past few years as I’ve switched to Quarto for my writing.\nI store all my bibliographic references, books, and articles in Zotero (see here for why).\nI read and annotate all my PDFs with Zotero, both on desktop and on iOS, since it can export annotations as clean plain text.\nI store all my notes in Obsidian. Before switching to Obsidian I used Bear, which was great but didn’t support fancier things like math or syntax highlighting. Before that, I used Evernote, but I abandoned it in September 2018 after 9 years of heavy use, given their ongoing privacy controversies and mass layoffs."
  },
  {
    "objectID": "uses/index.html#development",
    "href": "uses/index.html#development",
    "title": "What I use",
    "section": "Development",
    "text": "Development\nScience and research\n\nI post almost everything I write or develop on GitHub.\nI use R and either RStudio or Positron for most of my statistical computing, and I’m a dedicated devotee of the tidyverse. In the interest of full reproducibility and transparency, I make Quarto websites for each of my projects. See a list of these websites.\nI also use Python occasionally. Every few months I play with pandas and numpy and Jupyter, but I’m far more comfortable with R for scientific computing.\nI adapted the idea for research haikus from Kirby Nielsen.\nI use The Rogue Scholar to create stable DOIs for each of my blog posts.\nWeb\n\nI run my main web server on a DigitalOcean droplet, and I spin up temporary droplets all the time to offload scraping scripts, complicated R models, and to create on-the-fly VPNs.\nI normally access my remote files through SSH in a terminal, but for more complicated things, I’ve found that Mountain Duck is indispensable.\nMy website uses Pelican. My teaching websites all use blogdown + Hugo.\nI use Let’s Encrypt for SSL.\nMiscellaneous\n\nI use a system-wide hotkey (ctrl + `) to open iTerm2 from anywhere.\nI use Homebrew to install Unix-y programs.\nI’m partial to both Fira Code and Consolas for my monospaced fonts."
  },
  {
    "objectID": "uses/index.html#desktop-apps",
    "href": "uses/index.html#desktop-apps",
    "title": "What I use",
    "section": "Desktop apps",
    "text": "Desktop apps\nGraphic design\n\nThough I regularly use LaTeX (through pandoc), I adore InDesign CC and use it to make fancier academic and policy documents. I also used it for all the typesetting I did for BYU’s Neal A. Maxwell Institute.\nI use Illustrator CC all the time to enhance graphics I make in R and to make non-data-driven figures and diagrams.\nI use Lightroom and Photoshop too, but less often nowadays.\nDespite my dislike for Word and Excel, I use PowerPoint for all my presentations. It’s not my favorite, but in the apocryphal words of Churchill, “PowerPoint is the worst form of slide editor, except for all the others.”\nProductivity\n\nMy secret for avoiding the siren call of the internet is Freedom. I have two blocklists: (1) antisocial, which blocks Facebook and Twitter, and (2) nuclear, which blocks everything. I have the antisocial blocklist enabled on my laptop and phone from 8:00 AM–6:00 PM and 8:30 PM–11:30 PM. Since I accidentally discovered that it’s relatively easy to circumvent the blocking on the Mac, I also use Focus with the same schedule.\nI was an early convert to Todo.txt and used it for years until my tasks and projects got too unwieldy. I switched to Taskpaper for a while, used 2Do for a couple years, and now I’m a convert to OmniFocus.\n\nFantastical 2’s natural language input is a glorious thing.\nI use Timery as an interface to Toggl to track my time during the day\nI keep a log of what I work on (and occasionally do more traditional diary-like entries) with Day One on both iOS and macOS.\nI use TextExpander to replace and expand a ton of snippets, and I use Keyboard Maestro to run dozens of little scripts that help control my computer with the keyboard.\nI use Übersicht to show weather, iTunes track information, and my todo lists on my desktop.\nI use Dropbox religiously and use Backblaze to back up all the computers in our house to the cloud.\nWith all these little helper apps, I use Bartender to keep my menubar clean."
  },
  {
    "objectID": "uses/index.html#hardware",
    "href": "uses/index.html#hardware",
    "title": "What I use",
    "section": "Hardware",
    "text": "Hardware\n\nI use a 2021 14″ M1 Max MacBook Pro, a 2018 15″ MacBook Pro, a 5th generation iPad, and an iPhone 8."
  },
  {
    "objectID": "blog/2007/10/06/populating-a-livecycle-pdf-with-php-and-mysql/index.html",
    "href": "blog/2007/10/06/populating-a-livecycle-pdf-with-php-and-mysql/index.html",
    "title": "Populating a LiveCycle PDF with PHP and MySQL",
    "section": "",
    "text": "Warning\n\n\n\nThis tutorial is officially defunct. It is only here for archival purposes. The main script has been consolidated into one PHP class—pdftk-php. Please see the updated tutorial.\n\n\nI work in the Harold B. Lee Library Multimedia Lab where we check out digital video and still cameras, tripods, external hard drives, digital voice recorders, and let people use $8,000 Quad Core Intel Macs. Expensive stuff…\nTo insure a “you break it, you pay for it” system, we require all patrons to fill out a loan agreement that we then keep on file. We’ve been using this system for several years and now have more than a thousand forms—all completely unorganized and out of date. We have no way of knowing if a patron has graduated. We have no way of seeing if a patron has filled out a form previously.\nSo, I volunteered to fix the problem and move the entire loan agreement system to an online database. I had dabbled in LiveCycle and PHP but had never touched MySQL. So I decided to figure it all out.\nI’m assuming you already have a server set up with PHP and MySQL. If not, you can download WAMP or MAMP and set up a local server on your computer for testing.\nI’m also assuming you have some knowledge of HTML and PHP. If not, search for some PHP tutorials on Google and get a foundation there.\n\nAdobe LiveCycle\nAdobe’s LiveCycle PDF Form software works great creating fillable PDF forms and then gathering the form data electronically. Collecting the data is relatively easy since LiveCycle uses semi-open file formats for data storage. For example, if you have an e-mail submit button in your LiveCycle form, a specially formatted XML file will be e-mailed to whatever address you set in the button properties. LiveCycle can then input that XML and repopulate an empty form.\nAdobe even sells the LiveCycle Enterprise Suite, which is basically a specialized server made for generating and repopulating PDFs from submitted data. The Enterprise Suite is extremely expensive though\n\n\nCreate form in LiveCycle\nUnfortunately LiveCycle Designer does not work like the rest of the Adobe CS3 products. I spend most of my time in InDesign, and from a typographic point of view, Designer is pathetic and a little difficult to work with.\nCreating the form is relatively straightforward, regardless of the limitations. Drag text boxes and image boxes from the Library panel to add static text and images. Drag text input boxes from the Library to make fillable fields. Actually laying out and designing the form is not the scope of this tutorial, so I won’t go any further with that.\n\n\nData Bindings\nWhat concerns us most is data submission and population with form fields, so we need to set up our fields to work. If you click on a text field, you should have three tabs in the Object panel (if you don’t have an Object panel, go to Window &gt; Object).\nThe Field tab lets you set some basic properties for the field, like whether or not the field can have multiple lines, the line length, the display pattern for phone numbers or other number patterns, the caption, and a plethora of other things. The Value tab allows you to do some scripting for validation of your fields. I tried getting this to work, but since I’m not a programmer at all and don’t really know Javascript, I gave up.\nThe Binding tab is the most important for our purposes. By default your field will have a basic name like TextField1. You should change the names of all your fields to a more canonical naming system, like FirstName, LastName, EMail, etc. You can also change the data patterns and formats for text, XHTML, or dates.\n\n\nSubmit Through HTTP\nFor my purposes, I wanted patrons to be able to fill out this PDF from anywhere and then submit it online, regardless of e-mail accounts. To best do this, drag an HTTP Submit Button from the Standard section of your Library. For this button to do anything, you need to set a URL that will receive the data in the form of HTTP POST.\n\n\nBegin Setting Up Your Web Application\nI made several different PHP files in the process of getting this all to work. First I wanted to verify what the HTTP POST values were before I started trying to process them. I adapted this code from Steve Tibbett, an actual Adobe guy.\nDownload text for dump.php\nCreate a file called dump.php and paste this code into it (or remove the .txt extension). Set your HTTP Submit Button URL to dump.php (in my case it was http://localhost/PDFStuff/dump.php) and preview your PDF in Designer. Submit your data and you’ll see all the variables and the raw post data. This step is only to verify that the HTTP POST variables actually match up with your Designer field names.\nNow that we know that the HTTP POST variables are actually working, we need to save them to a database. You’ll need to first set up a MySQL table or database. Since I had never done this, I followed some tutorials here, which were extremely helpful in understanding how to actually use MySQL.\nUse PHPmyadmin and create a new PDFStuff database with a username and password and then create a table in it with the following command (or use the GUI form in PHPmyadmin to create the table—either way works):\nCREATE TABLE `PDF_Loans` (\n`id` int(4) NOT NULL auto_increment primary key,\n`FirstName` varchar(65) NOT NULL default '',\n`LastName` varchar(65) NOT NULL default '',\n`EMail` varchar(65) NOT NULL default '',\n) TYPE=MyISAM AUTO_INCREMENT=0 ;\nYou now have a table in your database where we can store our PDF form variables. Paste this code into a file called insert.php and change the PHP variables as necessary, both for your MySQL connection information and your HTTP POST variables (here I just use FirstName, LastName, and Email).\nDownload text for insert.php\nChange the URL of the HTTP POST button to insert.php (again, in my case it’s http://localhost/PDFStuff/insert.php) and try submitting some data with your PDF form. It should work.\n\n\nViewing Data in the Database\nTo actually see your submitted data, create a new php file called view.php and paste this code in, changing it as necessary.\nDownload text for view.php\nThis page will take all the data from your database and display it in a table. You should see one record—the one you just added. There is also a column for a link to view the PDF, although the link is blank for now. Add several more through LiveCycle to make sure it’s working.\n\n\nPDF Madness\nIf you’ve already had PHP/MySQL experience, all of that was easy. Now comes the tricky part—repopulating the PDF form from the MySQL.\nTo get this to work, we need to convert the MySQL data into an FDF file, or the Adobe file format for storing form data. We then need to infuse the FDF file into our empty PDF form and allow the user to download it.\nFortunately, someone else figured out the bulk of this, at www.pdfhacks.com. Download pdftk and forge_fdf.php place them in your main site directory. Forge_fdf.php will take your data and transform it into an FDF file while fdftk will insert that FDF into your PDF. All you need to do is add some variables.\nBefore creating your variables, you need to discover the real names for all of your fields. If you made your PDF in Acrobat, the field names should be identical, but if you used Designer, the official code-based names will be much longer. fdftk can discover those names for you and dump them in a text file.\nPlace your empty PDF form in your main site folder. Open up a command prompt or terminal and run this command in the site folder, changing file names as necessary:\n$ pdftk form.pdf dump_data_fields &gt; form.pdf.fields\nOpen up the newly created form.pdf.fields file in Notepad and you’ll see the automatic fdftk output, which will look something like this:\nFieldType: Text\nFieldName: form1[0].#subform[0].#area[0].FirstName[0]\nFieldNameAlt: First Name:\nFieldFlags: 2\nFieldJustification: Left\nThe FieldName in this case is long and hairy, but we’ll need that full name for the data insertion to work.\n\n\nDynamic Data Insertion\nNow we’re ready to put all the pieces together. Make a file called viewpdf.php and paste this code in, changing as necessary:\nDownload text for viewpdf.php\nIf you don’t want the form flattened (i.e. you want to maintain the form fields), take out the – flatten command.\nNotice how the long names had to go in to the $fdf_data_strings array.\nTo populate the PDF from your view.php table, you need to pass those long field names into viewpdf.php. Technically you would need to change the links in view.php to include the row id number for each row, so the PDF is generated using only the information from that row. Fortunately, we can have PHP and MySQL write all those links dynamically.\nWe originally set up the database so that every time you insert a record, an auto-id number would be assigned. We can reference that id number to view the PDF for that specific row entry.\nYou pass the id variable into the viewpdf.php file by adding ?id=1 onto the URL (for example, viewing the PDF for record number three would be viewpdf.php?id=3).\nPHP and MySQL can automatically generate that messy link for every record in the table. Just replace &lt;a href=\"#\"&gt;View PDF&lt;/a&gt; in view.php with &lt;a href=\"viewpdf.php?id=&lt;?php echo $rows['id']; ?&gt;\"&gt;View PDF&lt;/a&gt;\nOpen up view.php and see your dynamically generated table. You should have dynamic links that point to viewpdf.php?id=whatever. If you click on one of the links, the code in viewpdf.php will be processed for that row and a PDF will be generated and flattened and downloaded.\nVoila!\nI was only able to get this to work with text fields, although it is possible to do this with check boxes and other form elements. You’ll have to consult the fdftk documentation to see what variables need to be set for other form elements.\nThis is a bare bones implementation of PDF population. In real life this is implemented a lot better—i.e. I have my view.php file accessible only after logging in and all the pages are styled with CSS to look nicer.\nHopefully this all made sense. If you want to view the original tutorials I used for this, visit:\n\nMacTech Tutorial—Explains how to do this using an HTML submission form rather than a PDF form. This was the basis for my tutorial.\nMacTech Example—Working example of a PDF being populated by HTML.\nPHPeasy—Basic PHP/MySQL tutorials."
  },
  {
    "objectID": "blog/2009/03/15/do-not-succumb-to-economic-stupidity/index.html",
    "href": "blog/2009/03/15/do-not-succumb-to-economic-stupidity/index.html",
    "title": "Do not succumb to economic stupidity",
    "section": "",
    "text": "Yikes. It’s been a week since I launched my new site and blog and I haven’t posted yet. I think this is partially because I don’t know what I’m going to be focusing on with this blog. Middle East Studies? Technology? Hmmm…\nI’ve had this blog post on my mind for a while, though, so I’d better post it. The best part is that it has nothing to do with MES or tech topics.\nI’ve felt somewhat immune and aloof from all the American economic troubles. We moved to Egypt in August, a few months after the Bear Stern collapse, but before the September Lehman Bros./Commercial Paper collapse and subsequent bailouts.\nDespite the rigors of grad school, I’ve had plenty of time to study up on the causes of this crisis. Having a bus daily 45 minute–1 hour bus commute has given me plenty of time to follow some awesome podcasts. I’ll post about my podcast routine later, but suffice it to say, I feel way more informed about this crisis because of the time I’ve spent listening and studying.\nThat doesn’t mean I know everything—or anything—about this crisis. I don’t fully comprehend all the crazy financial machinations that got us here. I can follow the news a bit better, though.\nHere are some of the best resources I’ve found so far:\nNPR’s Planet Money: Amazing podcast that comes out 3–4 times a week. Excellent reporting. Easy to understand. Awesome. Their blog rocks, too.\nThis American Life: Normally their hour long radio shows highlight a variety of interesting stories on random topics and have little to do with news, but they cosponsor Planet Money and have dedicated two entire shows to the economic crisis, The Giant Pool of Money and Bad Bank. US Treasury Secretary Tim Geithner and Senator Max Baucus even praised Bad Bank in front of a joint session of Congress. It was that good.\nThe Crisis of Credit: Brilliant 10 minute animation inspired by Planet Money.\nInside the Meltdown: PBS did a great hour long documentary on the crisis. Really good, as well.\nSo check out those links and get informed. Found any other useful resources out there? Let us know in the comments…"
  },
  {
    "objectID": "blog/2009/04/26/typing-transliterated-arabic-quickly/index.html",
    "href": "blog/2009/04/26/typing-transliterated-arabic-quickly/index.html",
    "title": "Typing transliterated Arabic quickly",
    "section": "",
    "text": "Since Arabic doesn’t use the Latin alphabet, and lots of the letters don’t have Latin equivalents (خ, ع, ق, ط, for example), transliteration is necessary to show Arabic words and sounds in English writing. There is an easy way to type transliterated Arabic quickly, though, using macros to locate hidden Unicode characters used by many of the standard transliteration systems.\nUnfortunately, there is no universally standard system for transliteration, and most systems use letters that aren’t found on normal keyboards. One of the rising systems in the Middle East, nicknamed Franco Arab in Egypt, is my least favorite. It only uses standard English letters, meaning it’s useful for texting, e-mailing, and other things where it’s difficult to write in real Arabic script. Biggest problem: it’s ugly and hard to read.\nFor example, the name Great Britain (بريطانيا العظمى) uses several non-Latin letters. Written in Franco it looks like this: bri6ania al3o’6ma. For readers unfamiliar with Arabic (or even those who are, like me), it’s always hard to remember what the random uppercase letters and numbers mean.\nFortunately, there are better systems. Here’s Great Britain written using the IJMES (International Journal of Middle East Studies) system, also used in the Encyclopedia of Islam: Brīṭānīyā al-ʿuẓmá. Much easier to read.\nSince the nonstandard Latin letters use Unicode glyphs, you need to use a font that has a full set of Unicode glyphs, like Times, Arial, Helvetica, and other standard fonts. You also have to hunt down all the special characters either in Word’s Insert Special Character dialog or in the Glyphs panel in InDesign.\nYou can speed up the process of hunting for and inserting special characters by using a text substitution app like Texter for Windows (free, open source) or Typinator for Mac (not free). These programs can replace abbreviations that you type with preset phrases. For example, if you wanted to quickly type today’s date you could set up a shortcut that would replace %date with the full date.\nI set up a list of text replacements in my copy of Typinator that automatically change certain combinations of characters into IJMES standard transliterated rules. Here’s my list of text transformation rules (all with the prefix -ij, short for IJMES):\n\n-ij' = ʿ\n-ij` = ʾ\n-ija = ā\n-ijd = ḍ\n-ijh = ḥ\n-iji = ī\n-ijs = ṣ\n-ijt = ṭ\n-iju = ū\n-ijz = ẓ\n\nHere’s a (very) quick example of this in action:\n\n\n\n\nYou could set up similar rules for transliteration with different systems (even Franco), or even different languages. Typing IJMES transliterated words for academic papers just got infinitely easier.\n\n\n\n\n\n\nWarning\n\n\n\nYou will probably only want to use IJMES transliteration in print because of font encoding issues on different platforms and browsers. For online text you’ll have to stick with Franco or something like it."
  },
  {
    "objectID": "blog/2009/05/14/a-tale-of-three-taxis/index.html",
    "href": "blog/2009/05/14/a-tale-of-three-taxis/index.html",
    "title": "A Tale of Three Taxis",
    "section": "",
    "text": "Traffic in Cairo is horrible, especially when all the taxi drivers, the ubiquitous life-blood of the Egyptian streets, have a deathwish for you.\nRead about my recent near misses at my post at our family blog, Heissatopia."
  },
  {
    "objectID": "blog/2009/07/19/converting-a-blogger-blog-to-indesign-tagged-text/index.html",
    "href": "blog/2009/07/19/converting-a-blogger-blog-to-indesign-tagged-text/index.html",
    "title": "Import a Blogger Blog to InDesign with Perl",
    "section": "",
    "text": "Our family has a fairly sizable blog that we (actually, mostly my wife, Nancy) have kept updated for several years. Since it contains so much family history we wanted an easy way to preserve it in print form, just in case Blogger gets the boot from Google some day (not that that will ever really happen…).\nSince we’re both hobbyist graphic designers—I taught a couple print layout and design classes as an undergrad at BYU and have made several books at Lulu.com—we decided to layout and print each year of our blog, to keep for posterity.\nA couple years ago Nancy attempted this with our smaller Jordan blog for a print publishing class she took at BYU. We spent the bulk of our time manually copying and pasting each post and the subsequent comments into a huge Word document. She then ran a long series of find/replaces to clean up the messy, inconsistent typography, and then finally placed it into Quark (that evil program). Through a series of unfortunate events, Quark crashed repeatedly and corrupted her file multiple times—she was lucky to get her first draft turned in for her final project (she got an A, though. Phew!).\nI knew there had to be a faster, more efficient way to wrangle all the blog text, but this was back in 2006, before Blogger had an open API or options to backup a blog. Primitive, dark days indeed :).\nHowever, last year, Blogger introduced a fantastic new option—the ability to backup and export your entire blog, comments and all. Blogger spits out an Atom-formatted XML file that you can use to recreate your blog later on (or possibly import onto other platforms, like WordPress, I think). This was the key to simplifying the daunting task of collecting the text for our blog books. All we needed was a way to mangle the text in the XML file to create an InDesign-ready file.\nSo, I whipped up a semi-complicated Perl script that can parse an Atom-formatted XML file from Blogger and create a text file using InDesign Tagged Text to preapply paragraph and character styles. It also cleans up the typographic elements of the text, adding em and en dashes, removing empty paragraphs, etc. Additionally, it can add hidden index entries for each tag, essentially creating a barebones index for your book. And it only takes 10ish seconds to run on a large blog. It’s not perfect and could stand some good optimization, but it works.\nAdditionally, since InDesign tagged text works with, well, text, it won’t place your images for you. Instead it will insert the location of the image (the src=whatever.jpg of the img tags) in between curly braces { }. You’ll then need to manually place all the images later, deleting the braced text.\nIn the future, the script could be changed to output XML, which does let you include pictures, but you’d have to have all your images on your hard drive already. The script could go and download all the linked images, but it’s not really a good idea to place low resolution, web-optimized images in a print document. In our case we have high-res copies of all the pictures on the blog stored on an external hard drive, so we just have to go and find and place the images we want. It takes more time, but it makes better quality documents in the end.\nAlso, links are preserved as footnotes—all href=\"whatever.html\"s show up as the footnote text.\n\nHow to use the script\nFirst, download the script and its supporting files from Github. If you’re using Mac OS or Linux, make sure the main script file, format_for_id.pl is executable—type chmod +x format_for_id.pl at the terminal.\nNext, make sure you have Perl installed on your system. If you are using Linux or Mac OS X, you’re good to go. If you’re using Windows, download and install Strawberry Perl for Windows. You can also use ActivePerl, but installing modules is a little more difficult.\nThe script uses several additional CPAN modules that you’ll need to install. You’ll need to use the CPAN shell to do so.\n\nOn Windows with Strawberry Perl: open the packaged CPAN client in the Start Menu folder\nOn Windows with ActivePerl: Good luck. There is a large repository of specially compiled CPAN modules for ActiveState, and reportedly there is a kind of CPAN shell, but I haven’t gotten either to work too well. Stick with Strawberry Perl. It’s better :)\nOn Mac OS X: type perl -MCPAN -e shell at a terminal window\nOn Linux: type sudo cpan at a terminal window\n\n(If it’s your first time running the CPAN shell you’ll be asked to configure the installation environment. Choose the option to automatically configure everything.)\nOnce everything is set up and you see the cpan&gt; shell prompt, type install Package::Name (eg. install Date::Format) for each of the dependent CPAN packages listed at the beginning of format_for_id.pl.\nLog in to your Blogger Dashboard and export your blog as an XML file by going to Settings &gt; Basic &gt; Export blog. Place the XML file in the script folder.\nOpen config.cfg with a text editor and change the settings as needed. Set the input file to your newly downloaded XML file, choose the year you want to extract, set an output file, and set the file header, either &lt;UNICODE-MAC&gt; or &lt;UNICODE-WIN&gt;, depending on what platform you use InDesign on.\nFor now, leave all the style tags as they are so you can place the text into the example InDesign file and see how everything works. You can change them later and rerun the script\nFinally, using the terminal or command prompt, navigate to the folder with the script and and run it by typing perl format_for_id.pl. If everything goes well you should have an output file at the location you specified, full of InDesign tags.\nOpen up Example.indd in InDesign CS3 or above and place the generated text file. All the text should come in perfectly with all the needed paragraph and character styles applied. Bravo!\n\n\nAdvanced usage\nObviously you’ll want to make some changes to the format of the output text. You might not want the post URL right after the tag—you might want it at the end, or not want it at all. With a little knowledge of Perl, you can edit the main script directly, mostly the combineSortClean() sub near the end of the script, to change the order of the output elements.\nYou can also disable tag indexing and allow the tags to be output with a paragraph style. Just comment and uncomment the appropriate sections in the code. The same goes for the author-specific character styles—comment and uncomment the needed lines in the script.\nYou can rename the styles and use your own—just make sure the styles exist in your InDesign document before you place the output file. InDesign will throw away any tags that don’t already exist in the document.\nI made the script for our specific blog, so it doesn’t take every possible paragraph or character style into account. If you want additional functionality, you’ll have to add it. Feel free to fork the project off of GitHub and add to/improve it. That’s why it’s open source :)\nIf you have any questions, ask in the comments. Report any issues at the project GitHub page. I’ll try to respond quickly—I generally do, as evidenced by my pdftk-php project :)\nGood luck!"
  },
  {
    "objectID": "blog/2009/07/29/installing-pdftk-php/index.html",
    "href": "blog/2009/07/29/installing-pdftk-php/index.html",
    "title": "Installing pdftk-php",
    "section": "",
    "text": "Upon popular request, I’ve decided to update the original tutorial for populating a LiveCycle PDF with PHP to apply to the new release of pdftk-php. The installation instructions should be mostly clear in the readme and in the inline comments in the example included with the script; this post is merely supplemental."
  },
  {
    "objectID": "blog/2009/07/29/installing-pdftk-php/index.html#basic-usage",
    "href": "blog/2009/07/29/installing-pdftk-php/index.html#basic-usage",
    "title": "Installing pdftk-php",
    "section": "Basic usage",
    "text": "Basic usage\n\nInitial set up\nDownload the most recent version of pdftk-php from GitHub and download and install pdftk on your server.\nUnzip the download from GitHub and place the folder on your server. I’ve placed mine in a folder called pdftk-php.\nCreate a MySQL user and database and run the SQL found in /example/database.sql in a MySQL client (like phpMyAdmin) to create the sample database.\n\n\n\nExample Query in phpMyAdmin\n\n\nModify the information in example/_dbConfig.php so that the application can connect to your database.\n$host = \"localhost\";\n$username = \"pdftk-user\";\n$password = \"supersecure\";\n$db_name = \"pdftk-php\";\nBrowse to the example site (in my case, http://localhost/pdftk-php/example/index.php) and add some entries to populate the database a little.\n\n\nSet up the script\nOpen pdftk-php.php and insert the full path to your working pdftk installation at the beginning part of the passthru() command near line 71. Here are some examples for different scenarios on server platforms:\n// On a typical Unix-based installation\npassthru(\"/usr/local/bin/pdftk ...\");\n\n// On Windows, with an absolute path\npassthru(\"c:\\pdftk\\pdftk.exe ...\");\n\n// On Windows, with a relative path (useful if you place pdftk.exe in the server folder structure)\npassthru(\"../pdftk.exe ...\");\nIf you’re on a Unix-based server and don’t know where pdftk is, type one of the following commands, which should result in the absolute path to the program:\nwhich pdftk\n# or\nwhereis pdftk\nIn example/download.php verify that the path to the required pdftk-php.php is correct, near line 18. In the example, pdftk-php.php is located a directory below the example directory. If you like to store your included files elsewhere, make sure that you modify the require() path here.\npdftk-php.php needs to be able to write to a temporary directory on your server in order to create a temporary FDF file. This directory is specified near line 58, with the tempnam() function.\nIf you are on a Windows server you should already be able to write to pretty much any directory (I think… I’ve never worked with IIS permissions), so you should be good to go. If you are on a Unix-based server you’ll need to be more explicit with directory permissions. To make things easier, create a temporary folder on your server and give it write permissions:\ncd pdftk-php\nmkdir tmp\nchmod 777 tmp\nThen set the path in tempnam() to the new temporary folder.\n// If at the same level as download.php\n$fdf_fn = tempnam(\"tmp\", \"fdf\");\n\n// If one directory behind download.php\n$fdf_fn = tempnam(\"../tmp\", \"fdf\");\n\n// You can also use an absolute path\n$fdf_fn = tempnam(\"/Library/WebServer/www/pdftk-php/tmp\", \"fdf\");\n\n\nSet up the PDF\nCreate a fillable form in either Acrobat Professional or LiveCycle Designer, or use the included example PDF form. Give each field a unique and significant name so that you can work with the form more easily later on. You can modify field attributes by double clicking on the field using the Forms toolbar in Acrobat; in LiveCycle, use the Object panel.\n\n\n\nAcrobat Form Field Options\n\n\n\n\n\nLiveCycle Form Field Options\n\n\nIf you are using LiveCycle, you’ll need to save the final PDF as a static form compatible with Acrobat 7. pdftk doesn’t work with dynamic forms or PDFs from later versions of Acrobat.\n\n\n\nLiveCycle Save Options\n\n\n\n\nConnect PDF to script\nexample/download.php connects to your database, retrieves a row based on a passed GET variable, saves the data from the fetched row into variables, finally calling pdftk-php.php, which does the heavy lifting of creating an FDF file and injecting it into the PDF.\nStarting at around line 30 the script assigns the fetched values to variables. Each of those retrieved variables needs to be paired with a form field in your PDF (near line 39). In a basic Acrobat form this is simple:\n$fdf_data_strings= array('firstname' =&gt; $pdf_firstname,  'lastname' =&gt; $pdf_lastname, 'email' =&gt; $pdf_email);\nLiveCycle tends to complicate the form names slightly. You can use pdftk from the command line to retrieve the official form field names. Run this command from the directory containing your PDF file:\npdftk form.pdf dump_data_fields &gt; form-fields.txt\nWhen you open the resultant .txt file you should see a report of all the fields\n...\n---\nFieldType: Text\nFieldName: form1[0].#subform[0].firstname[0]\nFieldNameAlt: First name&#9;\nFieldFlags: 0\nFieldJustification: Left\n---\n...\nUse those long, hairy FieldNames in the $fdf_data_strings array, like so:\n$fdf_data_strings= array('form1[0].#subform[0].#area[0].FirstName[0]' =&gt; $pdf_firstname, 'form1[0].#subform[0].#area[0].LastName[0]' =&gt; $pdf_lastname, 'form1[0].#subform[0].#area[0].EMail[0]' =&gt; $pdf_email, );\nFinally, check the values of $pdf_filename and $pdf_original near lines 62 and 65.\nGo to http://localhost/pdftk-php/example/view.php and click on the download links for one of entries. You should be prompted to download a PDF file, dynamically generated using pdftk-php.php. Success!"
  },
  {
    "objectID": "blog/2009/07/29/installing-pdftk-php/index.html#advanced-customization",
    "href": "blog/2009/07/29/installing-pdftk-php/index.html#advanced-customization",
    "title": "Installing pdftk-php",
    "section": "Advanced customization",
    "text": "Advanced customization\n\nUsing checkboxes or radio buttons\n$fdf_data_strings works great for text fields, but can’t handle radio buttons or check boxes. For that you’ll need to use the $fdf_data_names array near line 49.\nNB: The logic for manipulating the form data in PHP and MySQL might be a little convoluted and could easily be optimized, but it works for clear demonstration purposes.\nTo demonstrate this how to do this, we’ll add a checkbox to our form and extend the database. Run this query in a MySQL client to add a couple columns to our table:\nALTER TABLE `users` ADD `option1` TINYINT( 1 ) NOT NULL, ADD `option2` TINYINT( 1 ) NOT NULL ;\nOpen /example/example.pdf in Acrobat Professional and add two checkbox fields named option1 and option2.\n\n\n\nHuge Checkboxes\n\n\nWe need to modify our web form and the table that displays the data, just to make sure everything is getting saved to the database correctly.\nFirst, make a couple changes to example/index.php After the section near lines 104–107, add\n&lt;p&gt;\n    &lt;label for=\"option1\"&gt;Option 1&lt;/label&gt;\n    &lt;input type=\"checkbox\" name=\"option1\" value=\"1\" id=\"option1\" /&gt;\n&lt;/p&gt;\n&lt;p&gt;\n    &lt;label for=\"option2\"&gt;Option 2&lt;/label&gt;\n    &lt;input type=\"checkbox\" name=\"option2\" value=\"1\" id=\"option2\" /&gt;\n&lt;/p&gt;\nThen, up near the top of example/index.php after line 34, add this:\nif ($_POST['option1'] == 1) {\n    $option1 = 1;\n} else {\n    $option1 = 0;\n}\n\nif ($_POST['option2'] == 1) {\n    $option2 = 1;\n} else {\n    $option2 = 0;\n}\nThis checks the value of the submitted checkboxes and sets the $optionx variables to either 1 or 0, which fit into the TINYINT columns in our table. You could use actual text as well and set the columns to VARCHAR.\nChange the SQL query from\n$sql = \"INSERT INTO users (firstname, lastname, email) VALUES ('$firstname', '$lastname', '$email')\";\nto\n$sql = \"INSERT INTO users (firstname, lastname, email, option1, option2) VALUES ('$firstname', '$lastname', '$email', '$option1', '$option2')\";\nGo ahead and insert some dummy submissions with boxes checked and unchecked to make sure everything is working.\nOptionally we need to modify example/view.php to show the stored values. Add the following table header cells after line 29:\n&lt;!-- Already here --&gt;&lt;th&gt;E-mail Address&lt;/th&gt;\n   &lt;th&gt;Option 1&lt;/th&gt;\n   &lt;th&gt;Option 2&lt;/th&gt;\n&lt;!-- Already here --&gt;&lt;th&gt;Download PDF&lt;/th&gt;\nIn the while loop a few lines later, add this code:\n&lt;!-- Already here --&gt;&lt;td&gt;&lt;?php echo $user[\"lastname\"]; ?&gt;&lt;/td&gt;\n&lt;td&gt;&lt;?php echo ($user['option1'] == 1) ? \"Yes\" : \"No\"; ?&gt;&lt;/td&gt;\n&lt;td&gt;&lt;?php echo ($user['option2'] == 1) ? \"Yes\" : \"No\"; ?&gt;&lt;/td&gt;\n&lt;!-- Already here --&gt;&lt;td&gt;&lt;?php echo $user[\"email\"]; ?&gt;&lt;/td&gt;\nThis is just PHP ternary notation, which essentially says that if the value of optionx is equal to one, echo “Yes,” otherwise, echo “No”.\nFinally we need to modify example/download.php to handle our checkboxes. Like I said above, the $fdf_data_names variable handles checkbox and radio button data. In PDF forms, the two allowed values for checkboxes are “Yes” and “Off” (not really opposites, but oh well), so you’ll need to set variables accordingly. Replace $fdf_data_names = array(); near line 49, with this, which checks the values of optionx and sets $pdf_optionx to either “Yes” or “Off” and then defines the $fdf_data_names array appropriately:\nif ($data['option1'] == 1) {\n    $pdf_option1 = \"Yes\";\n} else {\n    $pdf_option1 = \"Off\";\n}\n\nif ($data['option2'] == 1) {\n    $pdf_option2 = \"Yes\";\n} else {\n    $pdf_option2 = \"Off\";\n}\n\n$fdf_data_names = array('option1' =&gt; $pdf_option1, 'option2' =&gt; $pdf_option2);\nAnd that should do it! Visit http://localhost/pdftk-php/example/view.php and download one of the forms. The checkboxes should populate perfectly.\n\n\nOther types of form fields\nCombo boxes and radio buttons act similarly to checkboxes. If you run the dump_data_fields command with pdftk again on a form with these more advanced options, you’ll see a few differences in the results.\nFieldType: Text\nFieldName: email\nFieldFlags: 0\nFieldJustification: Left\n---\nFieldType: Button\nFieldName: option1\nFieldFlags: 0\nFieldValue: Yes\nFieldJustification: Left\nFieldStateOption: Off\nFieldStateOption: Yes\n---\nFieldType: Choice\nFieldName: favoriteColor\nFieldFlags: 131072\nFieldValue: blue\nFieldValueDefault: red\nFieldJustification: Left\nYou can see the “Yes” vs. “Off” values in our checkbox (called “Button” in PDF lingo). Drop down lists (“Choice” in PDF-speak) have multiple values, specified by you when you create the field.\n\n\n\nCombo box properties\n\n\nRadio buttons are hybrids. They are considered “Buttons,” like checkboxes, but can have custom values, like drop down lists.\nYour final PHP script will need to take these different values into account and assign the correct values in the $fdf_data_names array."
  },
  {
    "objectID": "blog/2009/07/29/installing-pdftk-php/index.html#conclusion",
    "href": "blog/2009/07/29/installing-pdftk-php/index.html#conclusion",
    "title": "Installing pdftk-php",
    "section": "Conclusion",
    "text": "Conclusion\nYou can do a ton with the pdftk-php.php class once you get it set up initially and get past the slight learning curve. If you have any questions, feel free to ask in the comments. If you find any problems, comment here or open an issue at the GitHub project page. Additionally, you can fork the project and contribute.\nGood luck!"
  },
  {
    "objectID": "blog/2009/08/01/using-google-voice-and-gizmo-project-together-internationally/index.html",
    "href": "blog/2009/08/01/using-google-voice-and-gizmo-project-together-internationally/index.html",
    "title": "Using Google Voice and Gizmo Project Together",
    "section": "",
    "text": "Note\n\n\n\nSee update below (skip to update)\n\n\nGoogle Voice, the Google-ized incarnation of GrandCentral, is a fantastic service that aims to become your virtual phone switchboard. It gives you a free phone number that can receive regular phone calls and route them to any other actual phones you have connected to your account. Powerful stuff.\nUnfortunately, though, its forwarding abilities are limited to US phones. Using some VoIP magic, though, you can create a semblance of international forwarding and get free (or nearly free) phone calls to the US while abroad. If you’re not in a foreign country, you can harness the same VoIP magic to get a nearly free phone service.\n\nThe Gizmo Project—background\nGizmo (formerly http://gizmo5.com/pc/) is normally a VoIP provider that lets you make free (or super cheap—something like $0.019 a minute) phone calls. When you sign up for an account you get a special phone number in the 747 area code as your VoIP/Gizmo username. While any phone on the Gizmo network can call your 747 number for free, regular phones can’t connect to it.\nGizmo offers a Call In service that lets you buy a phone number in most US area codes (or one of dozens of countries), which then lets you receive phone calls from standard phones. Call In numbers start at $35 a year (or $12 a year for three months), but prices can be higher depending on demand.\nGizmo touts itself primarily as software—it provides a “soft phone” program that you run on your computer. As long as the program is open you can make and receive phone calls (much like an IM program or Skype) using a microphone and your computer’s speakers or headphones.\n\n\nGizmo without a computer\nIt’s impractical to keep your computer on all the time and it can be awkward to use your computer as a phone. You can get around this limitation by buying an ATA adapter—a little box that plugs into your network with the sole purpose of running phone services. It’s essentially a hardware version of the Gizmo soft phone.\nFancy corporate VoIP phones (like the ubiquitous Cisco ones) have ATAs built in (kind of. The real ATA is somewhere on the network letting these computer-phones connect to it). You don’t need a fancy VoIP phone, though. Standard consumer ATAs let you plug regular phones directly into the adapter.\nAfter configuring the ATA with your Gizmo information you can make and receive calls using your Call In number. Rather than pay $40+ a month for regular phone service, you can have a fully featured phone that only costs $35 a year plus &lt;$0.02 a minute.\n\n\nBefore Google Voice\nFor the past three years we’ve been using Gizmo as our full-time phone. We bought a Linksys/Sipura ATA (our model, the SPA1000 is no longer manufactured), got a $10 phone from Target, and bought a Utah county 801 Call In number. The phone worked perfectly. I could plug the ATA in to any internet connection and get cheap/free phone service.\nOur system even works (mostly) in Egypt. I have two phones on my desk: the $10 Target phone plugged into the ATA (which is plugged into the router) and a 20 EGP neon pink phone (plugged into our Egyptian phone line). Family, friends, and unsuspecting telemarketers can reach us at our Utah number and pay only what it costs them to call an 801 number.\n\nOur current phone set up\nThe only problem with the system our system in Egypt is a bizarre limitation with Egyptian (or at least Link.net’s) internet infrastructure. We don’t have any bandwidth issues when someone calls us, but when we call out, the connection drops within the first ten seconds of the call 90% of the time. To get around this, we used some SkypeOut credits—we’d call someone in the States with Skype (using my computer), tell them to call us on our Utah number, hang up, and wait for their call.\nThis worked when we called actual people, but doesn’t work when calling banks, insurance companies, airline companies, or anything else with a phone tree—phone trees can’t call you back. SkypeOut works for those, but it’s more expensive than Gizmo.\nGoogle Voice changes all this.\n\n\nEnter Gizmo Voice\nGoogle and Gizmo have joined up to let you hook Google Voice directly into your Gizmo account. Rather than buy a Gizmo Call In number, I can use my free Google Voice number with my Gizmo Account. After following Gizmo’s instructions on connecting the two accounts, now when people call my Google Voice number, the call is routed to the normally inaccessible 747 Gizmo number, which is already associated with my ATA box.\nThis means I can stop paying $35 a year for my Call In number. The only thing I pay for is the phone use itself. Gizmo just changed their phone rates for users using Google Voice—apparently all calls under three minutes are free, while longer phone calls follow their normal low rates.\nAdditionally, now that I can have my Gizmo phone connected to my Google Voice account, my bizarre issue with calling out on Egyptian internet can be solved. In order to call people and have your Google Voice phone number appear on their caller IDs, you need to use Google Voice as an intermediary. You type in the number you want to call on their website and they’ll call one of your linked phones. When you pick up, your phone will start dialing the outbound number. Since Google calls my Gizmo phone now to make outbound calls, Link.net considers it an inbound call and it doesn’t get cut off.\nSo now, Gizmo combined with Google Voice gives me free short calls and cheap long calls to the US and a free US number that can replace my Gizmo Call In number. Everything works both in the States and internationally. It’s a nearly perfect system.\n\n\nStill untested Now tested…\nIn theory, since I have my Gizmo and GV accounts linked, my Google Voice number should show up on the recipient’s caller ID when I call out with my US phone, circumventing the need to use the Google Voice web site as the middleman. I can’t test it, though, since my internet connection won’t let me make any outbound calls with my ATA. I’ll keep trying over the next week, since I can make like 10–15 second calls 10% of the time.\nIt works! My Google Voice number shows up, just like it should…\n\nUpdate\n\nGoogle has acquired Gizmo5, which hopefully means that the link between Gizmo and GV will be more permanent and more official. Awesome."
  },
  {
    "objectID": "blog/2009/09/23/flashbakectl-released/index.html",
    "href": "blog/2009/09/23/flashbakectl-released/index.html",
    "title": "flashbakectl released",
    "section": "",
    "text": "Adding to my apparent series of Flashbake addons, I’ve just released flashbakectl.\nNormally to run Flashbake consistently you need to set up a cron job. While OS X is built on Unix and has cron, Apple recommends using launchd and property list (plist) files to run system agents and daemons. flashbakectl is a handy little script that loads and unloads a plist for you.\nBefore working on your project, run flashbakectl -l to load the plist and start the daemon, which will commit your unsaved changes every 15 minutes (or whatever you set it to). When you’re done for the day, run flashbakectl -u to stop the daemon, saving your computer from unnecessarily running Flashbake ad infinitum.\nflashbakectl only works on Mac OS X. You can get it at GitHub. Enjoy!"
  },
  {
    "objectID": "blog/2010/09/24/ios4-multitasking-battery-life/index.html",
    "href": "blog/2010/09/24/ios4-multitasking-battery-life/index.html",
    "title": "iOS 4, Multitasking, and Battery Life",
    "section": "",
    "text": "I recently got a brand new 4G iPod Touch for my initial foray into the amazing world of iOS. I’ve been sitting on the sidelines far too long and I’m so excited to finally have an almost-iPhone.\nHowever, I ran into my first hitch this morning and my Google skills have proven useless to solve it. I’ve been using the iPod as an alarm clock for the past few days since its UI is superior to the ancient digital clock in our bedroom :). When I went to bed last night the battery was probably at 80% capacity. Apple touts that the new iPod has 40 hours of battery life when playing music—quite impressive. I had put the iPod in airplane mode, turning off the WiFi so that I wouldn’t get any mail or Facebook notifications. With no internet, no music, and no apps (supposedly) running, the iPod’s battery theoretically should have lasted for months—or even years :)\nHowever, I didn’t wake up to the iPod’s alarm this morning. I woke up to Rachel pulling on my arm and saying “Wake up, Dad! I want cereal!” 1.5 hours later than when I was planning to wake up.\nThe iPod’s battery had died in less than 7 hours.\nI’m assuming this is linked to the new multitasking system in iOS 4, where apps that aren’t running are put into memory so you can open them where you left off. Sure enough, I had like 25 apps in the multitasking tray, which I quickly cleared out after plugging in the depleted iPod.\nSo, iOS users out there: what experience have you had with multitasking and battery life? Is there an easy way to “officially” quit an application rather than send it to the tray? What should I do avoid something like this again?"
  },
  {
    "objectID": "blog/2011/02/03/in-tahrir-square/index.html",
    "href": "blog/2011/02/03/in-tahrir-square/index.html",
    "title": "In Tahrir Square",
    "section": "",
    "text": "My awesome wife just wrote up a fantastic poem dedicated to the #Jan25 Tahrir protestors. This past week has been riveting and emotional for us, even though we live far from Egypt now. Cairo was our home for two years—we drank from the Nile (شربنا من النيل) and feel part of it. This horrific carnage is absolutely sickening.\nTa7ya Masr!\n\n\nIn Tahrir Square\nIn Tahrir Square the people fight\nFor freedom, truth, and human rights\nWe all should have; While here I sit\nAnd watch the robins peck and flit\nNot knowing what went on last night.\nHow many dead? We’ll never know.\nThe guns, the knives, the bombs aglow\nSeek and will seek to thwart hearts knit\nIn Tahrir square.\nTake up their quarrel with the foe:\nTo you from failing hands they fling\nThe torch; be yours to hold it high.\nIf ye break faith with those who die\nThey shall not sleep, when silence rings\nIn Tahrir Square.\nNancy Heiss\nFebruary 3, 2011\n(With thanks to John McCrae)"
  },
  {
    "objectID": "blog/2011/06/24/using-arabic-in-indesign-cs5-without-indesign-me/index.html",
    "href": "blog/2011/06/24/using-arabic-in-indesign-cs5-without-indesign-me/index.html",
    "title": "Using Arabic in InDesign CS5 without InDesign ME",
    "section": "",
    "text": "Nearly four years ago, I posted a workaround for InDesign CS3’s lack of support for RTL support. The only way to get proper Arabic or Hebrew text back then was to type backwards—something trivial (and scriptable) for Hebrew, but far more complicated for Arabic, with all its different letter forms that change depending on their position in the word. Rather than type backwards, you had to manually insert glyphs from the glyphs panel backwards. While this was extraordinarily tedious, it worked for times when you only needed to deal with a few words—maybe a short sentence.\nFor my job I’ve been using InDesign CS3 ME with Tasmeem and have fallen in love with the super advanced typographic tools that it provides. But, I’m not planning on keeping that job forever (I’m graduating in less than a year!), and I don’t want to go back to hunting for glyphs. I’ve been spoiled :)\nI heard rumors that CS4 and CS5 had a mysterious RTL editing mode that was only accessible through scripting, but I was in Egypt for both launches and wasn’t able to get the educational upgrade. I finally upgraded to CS5 a couple days ago and immediately got to work figuring out this rumored Arabic mode.\nStarting with CS4, Adobe began including a new “world ready” paragraph composer, which provides many of the same typographic controls offered by Winsoft’s Middle Eastern editions. However, CS4 shipped before Adobe could polish off any of the controls. The internet world hoped that there’d be some new world ready panel for CS5, but to no avail. The world ready composer was included (and improved?) with CS5, but is still mostly inaccessible.\nUnless you do a cool trick :)\nYou can programmatically create paragraph styles that use the world ready composer, allowing you to typeset Arabic, Hebrew, Thai, Devangari-like languages, and a ton of other more complex scripts. Here’s how you can enable native Arabic typography inside CS5 (courtesy of Thomas Phinney):\n\nDownload these scripts by Thomas Phinney and Peter Kahrel and install them in your script folder.\nRun the “r2l Paragraph Style Arabic” script from the Scripts panel. A new paragraph style named “RTL Arabic” should appear in your Paragraph Styles panel.\nCheck the style settings and verify that the “Adobe World-Ready Paragraph Composer” is being used for the style.\n\nStart using Arabic text!\nBonus: To use the composer in other documents you can either run the script again or just copy the style. The composer is activated by the style. Theoretically you could save the Arabic style to a new template and never have to run the script again.\n\nThis is definitely not a full replacement for Winsoft’s CS5 InDesign ME, since it doesn’t give you any graphical control over kashidas, digits, diacritic positions, or any of the other more detailed options available in the world ready composer. You can edit the script, change the options around, and create a new style, but you’d need to know the API for the world ready composer. Phinney’s script, for example, sets the default digits for the style as Arabic digits with ps.digitsType = DigitsTypeOptions.arabicDigits;. If you knew the option for Farsi-style numbers, you could edit the script. Unfortunately the whole composer is completely undocumented and unsupported. This site includes many of the hidden options—you’d just need to guess about how to use them in the script. Maybe Adobe will finally make all these options accessible in CS6?\nThere are a couple options for more graphical control over the hidden settings, but they cost money. idRTL created a plugin that provides a useful panel to edit all the hidden Middle Eastern settings. It’s $50 and says it’s made for CS4, although I’m assuming it’ll work with CS5 too. Word Tools creates a similar panel, costs $100, and supports CS5.\nSo, even though activating the world ready composer can be a little tricky, it’s a fantastic little trick that lets you use real RTL text without tedious backward typing."
  },
  {
    "objectID": "blog/2012/04/17/install-r-rstudio-r-commander-windows-osx/index.html",
    "href": "blog/2012/04/17/install-r-rstudio-r-commander-windows-osx/index.html",
    "title": "Install R, RStudio, and R Commander in Windows and OS X",
    "section": "",
    "text": "R is an incredibly powerful open source program for statistics and graphics. It can run on pretty much any computer and has a very active and friendly support community online. Graphics created by R are extremely extensible and are used in high level publications like the New York Times (as explained by this former NYT infographic designer).\nRStudio is an integrated development environment (IDE) for R. It’s basically a nice front-end for R, giving you a console, a scripting window, a graphics window, and an R workspace, among other options.\nR Commander is a basic graphical user interface (GUI) for R. It provides a series of menus that allow you to run lots of statistic tests and create graphics without typing a line of code. More advanced features of R aren’t accessible through R Commander, but you can use it for the majority of your statistics. (Lots of people (like me) use R Commander as a crutch for a few months before they get the hang of the R language. As intimidating as it might be to constantly type stuff at the console, it really is a lot faster.)\nHowever, as is the case with lots of free and open source software, it can be a little tricky to install all of these different programs and get them to work nicely together. The simple instructions below explain how to get everything working right.\n\nInstall R, RStudio, and R Commander in Windows\n\nDownload R from http://cran.us.r-project.org/ (click on “Download R for Windows” &gt; “base” &gt; “Download R 2.x.x for Windows”)\nInstall R. Leave all default settings in the installation options.\nDownload RStudio from http://rstudio.org/download/desktop and install it. Leave all default settings in the installation options.\nOpen RStudio.\nGo to the “Packages” tab and click on “Install Packages”. The first time you’ll do this you’ll be prompted to choose a CRAN mirror. R will download all necessary files from the server you select here. Choose the location closest to you (probably “USA CA 1” or “USA CA 2”, which are housed at UC Berkeley and UCLA, respectively).\n\nStart typing “Rcmdr” until you see it appear in a list. Select the first option (or finish typing Rcmdr), ensure that “Install dependencies” is checked, and click “Install”.\n\nWait while all the parts of the R Commander package are installed.\nIf you get permission errors while installing packages, close R Studio and reopen it with administrator privileges.\n\n\n\n\nInstall R, RStudio, and R Commander in Mac OS X\n\nDownload R from http://cran.us.r-project.org/ (click on “Download R for Mac OS X” &gt; “R-2.x.x.pkg (latest version)”)\nInstall R.\nDownload RStudio from http://rstudio.org/download/desktop.\nInstall RStudio by dragging the application icon to your Applications folder.\nDownload Tcl/Tk from http://cran.r-project.org/bin/macosx/tools/ (click on tcltk-8.x.x-x11.dmg; OS X needs this to run R Commander.)\nInstall Tcl/Tk.\nGo to your Applications folder and find a folder named Utilities. Verify that you have a program named “X11” there. If not, go to http://xquartz.macosforge.org/ and download and install the latest version of XQuartz.\n\nOpen RStudio.\nGo to the “Packages” tab and click on “Install Packages”. The first time you’ll do this you’ll be prompted to choose a CRAN mirror. R will download all necessary files from the server you select here. Choose the location closest to you (probably “USA CA 1” or “USA CA 2”, which are housed at UC Berkeley and UCLA, respectively).\n\nStart typing “Rcmdr” until you see it appear in a list. Select the first option (or finish typing Rcmdr), ensure that “Install dependencies” is checked, and click “Install”.\n\nWait while all the parts of the R Commander package are installed.\n\n\n\nOpen R Commander in Windows and OS X\nOnce you’ve installed R Commander, you won’t have to go through all those steps again! Running R Commander from this point on is simple—follow the instructions below.\nIf you decide to stop using R Commander and just stick with R, all you ever need to do is open RStudio—even simpler!\n\nOpen R Studio\nIn the console, type windows() if using Windows, quartz() if using Mac OS X. (This tells R Commander to output all graphs to a new window). If you don’t do this, R Commander graphs will be output to the graphics window in RStudio.\nGo to the “Packages” tab, scroll down to “Rcmdr,” and check the box to load the plugin. (Alternatively, type library(Rcmdr) at the console.)"
  },
  {
    "objectID": "blog/2013/03/15/side-by-side-page-numbers-indesign/index.html",
    "href": "blog/2013/03/15/side-by-side-page-numbers-indesign/index.html",
    "title": "True side-by-side page numbers in InDesign",
    "section": "",
    "text": "The books I make for the Middle East Texts Initiative contain side-by-side English-Arabic translations of old Arabic, Hebrew, and Latin texts. InDesign can generally handle the side-by-side parallel stories and text frames, but it cannot properly number the pages. After all the front matter and introductory text, the English translation starts on a verso page (left) with page 1, followed by a page of Arabic on the recto (right), also on page 1. The next verso is page 2.\nInDesign doesn’t include a way to insert automatic spread numbers instead of page numbers, which means there’s no easy way to have automatic parallel, same-numbered pages. For years users have come up with kludgy solutions, like:\n\nMaking a list of page numbers in Excel and placing that list in a threaded text box on every master page (kind of like this)\nPlacing a document of empty lines to take advantage of InDesign’s automatic paragraph numbering\nMaking a special text variable for each page (automated version)\n\nThese methods all work, but with one big caveat—they don’t deal with any of the actual page numbers. If you try to build an automatic table of contents after using any of these methods, you’ll get page numbers, not the spread numbers. Similarly, the exported PDF will show page numbers instead of spread numbers.\nThe only real way to get true side-by-side numbering is to create new sections for each page. Right click on one of your pages, start a section at page 1. Right click on the next page, start a section at page 1 with some prefix (so there aren’t duplicate pages). Right click on the next page, start a section at page 2. And so on for the all the side-by-side pages.\nCrazy tedious. But totally automatable with a script. Find said script at GitHub."
  },
  {
    "objectID": "blog/2016/02/10/libreoffice-base-sqlite-odbc-osx/index.html",
    "href": "blog/2016/02/10/libreoffice-base-sqlite-odbc-osx/index.html",
    "title": "Use LibreOffice Base as a GUI for an SQLite database in OS X",
    "section": "",
    "text": "As I conduct interviews for my dissertation research, I’ve been trying to figure out an open source database for storing interview notes and keeping track of the people and organizations I’m talking to. My ideal requirements are simple:\n\nThe format should be open source.\nThe format should be portable and not require an underlying server (sorry MongoDB and MySQL)—this way I can save the file in an encrypted file container for IRB data protection purposes.\nThe format should be easy to access with multiple languages (especially R and Python), ideally without external dependencies like Java.\nThe format should be compatible with some sort of Microsoft Access-esque form GUI to allow for easy data insertion.\n\nHowever, finding the right combination of programs and formats has been slightly more difficult. SQLite is the best format, given that it’s the most widely deployed and used database engine and is open source and has native support in both R1 and Python. The only thing it lacks is a nice form-based GUI front end.\n1 Technically RSQLite is a separate package, but it’s a dependency of dplyr, which is as important as base R in my book.There are plenty of SQLite viewers, but I haven’t found any that let you create Access-like forms. I could use Python to program my own GUI (or even get fancy and learn Swift and make a native Cocoa app), but that seems like an excessive amount of work.\nLibreOffice Base has excellent support for database-backed forms, but under the hood, LibreOffice uses the Java-based HSQLDB, which does not have native R and Python support and requires older Java runtime environments."
  },
  {
    "objectID": "blog/2016/02/10/libreoffice-base-sqlite-odbc-osx/index.html#the-problem",
    "href": "blog/2016/02/10/libreoffice-base-sqlite-odbc-osx/index.html#the-problem",
    "title": "Use LibreOffice Base as a GUI for an SQLite database in OS X",
    "section": "",
    "text": "As I conduct interviews for my dissertation research, I’ve been trying to figure out an open source database for storing interview notes and keeping track of the people and organizations I’m talking to. My ideal requirements are simple:\n\nThe format should be open source.\nThe format should be portable and not require an underlying server (sorry MongoDB and MySQL)—this way I can save the file in an encrypted file container for IRB data protection purposes.\nThe format should be easy to access with multiple languages (especially R and Python), ideally without external dependencies like Java.\nThe format should be compatible with some sort of Microsoft Access-esque form GUI to allow for easy data insertion.\n\nHowever, finding the right combination of programs and formats has been slightly more difficult. SQLite is the best format, given that it’s the most widely deployed and used database engine and is open source and has native support in both R1 and Python. The only thing it lacks is a nice form-based GUI front end.\n1 Technically RSQLite is a separate package, but it’s a dependency of dplyr, which is as important as base R in my book.There are plenty of SQLite viewers, but I haven’t found any that let you create Access-like forms. I could use Python to program my own GUI (or even get fancy and learn Swift and make a native Cocoa app), but that seems like an excessive amount of work.\nLibreOffice Base has excellent support for database-backed forms, but under the hood, LibreOffice uses the Java-based HSQLDB, which does not have native R and Python support and requires older Java runtime environments."
  },
  {
    "objectID": "blog/2016/02/10/libreoffice-base-sqlite-odbc-osx/index.html#the-solution",
    "href": "blog/2016/02/10/libreoffice-base-sqlite-odbc-osx/index.html#the-solution",
    "title": "Use LibreOffice Base as a GUI for an SQLite database in OS X",
    "section": "The solution",
    "text": "The solution\nFortunately there’s a way to use an SQLite database as the backend for LibreOffice Base using an ODBC driver, giving the best of both worlds: an open, universal, Java-free database behind a customizable form-based GUI.\nThere are official instructions for doing this on Linux and Windows, but there’s nothing about doing it in OS X. So here’s that missing tutorial.\n\nSQLite is already installed on OS X. Create a new SQLite database using sqlite3 in Terminal (or even easier, use a GUI program). Add some tables to it, or don’t—it doesn’t matter. You just some sort of database file.\nDownload the SQLite ODBC driver for OS X. The page includes a link to a precompiled version (currently it says “Steve Palm kindly provided a build of version 0.9993 for MacOSX 10.{6,7,8,9,10,11} on Intel as installer package (sqliteodbc-0.9993.dmg)”). Install the driver by opening sqliteodbc-0.9993.pkg.\nDownload an ODBC manager app. Prior to OS X 10.5, Apple included one of these, but for whatever reason they stopped with Snow Leopard. There are two that work equally well: ODBC Manager and iODBC Administrator.\nOpen the ODBC manager/administrator app. Add a new driver using these settings: \nAdd a new User DSN (Data Source Name). Create a new key named “database” and use the full absolute path to the SQLite database file as the value: \nQuit the ODBC manager. The SQLite file is now accessible in any program that uses ODBC.\nOpen LibreOffice and create a new Base database. In the wizard, select “Connect to an existing database” and choose “ODBC”: \nClick next to select which ODBC database to load. If you click on “Browse…”, you should see the name of the SQLite database you set up as a DSN earlier.\nClick on “Finish.” LibreOffice will prompt you to save an .odf database. This is fine—it’s not actually saving the database, just the accompanying form data.2\nCreate new tables and forms using LibreOffice: \n\n2 I think… I haven’t actually checked or tested this.Any time you save, all edits will occur on the SQLite file. Create a table, insert some records, and open the SQLite file in a GUI program to see all the changes. Magic!\nEven though there are 10 steps, it’s not too difficult. tl;dr version: (1) install an SQLite ODBC driver, (2) install an ODBC manager, (3) use the manager to configure the SQLite ODBC driver and connect to an existing SQLite database, and (4) connect to the SQLite database through ODBC with LibreOffice Base.\nPerfect!"
  },
  {
    "objectID": "blog/2016/04/03/drone-sightings-in-the-us-visualized/index.html",
    "href": "blog/2016/04/03/drone-sightings-in-the-us-visualized/index.html",
    "title": "Drone sightings in the US, visualized",
    "section": "",
    "text": "For my first entry into (hrbrmstr?)’s new weekly data visualization challenge, I made two plots related to the dataset of unmanned aircraft (UAS) sightings. The R code for these plots is on GitHub.\nFirst, I was interested in the type of drones being spotted. When I think of drones, I typically think of the ones the CIA and Air Force have flying over Yemen, Somalia, Afghanistan, and Pakistan shooting at suspected ISIS and Al-Qaeda members. To see if that’s the case with these UAS sightings, I mapped out all US-based Air Force bases and all drone sightings, assuming that any military drone sightings would happen near bases.\n\n\n\nUAS sightings and Air Force bases\n\n\nWhile some sightings do occur near bases (like in Eastern Washington, Eastern Colorado, and Nebraska, for example), most don’t. In fact, in Texas, there are almost no UAS sightings near Air Force bases. Thus, even though there are documented cases of military drones being used in the US, these sightings are most definitely not military drones (unless they’re all drones coming from unmapped CIA bases). They’re quadrocopters and other hobbyist drones.\nI noticed that in some cases (like Salt Lake City, Las Vegas, and Phoenix), almost all sightings were clustered around Air Force bases. However, this is not because the military is spying on everyone in the west, but because that’s where everyone lives—very few hobbyist drone operators live in the Utah, Nevada, or Arizona deserts. Drone sightings (both the number of sightings and their location) might just be a function of state population.\nFor my second plot, I looked at the relationship between drone sightings and state population to see if there are states that see more drones than normal. The results were surprising.\n\n\n\nDrone sightings per capita\n\n\nWhile more populous states like New York, California, Florida, and Texas predictably see more drones (since there are likely more hobbyists), Washington, DC by far sees the most drone activity per capita. Maybe everyone there really is just trying to spy on Congress and the President, even though drones are ostensibly banned in DC."
  },
  {
    "objectID": "blog/2016/12/08/save-base-graphics-as-pseudo-objects-in-r/index.html",
    "href": "blog/2016/12/08/save-base-graphics-as-pseudo-objects-in-r/index.html",
    "title": "Save base graphics as pseudo-objects in R",
    "section": "",
    "text": "tl;dr: Use pryr::%&lt;a-% to save a series of R commands as a kind of macro you can call repeatedly."
  },
  {
    "objectID": "blog/2016/12/08/save-base-graphics-as-pseudo-objects-in-r/index.html#saving-grid-based-plots-to-objects",
    "href": "blog/2016/12/08/save-base-graphics-as-pseudo-objects-in-r/index.html#saving-grid-based-plots-to-objects",
    "title": "Save base graphics as pseudo-objects in R",
    "section": "Saving grid-based plots to objects",
    "text": "Saving grid-based plots to objects\nOne nice thing about ggplot (and grid graphics in general) is that you can save plots as objects and use them later in other functions like gridExtra::grid.arrange():\nlibrary(tidyverse)\nlibrary(gridExtra)\n\ndf &lt;- data_frame(x = 1:100, y=rnorm(100),\n                 z=sample(LETTERS[1:3], 100, replace=TRUE))\n\n# Scatterplot\np1 &lt;- ggplot(df, aes(x=x, y=y, color=z)) + geom_point()\n\n# Distribution\np2 &lt;- ggplot(df, aes(x=y, fill=z)) + geom_density(alpha=0.5)\n\n# Combine plots\n# arrangeGrob() is basically grid.arrange(), but allows you to save as an object\np.both &lt;- arrangeGrob(p1, p2)\ngrid::grid.draw(p.both)\n\nThis is particularly useful when saving plots with ggsave; you can simultaneously make PDF and PNG versions of your plots for use in LaTeX (PDF) or Word, PowerPoint, or HTML (PNG).\nggsave(p.both, filename=\"blah.pdf\")\nggsave(p.both, filename=\"blah.png\")"
  },
  {
    "objectID": "blog/2016/12/08/save-base-graphics-as-pseudo-objects-in-r/index.html#saving-base-graphics-based-plots-to-objects",
    "href": "blog/2016/12/08/save-base-graphics-as-pseudo-objects-in-r/index.html#saving-base-graphics-based-plots-to-objects",
    "title": "Save base graphics as pseudo-objects in R",
    "section": "Saving base graphics-based plots to objects",
    "text": "Saving base graphics-based plots to objects\nHowever, saving base R graphics to objects for later reuse is a little trickier, since plots are built line-by-line into specific devices. One approach is to make plots on a null device, record the plot, and then display it later:\n# Save plot to an object using a null PDF device\n# http://stackoverflow.com/a/14742001/120898\npdf(NULL)\ndev.control(displaylist=\"enable\")\nplot(df$x, df$y)\ntext(40, 0, \"Random\")\ntext(60, 2, \"Text\")\nlines(stats::lowess(df$x, df$y))\np1.base &lt;- recordPlot()\ninvisible(dev.off())\n\n# Display the saved plot\ngrid::grid.newpage()\np1.base\n\nOne advantage of this is that you can then reuse the plot object to simultaneously save a PDF and a PNG without recreating the plot:\npdf(\"blah_base.pdf\")\np1.base\ndev.off()\n\npng(\"blah_base.png\")\np1.base\ndev.off()\nThis is particularly helpful if you want to use nicer fonts or higher resolutions with Cairo:\ncairo_pdf(\"blah_base.pdf\",\n          width=8, height=3.5, family=\"Comic Sans\")\np1.base\ndev.off()\n\npng(\"blah_base.png\", \n    width=8, height=3.5, family=\"Comic Sans\", \n    bg=\"white\", units=\"in\", res=300, type=\"cairo\")\np1.base\ndev.off()\nThere are situations, though, where recorded base graphics plot are clunky and don’t work well, since the plots have device information embedded in them. Rearranging saved plots with par(mfrow(), layout() or other multipanel techniques can be tricky. Saved plots that make heavy modifications to par() (like Sankey diagrams created from the riverplot package) can also be unwieldy when laying out with multiple base graphics plots. It seems that the best way to use multipanel techniques with base R is to use the actual plot commands rather than inserting recorded plots. Which means that if you want to simultaneously output a PNG and a PDF, you have to repeat the code twice, which is awful.\n\npdf(\"blah.pdf\")\nsplit.screen(c(1, 2))\n\nscreen(1)\nplot(df$x, df$y)\ntext(40, 0, \"Random\")\ntext(60, 2, \"Text\")\nlines(stats::lowess(df$x, df$y))\n\nscreen(2)\nplot(density(df$y))\n\nclose.screen(all=TRUE) \ndev.off()\n\n# Once more, with feeling\npng(\"blah.png\")\nsplit.screen(c(1, 2))\n\nscreen(1)\nplot(df$x, df$y)\ntext(40, 0, \"Random\")\ntext(60, 2, \"Text\")\nlines(stats::lowess(df$x, df$y))\n\nscreen(2)\nplot(density(df$y))\n\nclose.screen(all=TRUE) \ndev.off()"
  },
  {
    "objectID": "blog/2016/12/08/save-base-graphics-as-pseudo-objects-in-r/index.html#a-solution-active-bindings-in-pryr",
    "href": "blog/2016/12/08/save-base-graphics-as-pseudo-objects-in-r/index.html#a-solution-active-bindings-in-pryr",
    "title": "Save base graphics as pseudo-objects in R",
    "section": "A solution: active bindings in pryr",
    "text": "A solution: active bindings in pryr\nInstead of recording a plot and saving all its device-specific settings, you can use the %&lt;a-% function in Hadley Wickham’s pryr package to essentially save a chunk of R code as an object that can then be re-run later, almost like a macro.\nlibrary(pryr)\n\np1.pryr %&lt;a-% {\n  plot(df$x, df$y)\n  text(40, 0, \"Random\")\n  text(60, 2, \"Text\")\n  lines(stats::lowess(df$x, df$y))\n}\n\np2.pryr %&lt;a-% {\n  plot(density(df$y))\n}\nEvery time p1.pryr is called from now on, R will run everything assinged to it in {}. The plot itself is not saved as an object—rather, the code that generates the plot is saved as an object.\np1.pryr\n\np2.pryr\n\nIncluding these plots in a multipanel layout is trivial:\nsplit.screen(c(1, 2))\n\nscreen(1)\np1.pryr\n\nscreen(2)\np2.pryr\n\nclose.screen(all=TRUE) \n\nSaving these plots as PDFs and PNGs is the same as saving recorded plots:\npdf(\"blah_base_pryr.pdf\")\np1.pryr\ndev.off()\n\npng(\"blah_base_pryr.png\")\np1.pryr\ndev.off()\nBut even better is saving multipanel plots as PDFs and PNGs. Save the whole multipanel plot code as an actively bound object and then wrap it in pdf() or png():\np.both.pryr %&lt;a-% {\n  split.screen(c(1, 2))\n\n  screen(1)\n  p1.pryr\n  \n  screen(2)\n  p2.pryr\n  \n  close.screen(all=TRUE)\n}\n\npdf(\"blah_base_both_pryr.pdf\")\np.both.pryr\ndev.off()\n\npng(\"blah_base_both_pryr.png\")\np.both.pryr\ndev.off()\nAgain, actively bound objects are not the objects themselves, but an expression that gets run every time the object is accessed. It is far easier to deal with saved base graphics as code instead of as recorded plots, since there is no inherent device information to contend with.\nIn the end, it’s easiest to save grid-based graphics to objects, since they’re designed to do that. But you can fake it with base graphics using pryr.\n\nOne quick real-world use case. As mentioned, Sankey diagrams from riverplot package override all graphical parameters and can’t be easily plotted in multipanel layouts if you record them with recordPlot(). With pryr, though, it’s easy.\nlibrary(riverplot)\n\nx &lt;- riverplot.example()\n\nriverplot.saved %&lt;a-% {\n  plot(x)\n  text(3, 0, \"Some text\\n\", adj=c(0.5, -0.5), font=2)\n  points(1:5, rep(2, 5), pch=19)\n}\n\np.both.river %&lt;a-% {\n  split.screen(c(1, 2))\n\n  screen(1)\n  riverplot.saved\n  \n  screen(2)\n  plot(1:10)\n  \n  close.screen(all=TRUE)\n}\n\np.both.river"
  },
  {
    "objectID": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html",
    "href": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html",
    "title": "Exploring Minard’s 1812 plot with ggplot2",
    "section": "",
    "text": "For whatever reason, I decided to start reading Tolstoy’s War and Peace (via Audible) the week I had to turn in my dissertation. I still have a dozen or so hours to go, but the book has been incredible. I had no idea what it was about going into it, and was delighted to find that the “war” parts of the book deal with the Napolonic wars—both his 1804–1805 campaign in the War of the Third Coalition (like the Battle of Austerlitz), and his 1812 campaign to invade Russia, from whence we get Tchaikovsky’s 1812 Overture. I knew nothing about these wars and Tolstoy’s descriptions are incredible and gripping.\nIt’s been especially exciting because I’m preparing a course on data visualization this fall and had been looking forward to using Charles Minard’s famous plot about Napoleon’s 1812 winter retreat from Moscow, where the Grande Armée dropped from 422,000 to 10,000 troops.\nEdward Tufte has said that Minard’s plot “may well be the best statistical graphic ever drawn” because it manages to pack a ton of information into one dense figure. The plot contains six variables, each mapped to a different aesthetic:\nDesigners and statisticians have recreated this plot dozens of times—there are galleries of attempts all around the internet. It’s even included in Hadley Wickham’s original article introducting ggplot2. Creating the plot in R is fairly trivial and requires minimal code, thanks to ggplot’s clear grammar for data graphics.\nIn the seven years since Hadley’s original article, ggplot and R have matured significantly (thanks, in large part, due to the tidyverse). With these improvements, we can add fancier elements to the basic ggplot Minard plot and play around with some fun R features."
  },
  {
    "objectID": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#getting-started",
    "href": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#getting-started",
    "title": "Exploring Minard’s 1812 plot with ggplot2",
    "section": "Getting started",
    "text": "Getting started\nFirst, we load the necessary libraries and data (data available at Michael Friendly’s Minard gallery or in the GitHub repository for this notebook.)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggmap)\nlibrary(ggrepel)\nlibrary(gridExtra)\nlibrary(pander)\n\ncities &lt;- read.table(\"input/minard/cities.txt\",\n                     header = TRUE, stringsAsFactors = FALSE)\n\ntroops &lt;- read.table(\"input/minard/troops.txt\",\n                     header = TRUE, stringsAsFactors = FALSE)\n\ntemps &lt;- read.table(\"input/minard/temps.txt\",\n                    header = TRUE, stringsAsFactors = FALSE) %&gt;%\n  mutate(date = dmy(date))  # Convert string to actual date"
  },
  {
    "objectID": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#geography",
    "href": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#geography",
    "title": "Exploring Minard’s 1812 plot with ggplot2",
    "section": "Geography",
    "text": "Geography\nThe troops data includes five variables about troop movement: location, number of survivors, direction (advancing or retreating) and group (since Napoleon had generals commanding different elements of the army).\ntroops %&gt;% head() %&gt;% pandoc.table()\n\n\n\nlong\nlat\nsurvivors\ndirection\ngroup\n\n\n\n\n24\n54.9\n340000\nA\n1\n\n\n24.5\n55\n340000\nA\n1\n\n\n25.5\n54.5\n340000\nA\n1\n\n\n26\n54.7\n320000\nA\n1\n\n\n27\n54.8\n300000\nA\n1\n\n\n28\n54.9\n280000\nA\n1\n\n\n\nEach of these variables maps well into ggplot’s aesthetic-based paradigm. If we include just geographic and group information (so there are separate lines for the different divisions), we get a basic skeleton of the original plot:\nggplot(troops, aes(x = long, y = lat, group = group)) +\n  geom_path()\n\n\n\nTroops, line\n\n\nWe can map data to other aesthetics, like color and size:\nggplot(troops, aes(x = long, y = lat, group = group,\n                   color = direction, size = survivors)) +\n  geom_path()\n\n\n\nTroops, line with color and thickness\n\n\nThe individual segments of the path don’t fit together very well and leave big gaps. We can fix that by adding a rounded line ending to each segment.\nggplot(troops, aes(x = long, y = lat, group = group,\n                   color = direction, size = survivors)) +\n  geom_path(lineend = \"round\")\n\n\n\nTroops, line with color and thickness and rounded ends\n\n\nThe size of the path hides the drama of the plot. Napoleon started the 1812 campaign with 422,000 troops and returned with only 10,000. ggplot automatically makes discrete categories for the survivors variable, resulting in three not-very-granular categories. We can adjust the scale to allow for more categories, thus showing more variation in size and highlighting the devasation of the army:\nggplot(troops, aes(x = long, y = lat, group = group,\n                   color = direction, size = survivors)) +\n  geom_path(lineend = \"round\") +\n  scale_size(range = c(0.5, 15))\n\n\n\nTroops, big lines\n\n\nFinally, we can remove the labels, legends, and change the colors to match the shade of brown from Minard’s original plot (which I figured out with Photoshop’s eyedropper tool).\nggplot(troops, aes(x = long, y = lat, group = group,\n                   color = direction, size = survivors)) +\n  geom_path(lineend = \"round\") +\n  scale_size(range = c(0.5, 15)) +\n  scale_colour_manual(values = c(\"#DFC17E\", \"#252523\")) +\n  labs(x = NULL, y = NULL) +\n  guides(color = FALSE, size = FALSE)\n\n\n\nTroops, correct colors\n\n\nOne of the amazing things about this plot is that it is actually a map—the x and y axes show the longitude and latitude of the troops. This means we can overlay geographic details, like cities. The cities in the original data can easily be added with geom_point() and geom_text(). We use vjust in geom_text() to move the labels down away from their points.\n(Now that we’re adding graphical layers from different sources, it’s good to move the aesthetics defined in aes() to the layers where they’re actually used.)\nggplot() +\n  geom_path(data = troops, aes(x = long, y = lat, group = group,\n                               color = direction, size = survivors),\n            lineend = \"round\") +\n  geom_point(data = cities, aes(x = long, y = lat)) +\n  geom_text(data = cities, aes(x = long, y = lat, label = city), vjust = 1.5) +\n  scale_size(range = c(0.5, 15)) +\n  scale_colour_manual(values = c(\"#DFC17E\", \"#252523\")) +\n  labs(x = NULL, y = NULL) +\n  guides(color = FALSE, size = FALSE)\n\n\n\nTroops, with city names\n\n\nAlternatively, we can use geom_text_repel from the ggrepel package to automatically move the labels away from points and to ensure none of the labels overlap. We can also adjust the labels so they’re easier to read (using Open Sans).\nggplot() +\n  geom_path(data = troops, aes(x = long, y = lat, group = group,\n                               color = direction, size = survivors),\n            lineend = \"round\") +\n  geom_point(data = cities, aes(x = long, y = lat),\n             color = \"#DC5B44\") +\n  geom_text_repel(data = cities, aes(x = long, y = lat, label = city),\n                  color = \"#DC5B44\", family = \"Open Sans Condensed Bold\") +\n  scale_size(range = c(0.5, 15)) +\n  scale_colour_manual(values = c(\"#DFC17E\", \"#252523\")) +\n  labs(x = NULL, y = NULL) +\n  guides(color = FALSE, size = FALSE)\n\n\n\nTroops, with nicer city names\n\n\nAlso, because this is a map, we can overlay it on other maps. It’s fairly easy to get map data from Google or from the OpenStreetMap project (through the Stamen project) with the ggmap package. There are some weird quirks you have to deal with, though:\n\nYou can supply ggmap with a four-number bounding box to get a specific region of a map. OpenStreetMap makes this really easy to do. Vavigate to the area you want to map at openstreetmap.org and click on “Export” in the top toolbar. The left sidebar should show the latitudes and longitudes for the current view. If you click on “Manually select a different area,” you can create your own bounding box.\n\n\n\nOpenStreetMap screenshot\n\n\nOpenStreetMap is the only data source ggmap uses that can use exact bounding boxes. When you use Google as a source, Google will find the center of the bounding box and estimate the region you want, and it’s often wrong and will get too much of the map (or too little). OpenStreetMap is thus better for getting exact areas.\nBUUUUUT OpenStreetMap no longer allows ggmap to access its API, which stinks. Fortunatley, the Stamen project does work with ggmap, and it’s based on OpenStreetMap data, so all is well(ish).\n\nWith those caveats, we can get map tiles from Stamen with get_stamenmap():\nmarch.1812.europe &lt;- c(left = -13.10, bottom = 35.75, right = 41.04, top = 61.86)\n\n# \"zoom\" ranges from 3 (continent) to 21 (building)\n# \"where\" is a path to a folder where the downloaded tiles are cached\nmarch.1812.europe.map &lt;- get_stamenmap(bbox = march.1812.europe, zoom = 5,\n                                       maptype = \"terrain\", where = \"cache\")\nOnce we have the tiles, the ggmap() function plots them nicely:\nggmap(march.1812.europe.map)\n\n\n\nMap of Europe\n\n\nWe can even use Stamen’s fancier map types, like watercolor:\nmarch.1812.europe.map.wc &lt;- get_stamenmap(bbox = march.1812.europe, zoom = 5,\n                                          maptype = \"watercolor\", where = \"cache\")\nggmap(march.1812.europe.map.wc)\n\n\n\nWatercolor map of Europe\n\n\nNow we can overlay the Minard plot to see where the march took place in relation to the rest of Europe:\nggmap(march.1812.europe.map.wc) +\n  geom_path(data = troops, aes(x = long, y = lat, group = group,\n                               color = direction, size = survivors),\n            lineend = \"round\") +\n  scale_size(range = c(0.5, 5)) +\n  scale_colour_manual(values = c(\"#DFC17E\", \"#252523\")) +\n  guides(color = FALSE, size = FALSE) +\n  theme_nothing()  # This is a special theme that comes with ggmap\n\n\n\nWatercolor map with troops\n\n\nWe can also zoom in on just northeastern Europe and add the cities back in. We’ll save this plot to an object (march.1812.plot) so we can use it later.\nmarch.1812.ne.europe &lt;- c(left = 23.5, bottom = 53.4, right = 38.1, top = 56.3)\n\nmarch.1812.ne.europe.map &lt;- get_stamenmap(bbox = march.1812.ne.europe, zoom = 8,\n                                          maptype = \"terrain-background\", where = \"cache\")\n\nmarch.1812.plot &lt;- ggmap(march.1812.ne.europe.map) +\n  geom_path(data = troops, aes(x = long, y = lat, group = group,\n                               color = direction, size = survivors),\n            lineend = \"round\") +\n  geom_point(data = cities, aes(x = long, y = lat),\n             color = \"#DC5B44\") +\n  geom_text_repel(data = cities, aes(x = long, y = lat, label = city),\n                  color = \"#DC5B44\", family = \"Open Sans Condensed Bold\") +\n  scale_size(range = c(0.5, 10)) +\n  scale_colour_manual(values = c(\"#DFC17E\", \"#252523\")) +\n  guides(color = FALSE, size = FALSE) +\n  theme_nothing()\n\nmarch.1812.plot\n\n\n\nTroops with map background\n\n\nMagic!"
  },
  {
    "objectID": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#temperatures-and-time",
    "href": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#temperatures-and-time",
    "title": "Exploring Minard’s 1812 plot with ggplot2",
    "section": "Temperatures and time",
    "text": "Temperatures and time\nSo far we have four of the variables from Minard’s original plot—we’re still missing the temperatures during the retreat and the days of the retreat. Minard put this infomration in a separate plot under the map, which is fairly easy to do with gridExtra.\nFirst we have to create the panel, which is a basic line graph with longitude along the x-axis and temperature along the y-axis, with text added at each point.\nggplot(data = temps, aes(x = long, y = temp)) +\n  geom_line() +\n  geom_text(aes(label = temp), vjust = 1.5)\n\n\n\nTemperatures\n\n\nWe can create a new variable for nicer labels, combining temperature with the date. We’ll also clean up the theme, move the axis label to the right, and only include major horizontal gridlines. When we overlay the two plots, we have to make sure the x-axes align, so we need to use the same x-axis limits used in march.1812.plot. Those limits are buried inside the plot object, the parts of which can be accessed with ggplot_build():\nggplot_build(march.1812.plot)$layout$panel_ranges[[1]]$x.range\n## [1] 23.5 38.1\ntemps.nice &lt;- temps %&gt;%\n  mutate(nice.label = paste0(temp, \"°, \", month, \". \", day))\n\ntemps.1812.plot &lt;- ggplot(data = temps.nice, aes(x = long, y = temp)) +\n  geom_line() +\n  geom_label(aes(label = nice.label),\n            family = \"Open Sans Condensed Bold\", size = 2.5) +\n  labs(x = NULL, y = \"° Celsius\") +\n  scale_x_continuous(limits = ggplot_build(march.1812.plot)$layout$panel_ranges[[1]]$x.range) +\n  scale_y_continuous(position = \"right\") +\n  coord_cartesian(ylim = c(-35, 5)) +  # Add some space above/below\n  theme_bw(base_family = \"Open Sans Condensed Light\") +\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.text.x = element_blank(), axis.ticks = element_blank(),\n        panel.border = element_blank())\n\ntemps.1812.plot\n\n\n\nTemperatures, cleaner"
  },
  {
    "objectID": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#combining-the-plots",
    "href": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#combining-the-plots",
    "title": "Exploring Minard’s 1812 plot with ggplot2",
    "section": "Combining the plots",
    "text": "Combining the plots\nFinally, we use functions in gridExtra to combine the two plots. The easiest way to combine plot objects with gridExtra is to use grid.arrange(), but doing so doesn’t align the axes of the plot. For instance, look at these two example plots—they’re no longer comparable vertically because the left side of the bottom plot extends to the edge of the plot, expanding under the long axis label in the top plot:\nexample.data &lt;- data_frame(x = 1:10, y = rnorm(10))\n\nplot1 &lt;- ggplot(example.data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(y = \"This is a really\\nreally really really\\nreally tall label\")\n\nplot2 &lt;- ggplot(example.data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(y = NULL)\n\ngrid.arrange(plot1, plot2)\n\n\n\nCombined plots\n\n\nInstead of using grid.arrange, we can use gridExtra’s special version of rbind() (or cbind()) for ggplotGrob objects:\nplot.both &lt;- rbind(ggplotGrob(plot1),\n                   ggplotGrob(plot2))\n\ngrid::grid.newpage()\ngrid::grid.draw(plot.both)\n\n\n\nCombined with gtable\n\n\nNow that we can align plots correctly, we can combine the map and the temperature:\nboth.1812.plot &lt;- rbind(ggplotGrob(march.1812.plot),\n                        ggplotGrob(temps.1812.plot))\n\ngrid::grid.newpage()\ngrid::grid.draw(both.1812.plot)\n\n\n\nAll combined\n\n\nThey’re aligned, but there’s an obvious problem—the map is way too small and the temperatures are too tall. With grid.arrange it’s possible to pass a vector of relative panel heights, which would let us shrink the bottom panel. While using gtable::rbind() does let us align the two plots, it doesn’t provide an easy way to mess with panel heights. Following this StackOverflow answer, though, we can mess with the ggplot object and adjust the panels manually.\n# Identify which layout elements are panels\npanels &lt;- both.1812.plot$layout$t[grep(\"panel\", both.1812.plot$layout$name)]\n\n# Normally we can pass a vector of null units that represent relative heights.\n# For instance, `unit(c(3, 1), \"null\")` would make the top panel 3 times as\n# tall as the bottom.\n\n# But, the map here uses coord_equal() to show the correct dimensions of the\n# map, and this messes with the panel height for whatever reason. So instead,\n# we extract the original map panel height, which is really small, and then\n# make the bottom panel smaller in the same scale.\nmap.panel.height &lt;- both.1812.plot$heights[panels][1]\n\n# See, super small\nmap.panel.height\n## [1] 0.345197879241894null\n# Apply new panel heights to object\nboth.1812.plot$heights[panels] &lt;- unit(c(map.panel.height, 0.1), \"null\")\n\ngrid::grid.newpage()\ngrid::grid.draw(both.1812.plot)\n\n\n\nCombined with correct heights\n\n\nWe can follow the same process to create a backgroundless version of the map:\n# No map this time\nmarch.1812.plot.simple &lt;- ggplot() +\n  geom_path(data = troops, aes(x = long, y = lat, group = group,\n                               color = direction, size = survivors),\n            lineend = \"round\") +\n  geom_point(data = cities, aes(x = long, y = lat),\n             color = \"#DC5B44\") +\n  geom_text_repel(data = cities, aes(x = long, y = lat, label = city),\n                  color = \"#DC5B44\", family = \"Open Sans Condensed Bold\") +\n  scale_size(range = c(0.5, 10)) +\n  scale_colour_manual(values = c(\"#DFC17E\", \"#252523\")) +\n  guides(color = FALSE, size = FALSE) +\n  theme_nothing()\n\n# Change the x-axis limits to match the simple map\ntemps.1812.plot &lt;- ggplot(data = temps.nice, aes(x = long, y = temp)) +\n  geom_line() +\n  geom_label(aes(label = nice.label),\n            family = \"Open Sans Condensed Bold\", size = 2.5) +\n  labs(x = NULL, y = \"° Celsius\") +\n  scale_x_continuous(limits = ggplot_build(march.1812.plot.simple)$layout$panel_ranges[[1]]$x.range) +\n  scale_y_continuous(position = \"right\") +\n  coord_cartesian(ylim = c(-35, 5)) +  # Add some space above/below\n  theme_bw(base_family = \"Open Sans Condensed Light\") +\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.text.x = element_blank(), axis.ticks = element_blank(),\n        panel.border = element_blank())\n\n# Combine the two plots\nboth.1812.plot.simple &lt;- rbind(ggplotGrob(march.1812.plot.simple),\n                               ggplotGrob(temps.1812.plot))\n\n# Adjust panels\npanels &lt;- both.1812.plot.simple$layout$t[grep(\"panel\", both.1812.plot.simple$layout$name)]\n\n# Because this plot doesn't use coord_equal, since it's not a map, we can use\n# whatever relative numbers we want, like a 3:1 ratio\nboth.1812.plot.simple$heights[panels] &lt;- unit(c(3, 1), \"null\")\n\ngrid::grid.newpage()\ngrid::grid.draw(both.1812.plot.simple)\n\n\n\nRecreation of Minard’s plot"
  },
  {
    "objectID": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#conclusion",
    "href": "blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/index.html#conclusion",
    "title": "Exploring Minard’s 1812 plot with ggplot2",
    "section": "Conclusion",
    "text": "Conclusion\nRecreating Minard’s famous 1812 plot is relatively easy to do with ggplot. Adding fancy bells and whistles like maps and aligned panels is a little trickier, but the end result is worth the extra effort."
  },
  {
    "objectID": "blog/2018/12/26/tidytext-pos-john/index.html",
    "href": "blog/2018/12/26/tidytext-pos-john/index.html",
    "title": "Tidy text, parts of speech, and unique words in the Bible",
    "section": "",
    "text": "(See this notebook on GitHub)\nAs part of my goal to read some sort of religiously themed book every day (what I’ve read so far), I’ve been reading Eric Huntsman’s new Becoming the Beloved Disciple, a close reading of the Gospel of John from an LDS perspective.\nNear the beginning, Huntsman discusses several word frequencies that make John unique compared to the synoptic gospels of Matthew, Mark, and Luke (which all draw on the same Q source). For instance, Huntsman states that John focuses more on themes of discipleship (since the word “disciple” appears 87 times in John), and on “knowing,” “believing,” and “doing,” which appear more often in John than the other gospels.\nIn the course of teaching data visualization, I’ve dabbled in text-based analysis with R, and as a PhD student I wrote a couple of now-dormant papers that used cool digital humanities methods to analyze large corpora of text, so my curiosity was piqued. How unique is the word “disciple” in John compared to the synoptic gospels? What are the most unique verbs in John? What words are the most predictive that we’re in John?\nLet’s explore with R!\nAs I started writing this post, I also accidentally created an R package. The complete LDS scriptures are available online for free as an open source database, and I’ve downloaded that CSV file so many times for other little mini projects I’ve done, so I decided to finally just stick it all in a new package so I wouldn’t need to keep downloading the data by hand. So, behold: scriptuRs. Install it with remotes::install_github(\"andrewheiss/scriptuRs\") or devtools::install_github(\"andrewheiss/scriptuRs\"). It’ll be on CRAN once they open up for submissions again in January."
  },
  {
    "objectID": "blog/2018/12/26/tidytext-pos-john/index.html#load-packages-and-data",
    "href": "blog/2018/12/26/tidytext-pos-john/index.html#load-packages-and-data",
    "title": "Tidy text, parts of speech, and unique words in the Bible",
    "section": "Load packages and data",
    "text": "Load packages and data\nFirst, we’ll load the necessary packages and data:\nlibrary(tidyverse)  # For dplyr, ggplot2, and friends\nlibrary(scriptuRs)  # For full text of bible\nlibrary(tidytext)   # For analyzing text\nlibrary(cleanNLP)   # For fancier natural language processing\n\n# Load data\ngospels &lt;- kjv_bible() %&gt;% \n  filter(book_title %in% c(\"Matthew\", \"Mark\", \"Luke\", \"John\"))"
  },
  {
    "objectID": "blog/2018/12/26/tidytext-pos-john/index.html#part-of-speech-tagging",
    "href": "blog/2018/12/26/tidytext-pos-john/index.html#part-of-speech-tagging",
    "title": "Tidy text, parts of speech, and unique words in the Bible",
    "section": "Part-of-speech tagging",
    "text": "Part-of-speech tagging\nBecause I want to know what the most unique/common verbs are in John, we need to identify the grammatical purpose of each word. There are incredible algorithms for tagging parts of speech, such as Stanford NLP or spaCy, and the cleanNLP package provides an easy frontend for working with any of them.\nInstalling cleanNLP is trivial—it’s just a normal R package—but connecting it with external NLP algorithms is a little trickier. To install spaCy, which is a really fast tagging library, follow these steps:\n\nMake sure Python is installed.\nOpen Terminal and run this command to install spaCy:\npip install -U spacy\nRun this command to download spaCy’s English algorithms:\npython -m spacy download en\n\nThen, in RStudio, we can point R to the version of Python that has spaCy installed and tell cleanNLP to use spaCy as the NLP backend:\n# Set up NLP backend\nreticulate::use_python(\"/usr/local/bin/python3\")  # I use homebrew python3\ncnlp_init_spacy()  # Use spaCy\n# cnlp_init_udpipe()  # Or use this R-only one without external dependencies\nWith all that set up, we can now use cnlp_annotate() to do the actual tagging:\n# Determine the parts of speech of the \"text\" column and use \"verse_title\" as the id\ngospels_annotated &lt;- cnlp_annotate(gospels,\n                                   text_name = \"text\", doc_name = \"verse_title\")\nThe resulting object is a large annotation, which is a custom class for cleanNLP that‘s not very usable with tidy analysis. Fortunately there’s a data frame in the object in the $token slot:\ngospel_terms &lt;- gospels_annotated$token\nhead(gospel_terms)\n## # A tibble: 6 x 10\n##   doc_id       sid   tid token     token_with_ws lemma     upos  xpos  tid_source relation\n##   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;   \n## 1 Matthew 1…     1     1 THE       \"THE \"        the       DET   DT             2 det     \n## 2 Matthew 1…     1     2 book      \"book \"       book      NOUN  NN             0 root    \n## 3 Matthew 1…     1     3 of        \"of \"         of        ADP   IN             2 prep    \n## 4 Matthew 1…     1     4 the       \"the \"        the       DET   DT             5 det     \n## 5 Matthew 1…     1     5 generati… \"generation \" generati… NOUN  NN             3 pobj    \n## 6 Matthew 1…     1     6 of        \"of \"         of        ADP   IN             5 prep\nI think this is amazing. There are columns for each word, its lemma (an uncapitalized, unconjugated base form of the word), and the part of speech. The upos column shows the universal part of speech code (like NOUN, PROPN (for proper nouns), VERB, etc.), and the pos column shows a more detailed part of speech code, based on the Penn Treebank codes (you can get tenses, plurals, types of adverbs, etc.)."
  },
  {
    "objectID": "blog/2018/12/26/tidytext-pos-john/index.html#most-predictive-words",
    "href": "blog/2018/12/26/tidytext-pos-john/index.html#most-predictive-words",
    "title": "Tidy text, parts of speech, and unique words in the Bible",
    "section": "Most predictive words",
    "text": "Most predictive words\nBeyond just counting words and calculating tf-idf scores, we can use fancier statistical and machine learning techniques to discover which words are the most predictive of being from John. If we stumbled on a random New Testament verse, what words would tip us off that the verse might be from John? If “disciple” isn’t that unique of a word for John, what words are?\nTo do this, we’ll adapt a cool new blog post by Julia Silge and use a logistic regression model with LASSO regularization to categorize John vs. the synoptic gospels. LASSOing gives us a measure of variable importance and lets us see which words are most important for predicting if text comes from John or not.\nBefore running the model with glmnet’s cv.glmnet(), we have to restructure our tidy data into a sparse matrix. Following Julia’s approach, we’ll also split our data into a training set and a test set:\nlibrary(rsample)\n\n# Make this random split reproducible\nset.seed(1234)\n\nverses_split &lt;- gospels %&gt;% \n  select(id = verse_title) %&gt;% \n  initial_split(prop = 3/4)\n\ntrain_data &lt;- training(verses_split)\ntest_data &lt;- testing(verses_split)\nNow we can make a spare matrix based on the training set:\nsparse_words &lt;- gospel_terms %&gt;%\n  count(doc_id, lemma) %&gt;%\n  inner_join(train_data, by = c(\"doc_id\" = \"id\")) %&gt;%\n  cast_sparse(doc_id, lemma, n)\n\ndim(sparse_words)\n## [1] 2835 2590\nWe have 2,835 rows and 2,590 columns to work with. Phew.\nWe need an outcome variable here, too, or a binary variable indicating if the verse is in John or one of the synoptic gospels.\nverses_books &lt;- tibble(verse_title = rownames(sparse_words)) %&gt;% \n  left_join(gospels %&gt;% select(verse_title, book_title), by = \"verse_title\") %&gt;% \n  mutate(book_type = ifelse(str_detect(book_title, \"John\"), \"John\", \"Synoptic gospels\")) %&gt;% \n  mutate(is_john = book_type == \"John\")\nWe can finally run the model now with the sparse_words matrix and the binary verses_books$is_john variable:\nlibrary(glmnet)\nlibrary(doMC)\nregisterDoMC(cores = parallel::detectCores())\n\nmodel &lt;- cv.glmnet(sparse_words, verses_books$is_john,\n                   family = \"binomial\",\n                   parallel = TRUE, keep = TRUE\n)\nWe can then extract the coefficients that have the highest lambda within 1 standard error of the minimum (glmnet goes through a sequence of possible lambda values for each iteration of the model—we want the one with the best, or where it’s big, but still close to the minimum).\nlibrary(glmnet)\nlibrary(broom)\n\ncoefs &lt;- model$glmnet.fit %&gt;%\n  tidy() %&gt;%\n  filter(lambda == model$lambda.1se) %&gt;% \n  filter(term != \"(Intercept)\")\n\ntop_coefs &lt;- coefs %&gt;% \n  group_by(estimate &gt; 0) %&gt;%\n  top_n(10, abs(estimate)) %&gt;%\n  ungroup() %&gt;% \n  arrange(desc(estimate)) %&gt;% \n  mutate(term = fct_inorder(term)) %&gt;% \n  mutate(prob_type = ifelse(estimate &gt; 0, \"Increases likelihood of being from John\", \n                            \"Increases likelihood of being from Synoptic Gospels\"),\n         prob_type = fct_inorder(prob_type))\n\ntop_coefs %&gt;%\n  ggplot(aes(x = fct_rev(term), y = estimate, fill = prob_type)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"#6d80b0\", \"#6c9d53\"), name = NULL) +\n  labs(x = NULL, y = \"Coefficient\",\n       title = \"Words that change the likelihood of being in John\",\n       subtitle = \"A verse with “changer” in it is probably from John\") +\n  theme_minimal(base_family = \"IBM Plex Sans\") +\n  theme(plot.title = element_text(face = \"bold\"),\n        legend.position = \"top\",\n        legend.justification = \"left\",\n        legend.box.margin = margin(l = -0.75, t = -0.25, unit = \"lines\"),\n        legend.key.size = unit(0.65, \"lines\")) +\n  coord_flip()\n\nFor whatever reason, “changer” is a very John-like word (even though the incident of the money changers at the temple appears in all four gospels), and nouns like “manna” and “leg” are also very John-like. Meanwhile, words like “parable” and “Herodians” seem to be more Synoptic-like.\nWhere do words like “abide” or “disciple” fit in this model?\ncoefs %&gt;% \n  filter(term %in% c(\"disciple\", \"abideth\"))\n## # A tibble: 2 x 5\n##   term      step estimate  lambda dev.ratio\n##   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n## 1 disciple    29    0.582 0.00646     0.617\n## 2 abideth     29    2.78  0.00646     0.617\nThe coefficient for “disciple” is positive, but not really that high—just ≈0.6—so it doesn’t boost the likelihood that we’re in John. “Abideth,” though, has a fairly strong effect, just as we found with the tf-idf.\nNeat!"
  }
]